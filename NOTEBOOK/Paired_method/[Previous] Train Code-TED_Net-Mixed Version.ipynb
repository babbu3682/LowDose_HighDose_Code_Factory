{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels==0.7.4\n",
    "# !pip install efficientnet-pytorch==0.6.3\n",
    "# !pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CUDA 11.1\n",
    "# !pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE -> MAE Loss 꿀팁!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  3 12:35:29 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:18:00.0 Off |                  Off |\r\n",
      "| 49%   76C    P2   292W / 300W |  48324MiB / 48685MiB |    100%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:3B:00.0 Off |                  Off |\r\n",
      "| 58%   82C    P2   264W / 300W |  46888MiB / 48685MiB |     45%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:86:00.0 Off |                  Off |\r\n",
      "| 55%   80C    P2   267W / 300W |  46888MiB / 48685MiB |     59%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:AF:00.0 Off |                  Off |\r\n",
      "| 30%   29C    P8    19W / 300W |      1MiB / 48685MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/4.Dose_img2img/scripts study\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/4.Dose_img2img/scripts study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 갯수 =  64\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"CPU 갯수 = \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED_Net_Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "Dataset Name:  Sinogram_DCM\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output To:  /workspace/sunggu/4.Dose_img2img/model/[Previous]TED_Net_Mixed\n",
      "Save   To:  /workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0001\n",
      "Batchsize:  54\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6899\n",
      "Valid [Total]  number =  14\n",
      "Creating criterion: Perceptual+L1 Loss\n",
      "Creating model: TED_Net\n",
      "adopt performer encoder for tokens-to-token\n",
      "adopt performer encoder for tokens-to-token\n",
      "Load feature extractor...!\n",
      "Number of Learnable Params: 4481333\n",
      "TED_Net_Mixed(\n",
      "  (tokens_to_token): T2T_module(\n",
      "    (soft_split0): Unfold(kernel_size=(7, 7), dilation=1, padding=0, stride=(2, 2))\n",
      "    (soft_split1): Unfold(kernel_size=(3, 3), dilation=(2, 2), padding=0, stride=(1, 1))\n",
      "    (soft_split2): Unfold(kernel_size=(3, 3), dilation=1, padding=0, stride=(1, 1))\n",
      "    (attention1): Token_performer(\n",
      "      (kqv): Linear(in_features=49, out_features=192, bias=True)\n",
      "      (dp): Dropout(p=0.1, inplace=False)\n",
      "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (norm1): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (attention2): Token_performer(\n",
      "      (kqv): Linear(in_features=576, out_features=192, bias=True)\n",
      "      (dp): Dropout(p=0.1, inplace=False)\n",
      "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (project): Linear(in_features=576, out_features=512, bias=True)\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_extractor): Revised_UNet(\n",
      "    (enc1_1): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (enc1_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (pool1): DownsampleBlock(\n",
      "      (downsample): Sequential(\n",
      "        (0): PixelUnshuffle(downscale_factor=2)\n",
      "        (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (enc2_1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (enc2_2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (pool2): DownsampleBlock(\n",
      "      (downsample): Sequential(\n",
      "        (0): PixelUnshuffle(downscale_factor=2)\n",
      "        (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (enc3_1): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (enc3_2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (pool3): DownsampleBlock(\n",
      "      (downsample): Sequential(\n",
      "        (0): PixelUnshuffle(downscale_factor=2)\n",
      "        (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (enc4_1): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (enc4_2): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (pool4): DownsampleBlock(\n",
      "      (downsample): Sequential(\n",
      "        (0): PixelUnshuffle(downscale_factor=2)\n",
      "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (enc5_1): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec5_1): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (unpool4): UpsampleBlock(\n",
      "      (upsample): Sequential(\n",
      "        (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PixelShuffle(upscale_factor=2)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dec4_2): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec4_1): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (unpool3): UpsampleBlock(\n",
      "      (upsample): Sequential(\n",
      "        (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PixelShuffle(upscale_factor=2)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dec3_2): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec3_1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (unpool2): UpsampleBlock(\n",
      "      (upsample): Sequential(\n",
      "        (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PixelShuffle(upscale_factor=2)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dec2_2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec2_1): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (unpool1): UpsampleBlock(\n",
      "      (upsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PixelShuffle(upscale_factor=2)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dec1_2): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec1_1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (fc): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (dconv1): Token_back_Image(\n",
      "    (soft_split0): Fold(output_size=(64, 64), kernel_size=(7, 7), dilation=1, padding=0, stride=(2, 2))\n",
      "    (soft_split1): Fold(output_size=(29, 29), kernel_size=(3, 3), dilation=(2, 2), padding=0, stride=(1, 1))\n",
      "    (soft_split2): Fold(output_size=(25, 25), kernel_size=(3, 3), dilation=1, padding=0, stride=(1, 1))\n",
      "    (attention1): Token_performer(\n",
      "      (kqv): Linear(in_features=64, out_features=147, bias=True)\n",
      "      (dp): Dropout(p=0.1, inplace=False)\n",
      "      (proj): Linear(in_features=49, out_features=49, bias=True)\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=49, out_features=49, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=49, out_features=49, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (attention2): Token_performer(\n",
      "      (kqv): Linear(in_features=64, out_features=1728, bias=True)\n",
      "      (dp): Dropout(p=0.1, inplace=False)\n",
      "      (proj): Linear(in_features=576, out_features=576, bias=True)\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=576, out_features=576, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=576, out_features=576, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (project): Linear(in_features=512, out_features=576, bias=True)\n",
      "  )\n",
      "  (head): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 1000 epochs\n",
      "Train: [epoch:0]  [  0/127]  eta: 0:06:51  lr: 0.000000  loss: 1.7690 (1.7690)  time: 3.2421  data: 2.1983  max mem: 34204\n",
      "Train: [epoch:0]  [ 10/127]  eta: 0:02:20  lr: 0.000000  loss: 1.7734 (1.7742)  time: 1.1980  data: 0.2000  max mem: 34254\n",
      "Train: [epoch:0]  [ 20/127]  eta: 0:01:57  lr: 0.000000  loss: 1.7748 (1.7762)  time: 0.9905  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:0]  [ 30/127]  eta: 0:01:43  lr: 0.000000  loss: 1.7757 (1.7761)  time: 0.9894  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:0]  [ 40/127]  eta: 0:01:30  lr: 0.000000  loss: 1.7740 (1.7754)  time: 0.9908  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:0]  [ 50/127]  eta: 0:01:19  lr: 0.000000  loss: 1.7716 (1.7752)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:0]  [ 60/127]  eta: 0:01:17  lr: 0.000000  loss: 1.7716 (1.7749)  time: 1.3991  data: 0.4238  max mem: 34254\n",
      "Train: [epoch:0]  [ 70/127]  eta: 0:01:13  lr: 0.000000  loss: 1.7716 (1.7744)  time: 1.9411  data: 0.9780  max mem: 34254\n",
      "Train: [epoch:0]  [ 80/127]  eta: 0:01:03  lr: 0.000000  loss: 1.7797 (1.7752)  time: 1.9138  data: 0.9443  max mem: 34254\n",
      "Train: [epoch:0]  [ 90/127]  eta: 0:00:53  lr: 0.000000  loss: 1.7779 (1.7743)  time: 1.9457  data: 0.9694  max mem: 34254\n",
      "Train: [epoch:0]  [100/127]  eta: 0:00:39  lr: 0.000000  loss: 1.7688 (1.7741)  time: 2.0037  data: 1.0257  max mem: 34254\n",
      "Train: [epoch:0]  [110/127]  eta: 0:00:26  lr: 0.000000  loss: 1.7727 (1.7744)  time: 1.9681  data: 0.9904  max mem: 34254\n",
      "Train: [epoch:0]  [120/127]  eta: 0:00:10  lr: 0.000000  loss: 1.7734 (1.7743)  time: 1.8915  data: 0.9106  max mem: 34254\n",
      "Train: [epoch:0]  [126/127]  eta: 0:00:01  lr: 0.000000  loss: 1.7734 (1.7744)  time: 1.8180  data: 0.8376  max mem: 34254\n",
      "Train: [epoch:0] Total time: 0:03:18 (1.5643 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 1.7734 (1.7744)\n",
      "Valid: [epoch:0]  [ 0/14]  eta: 0:00:39  loss: 1.2457 (1.2457)  time: 2.8500  data: 0.4268  max mem: 34254\n",
      "Valid: [epoch:0]  [13/14]  eta: 0:00:02  loss: 1.2018 (1.1959)  time: 2.3119  data: 0.0306  max mem: 34254\n",
      "Valid: [epoch:0] Total time: 0:00:32 (2.3236 s / it)\n",
      "Averaged stats: loss: 1.2018 (1.1959)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_0_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.196%\n",
      "Min loss: 1.196\n",
      "Best Epoch: 0.000\n",
      "/home/sunggu/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Train: [epoch:1]  [  0/127]  eta: 0:05:37  lr: 0.000000  loss: 1.7802 (1.7802)  time: 2.6552  data: 1.6774  max mem: 34254\n",
      "Train: [epoch:1]  [ 10/127]  eta: 0:02:13  lr: 0.000000  loss: 1.7683 (1.7689)  time: 1.1405  data: 0.1526  max mem: 34254\n",
      "Train: [epoch:1]  [ 20/127]  eta: 0:01:54  lr: 0.000000  loss: 1.7708 (1.7723)  time: 0.9912  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [ 30/127]  eta: 0:01:41  lr: 0.000000  loss: 1.7721 (1.7718)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [ 40/127]  eta: 0:01:29  lr: 0.000000  loss: 1.7739 (1.7735)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [ 50/127]  eta: 0:01:18  lr: 0.000000  loss: 1.7719 (1.7728)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [ 60/127]  eta: 0:01:08  lr: 0.000000  loss: 1.7705 (1.7735)  time: 0.9997  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [ 70/127]  eta: 0:00:58  lr: 0.000000  loss: 1.7704 (1.7729)  time: 1.0018  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [ 80/127]  eta: 0:00:47  lr: 0.000000  loss: 1.7714 (1.7733)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [ 90/127]  eta: 0:00:37  lr: 0.000000  loss: 1.7711 (1.7726)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [100/127]  eta: 0:00:27  lr: 0.000000  loss: 1.7710 (1.7726)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [110/127]  eta: 0:00:17  lr: 0.000000  loss: 1.7724 (1.7723)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [120/127]  eta: 0:00:07  lr: 0.000000  loss: 1.7694 (1.7722)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1]  [126/127]  eta: 0:00:01  lr: 0.000000  loss: 1.7738 (1.7723)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:1] Total time: 0:02:08 (1.0107 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 1.7738 (1.7723)\n",
      "Valid: [epoch:1]  [ 0/14]  eta: 0:00:35  loss: 1.1671 (1.1671)  time: 2.5118  data: 0.3853  max mem: 34254\n",
      "Valid: [epoch:1]  [13/14]  eta: 0:00:02  loss: 1.1986 (1.1928)  time: 2.1385  data: 0.0276  max mem: 34254\n",
      "Valid: [epoch:1] Total time: 0:00:30 (2.1497 s / it)\n",
      "Averaged stats: loss: 1.1986 (1.1928)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_1_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.193%\n",
      "Min loss: 1.193\n",
      "Best Epoch: 1.000\n",
      "Train: [epoch:2]  [  0/127]  eta: 0:06:41  lr: 0.000010  loss: 1.7594 (1.7594)  time: 3.1627  data: 2.1831  max mem: 34254\n",
      "Train: [epoch:2]  [ 10/127]  eta: 0:02:19  lr: 0.000010  loss: 1.6624 (1.6614)  time: 1.1940  data: 0.1986  max mem: 34254\n",
      "Train: [epoch:2]  [ 20/127]  eta: 0:01:57  lr: 0.000010  loss: 1.5525 (1.5875)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [ 30/127]  eta: 0:01:43  lr: 0.000010  loss: 1.4452 (1.5269)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [ 40/127]  eta: 0:01:31  lr: 0.000010  loss: 1.3498 (1.4722)  time: 1.0053  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [ 50/127]  eta: 0:01:20  lr: 0.000010  loss: 1.2497 (1.4181)  time: 1.0055  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [ 60/127]  eta: 0:01:09  lr: 0.000010  loss: 1.1498 (1.3669)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [ 70/127]  eta: 0:00:58  lr: 0.000010  loss: 1.0533 (1.3185)  time: 1.0014  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [ 80/127]  eta: 0:00:48  lr: 0.000010  loss: 0.9868 (1.2735)  time: 1.0023  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [ 90/127]  eta: 0:00:37  lr: 0.000010  loss: 0.9228 (1.2319)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [100/127]  eta: 0:00:27  lr: 0.000010  loss: 0.8686 (1.1944)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [110/127]  eta: 0:00:17  lr: 0.000010  loss: 0.8320 (1.1608)  time: 1.0015  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [120/127]  eta: 0:00:07  lr: 0.000010  loss: 0.7988 (1.1301)  time: 1.0018  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2]  [126/127]  eta: 0:00:01  lr: 0.000010  loss: 0.7857 (1.1133)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:2] Total time: 0:02:09 (1.0179 s / it)\n",
      "Averaged stats: lr: 0.000010  loss: 0.7857 (1.1133)\n",
      "Valid: [epoch:2]  [ 0/14]  eta: 0:00:36  loss: 0.4323 (0.4323)  time: 2.6051  data: 0.3538  max mem: 34254\n",
      "Valid: [epoch:2]  [13/14]  eta: 0:00:02  loss: 0.4399 (0.4424)  time: 2.2399  data: 0.0254  max mem: 34254\n",
      "Valid: [epoch:2] Total time: 0:00:31 (2.2487 s / it)\n",
      "Averaged stats: loss: 0.4399 (0.4424)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_2_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.442%\n",
      "Min loss: 0.442\n",
      "Best Epoch: 2.000\n",
      "Train: [epoch:3]  [  0/127]  eta: 0:07:11  lr: 0.000020  loss: 0.7611 (0.7611)  time: 3.3944  data: 2.3772  max mem: 34254\n",
      "Train: [epoch:3]  [ 10/127]  eta: 0:02:23  lr: 0.000020  loss: 0.7427 (0.7450)  time: 1.2232  data: 0.2162  max mem: 34254\n",
      "Train: [epoch:3]  [ 20/127]  eta: 0:01:59  lr: 0.000020  loss: 0.7340 (0.7316)  time: 1.0001  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [ 30/127]  eta: 0:01:44  lr: 0.000020  loss: 0.7120 (0.7214)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [ 40/127]  eta: 0:01:31  lr: 0.000020  loss: 0.6886 (0.7115)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [ 50/127]  eta: 0:01:20  lr: 0.000020  loss: 0.6703 (0.7023)  time: 0.9950  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 60/127]  eta: 0:01:09  lr: 0.000020  loss: 0.6622 (0.6958)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [ 70/127]  eta: 0:00:58  lr: 0.000020  loss: 0.6570 (0.6896)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [ 80/127]  eta: 0:00:48  lr: 0.000020  loss: 0.6471 (0.6840)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [ 90/127]  eta: 0:00:37  lr: 0.000020  loss: 0.6414 (0.6787)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [100/127]  eta: 0:00:27  lr: 0.000020  loss: 0.6328 (0.6738)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [110/127]  eta: 0:00:17  lr: 0.000020  loss: 0.6284 (0.6694)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [120/127]  eta: 0:00:07  lr: 0.000020  loss: 0.6212 (0.6649)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3]  [126/127]  eta: 0:00:01  lr: 0.000020  loss: 0.6163 (0.6625)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:3] Total time: 0:02:09 (1.0168 s / it)\n",
      "Averaged stats: lr: 0.000020  loss: 0.6163 (0.6625)\n",
      "Valid: [epoch:3]  [ 0/14]  eta: 0:00:35  loss: 0.3341 (0.3341)  time: 2.5714  data: 0.4298  max mem: 34254\n",
      "Valid: [epoch:3]  [13/14]  eta: 0:00:02  loss: 0.3339 (0.3355)  time: 2.1674  data: 0.0308  max mem: 34254\n",
      "Valid: [epoch:3] Total time: 0:00:30 (2.1759 s / it)\n",
      "Averaged stats: loss: 0.3339 (0.3355)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_3_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.336%\n",
      "Min loss: 0.336\n",
      "Best Epoch: 3.000\n",
      "Train: [epoch:4]  [  0/127]  eta: 0:05:20  lr: 0.000030  loss: 0.6105 (0.6105)  time: 2.5223  data: 1.5431  max mem: 34254\n",
      "Train: [epoch:4]  [ 10/127]  eta: 0:02:12  lr: 0.000030  loss: 0.6073 (0.6078)  time: 1.1326  data: 0.1404  max mem: 34254\n",
      "Train: [epoch:4]  [ 20/127]  eta: 0:01:54  lr: 0.000030  loss: 0.6060 (0.6064)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [ 30/127]  eta: 0:01:41  lr: 0.000030  loss: 0.5996 (0.6027)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [ 40/127]  eta: 0:01:29  lr: 0.000030  loss: 0.5931 (0.6000)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [ 50/127]  eta: 0:01:18  lr: 0.000030  loss: 0.5878 (0.5973)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [ 60/127]  eta: 0:01:08  lr: 0.000030  loss: 0.5848 (0.5948)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [ 70/127]  eta: 0:00:57  lr: 0.000030  loss: 0.5810 (0.5927)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [ 80/127]  eta: 0:00:47  lr: 0.000030  loss: 0.5773 (0.5905)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [ 90/127]  eta: 0:00:37  lr: 0.000030  loss: 0.5721 (0.5880)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [100/127]  eta: 0:00:27  lr: 0.000030  loss: 0.5671 (0.5857)  time: 1.0014  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [110/127]  eta: 0:00:17  lr: 0.000030  loss: 0.5636 (0.5836)  time: 1.0018  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [120/127]  eta: 0:00:07  lr: 0.000030  loss: 0.5606 (0.5816)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4]  [126/127]  eta: 0:00:01  lr: 0.000030  loss: 0.5606 (0.5807)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:4] Total time: 0:02:08 (1.0096 s / it)\n",
      "Averaged stats: lr: 0.000030  loss: 0.5606 (0.5807)\n",
      "Valid: [epoch:4]  [ 0/14]  eta: 0:00:35  loss: 0.2984 (0.2984)  time: 2.5123  data: 0.3741  max mem: 34254\n",
      "Valid: [epoch:4]  [13/14]  eta: 0:00:02  loss: 0.2984 (0.2996)  time: 2.1607  data: 0.0268  max mem: 34254\n",
      "Valid: [epoch:4] Total time: 0:00:30 (2.1713 s / it)\n",
      "Averaged stats: loss: 0.2984 (0.2996)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_4_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.300%\n",
      "Min loss: 0.300\n",
      "Best Epoch: 4.000\n",
      "Train: [epoch:5]  [  0/127]  eta: 0:07:32  lr: 0.000040  loss: 0.5579 (0.5579)  time: 3.5651  data: 2.5590  max mem: 34254\n",
      "Train: [epoch:5]  [ 10/127]  eta: 0:02:23  lr: 0.000040  loss: 0.5527 (0.5540)  time: 1.2267  data: 0.2327  max mem: 34254\n",
      "Train: [epoch:5]  [ 20/127]  eta: 0:01:59  lr: 0.000040  loss: 0.5519 (0.5535)  time: 0.9924  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [ 30/127]  eta: 0:01:44  lr: 0.000040  loss: 0.5492 (0.5511)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [ 40/127]  eta: 0:01:31  lr: 0.000040  loss: 0.5457 (0.5498)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [ 50/127]  eta: 0:01:20  lr: 0.000040  loss: 0.5438 (0.5485)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [ 60/127]  eta: 0:01:09  lr: 0.000040  loss: 0.5403 (0.5472)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [ 70/127]  eta: 0:00:58  lr: 0.000040  loss: 0.5383 (0.5459)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [ 80/127]  eta: 0:00:48  lr: 0.000040  loss: 0.5344 (0.5441)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [ 90/127]  eta: 0:00:37  lr: 0.000040  loss: 0.5304 (0.5423)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [100/127]  eta: 0:00:27  lr: 0.000040  loss: 0.5268 (0.5407)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [110/127]  eta: 0:00:17  lr: 0.000040  loss: 0.5244 (0.5392)  time: 0.9997  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [120/127]  eta: 0:00:07  lr: 0.000040  loss: 0.5221 (0.5376)  time: 1.0002  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5]  [126/127]  eta: 0:00:01  lr: 0.000040  loss: 0.5194 (0.5368)  time: 0.9997  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:5] Total time: 0:02:09 (1.0174 s / it)\n",
      "Averaged stats: lr: 0.000040  loss: 0.5194 (0.5368)\n",
      "Valid: [epoch:5]  [ 0/14]  eta: 0:00:35  loss: 0.2324 (0.2324)  time: 2.5330  data: 0.4031  max mem: 34254\n",
      "Valid: [epoch:5]  [13/14]  eta: 0:00:02  loss: 0.2422 (0.2464)  time: 2.1412  data: 0.0289  max mem: 34254\n",
      "Valid: [epoch:5] Total time: 0:00:30 (2.1501 s / it)\n",
      "Averaged stats: loss: 0.2422 (0.2464)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_5_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.246%\n",
      "Min loss: 0.246\n",
      "Best Epoch: 5.000\n",
      "Train: [epoch:6]  [  0/127]  eta: 0:07:04  lr: 0.000050  loss: 0.5150 (0.5150)  time: 3.3394  data: 2.3476  max mem: 34254\n",
      "Train: [epoch:6]  [ 10/127]  eta: 0:02:21  lr: 0.000050  loss: 0.5162 (0.5171)  time: 1.2114  data: 0.2135  max mem: 34254\n",
      "Train: [epoch:6]  [ 20/127]  eta: 0:01:58  lr: 0.000050  loss: 0.5148 (0.5161)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [ 30/127]  eta: 0:01:43  lr: 0.000050  loss: 0.5121 (0.5146)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [ 40/127]  eta: 0:01:31  lr: 0.000050  loss: 0.5094 (0.5132)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [ 50/127]  eta: 0:01:20  lr: 0.000050  loss: 0.5056 (0.5113)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [ 60/127]  eta: 0:01:09  lr: 0.000050  loss: 0.5030 (0.5098)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [ 70/127]  eta: 0:00:58  lr: 0.000050  loss: 0.5001 (0.5083)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [ 80/127]  eta: 0:00:48  lr: 0.000050  loss: 0.4998 (0.5072)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [ 90/127]  eta: 0:00:37  lr: 0.000050  loss: 0.4976 (0.5059)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [100/127]  eta: 0:00:27  lr: 0.000050  loss: 0.4942 (0.5045)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [110/127]  eta: 0:00:17  lr: 0.000050  loss: 0.4887 (0.5030)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [120/127]  eta: 0:00:07  lr: 0.000050  loss: 0.4871 (0.5016)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6]  [126/127]  eta: 0:00:01  lr: 0.000050  loss: 0.4856 (0.5006)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:6] Total time: 0:02:09 (1.0164 s / it)\n",
      "Averaged stats: lr: 0.000050  loss: 0.4856 (0.5006)\n",
      "Valid: [epoch:6]  [ 0/14]  eta: 0:00:37  loss: 0.1808 (0.1808)  time: 2.6816  data: 0.3874  max mem: 34254\n",
      "Valid: [epoch:6]  [13/14]  eta: 0:00:02  loss: 0.1633 (0.1704)  time: 2.3495  data: 0.0278  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:6] Total time: 0:00:33 (2.3581 s / it)\n",
      "Averaged stats: loss: 0.1633 (0.1704)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_6_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.170%\n",
      "Min loss: 0.170\n",
      "Best Epoch: 6.000\n",
      "Train: [epoch:7]  [  0/127]  eta: 0:07:19  lr: 0.000060  loss: 0.4823 (0.4823)  time: 3.4613  data: 2.4861  max mem: 34254\n",
      "Train: [epoch:7]  [ 10/127]  eta: 0:02:22  lr: 0.000060  loss: 0.4815 (0.4817)  time: 1.2176  data: 0.2261  max mem: 34254\n",
      "Train: [epoch:7]  [ 20/127]  eta: 0:01:58  lr: 0.000060  loss: 0.4808 (0.4796)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [ 30/127]  eta: 0:01:44  lr: 0.000060  loss: 0.4744 (0.4774)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [ 40/127]  eta: 0:01:31  lr: 0.000060  loss: 0.4728 (0.4754)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [ 50/127]  eta: 0:01:20  lr: 0.000060  loss: 0.4651 (0.4732)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [ 60/127]  eta: 0:01:09  lr: 0.000060  loss: 0.4635 (0.4715)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [ 70/127]  eta: 0:00:58  lr: 0.000060  loss: 0.4619 (0.4700)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [ 80/127]  eta: 0:00:48  lr: 0.000060  loss: 0.4601 (0.4687)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [ 90/127]  eta: 0:00:37  lr: 0.000060  loss: 0.4579 (0.4672)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [100/127]  eta: 0:00:27  lr: 0.000060  loss: 0.4526 (0.4657)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [110/127]  eta: 0:00:17  lr: 0.000060  loss: 0.4506 (0.4643)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [120/127]  eta: 0:00:07  lr: 0.000060  loss: 0.4478 (0.4629)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7]  [126/127]  eta: 0:00:01  lr: 0.000060  loss: 0.4478 (0.4621)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:7] Total time: 0:02:09 (1.0164 s / it)\n",
      "Averaged stats: lr: 0.000060  loss: 0.4478 (0.4621)\n",
      "Valid: [epoch:7]  [ 0/14]  eta: 0:00:35  loss: 0.0958 (0.0958)  time: 2.5102  data: 0.3638  max mem: 34254\n",
      "Valid: [epoch:7]  [13/14]  eta: 0:00:02  loss: 0.0997 (0.1044)  time: 2.1245  data: 0.0261  max mem: 34254\n",
      "Valid: [epoch:7] Total time: 0:00:29 (2.1327 s / it)\n",
      "Averaged stats: loss: 0.0997 (0.1044)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_7_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.104%\n",
      "Min loss: 0.104\n",
      "Best Epoch: 7.000\n",
      "Train: [epoch:8]  [  0/127]  eta: 0:05:37  lr: 0.000070  loss: 0.4425 (0.4425)  time: 2.6566  data: 1.6815  max mem: 34254\n",
      "Train: [epoch:8]  [ 10/127]  eta: 0:02:13  lr: 0.000070  loss: 0.4434 (0.4435)  time: 1.1444  data: 0.1530  max mem: 34254\n",
      "Train: [epoch:8]  [ 20/127]  eta: 0:01:54  lr: 0.000070  loss: 0.4427 (0.4433)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [ 30/127]  eta: 0:01:41  lr: 0.000070  loss: 0.4414 (0.4418)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [ 40/127]  eta: 0:01:29  lr: 0.000070  loss: 0.4354 (0.4398)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [ 50/127]  eta: 0:01:19  lr: 0.000070  loss: 0.4322 (0.4382)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [ 60/127]  eta: 0:01:08  lr: 0.000070  loss: 0.4317 (0.4371)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [ 70/127]  eta: 0:00:58  lr: 0.000070  loss: 0.4290 (0.4358)  time: 1.0058  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [ 80/127]  eta: 0:00:47  lr: 0.000070  loss: 0.4281 (0.4349)  time: 1.0161  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [ 90/127]  eta: 0:00:37  lr: 0.000070  loss: 0.4248 (0.4336)  time: 1.0086  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [100/127]  eta: 0:00:27  lr: 0.000070  loss: 0.4223 (0.4324)  time: 1.0143  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [110/127]  eta: 0:00:17  lr: 0.000070  loss: 0.4210 (0.4312)  time: 1.0140  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [120/127]  eta: 0:00:07  lr: 0.000070  loss: 0.4168 (0.4299)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8]  [126/127]  eta: 0:00:01  lr: 0.000070  loss: 0.4155 (0.4292)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:8] Total time: 0:02:09 (1.0160 s / it)\n",
      "Averaged stats: lr: 0.000070  loss: 0.4155 (0.4292)\n",
      "Valid: [epoch:8]  [ 0/14]  eta: 0:00:34  loss: 0.0564 (0.0564)  time: 2.4953  data: 0.3640  max mem: 34254\n",
      "Valid: [epoch:8]  [13/14]  eta: 0:00:02  loss: 0.0602 (0.0636)  time: 2.1289  data: 0.0261  max mem: 34254\n",
      "Valid: [epoch:8] Total time: 0:00:29 (2.1372 s / it)\n",
      "Averaged stats: loss: 0.0602 (0.0636)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_8_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.064%\n",
      "Min loss: 0.064\n",
      "Best Epoch: 8.000\n",
      "Train: [epoch:9]  [  0/127]  eta: 0:06:29  lr: 0.000080  loss: 0.4151 (0.4151)  time: 3.0652  data: 2.0043  max mem: 34254\n",
      "Train: [epoch:9]  [ 10/127]  eta: 0:02:18  lr: 0.000080  loss: 0.4135 (0.4132)  time: 1.1823  data: 0.1823  max mem: 34254\n",
      "Train: [epoch:9]  [ 20/127]  eta: 0:01:56  lr: 0.000080  loss: 0.4100 (0.4109)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [ 30/127]  eta: 0:01:42  lr: 0.000080  loss: 0.4055 (0.4087)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [ 40/127]  eta: 0:01:30  lr: 0.000080  loss: 0.4020 (0.4068)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [ 50/127]  eta: 0:01:19  lr: 0.000080  loss: 0.3984 (0.4049)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [ 60/127]  eta: 0:01:08  lr: 0.000080  loss: 0.3966 (0.4032)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [ 70/127]  eta: 0:00:58  lr: 0.000080  loss: 0.3927 (0.4015)  time: 1.0018  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [ 80/127]  eta: 0:00:48  lr: 0.000080  loss: 0.3892 (0.3998)  time: 1.0014  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [ 90/127]  eta: 0:00:37  lr: 0.000080  loss: 0.3860 (0.3981)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [100/127]  eta: 0:00:27  lr: 0.000080  loss: 0.3821 (0.3963)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [110/127]  eta: 0:00:17  lr: 0.000080  loss: 0.3781 (0.3946)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [120/127]  eta: 0:00:07  lr: 0.000080  loss: 0.3738 (0.3927)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9]  [126/127]  eta: 0:00:01  lr: 0.000080  loss: 0.3715 (0.3917)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:9] Total time: 0:02:08 (1.0141 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.3715 (0.3917)\n",
      "Valid: [epoch:9]  [ 0/14]  eta: 0:00:35  loss: 0.0398 (0.0398)  time: 2.5190  data: 0.3794  max mem: 34254\n",
      "Valid: [epoch:9]  [13/14]  eta: 0:00:02  loss: 0.0398 (0.0425)  time: 2.1222  data: 0.0272  max mem: 34254\n",
      "Valid: [epoch:9] Total time: 0:00:29 (2.1319 s / it)\n",
      "Averaged stats: loss: 0.0398 (0.0425)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_9_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.042%\n",
      "Min loss: 0.042\n",
      "Best Epoch: 9.000\n",
      "Train: [epoch:10]  [  0/127]  eta: 0:07:09  lr: 0.000090  loss: 0.3703 (0.3703)  time: 3.3795  data: 2.4019  max mem: 34254\n",
      "Train: [epoch:10]  [ 10/127]  eta: 0:02:21  lr: 0.000090  loss: 0.3686 (0.3683)  time: 1.2108  data: 0.2185  max mem: 34254\n",
      "Train: [epoch:10]  [ 20/127]  eta: 0:01:58  lr: 0.000090  loss: 0.3660 (0.3667)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [ 30/127]  eta: 0:01:44  lr: 0.000090  loss: 0.3619 (0.3642)  time: 1.0004  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [ 40/127]  eta: 0:01:31  lr: 0.000090  loss: 0.3550 (0.3612)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [ 50/127]  eta: 0:01:20  lr: 0.000090  loss: 0.3494 (0.3588)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [ 60/127]  eta: 0:01:09  lr: 0.000090  loss: 0.3472 (0.3565)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [ 70/127]  eta: 0:00:58  lr: 0.000090  loss: 0.3416 (0.3542)  time: 0.9990  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:10]  [ 80/127]  eta: 0:00:48  lr: 0.000090  loss: 0.3379 (0.3520)  time: 1.0004  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [ 90/127]  eta: 0:00:37  lr: 0.000090  loss: 0.3343 (0.3497)  time: 0.9993  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [100/127]  eta: 0:00:27  lr: 0.000090  loss: 0.3287 (0.3474)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [110/127]  eta: 0:00:17  lr: 0.000090  loss: 0.3237 (0.3450)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [120/127]  eta: 0:00:07  lr: 0.000090  loss: 0.3182 (0.3427)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10]  [126/127]  eta: 0:00:01  lr: 0.000090  loss: 0.3166 (0.3412)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:10] Total time: 0:02:09 (1.0178 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.3166 (0.3412)\n",
      "Valid: [epoch:10]  [ 0/14]  eta: 0:00:35  loss: 0.0357 (0.0357)  time: 2.5065  data: 0.3622  max mem: 34254\n",
      "Valid: [epoch:10]  [13/14]  eta: 0:00:02  loss: 0.0295 (0.0316)  time: 2.1113  data: 0.0259  max mem: 34254\n",
      "Valid: [epoch:10] Total time: 0:00:29 (2.1212 s / it)\n",
      "Averaged stats: loss: 0.0295 (0.0316)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_10_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.032%\n",
      "Min loss: 0.032\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:11]  [  0/127]  eta: 0:07:17  lr: 0.000100  loss: 0.3121 (0.3121)  time: 3.4450  data: 2.4715  max mem: 34254\n",
      "Train: [epoch:11]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.3090 (0.3099)  time: 1.2130  data: 0.2248  max mem: 34254\n",
      "Train: [epoch:11]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.3064 (0.3078)  time: 0.9906  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.3014 (0.3051)  time: 0.9926  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.2972 (0.3029)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.2933 (0.3008)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.2884 (0.2983)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.2820 (0.2956)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.2773 (0.2932)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.2740 (0.2908)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.2675 (0.2882)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.2613 (0.2857)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.2589 (0.2834)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.2564 (0.2819)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:11] Total time: 0:02:09 (1.0158 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.2564 (0.2819)\n",
      "Valid: [epoch:11]  [ 0/14]  eta: 0:00:35  loss: 0.0238 (0.0238)  time: 2.5058  data: 0.3941  max mem: 34254\n",
      "Valid: [epoch:11]  [13/14]  eta: 0:00:02  loss: 0.0244 (0.0261)  time: 2.0996  data: 0.0282  max mem: 34254\n",
      "Valid: [epoch:11] Total time: 0:00:29 (2.1089 s / it)\n",
      "Averaged stats: loss: 0.0244 (0.0261)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_11_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.026%\n",
      "Min loss: 0.026\n",
      "Best Epoch: 11.000\n",
      "Train: [epoch:12]  [  0/127]  eta: 0:06:50  lr: 0.000100  loss: 0.2535 (0.2535)  time: 3.2322  data: 2.2560  max mem: 34254\n",
      "Train: [epoch:12]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.2510 (0.2503)  time: 1.1937  data: 0.2052  max mem: 34254\n",
      "Train: [epoch:12]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.2461 (0.2472)  time: 0.9907  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.2413 (0.2448)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.2390 (0.2429)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.2355 (0.2404)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.2288 (0.2386)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.2274 (0.2370)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.2260 (0.2355)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.2223 (0.2338)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.2163 (0.2318)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.2115 (0.2299)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.2092 (0.2280)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.2054 (0.2268)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:12] Total time: 0:02:08 (1.0132 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.2054 (0.2268)\n",
      "Valid: [epoch:12]  [ 0/14]  eta: 0:00:34  loss: 0.0204 (0.0204)  time: 2.4347  data: 0.3605  max mem: 34254\n",
      "Valid: [epoch:12]  [13/14]  eta: 0:00:02  loss: 0.0214 (0.0227)  time: 2.0735  data: 0.0258  max mem: 34254\n",
      "Valid: [epoch:12] Total time: 0:00:29 (2.0817 s / it)\n",
      "Averaged stats: loss: 0.0214 (0.0227)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_12_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.023%\n",
      "Min loss: 0.023\n",
      "Best Epoch: 12.000\n",
      "Train: [epoch:13]  [  0/127]  eta: 0:06:53  lr: 0.000100  loss: 0.2003 (0.2003)  time: 3.2539  data: 2.2819  max mem: 34254\n",
      "Train: [epoch:13]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.2001 (0.2005)  time: 1.1956  data: 0.2075  max mem: 34254\n",
      "Train: [epoch:13]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.1991 (0.1991)  time: 0.9903  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.1953 (0.1976)  time: 0.9917  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.1920 (0.1959)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.1885 (0.1942)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.1869 (0.1928)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.1841 (0.1912)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.1800 (0.1897)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.1781 (0.1882)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.1752 (0.1869)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.1710 (0.1854)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.1698 (0.1841)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.1687 (0.1833)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:13] Total time: 0:02:08 (1.0132 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1687 (0.1833)\n",
      "Valid: [epoch:13]  [ 0/14]  eta: 0:00:34  loss: 0.0218 (0.0218)  time: 2.4320  data: 0.3513  max mem: 34254\n",
      "Valid: [epoch:13]  [13/14]  eta: 0:00:02  loss: 0.0221 (0.0232)  time: 2.1173  data: 0.0252  max mem: 34254\n",
      "Valid: [epoch:13] Total time: 0:00:29 (2.1274 s / it)\n",
      "Averaged stats: loss: 0.0221 (0.0232)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_13_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 0.023%\n",
      "Min loss: 0.023\n",
      "Best Epoch: 12.000\n",
      "Train: [epoch:14]  [  0/127]  eta: 0:06:47  lr: 0.000100  loss: 0.1653 (0.1653)  time: 3.2116  data: 2.2319  max mem: 34254\n",
      "Train: [epoch:14]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.1637 (0.1633)  time: 1.1918  data: 0.2030  max mem: 34254\n",
      "Train: [epoch:14]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.1604 (0.1617)  time: 0.9904  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.1592 (0.1604)  time: 0.9924  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.1563 (0.1591)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.1533 (0.1577)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.1511 (0.1564)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.1477 (0.1550)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.1459 (0.1539)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.1449 (0.1529)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.1438 (0.1518)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.1416 (0.1507)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.1384 (0.1495)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.1351 (0.1488)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:14] Total time: 0:02:08 (1.0130 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1351 (0.1488)\n",
      "Valid: [epoch:14]  [ 0/14]  eta: 0:00:35  loss: 0.0185 (0.0185)  time: 2.5157  data: 0.3683  max mem: 34254\n",
      "Valid: [epoch:14]  [13/14]  eta: 0:00:02  loss: 0.0185 (0.0194)  time: 2.1585  data: 0.0264  max mem: 34254\n",
      "Valid: [epoch:14] Total time: 0:00:30 (2.1674 s / it)\n",
      "Averaged stats: loss: 0.0185 (0.0194)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_14_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.019%\n",
      "Min loss: 0.019\n",
      "Best Epoch: 14.000\n",
      "Train: [epoch:15]  [  0/127]  eta: 0:06:49  lr: 0.000100  loss: 0.1348 (0.1348)  time: 3.2222  data: 2.2237  max mem: 34254\n",
      "Train: [epoch:15]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.1328 (0.1325)  time: 1.1936  data: 0.2022  max mem: 34254\n",
      "Train: [epoch:15]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.1324 (0.1319)  time: 0.9908  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.1301 (0.1311)  time: 0.9918  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.1278 (0.1300)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.1254 (0.1290)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.1238 (0.1280)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.1229 (0.1273)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.1208 (0.1264)  time: 1.0010  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.1194 (0.1256)  time: 0.9999  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.1179 (0.1248)  time: 1.0039  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.1158 (0.1239)  time: 1.0038  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.1148 (0.1231)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.1128 (0.1225)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:15] Total time: 0:02:08 (1.0156 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1128 (0.1225)\n",
      "Valid: [epoch:15]  [ 0/14]  eta: 0:00:35  loss: 0.0169 (0.0169)  time: 2.5041  data: 0.3763  max mem: 34254\n",
      "Valid: [epoch:15]  [13/14]  eta: 0:00:02  loss: 0.0176 (0.0184)  time: 2.1491  data: 0.0270  max mem: 34254\n",
      "Valid: [epoch:15] Total time: 0:00:30 (2.1578 s / it)\n",
      "Averaged stats: loss: 0.0176 (0.0184)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_15_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.018%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:16]  [  0/127]  eta: 0:06:53  lr: 0.000100  loss: 0.1122 (0.1122)  time: 3.2597  data: 2.2805  max mem: 34254\n",
      "Train: [epoch:16]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.1103 (0.1100)  time: 1.1970  data: 0.2074  max mem: 34254\n",
      "Train: [epoch:16]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.1089 (0.1093)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.1082 (0.1086)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.1064 (0.1079)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.1042 (0.1070)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.1028 (0.1063)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.1019 (0.1057)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.1014 (0.1050)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0998 (0.1045)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0995 (0.1039)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0953 (0.1031)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0948 (0.1024)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0938 (0.1019)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:16] Total time: 0:02:08 (1.0150 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0938 (0.1019)\n",
      "Valid: [epoch:16]  [ 0/14]  eta: 0:00:37  loss: 0.0189 (0.0189)  time: 2.6525  data: 0.3623  max mem: 34254\n",
      "Valid: [epoch:16]  [13/14]  eta: 0:00:02  loss: 0.0173 (0.0180)  time: 2.2077  data: 0.0260  max mem: 34254\n",
      "Valid: [epoch:16] Total time: 0:00:31 (2.2167 s / it)\n",
      "Averaged stats: loss: 0.0173 (0.0180)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_16_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.018%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 16.000\n",
      "Train: [epoch:17]  [  0/127]  eta: 0:05:52  lr: 0.000100  loss: 0.0949 (0.0949)  time: 2.7749  data: 1.7894  max mem: 34254\n",
      "Train: [epoch:17]  [ 10/127]  eta: 0:02:14  lr: 0.000100  loss: 0.0921 (0.0924)  time: 1.1505  data: 0.1628  max mem: 34254\n",
      "Train: [epoch:17]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0915 (0.0918)  time: 0.9890  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0906 (0.0913)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0896 (0.0907)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0891 (0.0904)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0878 (0.0899)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0862 (0.0893)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0853 (0.0888)  time: 1.0008  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0840 (0.0882)  time: 1.0010  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0834 (0.0877)  time: 0.9997  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0819 (0.0872)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0807 (0.0866)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0807 (0.0863)  time: 0.9979  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:17] Total time: 0:02:08 (1.0117 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0807 (0.0863)\n",
      "Valid: [epoch:17]  [ 0/14]  eta: 0:00:35  loss: 0.0179 (0.0179)  time: 2.5610  data: 0.3934  max mem: 34254\n",
      "Valid: [epoch:17]  [13/14]  eta: 0:00:02  loss: 0.0164 (0.0170)  time: 2.1521  data: 0.0282  max mem: 34254\n",
      "Valid: [epoch:17] Total time: 0:00:30 (2.1616 s / it)\n",
      "Averaged stats: loss: 0.0164 (0.0170)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_17_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.017%\n",
      "Min loss: 0.017\n",
      "Best Epoch: 17.000\n",
      "Train: [epoch:18]  [  0/127]  eta: 0:05:21  lr: 0.000100  loss: 0.0795 (0.0795)  time: 2.5318  data: 1.5558  max mem: 34254\n",
      "Train: [epoch:18]  [ 10/127]  eta: 0:02:12  lr: 0.000100  loss: 0.0785 (0.0785)  time: 1.1301  data: 0.1415  max mem: 34254\n",
      "Train: [epoch:18]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0781 (0.0784)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0779 (0.0782)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [ 40/127]  eta: 0:01:29  lr: 0.000100  loss: 0.0765 (0.0776)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [ 50/127]  eta: 0:01:18  lr: 0.000100  loss: 0.0760 (0.0773)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0748 (0.0768)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0736 (0.0763)  time: 0.9983  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0732 (0.0760)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0730 (0.0756)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0724 (0.0752)  time: 1.0000  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0710 (0.0748)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0697 (0.0744)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0695 (0.0741)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:18] Total time: 0:02:08 (1.0107 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0695 (0.0741)\n",
      "Valid: [epoch:18]  [ 0/14]  eta: 0:00:36  loss: 0.0155 (0.0155)  time: 2.6112  data: 0.4247  max mem: 34254\n",
      "Valid: [epoch:18]  [13/14]  eta: 0:00:02  loss: 0.0164 (0.0169)  time: 2.1919  data: 0.0304  max mem: 34254\n",
      "Valid: [epoch:18] Total time: 0:00:30 (2.2013 s / it)\n",
      "Averaged stats: loss: 0.0164 (0.0169)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_18_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.017%\n",
      "Min loss: 0.017\n",
      "Best Epoch: 18.000\n",
      "Train: [epoch:19]  [  0/127]  eta: 0:07:21  lr: 0.000100  loss: 0.0659 (0.0659)  time: 3.4778  data: 2.4988  max mem: 34254\n",
      "Train: [epoch:19]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0674 (0.0677)  time: 1.2155  data: 0.2272  max mem: 34254\n",
      "Train: [epoch:19]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0674 (0.0674)  time: 0.9909  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0668 (0.0671)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0661 (0.0669)  time: 1.0006  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0660 (0.0667)  time: 1.0012  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0654 (0.0664)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0643 (0.0661)  time: 0.9987  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0642 (0.0658)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0627 (0.0655)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0621 (0.0651)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0618 (0.0648)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0613 (0.0645)  time: 0.9979  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0601 (0.0642)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:19] Total time: 0:02:09 (1.0181 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0601 (0.0642)\n",
      "Valid: [epoch:19]  [ 0/14]  eta: 0:00:35  loss: 0.0155 (0.0155)  time: 2.5332  data: 0.3643  max mem: 34254\n",
      "Valid: [epoch:19]  [13/14]  eta: 0:00:02  loss: 0.0164 (0.0169)  time: 2.1720  data: 0.0261  max mem: 34254\n",
      "Valid: [epoch:19] Total time: 0:00:30 (2.1805 s / it)\n",
      "Averaged stats: loss: 0.0164 (0.0169)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_19_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.017%\n",
      "Min loss: 0.017\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:20]  [  0/127]  eta: 0:06:22  lr: 0.000100  loss: 0.0586 (0.0586)  time: 3.0094  data: 1.9762  max mem: 34254\n",
      "Train: [epoch:20]  [ 10/127]  eta: 0:02:17  lr: 0.000100  loss: 0.0602 (0.0605)  time: 1.1736  data: 0.1797  max mem: 34254\n",
      "Train: [epoch:20]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0602 (0.0604)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0594 (0.0600)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0585 (0.0595)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0577 (0.0591)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0571 (0.0588)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0566 (0.0584)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0557 (0.0581)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0556 (0.0579)  time: 1.0029  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0555 (0.0576)  time: 1.0041  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0545 (0.0573)  time: 1.0008  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0544 (0.0571)  time: 1.0003  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0544 (0.0569)  time: 0.9996  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:20] Total time: 0:02:08 (1.0148 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0544 (0.0569)\n",
      "Valid: [epoch:20]  [ 0/14]  eta: 0:00:35  loss: 0.0155 (0.0155)  time: 2.5644  data: 0.4193  max mem: 34254\n",
      "Valid: [epoch:20]  [13/14]  eta: 0:00:02  loss: 0.0162 (0.0165)  time: 2.1636  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:20] Total time: 0:00:30 (2.1726 s / it)\n",
      "Averaged stats: loss: 0.0162 (0.0165)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_20_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.017%\n",
      "Min loss: 0.017\n",
      "Best Epoch: 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [  0/127]  eta: 0:06:10  lr: 0.000100  loss: 0.0534 (0.0534)  time: 2.9152  data: 1.9431  max mem: 34254\n",
      "Train: [epoch:21]  [ 10/127]  eta: 0:02:16  lr: 0.000100  loss: 0.0542 (0.0541)  time: 1.1656  data: 0.1767  max mem: 34254\n",
      "Train: [epoch:21]  [ 20/127]  eta: 0:01:55  lr: 0.000100  loss: 0.0536 (0.0535)  time: 0.9916  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0524 (0.0530)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0519 (0.0528)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0519 (0.0526)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0510 (0.0524)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0506 (0.0521)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0503 (0.0519)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0500 (0.0517)  time: 1.0002  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0495 (0.0515)  time: 1.0003  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0489 (0.0512)  time: 0.9991  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0482 (0.0510)  time: 0.9992  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0483 (0.0509)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:21] Total time: 0:02:08 (1.0137 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0483 (0.0509)\n",
      "Valid: [epoch:21]  [ 0/14]  eta: 0:00:35  loss: 0.0153 (0.0153)  time: 2.5188  data: 0.3825  max mem: 34254\n",
      "Valid: [epoch:21]  [13/14]  eta: 0:00:02  loss: 0.0162 (0.0165)  time: 2.1106  data: 0.0274  max mem: 34254\n",
      "Valid: [epoch:21] Total time: 0:00:29 (2.1206 s / it)\n",
      "Averaged stats: loss: 0.0162 (0.0165)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_21_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 21.000\n",
      "Train: [epoch:22]  [  0/127]  eta: 0:06:53  lr: 0.000100  loss: 0.0483 (0.0483)  time: 3.2598  data: 2.2878  max mem: 34254\n",
      "Train: [epoch:22]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0483 (0.0483)  time: 1.1961  data: 0.2081  max mem: 34254\n",
      "Train: [epoch:22]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0480 (0.0483)  time: 0.9910  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0477 (0.0481)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0474 (0.0479)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0468 (0.0476)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0463 (0.0474)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0460 (0.0471)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0458 (0.0470)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0456 (0.0468)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0450 (0.0467)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0450 (0.0465)  time: 0.9993  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0448 (0.0463)  time: 0.9993  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0442 (0.0462)  time: 0.9995  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:22] Total time: 0:02:08 (1.0156 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0442 (0.0462)\n",
      "Valid: [epoch:22]  [ 0/14]  eta: 0:00:35  loss: 0.0168 (0.0168)  time: 2.5115  data: 0.3645  max mem: 34254\n",
      "Valid: [epoch:22]  [13/14]  eta: 0:00:02  loss: 0.0160 (0.0163)  time: 2.1322  data: 0.0261  max mem: 34254\n",
      "Valid: [epoch:22] Total time: 0:00:29 (2.1411 s / it)\n",
      "Averaged stats: loss: 0.0160 (0.0163)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_22_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 22.000\n",
      "Train: [epoch:23]  [  0/127]  eta: 0:05:53  lr: 0.000100  loss: 0.0441 (0.0441)  time: 2.7824  data: 1.8025  max mem: 34254\n",
      "Train: [epoch:23]  [ 10/127]  eta: 0:02:14  lr: 0.000100  loss: 0.0441 (0.0445)  time: 1.1538  data: 0.1640  max mem: 34254\n",
      "Train: [epoch:23]  [ 20/127]  eta: 0:01:55  lr: 0.000100  loss: 0.0437 (0.0440)  time: 0.9910  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0432 (0.0440)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0431 (0.0437)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0430 (0.0436)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0434 (0.0435)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0430 (0.0434)  time: 0.9983  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0420 (0.0432)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0417 (0.0430)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0415 (0.0428)  time: 0.9996  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0409 (0.0427)  time: 1.0007  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0409 (0.0425)  time: 1.0008  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0408 (0.0424)  time: 1.0006  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:23] Total time: 0:02:08 (1.0119 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0408 (0.0424)\n",
      "Valid: [epoch:23]  [ 0/14]  eta: 0:00:36  loss: 0.0150 (0.0150)  time: 2.5901  data: 0.3821  max mem: 34254\n",
      "Valid: [epoch:23]  [13/14]  eta: 0:00:02  loss: 0.0159 (0.0161)  time: 2.1928  data: 0.0274  max mem: 34254\n",
      "Valid: [epoch:23] Total time: 0:00:30 (2.2020 s / it)\n",
      "Averaged stats: loss: 0.0159 (0.0161)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_23_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 23.000\n",
      "Train: [epoch:24]  [  0/127]  eta: 0:06:44  lr: 0.000100  loss: 0.0402 (0.0402)  time: 3.1846  data: 2.2118  max mem: 34254\n",
      "Train: [epoch:24]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0411 (0.0407)  time: 1.1894  data: 0.2012  max mem: 34254\n",
      "Train: [epoch:24]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0401 (0.0404)  time: 0.9911  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0399 (0.0403)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0399 (0.0402)  time: 0.9995  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0399 (0.0401)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0394 (0.0400)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0390 (0.0398)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0389 (0.0398)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0390 (0.0397)  time: 0.9993  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:24]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0390 (0.0396)  time: 0.9988  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0386 (0.0395)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0382 (0.0393)  time: 1.0033  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0381 (0.0393)  time: 1.0040  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:24] Total time: 0:02:09 (1.0163 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0381 (0.0393)\n",
      "Valid: [epoch:24]  [ 0/14]  eta: 0:00:35  loss: 0.0163 (0.0163)  time: 2.5092  data: 0.3816  max mem: 34254\n",
      "Valid: [epoch:24]  [13/14]  eta: 0:00:02  loss: 0.0158 (0.0160)  time: 2.1419  data: 0.0273  max mem: 34254\n",
      "Valid: [epoch:24] Total time: 0:00:30 (2.1508 s / it)\n",
      "Averaged stats: loss: 0.0158 (0.0160)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_24_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 24.000\n",
      "Train: [epoch:25]  [  0/127]  eta: 0:07:24  lr: 0.000100  loss: 0.0388 (0.0388)  time: 3.5026  data: 2.4552  max mem: 34254\n",
      "Train: [epoch:25]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0373 (0.0376)  time: 1.2190  data: 0.2233  max mem: 34254\n",
      "Train: [epoch:25]  [ 20/127]  eta: 0:01:59  lr: 0.000100  loss: 0.0373 (0.0375)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0374 (0.0375)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0374 (0.0376)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0373 (0.0375)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0367 (0.0374)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0367 (0.0373)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0368 (0.0373)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0366 (0.0372)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0362 (0.0371)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0358 (0.0370)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0358 (0.0369)  time: 1.0001  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0354 (0.0368)  time: 1.0001  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:25] Total time: 0:02:09 (1.0183 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0354 (0.0368)\n",
      "Valid: [epoch:25]  [ 0/14]  eta: 0:00:35  loss: 0.0186 (0.0186)  time: 2.5230  data: 0.3780  max mem: 34254\n",
      "Valid: [epoch:25]  [13/14]  eta: 0:00:02  loss: 0.0156 (0.0159)  time: 2.1863  data: 0.0271  max mem: 34254\n",
      "Valid: [epoch:25] Total time: 0:00:30 (2.1960 s / it)\n",
      "Averaged stats: loss: 0.0156 (0.0159)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_25_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 25.000\n",
      "Train: [epoch:26]  [  0/127]  eta: 0:06:47  lr: 0.000100  loss: 0.0373 (0.0373)  time: 3.2062  data: 2.2257  max mem: 34254\n",
      "Train: [epoch:26]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0364 (0.0361)  time: 1.1965  data: 0.2024  max mem: 34254\n",
      "Train: [epoch:26]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0361 (0.0358)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0349 (0.0357)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0351 (0.0357)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0351 (0.0354)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0344 (0.0354)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0344 (0.0352)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0345 (0.0351)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0345 (0.0351)  time: 0.9995  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0344 (0.0350)  time: 1.0044  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0342 (0.0350)  time: 1.0094  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0342 (0.0349)  time: 1.0055  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0343 (0.0349)  time: 1.0009  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:26] Total time: 0:02:09 (1.0182 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0343 (0.0349)\n",
      "Valid: [epoch:26]  [ 0/14]  eta: 0:00:36  loss: 0.0149 (0.0149)  time: 2.6274  data: 0.4553  max mem: 34254\n",
      "Valid: [epoch:26]  [13/14]  eta: 0:00:02  loss: 0.0159 (0.0160)  time: 2.1533  data: 0.0326  max mem: 34254\n",
      "Valid: [epoch:26] Total time: 0:00:30 (2.1626 s / it)\n",
      "Averaged stats: loss: 0.0159 (0.0160)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_26_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 25.000\n",
      "Train: [epoch:27]  [  0/127]  eta: 0:06:41  lr: 0.000100  loss: 0.0322 (0.0322)  time: 3.1591  data: 2.1822  max mem: 34254\n",
      "Train: [epoch:27]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0335 (0.0336)  time: 1.1881  data: 0.1985  max mem: 34254\n",
      "Train: [epoch:27]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0335 (0.0338)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0337 (0.0338)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0333 (0.0337)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0330 (0.0336)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0331 (0.0335)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0329 (0.0335)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0327 (0.0334)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0327 (0.0333)  time: 0.9993  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0334 (0.0333)  time: 0.9997  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0328 (0.0333)  time: 0.9996  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0321 (0.0332)  time: 1.0003  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0322 (0.0331)  time: 1.0056  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:27] Total time: 0:02:09 (1.0161 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0322 (0.0331)\n",
      "Valid: [epoch:27]  [ 0/14]  eta: 0:00:41  loss: 0.0157 (0.0157)  time: 2.9714  data: 0.4714  max mem: 34254\n",
      "Valid: [epoch:27]  [13/14]  eta: 0:00:02  loss: 0.0157 (0.0159)  time: 2.5205  data: 0.0338  max mem: 34254\n",
      "Valid: [epoch:27] Total time: 0:00:35 (2.5329 s / it)\n",
      "Averaged stats: loss: 0.0157 (0.0159)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_27_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 25.000\n",
      "Train: [epoch:28]  [  0/127]  eta: 0:07:24  lr: 0.000100  loss: 0.0320 (0.0320)  time: 3.4979  data: 2.5205  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:28]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0322 (0.0327)  time: 1.2195  data: 0.2292  max mem: 34254\n",
      "Train: [epoch:28]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0322 (0.0325)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0321 (0.0325)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0323 (0.0324)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0319 (0.0323)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0317 (0.0322)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0320 (0.0322)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0316 (0.0321)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0313 (0.0320)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0313 (0.0319)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0315 (0.0319)  time: 1.0003  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0313 (0.0319)  time: 1.0007  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0312 (0.0318)  time: 1.0002  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:28] Total time: 0:02:09 (1.0174 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0312 (0.0318)\n",
      "Valid: [epoch:28]  [ 0/14]  eta: 0:00:41  loss: 0.0155 (0.0155)  time: 2.9788  data: 0.4222  max mem: 34254\n",
      "Valid: [epoch:28]  [13/14]  eta: 0:00:02  loss: 0.0158 (0.0159)  time: 2.3686  data: 0.0303  max mem: 34254\n",
      "Valid: [epoch:28] Total time: 0:00:33 (2.3811 s / it)\n",
      "Averaged stats: loss: 0.0158 (0.0159)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_28_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 25.000\n",
      "Train: [epoch:29]  [  0/127]  eta: 0:07:00  lr: 0.000100  loss: 0.0307 (0.0307)  time: 3.3111  data: 2.3367  max mem: 34254\n",
      "Train: [epoch:29]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0314 (0.0315)  time: 1.2080  data: 0.2125  max mem: 34254\n",
      "Train: [epoch:29]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0311 (0.0313)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0311 (0.0311)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0304 (0.0310)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0307 (0.0310)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0312 (0.0310)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0307 (0.0310)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0306 (0.0309)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0304 (0.0309)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0302 (0.0308)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0303 (0.0308)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0303 (0.0308)  time: 0.9995  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0303 (0.0307)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:29] Total time: 0:02:09 (1.0161 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0303 (0.0307)\n",
      "Valid: [epoch:29]  [ 0/14]  eta: 0:00:37  loss: 0.0164 (0.0164)  time: 2.6999  data: 0.4392  max mem: 34254\n",
      "Valid: [epoch:29]  [13/14]  eta: 0:00:02  loss: 0.0157 (0.0158)  time: 2.2826  data: 0.0315  max mem: 34254\n",
      "Valid: [epoch:29] Total time: 0:00:32 (2.2940 s / it)\n",
      "Averaged stats: loss: 0.0157 (0.0158)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_29_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.016\n",
      "Best Epoch: 29.000\n",
      "Train: [epoch:30]  [  0/127]  eta: 0:07:09  lr: 0.000100  loss: 0.0323 (0.0323)  time: 3.3792  data: 2.3780  max mem: 34254\n",
      "Train: [epoch:30]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0300 (0.0303)  time: 1.2072  data: 0.2163  max mem: 34254\n",
      "Train: [epoch:30]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0297 (0.0300)  time: 0.9906  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0300 (0.0303)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0299 (0.0302)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0297 (0.0300)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0295 (0.0299)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0297 (0.0299)  time: 0.9956  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:30]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0294 (0.0298)  time: 0.9948  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:30]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0294 (0.0298)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0296 (0.0298)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0297 (0.0298)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0296 (0.0297)  time: 0.9990  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:30]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0296 (0.0297)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:30] Total time: 0:02:08 (1.0152 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0296 (0.0297)\n",
      "Valid: [epoch:30]  [ 0/14]  eta: 0:00:38  loss: 0.0162 (0.0162)  time: 2.7198  data: 0.4075  max mem: 34254\n",
      "Valid: [epoch:30]  [13/14]  eta: 0:00:02  loss: 0.0153 (0.0155)  time: 2.1668  data: 0.0292  max mem: 34254\n",
      "Valid: [epoch:30] Total time: 0:00:30 (2.1780 s / it)\n",
      "Averaged stats: loss: 0.0153 (0.0155)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_30_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 30.000\n",
      "Train: [epoch:31]  [  0/127]  eta: 0:07:10  lr: 0.000100  loss: 0.0286 (0.0286)  time: 3.3913  data: 2.4094  max mem: 34254\n",
      "Train: [epoch:31]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0286 (0.0289)  time: 1.2136  data: 0.2192  max mem: 34254\n",
      "Train: [epoch:31]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0294 (0.0293)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0295 (0.0293)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0289 (0.0292)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0286 (0.0291)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0286 (0.0291)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0288 (0.0290)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0285 (0.0290)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0284 (0.0290)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0284 (0.0290)  time: 0.9968  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:31]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0283 (0.0289)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0283 (0.0289)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0286 (0.0289)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:31] Total time: 0:02:09 (1.0166 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0286 (0.0289)\n",
      "Valid: [epoch:31]  [ 0/14]  eta: 0:00:35  loss: 0.0161 (0.0161)  time: 2.5083  data: 0.3813  max mem: 34254\n",
      "Valid: [epoch:31]  [13/14]  eta: 0:00:02  loss: 0.0156 (0.0157)  time: 2.1149  data: 0.0273  max mem: 34254\n",
      "Valid: [epoch:31] Total time: 0:00:29 (2.1267 s / it)\n",
      "Averaged stats: loss: 0.0156 (0.0157)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_31_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 30.000\n",
      "Train: [epoch:32]  [  0/127]  eta: 0:05:38  lr: 0.000100  loss: 0.0296 (0.0296)  time: 2.6688  data: 1.6869  max mem: 34254\n",
      "Train: [epoch:32]  [ 10/127]  eta: 0:02:14  lr: 0.000100  loss: 0.0296 (0.0291)  time: 1.1483  data: 0.1535  max mem: 34254\n",
      "Train: [epoch:32]  [ 20/127]  eta: 0:01:55  lr: 0.000100  loss: 0.0286 (0.0286)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0286 (0.0287)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0284 (0.0286)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0281 (0.0285)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0281 (0.0284)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0271 (0.0283)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0271 (0.0282)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0276 (0.0282)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0281 (0.0282)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0279 (0.0281)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0280 (0.0282)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0281 (0.0281)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:32] Total time: 0:02:08 (1.0106 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0281 (0.0281)\n",
      "Valid: [epoch:32]  [ 0/14]  eta: 0:00:35  loss: 0.0160 (0.0160)  time: 2.5384  data: 0.3790  max mem: 34254\n",
      "Valid: [epoch:32]  [13/14]  eta: 0:00:02  loss: 0.0153 (0.0155)  time: 2.1479  data: 0.0272  max mem: 34254\n",
      "Valid: [epoch:32] Total time: 0:00:30 (2.1563 s / it)\n",
      "Averaged stats: loss: 0.0153 (0.0155)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_32_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 32.000\n",
      "Train: [epoch:33]  [  0/127]  eta: 0:06:49  lr: 0.000100  loss: 0.0283 (0.0283)  time: 3.2273  data: 2.2518  max mem: 34254\n",
      "Train: [epoch:33]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0282 (0.0282)  time: 1.1944  data: 0.2048  max mem: 34254\n",
      "Train: [epoch:33]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0281 (0.0282)  time: 0.9915  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0278 (0.0281)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0277 (0.0280)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0275 (0.0280)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0279 (0.0280)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0275 (0.0279)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0272 (0.0278)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0272 (0.0277)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0271 (0.0277)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0272 (0.0277)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0268 (0.0276)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0270 (0.0276)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:33] Total time: 0:02:08 (1.0142 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0270 (0.0276)\n",
      "Valid: [epoch:33]  [ 0/14]  eta: 0:00:35  loss: 0.0159 (0.0159)  time: 2.5694  data: 0.3775  max mem: 34254\n",
      "Valid: [epoch:33]  [13/14]  eta: 0:00:02  loss: 0.0156 (0.0157)  time: 2.1584  data: 0.0271  max mem: 34254\n",
      "Valid: [epoch:33] Total time: 0:00:30 (2.1671 s / it)\n",
      "Averaged stats: loss: 0.0156 (0.0157)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_33_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 32.000\n",
      "Train: [epoch:34]  [  0/127]  eta: 0:06:23  lr: 0.000100  loss: 0.0249 (0.0249)  time: 3.0198  data: 2.0414  max mem: 34254\n",
      "Train: [epoch:34]  [ 10/127]  eta: 0:02:17  lr: 0.000100  loss: 0.0279 (0.0274)  time: 1.1774  data: 0.1857  max mem: 34254\n",
      "Train: [epoch:34]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0280 (0.0277)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0272 (0.0276)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0272 (0.0276)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0273 (0.0276)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0270 (0.0274)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0271 (0.0274)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0269 (0.0273)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0266 (0.0273)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0266 (0.0272)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0266 (0.0272)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0264 (0.0271)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0266 (0.0271)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:34] Total time: 0:02:08 (1.0129 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0266 (0.0271)\n",
      "Valid: [epoch:34]  [ 0/14]  eta: 0:00:35  loss: 0.0154 (0.0154)  time: 2.5625  data: 0.4449  max mem: 34254\n",
      "Valid: [epoch:34]  [13/14]  eta: 0:00:02  loss: 0.0158 (0.0159)  time: 2.1140  data: 0.0319  max mem: 34254\n",
      "Valid: [epoch:34] Total time: 0:00:29 (2.1222 s / it)\n",
      "Averaged stats: loss: 0.0158 (0.0159)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_34_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 32.000\n",
      "Train: [epoch:35]  [  0/127]  eta: 0:06:27  lr: 0.000100  loss: 0.0281 (0.0281)  time: 3.0516  data: 2.0783  max mem: 34254\n",
      "Train: [epoch:35]  [ 10/127]  eta: 0:02:18  lr: 0.000100  loss: 0.0274 (0.0272)  time: 1.1798  data: 0.1890  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0268 (0.0269)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0265 (0.0269)  time: 0.9926  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0264 (0.0269)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0266 (0.0269)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0266 (0.0268)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0265 (0.0267)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0262 (0.0267)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0261 (0.0267)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0259 (0.0266)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0266 (0.0266)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0263 (0.0266)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0262 (0.0266)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:35] Total time: 0:02:08 (1.0121 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0262 (0.0266)\n",
      "Valid: [epoch:35]  [ 0/14]  eta: 0:00:37  loss: 0.0158 (0.0158)  time: 2.6755  data: 0.4563  max mem: 34254\n",
      "Valid: [epoch:35]  [13/14]  eta: 0:00:02  loss: 0.0155 (0.0156)  time: 2.2442  data: 0.0327  max mem: 34254\n",
      "Valid: [epoch:35] Total time: 0:00:31 (2.2540 s / it)\n",
      "Averaged stats: loss: 0.0155 (0.0156)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_35_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 32.000\n",
      "Train: [epoch:36]  [  0/127]  eta: 0:06:36  lr: 0.000100  loss: 0.0266 (0.0266)  time: 3.1208  data: 2.1474  max mem: 34254\n",
      "Train: [epoch:36]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0266 (0.0266)  time: 1.1895  data: 0.1953  max mem: 34254\n",
      "Train: [epoch:36]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0263 (0.0264)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0262 (0.0264)  time: 0.9918  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0258 (0.0262)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0257 (0.0261)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0257 (0.0261)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0260 (0.0261)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0261 (0.0261)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0261 (0.0261)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0261 (0.0261)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0260 (0.0261)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0263 (0.0261)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0261 (0.0261)  time: 0.9991  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:36] Total time: 0:02:08 (1.0130 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0261 (0.0261)\n",
      "Valid: [epoch:36]  [ 0/14]  eta: 0:00:37  loss: 0.0154 (0.0154)  time: 2.6644  data: 0.4759  max mem: 34254\n",
      "Valid: [epoch:36]  [13/14]  eta: 0:00:02  loss: 0.0154 (0.0155)  time: 2.2633  data: 0.0341  max mem: 34254\n",
      "Valid: [epoch:36] Total time: 0:00:31 (2.2735 s / it)\n",
      "Averaged stats: loss: 0.0154 (0.0155)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_36_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 32.000\n",
      "Train: [epoch:37]  [  0/127]  eta: 0:07:20  lr: 0.000100  loss: 0.0261 (0.0261)  time: 3.4668  data: 2.4930  max mem: 34254\n",
      "Train: [epoch:37]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0254 (0.0255)  time: 1.2149  data: 0.2267  max mem: 34254\n",
      "Train: [epoch:37]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0257 (0.0260)  time: 0.9897  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0264 (0.0259)  time: 0.9901  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0251 (0.0257)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0251 (0.0257)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0253 (0.0257)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0257 (0.0257)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0255 (0.0257)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0256 (0.0258)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0256 (0.0257)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0252 (0.0257)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0252 (0.0257)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0252 (0.0257)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:37] Total time: 0:02:08 (1.0146 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0252 (0.0257)\n",
      "Valid: [epoch:37]  [ 0/14]  eta: 0:00:36  loss: 0.0185 (0.0185)  time: 2.6224  data: 0.4226  max mem: 34254\n",
      "Valid: [epoch:37]  [13/14]  eta: 0:00:02  loss: 0.0156 (0.0157)  time: 2.2194  data: 0.0303  max mem: 34254\n",
      "Valid: [epoch:37] Total time: 0:00:31 (2.2293 s / it)\n",
      "Averaged stats: loss: 0.0156 (0.0157)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_37_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 32.000\n",
      "Train: [epoch:38]  [  0/127]  eta: 0:06:47  lr: 0.000100  loss: 0.0247 (0.0247)  time: 3.2086  data: 2.2358  max mem: 34254\n",
      "Train: [epoch:38]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0255 (0.0257)  time: 1.1914  data: 0.2033  max mem: 34254\n",
      "Train: [epoch:38]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0255 (0.0257)  time: 0.9897  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0260 (0.0258)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0258 (0.0257)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0248 (0.0255)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0248 (0.0255)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0252 (0.0255)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0250 (0.0254)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0249 (0.0254)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0249 (0.0254)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0249 (0.0254)  time: 0.9978  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:38]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0251 (0.0254)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0251 (0.0254)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:38] Total time: 0:02:08 (1.0130 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0251 (0.0254)\n",
      "Valid: [epoch:38]  [ 0/14]  eta: 0:00:37  loss: 0.0156 (0.0156)  time: 2.6500  data: 0.4246  max mem: 34254\n",
      "Valid: [epoch:38]  [13/14]  eta: 0:00:02  loss: 0.0152 (0.0153)  time: 2.2275  data: 0.0304  max mem: 34254\n",
      "Valid: [epoch:38] Total time: 0:00:31 (2.2386 s / it)\n",
      "Averaged stats: loss: 0.0152 (0.0153)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_38_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 38.000\n",
      "Train: [epoch:39]  [  0/127]  eta: 0:07:03  lr: 0.000100  loss: 0.0258 (0.0258)  time: 3.3327  data: 2.3450  max mem: 34254\n",
      "Train: [epoch:39]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0250 (0.0253)  time: 1.2041  data: 0.2133  max mem: 34254\n",
      "Train: [epoch:39]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0250 (0.0253)  time: 0.9915  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0250 (0.0252)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0247 (0.0250)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0247 (0.0251)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0252 (0.0251)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0252 (0.0251)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0251 (0.0251)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0250 (0.0250)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0245 (0.0250)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0245 (0.0250)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0248 (0.0250)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0248 (0.0250)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:39] Total time: 0:02:08 (1.0143 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0248 (0.0250)\n",
      "Valid: [epoch:39]  [ 0/14]  eta: 0:00:38  loss: 0.0158 (0.0158)  time: 2.7295  data: 0.4482  max mem: 34254\n",
      "Valid: [epoch:39]  [13/14]  eta: 0:00:02  loss: 0.0153 (0.0154)  time: 2.2790  data: 0.0321  max mem: 34254\n",
      "Valid: [epoch:39] Total time: 0:00:32 (2.2885 s / it)\n",
      "Averaged stats: loss: 0.0153 (0.0154)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_39_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 38.000\n",
      "Train: [epoch:40]  [  0/127]  eta: 0:05:30  lr: 0.000100  loss: 0.0254 (0.0254)  time: 2.6050  data: 1.6233  max mem: 34254\n",
      "Train: [epoch:40]  [ 10/127]  eta: 0:02:13  lr: 0.000100  loss: 0.0247 (0.0245)  time: 1.1379  data: 0.1477  max mem: 34254\n",
      "Train: [epoch:40]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0247 (0.0249)  time: 0.9916  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0248 (0.0248)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [ 40/127]  eta: 0:01:29  lr: 0.000100  loss: 0.0249 (0.0249)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [ 50/127]  eta: 0:01:18  lr: 0.000100  loss: 0.0247 (0.0248)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0245 (0.0247)  time: 1.0022  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0243 (0.0247)  time: 1.0018  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0242 (0.0247)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0243 (0.0247)  time: 1.0022  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0246 (0.0247)  time: 1.0020  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0249 (0.0247)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0250 (0.0248)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0249 (0.0247)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:40] Total time: 0:02:08 (1.0107 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0249 (0.0247)\n",
      "Valid: [epoch:40]  [ 0/14]  eta: 0:00:37  loss: 0.0141 (0.0141)  time: 2.6705  data: 0.4463  max mem: 34254\n",
      "Valid: [epoch:40]  [13/14]  eta: 0:00:02  loss: 0.0156 (0.0157)  time: 2.2289  data: 0.0320  max mem: 34254\n",
      "Valid: [epoch:40] Total time: 0:00:31 (2.2390 s / it)\n",
      "Averaged stats: loss: 0.0156 (0.0157)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_40_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.016%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 38.000\n",
      "Train: [epoch:41]  [  0/127]  eta: 0:07:16  lr: 0.000100  loss: 0.0257 (0.0257)  time: 3.4398  data: 2.4551  max mem: 34254\n",
      "Train: [epoch:41]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0246 (0.0247)  time: 1.2144  data: 0.2233  max mem: 34254\n",
      "Train: [epoch:41]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0248 (0.0251)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0249 (0.0249)  time: 0.9924  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0241 (0.0247)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0239 (0.0246)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0240 (0.0246)  time: 1.0007  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0244 (0.0246)  time: 1.0000  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0244 (0.0245)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0241 (0.0245)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0242 (0.0245)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0238 (0.0245)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0238 (0.0244)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0238 (0.0244)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:41] Total time: 0:02:08 (1.0156 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0238 (0.0244)\n",
      "Valid: [epoch:41]  [ 0/14]  eta: 0:00:36  loss: 0.0157 (0.0157)  time: 2.6339  data: 0.4133  max mem: 34254\n",
      "Valid: [epoch:41]  [13/14]  eta: 0:00:02  loss: 0.0154 (0.0154)  time: 2.2390  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:41] Total time: 0:00:31 (2.2495 s / it)\n",
      "Averaged stats: loss: 0.0154 (0.0154)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_41_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 38.000\n",
      "Train: [epoch:42]  [  0/127]  eta: 0:05:25  lr: 0.000100  loss: 0.0239 (0.0239)  time: 2.5607  data: 1.5709  max mem: 34254\n",
      "Train: [epoch:42]  [ 10/127]  eta: 0:02:13  lr: 0.000100  loss: 0.0241 (0.0245)  time: 1.1374  data: 0.1429  max mem: 34254\n",
      "Train: [epoch:42]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0250 (0.0248)  time: 0.9937  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:42]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0244 (0.0245)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [ 40/127]  eta: 0:01:29  lr: 0.000100  loss: 0.0237 (0.0244)  time: 0.9987  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0237 (0.0242)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0235 (0.0242)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [ 70/127]  eta: 0:00:57  lr: 0.000100  loss: 0.0243 (0.0242)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0242 (0.0242)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0238 (0.0242)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0235 (0.0241)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0236 (0.0241)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0236 (0.0241)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0234 (0.0241)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:42] Total time: 0:02:08 (1.0090 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0234 (0.0241)\n",
      "Valid: [epoch:42]  [ 0/14]  eta: 0:00:37  loss: 0.0155 (0.0155)  time: 2.6987  data: 0.4429  max mem: 34254\n",
      "Valid: [epoch:42]  [13/14]  eta: 0:00:02  loss: 0.0150 (0.0151)  time: 2.2754  data: 0.0317  max mem: 34254\n",
      "Valid: [epoch:42] Total time: 0:00:32 (2.2862 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0151)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_42_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 42.000\n",
      "Train: [epoch:43]  [  0/127]  eta: 0:06:55  lr: 0.000100  loss: 0.0237 (0.0237)  time: 3.2753  data: 2.3017  max mem: 34254\n",
      "Train: [epoch:43]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0239 (0.0240)  time: 1.2024  data: 0.2093  max mem: 34254\n",
      "Train: [epoch:43]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0239 (0.0241)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0236 (0.0239)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0237 (0.0240)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0239 (0.0239)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0239 (0.0239)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0237 (0.0239)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0240 (0.0239)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0240 (0.0239)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0237 (0.0239)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0236 (0.0239)  time: 1.0109  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0236 (0.0238)  time: 1.0111  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0236 (0.0238)  time: 1.0019  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:43] Total time: 0:02:09 (1.0166 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0236 (0.0238)\n",
      "Valid: [epoch:43]  [ 0/14]  eta: 0:00:37  loss: 0.0156 (0.0156)  time: 2.6551  data: 0.4132  max mem: 34254\n",
      "Valid: [epoch:43]  [13/14]  eta: 0:00:02  loss: 0.0152 (0.0153)  time: 2.2657  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:43] Total time: 0:00:31 (2.2754 s / it)\n",
      "Averaged stats: loss: 0.0152 (0.0153)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_43_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 42.000\n",
      "Train: [epoch:44]  [  0/127]  eta: 0:07:11  lr: 0.000100  loss: 0.0240 (0.0240)  time: 3.3980  data: 2.4153  max mem: 34254\n",
      "Train: [epoch:44]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0239 (0.0237)  time: 1.2104  data: 0.2197  max mem: 34254\n",
      "Train: [epoch:44]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0238 (0.0237)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0237 (0.0237)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0233 (0.0235)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0229 (0.0234)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0232 (0.0235)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0237 (0.0235)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0237 (0.0235)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0237 (0.0235)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0238 (0.0236)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0235 (0.0236)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0234 (0.0236)  time: 0.9994  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0235 (0.0236)  time: 0.9995  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:44] Total time: 0:02:08 (1.0153 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0235 (0.0236)\n",
      "Valid: [epoch:44]  [ 0/14]  eta: 0:00:42  loss: 0.0145 (0.0145)  time: 3.0163  data: 0.4184  max mem: 34254\n",
      "Valid: [epoch:44]  [13/14]  eta: 0:00:02  loss: 0.0149 (0.0150)  time: 2.6666  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:44] Total time: 0:00:37 (2.6817 s / it)\n",
      "Averaged stats: loss: 0.0149 (0.0150)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_44_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 44.000\n",
      "Train: [epoch:45]  [  0/127]  eta: 0:06:32  lr: 0.000100  loss: 0.0228 (0.0228)  time: 3.0930  data: 2.1185  max mem: 34254\n",
      "Train: [epoch:45]  [ 10/127]  eta: 0:02:18  lr: 0.000100  loss: 0.0236 (0.0237)  time: 1.1805  data: 0.1927  max mem: 34254\n",
      "Train: [epoch:45]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0236 (0.0238)  time: 0.9899  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0232 (0.0236)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0232 (0.0236)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0234 (0.0236)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0234 (0.0235)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0234 (0.0235)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0227 (0.0234)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0226 (0.0234)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0229 (0.0233)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0231 (0.0234)  time: 0.9987  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0232 (0.0234)  time: 0.9983  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:45]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0230 (0.0233)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:45] Total time: 0:02:08 (1.0129 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0230 (0.0233)\n",
      "Valid: [epoch:45]  [ 0/14]  eta: 0:00:35  loss: 0.0144 (0.0144)  time: 2.5205  data: 0.3744  max mem: 34254\n",
      "Valid: [epoch:45]  [13/14]  eta: 0:00:02  loss: 0.0152 (0.0152)  time: 2.1340  data: 0.0268  max mem: 34254\n",
      "Valid: [epoch:45] Total time: 0:00:30 (2.1430 s / it)\n",
      "Averaged stats: loss: 0.0152 (0.0152)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_45_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 44.000\n",
      "Train: [epoch:46]  [  0/127]  eta: 0:06:13  lr: 0.000100  loss: 0.0225 (0.0225)  time: 2.9398  data: 1.9007  max mem: 34254\n",
      "Train: [epoch:46]  [ 10/127]  eta: 0:02:16  lr: 0.000100  loss: 0.0229 (0.0233)  time: 1.1679  data: 0.1729  max mem: 34254\n",
      "Train: [epoch:46]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0229 (0.0231)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0230 (0.0231)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0230 (0.0231)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0233 (0.0232)  time: 0.9992  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0237 (0.0233)  time: 1.0005  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0230 (0.0232)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0226 (0.0232)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0226 (0.0231)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0232 (0.0232)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0232 (0.0231)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0226 (0.0231)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0233 (0.0231)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:46] Total time: 0:02:08 (1.0123 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0233 (0.0231)\n",
      "Valid: [epoch:46]  [ 0/14]  eta: 0:00:37  loss: 0.0139 (0.0139)  time: 2.6504  data: 0.4461  max mem: 34254\n",
      "Valid: [epoch:46]  [13/14]  eta: 0:00:02  loss: 0.0150 (0.0150)  time: 2.1650  data: 0.0320  max mem: 34254\n",
      "Valid: [epoch:46] Total time: 0:00:30 (2.1775 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0150)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_46_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 44.000\n",
      "Train: [epoch:47]  [  0/127]  eta: 0:06:19  lr: 0.000100  loss: 0.0250 (0.0250)  time: 2.9891  data: 2.0041  max mem: 34254\n",
      "Train: [epoch:47]  [ 10/127]  eta: 0:02:17  lr: 0.000100  loss: 0.0233 (0.0233)  time: 1.1729  data: 0.1823  max mem: 34254\n",
      "Train: [epoch:47]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0228 (0.0233)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0228 (0.0232)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0227 (0.0230)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0227 (0.0230)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0226 (0.0229)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0226 (0.0229)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0226 (0.0229)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0226 (0.0228)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0222 (0.0228)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0223 (0.0228)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0228 (0.0228)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0225 (0.0228)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:47] Total time: 0:02:08 (1.0118 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0225 (0.0228)\n",
      "Valid: [epoch:47]  [ 0/14]  eta: 0:00:34  loss: 0.0157 (0.0157)  time: 2.4802  data: 0.3902  max mem: 34254\n",
      "Valid: [epoch:47]  [13/14]  eta: 0:00:02  loss: 0.0154 (0.0154)  time: 2.0931  data: 0.0279  max mem: 34254\n",
      "Valid: [epoch:47] Total time: 0:00:29 (2.1023 s / it)\n",
      "Averaged stats: loss: 0.0154 (0.0154)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_47_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 44.000\n",
      "Train: [epoch:48]  [  0/127]  eta: 0:07:17  lr: 0.000100  loss: 0.0241 (0.0241)  time: 3.4429  data: 2.4536  max mem: 34254\n",
      "Train: [epoch:48]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0237 (0.0235)  time: 1.2151  data: 0.2231  max mem: 34254\n",
      "Train: [epoch:48]  [ 20/127]  eta: 0:01:59  lr: 0.000100  loss: 0.0231 (0.0231)  time: 1.0016  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0228 (0.0230)  time: 1.0028  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [ 40/127]  eta: 0:01:32  lr: 0.000100  loss: 0.0226 (0.0230)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0229 (0.0229)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0220 (0.0228)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0220 (0.0227)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0220 (0.0226)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0221 (0.0226)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0223 (0.0226)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0225 (0.0226)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0227 (0.0226)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0223 (0.0226)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:48] Total time: 0:02:09 (1.0171 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0223 (0.0226)\n",
      "Valid: [epoch:48]  [ 0/14]  eta: 0:00:34  loss: 0.0140 (0.0140)  time: 2.4380  data: 0.3654  max mem: 34254\n",
      "Valid: [epoch:48]  [13/14]  eta: 0:00:02  loss: 0.0150 (0.0151)  time: 2.0920  data: 0.0262  max mem: 34254\n",
      "Valid: [epoch:48] Total time: 0:00:29 (2.1013 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0151)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_48_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 44.000\n",
      "Train: [epoch:49]  [  0/127]  eta: 0:06:36  lr: 0.000100  loss: 0.0219 (0.0219)  time: 3.1217  data: 2.1018  max mem: 34254\n",
      "Train: [epoch:49]  [ 10/127]  eta: 0:02:18  lr: 0.000100  loss: 0.0227 (0.0228)  time: 1.1841  data: 0.1912  max mem: 34254\n",
      "Train: [epoch:49]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0226 (0.0225)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0220 (0.0225)  time: 0.9943  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:49]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0225 (0.0225)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0227 (0.0226)  time: 1.0025  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0226 (0.0226)  time: 1.0033  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0221 (0.0225)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0221 (0.0225)  time: 0.9992  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0222 (0.0225)  time: 1.0022  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0220 (0.0224)  time: 1.0028  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0225 (0.0225)  time: 0.9997  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0230 (0.0224)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0223 (0.0224)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:49] Total time: 0:02:09 (1.0160 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0223 (0.0224)\n",
      "Valid: [epoch:49]  [ 0/14]  eta: 0:00:37  loss: 0.0146 (0.0146)  time: 2.6554  data: 0.4372  max mem: 34254\n",
      "Valid: [epoch:49]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.2324  data: 0.0313  max mem: 34254\n",
      "Valid: [epoch:49] Total time: 0:00:31 (2.2425 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_49_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 49.000\n",
      "Train: [epoch:50]  [  0/127]  eta: 0:06:38  lr: 0.000100  loss: 0.0246 (0.0246)  time: 3.1406  data: 2.1648  max mem: 34254\n",
      "Train: [epoch:50]  [ 10/127]  eta: 0:02:18  lr: 0.000100  loss: 0.0227 (0.0227)  time: 1.1855  data: 0.1969  max mem: 34254\n",
      "Train: [epoch:50]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0221 (0.0225)  time: 0.9912  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0218 (0.0224)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0218 (0.0224)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0223 (0.0224)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0223 (0.0224)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0221 (0.0224)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0225 (0.0224)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0227 (0.0223)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0219 (0.0223)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0217 (0.0222)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0219 (0.0222)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0217 (0.0222)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:50] Total time: 0:02:08 (1.0140 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0217 (0.0222)\n",
      "Valid: [epoch:50]  [ 0/14]  eta: 0:00:35  loss: 0.0155 (0.0155)  time: 2.5348  data: 0.3903  max mem: 34254\n",
      "Valid: [epoch:50]  [13/14]  eta: 0:00:02  loss: 0.0148 (0.0149)  time: 2.1264  data: 0.0280  max mem: 34254\n",
      "Valid: [epoch:50] Total time: 0:00:29 (2.1359 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_50_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 49.000\n",
      "Train: [epoch:51]  [  0/127]  eta: 0:05:30  lr: 0.000100  loss: 0.0210 (0.0210)  time: 2.6010  data: 1.6249  max mem: 34254\n",
      "Train: [epoch:51]  [ 10/127]  eta: 0:02:13  lr: 0.000100  loss: 0.0226 (0.0222)  time: 1.1418  data: 0.1478  max mem: 34254\n",
      "Train: [epoch:51]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0224 (0.0221)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0218 (0.0220)  time: 0.9918  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [ 40/127]  eta: 0:01:29  lr: 0.000100  loss: 0.0219 (0.0221)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [ 50/127]  eta: 0:01:18  lr: 0.000100  loss: 0.0222 (0.0221)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0222 (0.0222)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0223 (0.0223)  time: 0.9993  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0219 (0.0222)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0219 (0.0222)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0218 (0.0222)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0211 (0.0221)  time: 0.9991  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0213 (0.0220)  time: 0.9991  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0214 (0.0220)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:51] Total time: 0:02:08 (1.0100 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0214 (0.0220)\n",
      "Valid: [epoch:51]  [ 0/14]  eta: 0:00:37  loss: 0.0156 (0.0156)  time: 2.6954  data: 0.4408  max mem: 34254\n",
      "Valid: [epoch:51]  [13/14]  eta: 0:00:02  loss: 0.0148 (0.0149)  time: 2.2183  data: 0.0316  max mem: 34254\n",
      "Valid: [epoch:51] Total time: 0:00:31 (2.2319 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_51_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 49.000\n",
      "Train: [epoch:52]  [  0/127]  eta: 0:06:58  lr: 0.000100  loss: 0.0207 (0.0207)  time: 3.2968  data: 2.3256  max mem: 34254\n",
      "Train: [epoch:52]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0229 (0.0225)  time: 1.1995  data: 0.2115  max mem: 34254\n",
      "Train: [epoch:52]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0213 (0.0217)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0213 (0.0219)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0220 (0.0219)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0220 (0.0220)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0219 (0.0219)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0215 (0.0219)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0214 (0.0218)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0215 (0.0219)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0220 (0.0219)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0218 (0.0219)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0219 (0.0219)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:52]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0222 (0.0219)  time: 0.9966  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:52] Total time: 0:02:08 (1.0151 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0222 (0.0219)\n",
      "Valid: [epoch:52]  [ 0/14]  eta: 0:00:35  loss: 0.0145 (0.0145)  time: 2.5641  data: 0.3930  max mem: 34254\n",
      "Valid: [epoch:52]  [13/14]  eta: 0:00:02  loss: 0.0148 (0.0149)  time: 2.1793  data: 0.0282  max mem: 34254\n",
      "Valid: [epoch:52] Total time: 0:00:30 (2.1878 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_52_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 49.000\n",
      "Train: [epoch:53]  [  0/127]  eta: 0:06:55  lr: 0.000100  loss: 0.0207 (0.0207)  time: 3.2704  data: 2.3011  max mem: 34254\n",
      "Train: [epoch:53]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0221 (0.0227)  time: 1.1973  data: 0.2093  max mem: 34254\n",
      "Train: [epoch:53]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0220 (0.0225)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0218 (0.0221)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0212 (0.0219)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0214 (0.0219)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0218 (0.0218)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0218 (0.0218)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0217 (0.0218)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0211 (0.0218)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0215 (0.0218)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0215 (0.0217)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0210 (0.0217)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0210 (0.0217)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:53] Total time: 0:02:08 (1.0136 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0210 (0.0217)\n",
      "Valid: [epoch:53]  [ 0/14]  eta: 0:00:36  loss: 0.0151 (0.0151)  time: 2.6176  data: 0.4297  max mem: 34254\n",
      "Valid: [epoch:53]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.1999  data: 0.0308  max mem: 34254\n",
      "Valid: [epoch:53] Total time: 0:00:30 (2.2099 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_53_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 53.000\n",
      "Train: [epoch:54]  [  0/127]  eta: 0:06:57  lr: 0.000100  loss: 0.0213 (0.0213)  time: 3.2880  data: 2.3158  max mem: 34254\n",
      "Train: [epoch:54]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0213 (0.0215)  time: 1.1980  data: 0.2106  max mem: 34254\n",
      "Train: [epoch:54]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0216 (0.0217)  time: 0.9899  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0214 (0.0216)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0211 (0.0215)  time: 0.9979  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0212 (0.0215)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0216 (0.0215)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0213 (0.0215)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0211 (0.0215)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0211 (0.0214)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0211 (0.0214)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0212 (0.0214)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0215 (0.0214)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0215 (0.0215)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:54] Total time: 0:02:08 (1.0155 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0215 (0.0215)\n",
      "Valid: [epoch:54]  [ 0/14]  eta: 0:00:36  loss: 0.0154 (0.0154)  time: 2.6184  data: 0.4279  max mem: 34254\n",
      "Valid: [epoch:54]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.1889  data: 0.0307  max mem: 34254\n",
      "Valid: [epoch:54] Total time: 0:00:30 (2.1976 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_54_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 53.000\n",
      "Train: [epoch:55]  [  0/127]  eta: 0:07:15  lr: 0.000100  loss: 0.0201 (0.0201)  time: 3.4329  data: 2.4616  max mem: 34254\n",
      "Train: [epoch:55]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0216 (0.0219)  time: 1.2103  data: 0.2239  max mem: 34254\n",
      "Train: [epoch:55]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0215 (0.0217)  time: 0.9885  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0212 (0.0216)  time: 0.9909  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0212 (0.0216)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0212 (0.0216)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0210 (0.0215)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0205 (0.0214)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0211 (0.0214)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0209 (0.0213)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0208 (0.0214)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0215 (0.0214)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0210 (0.0214)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0205 (0.0213)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:55] Total time: 0:02:08 (1.0146 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0205 (0.0213)\n",
      "Valid: [epoch:55]  [ 0/14]  eta: 0:00:37  loss: 0.0141 (0.0141)  time: 2.6509  data: 0.4846  max mem: 34254\n",
      "Valid: [epoch:55]  [13/14]  eta: 0:00:02  loss: 0.0148 (0.0149)  time: 2.3400  data: 0.0347  max mem: 34254\n",
      "Valid: [epoch:55] Total time: 0:00:32 (2.3505 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_55_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 53.000\n",
      "Train: [epoch:56]  [  0/127]  eta: 0:06:33  lr: 0.000100  loss: 0.0260 (0.0260)  time: 3.1021  data: 2.1316  max mem: 34254\n",
      "Train: [epoch:56]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0217 (0.0218)  time: 1.1912  data: 0.1939  max mem: 34254\n",
      "Train: [epoch:56]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0217 (0.0216)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0214 (0.0215)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0208 (0.0214)  time: 0.9951  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:56]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0206 (0.0213)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0206 (0.0213)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0207 (0.0212)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0208 (0.0212)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0208 (0.0212)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0212 (0.0212)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0216 (0.0212)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0208 (0.0212)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0206 (0.0212)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:56] Total time: 0:02:08 (1.0132 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0206 (0.0212)\n",
      "Valid: [epoch:56]  [ 0/14]  eta: 0:00:37  loss: 0.0146 (0.0146)  time: 2.6501  data: 0.4275  max mem: 34254\n",
      "Valid: [epoch:56]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2186  data: 0.0306  max mem: 34254\n",
      "Valid: [epoch:56] Total time: 0:00:31 (2.2284 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_56_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 56.000\n",
      "Train: [epoch:57]  [  0/127]  eta: 0:06:57  lr: 0.000100  loss: 0.0223 (0.0223)  time: 3.2860  data: 2.2918  max mem: 34254\n",
      "Train: [epoch:57]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0215 (0.0214)  time: 1.1980  data: 0.2084  max mem: 34254\n",
      "Train: [epoch:57]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0209 (0.0210)  time: 0.9898  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0207 (0.0212)  time: 0.9912  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0209 (0.0211)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0209 (0.0211)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0209 (0.0211)  time: 1.0000  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0212 (0.0212)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0212 (0.0211)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0205 (0.0211)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0209 (0.0211)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0207 (0.0211)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0205 (0.0211)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0200 (0.0210)  time: 0.9988  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:57] Total time: 0:02:08 (1.0157 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0200 (0.0210)\n",
      "Valid: [epoch:57]  [ 0/14]  eta: 0:00:41  loss: 0.0175 (0.0175)  time: 2.9340  data: 0.4071  max mem: 34254\n",
      "Valid: [epoch:57]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.5372  data: 0.0292  max mem: 34254\n",
      "Valid: [epoch:57] Total time: 0:00:35 (2.5484 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_57_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 56.000\n",
      "Train: [epoch:58]  [  0/127]  eta: 0:07:05  lr: 0.000100  loss: 0.0204 (0.0204)  time: 3.3512  data: 2.3807  max mem: 34254\n",
      "Train: [epoch:58]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0210 (0.0209)  time: 1.2075  data: 0.2165  max mem: 34254\n",
      "Train: [epoch:58]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0210 (0.0212)  time: 0.9917  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0206 (0.0208)  time: 0.9911  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0202 (0.0207)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0203 (0.0207)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0203 (0.0207)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0208 (0.0207)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0209 (0.0208)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0207 (0.0208)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0199 (0.0207)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0202 (0.0207)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0208 (0.0207)  time: 0.9996  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0205 (0.0208)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:58] Total time: 0:02:08 (1.0157 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0205 (0.0208)\n",
      "Valid: [epoch:58]  [ 0/14]  eta: 0:00:41  loss: 0.0154 (0.0154)  time: 2.9342  data: 0.3855  max mem: 34254\n",
      "Valid: [epoch:58]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.5385  data: 0.0276  max mem: 34254\n",
      "Valid: [epoch:58] Total time: 0:00:35 (2.5482 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_58_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 56.000\n",
      "Train: [epoch:59]  [  0/127]  eta: 0:06:56  lr: 0.000100  loss: 0.0224 (0.0224)  time: 3.2780  data: 2.2999  max mem: 34254\n",
      "Train: [epoch:59]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0206 (0.0207)  time: 1.2071  data: 0.2092  max mem: 34254\n",
      "Train: [epoch:59]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0206 (0.0208)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0207 (0.0206)  time: 0.9890  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0202 (0.0206)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0204 (0.0206)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0205 (0.0206)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0206 (0.0206)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0205 (0.0206)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0208 (0.0206)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0205 (0.0206)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0203 (0.0206)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0202 (0.0206)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0203 (0.0206)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:59] Total time: 0:02:08 (1.0143 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0203 (0.0206)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:59]  [ 0/14]  eta: 0:00:37  loss: 0.0144 (0.0144)  time: 2.6944  data: 0.4678  max mem: 34254\n",
      "Valid: [epoch:59]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1734  data: 0.0335  max mem: 34254\n",
      "Valid: [epoch:59] Total time: 0:00:30 (2.1862 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_59_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 56.000\n",
      "Train: [epoch:60]  [  0/127]  eta: 0:06:46  lr: 0.000100  loss: 0.0197 (0.0197)  time: 3.2032  data: 2.2158  max mem: 34254\n",
      "Train: [epoch:60]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0198 (0.0203)  time: 1.1908  data: 0.2015  max mem: 34254\n",
      "Train: [epoch:60]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0201 (0.0203)  time: 0.9900  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0201 (0.0203)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0203 (0.0204)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0202 (0.0203)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0205 (0.0204)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0207 (0.0204)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0199 (0.0204)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0203 (0.0204)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0205 (0.0205)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0205 (0.0205)  time: 0.9997  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0200 (0.0204)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0203 (0.0205)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:60] Total time: 0:02:08 (1.0145 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0203 (0.0205)\n",
      "Valid: [epoch:60]  [ 0/14]  eta: 0:00:37  loss: 0.0131 (0.0131)  time: 2.6963  data: 0.4455  max mem: 34254\n",
      "Valid: [epoch:60]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2218  data: 0.0320  max mem: 34254\n",
      "Valid: [epoch:60] Total time: 0:00:31 (2.2357 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_60_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 56.000\n",
      "Train: [epoch:61]  [  0/127]  eta: 0:07:14  lr: 0.000100  loss: 0.0215 (0.0215)  time: 3.4209  data: 2.4496  max mem: 34254\n",
      "Train: [epoch:61]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0202 (0.0202)  time: 1.2090  data: 0.2228  max mem: 34254\n",
      "Train: [epoch:61]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0202 (0.0205)  time: 0.9887  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0208 (0.0206)  time: 0.9904  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0206 (0.0205)  time: 0.9911  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0203 (0.0205)  time: 0.9916  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0205 (0.0206)  time: 0.9926  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0198 (0.0204)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0199 (0.0205)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0202 (0.0204)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0202 (0.0204)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0207 (0.0205)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0206 (0.0204)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0196 (0.0204)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:61] Total time: 0:02:08 (1.0138 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0196 (0.0204)\n",
      "Valid: [epoch:61]  [ 0/14]  eta: 0:00:36  loss: 0.0154 (0.0154)  time: 2.5716  data: 0.4026  max mem: 34254\n",
      "Valid: [epoch:61]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.1964  data: 0.0289  max mem: 34254\n",
      "Valid: [epoch:61] Total time: 0:00:30 (2.2047 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_61_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 56.000\n",
      "Train: [epoch:62]  [  0/127]  eta: 0:06:57  lr: 0.000100  loss: 0.0203 (0.0203)  time: 3.2873  data: 2.2997  max mem: 34254\n",
      "Train: [epoch:62]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0203 (0.0205)  time: 1.2009  data: 0.2092  max mem: 34254\n",
      "Train: [epoch:62]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0199 (0.0202)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0203 (0.0205)  time: 0.9904  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0207 (0.0204)  time: 0.9921  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0199 (0.0204)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0200 (0.0204)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0199 (0.0202)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0197 (0.0202)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0202 (0.0202)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0204 (0.0203)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0204 (0.0202)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0198 (0.0202)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0200 (0.0202)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:62] Total time: 0:02:08 (1.0134 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0200 (0.0202)\n",
      "Valid: [epoch:62]  [ 0/14]  eta: 0:00:36  loss: 0.0153 (0.0153)  time: 2.5766  data: 0.3926  max mem: 34254\n",
      "Valid: [epoch:62]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1537  data: 0.0281  max mem: 34254\n",
      "Valid: [epoch:62] Total time: 0:00:30 (2.1660 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_62_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 62.000\n",
      "Train: [epoch:63]  [  0/127]  eta: 0:06:51  lr: 0.000100  loss: 0.0220 (0.0220)  time: 3.2418  data: 2.2715  max mem: 34254\n",
      "Train: [epoch:63]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0206 (0.0206)  time: 1.1943  data: 0.2066  max mem: 34254\n",
      "Train: [epoch:63]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0202 (0.0203)  time: 0.9895  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0196 (0.0203)  time: 0.9903  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0199 (0.0202)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0199 (0.0202)  time: 0.9935  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:63]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0194 (0.0201)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0197 (0.0201)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0197 (0.0200)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0197 (0.0200)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0198 (0.0200)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0204 (0.0201)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0205 (0.0201)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0202 (0.0201)  time: 0.9983  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:63] Total time: 0:02:08 (1.0137 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0202 (0.0201)\n",
      "Valid: [epoch:63]  [ 0/14]  eta: 0:00:35  loss: 0.0153 (0.0153)  time: 2.5535  data: 0.4084  max mem: 34254\n",
      "Valid: [epoch:63]  [13/14]  eta: 0:00:02  loss: 0.0149 (0.0149)  time: 2.1549  data: 0.0293  max mem: 34254\n",
      "Valid: [epoch:63] Total time: 0:00:30 (2.1649 s / it)\n",
      "Averaged stats: loss: 0.0149 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_63_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 62.000\n",
      "Train: [epoch:64]  [  0/127]  eta: 0:07:18  lr: 0.000100  loss: 0.0226 (0.0226)  time: 3.4564  data: 2.4847  max mem: 34254\n",
      "Train: [epoch:64]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0202 (0.0205)  time: 1.2183  data: 0.2260  max mem: 34254\n",
      "Train: [epoch:64]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0199 (0.0203)  time: 0.9924  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0195 (0.0202)  time: 0.9902  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0198 (0.0202)  time: 0.9918  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0199 (0.0200)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0196 (0.0202)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0202 (0.0202)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0196 (0.0201)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0194 (0.0200)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0197 (0.0200)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0196 (0.0200)  time: 0.9979  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0196 (0.0200)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0196 (0.0200)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:64] Total time: 0:02:08 (1.0155 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0196 (0.0200)\n",
      "Valid: [epoch:64]  [ 0/14]  eta: 0:00:40  loss: 0.0153 (0.0153)  time: 2.8747  data: 0.5169  max mem: 34254\n",
      "Valid: [epoch:64]  [13/14]  eta: 0:00:02  loss: 0.0148 (0.0148)  time: 2.3424  data: 0.0370  max mem: 34254\n",
      "Valid: [epoch:64] Total time: 0:00:32 (2.3541 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_64_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 62.000\n",
      "Train: [epoch:65]  [  0/127]  eta: 0:07:02  lr: 0.000100  loss: 0.0191 (0.0191)  time: 3.3302  data: 2.3535  max mem: 34254\n",
      "Train: [epoch:65]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0205 (0.0204)  time: 1.2021  data: 0.2141  max mem: 34254\n",
      "Train: [epoch:65]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0200 (0.0202)  time: 0.9896  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0194 (0.0199)  time: 0.9907  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0193 (0.0198)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0198 (0.0198)  time: 0.9917  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0199 (0.0199)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0202 (0.0199)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0200 (0.0199)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0193 (0.0199)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0198 (0.0199)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0201 (0.0199)  time: 0.9991  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0198 (0.0199)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0198 (0.0199)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:65] Total time: 0:02:08 (1.0135 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0198 (0.0199)\n",
      "Valid: [epoch:65]  [ 0/14]  eta: 0:00:35  loss: 0.0151 (0.0151)  time: 2.5409  data: 0.3779  max mem: 34254\n",
      "Valid: [epoch:65]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1477  data: 0.0271  max mem: 34254\n",
      "Valid: [epoch:65] Total time: 0:00:30 (2.1566 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_65_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:66]  [  0/127]  eta: 0:06:08  lr: 0.000100  loss: 0.0226 (0.0226)  time: 2.9051  data: 1.9258  max mem: 34254\n",
      "Train: [epoch:66]  [ 10/127]  eta: 0:02:16  lr: 0.000100  loss: 0.0201 (0.0205)  time: 1.1640  data: 0.1752  max mem: 34254\n",
      "Train: [epoch:66]  [ 20/127]  eta: 0:01:55  lr: 0.000100  loss: 0.0199 (0.0200)  time: 0.9904  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0193 (0.0199)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0194 (0.0199)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0199 (0.0200)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0200 (0.0199)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0194 (0.0198)  time: 0.9926  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0194 (0.0199)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0194 (0.0198)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0204 (0.0199)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0206 (0.0199)  time: 0.9979  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0194 (0.0199)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0199 (0.0199)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:66] Total time: 0:02:08 (1.0113 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0199 (0.0199)\n",
      "Valid: [epoch:66]  [ 0/14]  eta: 0:00:36  loss: 0.0137 (0.0137)  time: 2.5916  data: 0.3821  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:66]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1934  data: 0.0274  max mem: 34254\n",
      "Valid: [epoch:66] Total time: 0:00:30 (2.2065 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_66_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:67]  [  0/127]  eta: 0:07:08  lr: 0.000100  loss: 0.0190 (0.0190)  time: 3.3757  data: 2.4036  max mem: 34254\n",
      "Train: [epoch:67]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0202 (0.0201)  time: 1.2144  data: 0.2186  max mem: 34254\n",
      "Train: [epoch:67]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0202 (0.0201)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0196 (0.0200)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0193 (0.0198)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0189 (0.0197)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0199 (0.0198)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0202 (0.0198)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0198 (0.0198)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0193 (0.0197)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0188 (0.0196)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0187 (0.0196)  time: 1.0016  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0201 (0.0197)  time: 1.0029  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0201 (0.0197)  time: 1.0033  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:67] Total time: 0:02:09 (1.0163 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0201 (0.0197)\n",
      "Valid: [epoch:67]  [ 0/14]  eta: 0:00:37  loss: 0.0139 (0.0139)  time: 2.6631  data: 0.4418  max mem: 34254\n",
      "Valid: [epoch:67]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2109  data: 0.0316  max mem: 34254\n",
      "Valid: [epoch:67] Total time: 0:00:31 (2.2227 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_67_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:68]  [  0/127]  eta: 0:07:06  lr: 0.000100  loss: 0.0193 (0.0193)  time: 3.3575  data: 2.3835  max mem: 34254\n",
      "Train: [epoch:68]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0195 (0.0197)  time: 1.2033  data: 0.2168  max mem: 34254\n",
      "Train: [epoch:68]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0196 (0.0195)  time: 0.9884  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0198 (0.0197)  time: 0.9897  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0198 (0.0196)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0199 (0.0196)  time: 0.9921  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0195 (0.0195)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0201 (0.0196)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0196 (0.0195)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0193 (0.0195)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0195 (0.0196)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0195 (0.0196)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0195 (0.0196)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:68] Total time: 0:02:08 (1.0130 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0195 (0.0196)\n",
      "Valid: [epoch:68]  [ 0/14]  eta: 0:00:37  loss: 0.0142 (0.0142)  time: 2.6670  data: 0.4489  max mem: 34254\n",
      "Valid: [epoch:68]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2149  data: 0.0321  max mem: 34254\n",
      "Valid: [epoch:68] Total time: 0:00:31 (2.2254 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_68_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:69]  [  0/127]  eta: 0:07:08  lr: 0.000100  loss: 0.0207 (0.0207)  time: 3.3721  data: 2.3979  max mem: 34254\n",
      "Train: [epoch:69]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0197 (0.0197)  time: 1.2071  data: 0.2181  max mem: 34254\n",
      "Train: [epoch:69]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0192 (0.0197)  time: 0.9908  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0191 (0.0195)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0191 (0.0195)  time: 0.9915  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0194 (0.0195)  time: 0.9917  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0195 (0.0195)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0195 (0.0195)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0196 (0.0195)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0196 (0.0195)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0193 (0.0195)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0195 (0.0195)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:69] Total time: 0:02:08 (1.0132 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0195 (0.0195)\n",
      "Valid: [epoch:69]  [ 0/14]  eta: 0:00:37  loss: 0.0151 (0.0151)  time: 2.6518  data: 0.4719  max mem: 34254\n",
      "Valid: [epoch:69]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.1750  data: 0.0338  max mem: 34254\n",
      "Valid: [epoch:69] Total time: 0:00:30 (2.1850 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_69_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:70]  [  0/127]  eta: 0:06:37  lr: 0.000100  loss: 0.0189 (0.0189)  time: 3.1304  data: 2.1594  max mem: 34254\n",
      "Train: [epoch:70]  [ 10/127]  eta: 0:02:18  lr: 0.000100  loss: 0.0197 (0.0195)  time: 1.1838  data: 0.1964  max mem: 34254\n",
      "Train: [epoch:70]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0197 (0.0197)  time: 0.9898  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0194 (0.0196)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0194 (0.0197)  time: 0.9926  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0194 (0.0196)  time: 0.9915  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0193 (0.0196)  time: 0.9922  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:70]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0191 (0.0195)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0189 (0.0195)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0194 (0.0195)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0193 (0.0195)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:70] Total time: 0:02:08 (1.0108 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0193 (0.0195)\n",
      "Valid: [epoch:70]  [ 0/14]  eta: 0:00:36  loss: 0.0131 (0.0131)  time: 2.6164  data: 0.3944  max mem: 34254\n",
      "Valid: [epoch:70]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2075  data: 0.0283  max mem: 34254\n",
      "Valid: [epoch:70] Total time: 0:00:31 (2.2191 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_70_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:71]  [  0/127]  eta: 0:06:44  lr: 0.000100  loss: 0.0211 (0.0211)  time: 3.1837  data: 2.2090  max mem: 34254\n",
      "Train: [epoch:71]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0200 (0.0200)  time: 1.1965  data: 0.2009  max mem: 34254\n",
      "Train: [epoch:71]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0191 (0.0195)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0190 (0.0195)  time: 0.9901  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9904  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0194 (0.0195)  time: 0.9909  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0194 (0.0194)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0194 (0.0194)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:71] Total time: 0:02:08 (1.0118 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0194 (0.0194)\n",
      "Valid: [epoch:71]  [ 0/14]  eta: 0:00:37  loss: 0.0153 (0.0153)  time: 2.6515  data: 0.4334  max mem: 34254\n",
      "Valid: [epoch:71]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2156  data: 0.0311  max mem: 34254\n",
      "Valid: [epoch:71] Total time: 0:00:31 (2.2295 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_71_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:72]  [  0/127]  eta: 0:06:48  lr: 0.000100  loss: 0.0205 (0.0205)  time: 3.2197  data: 2.2474  max mem: 34254\n",
      "Train: [epoch:72]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0197 (0.0202)  time: 1.1932  data: 0.2044  max mem: 34254\n",
      "Train: [epoch:72]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0197 (0.0201)  time: 0.9901  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0196 (0.0198)  time: 0.9897  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0191 (0.0197)  time: 0.9907  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0190 (0.0196)  time: 0.9915  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0192 (0.0196)  time: 0.9917  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0193 (0.0196)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0189 (0.0195)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0189 (0.0194)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0189 (0.0194)  time: 0.9983  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0186 (0.0193)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:72] Total time: 0:02:08 (1.0120 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0186 (0.0193)\n",
      "Valid: [epoch:72]  [ 0/14]  eta: 0:00:37  loss: 0.0139 (0.0139)  time: 2.6442  data: 0.4302  max mem: 34254\n",
      "Valid: [epoch:72]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2375  data: 0.0308  max mem: 34254\n",
      "Valid: [epoch:72] Total time: 0:00:31 (2.2483 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_72_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:73]  [  0/127]  eta: 0:07:33  lr: 0.000100  loss: 0.0198 (0.0198)  time: 3.5704  data: 2.5972  max mem: 34254\n",
      "Train: [epoch:73]  [ 10/127]  eta: 0:02:23  lr: 0.000100  loss: 0.0196 (0.0195)  time: 1.2250  data: 0.2362  max mem: 34254\n",
      "Train: [epoch:73]  [ 20/127]  eta: 0:01:59  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9912  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0192 (0.0192)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0189 (0.0194)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0196 (0.0193)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0195 (0.0194)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:73] Total time: 0:02:09 (1.0160 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0191 (0.0193)\n",
      "Valid: [epoch:73]  [ 0/14]  eta: 0:00:37  loss: 0.0136 (0.0136)  time: 2.7122  data: 0.4396  max mem: 34254\n",
      "Valid: [epoch:73]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2703  data: 0.0315  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:73] Total time: 0:00:31 (2.2817 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_73_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:74]  [  0/127]  eta: 0:07:43  lr: 0.000100  loss: 0.0184 (0.0184)  time: 3.6514  data: 2.6775  max mem: 34254\n",
      "Train: [epoch:74]  [ 10/127]  eta: 0:02:24  lr: 0.000100  loss: 0.0199 (0.0197)  time: 1.2341  data: 0.2435  max mem: 34254\n",
      "Train: [epoch:74]  [ 20/127]  eta: 0:01:59  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0197 (0.0194)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0197 (0.0194)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0196 (0.0194)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0186 (0.0193)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0193 (0.0193)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:74] Total time: 0:02:09 (1.0164 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0193 (0.0193)\n",
      "Valid: [epoch:74]  [ 0/14]  eta: 0:00:37  loss: 0.0145 (0.0145)  time: 2.7137  data: 0.4615  max mem: 34254\n",
      "Valid: [epoch:74]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.2629  data: 0.0331  max mem: 34254\n",
      "Valid: [epoch:74] Total time: 0:00:31 (2.2747 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_74_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:75]  [  0/127]  eta: 0:07:18  lr: 0.000100  loss: 0.0192 (0.0192)  time: 3.4491  data: 2.4756  max mem: 34254\n",
      "Train: [epoch:75]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0204 (0.0200)  time: 1.2141  data: 0.2252  max mem: 34254\n",
      "Train: [epoch:75]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0195 (0.0194)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0191 (0.0195)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9921  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0193 (0.0193)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0199 (0.0194)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0199 (0.0195)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0187 (0.0194)  time: 0.9958  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:75]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0186 (0.0194)  time: 0.9951  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:75]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:75] Total time: 0:02:08 (1.0146 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0187 (0.0193)\n",
      "Valid: [epoch:75]  [ 0/14]  eta: 0:00:38  loss: 0.0137 (0.0137)  time: 2.7360  data: 0.4427  max mem: 34254\n",
      "Valid: [epoch:75]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.3011  data: 0.0317  max mem: 34254\n",
      "Valid: [epoch:75] Total time: 0:00:32 (2.3131 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_75_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:76]  [  0/127]  eta: 0:06:13  lr: 0.000100  loss: 0.0192 (0.0192)  time: 2.9415  data: 1.9705  max mem: 34254\n",
      "Train: [epoch:76]  [ 10/127]  eta: 0:02:16  lr: 0.000100  loss: 0.0187 (0.0189)  time: 1.1694  data: 0.1793  max mem: 34254\n",
      "Train: [epoch:76]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0188 (0.0191)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0192 (0.0192)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0186 (0.0191)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0193 (0.0193)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0199 (0.0193)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0194 (0.0193)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0194 (0.0193)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:76] Total time: 0:02:08 (1.0128 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0194 (0.0193)\n",
      "Valid: [epoch:76]  [ 0/14]  eta: 0:00:38  loss: 0.0153 (0.0153)  time: 2.7548  data: 0.4381  max mem: 34254\n",
      "Valid: [epoch:76]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2793  data: 0.0314  max mem: 34254\n",
      "Valid: [epoch:76] Total time: 0:00:32 (2.2977 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_76_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:77]  [  0/127]  eta: 0:07:10  lr: 0.000100  loss: 0.0177 (0.0177)  time: 3.3915  data: 2.2782  max mem: 34254\n",
      "Train: [epoch:77]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0195 (0.0195)  time: 1.2109  data: 0.2072  max mem: 34254\n",
      "Train: [epoch:77]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0196 (0.0196)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0193 (0.0193)  time: 0.9924  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0193 (0.0195)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0202 (0.0195)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9970  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:77]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0195 (0.0195)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0191 (0.0194)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0195 (0.0194)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:77] Total time: 0:02:08 (1.0153 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0190 (0.0193)\n",
      "Valid: [epoch:77]  [ 0/14]  eta: 0:00:36  loss: 0.0150 (0.0150)  time: 2.5727  data: 0.4226  max mem: 34254\n",
      "Valid: [epoch:77]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1604  data: 0.0303  max mem: 34254\n",
      "Valid: [epoch:77] Total time: 0:00:30 (2.1688 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_77_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:78]  [  0/127]  eta: 0:07:35  lr: 0.000100  loss: 0.0198 (0.0198)  time: 3.5833  data: 2.5038  max mem: 34254\n",
      "Train: [epoch:78]  [ 10/127]  eta: 0:02:23  lr: 0.000100  loss: 0.0194 (0.0193)  time: 1.2278  data: 0.2277  max mem: 34254\n",
      "Train: [epoch:78]  [ 20/127]  eta: 0:01:59  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9916  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0196 (0.0196)  time: 0.9916  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0196 (0.0194)  time: 0.9932  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0185 (0.0193)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0186 (0.0192)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:78] Total time: 0:02:09 (1.0163 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0190 (0.0192)\n",
      "Valid: [epoch:78]  [ 0/14]  eta: 0:00:37  loss: 0.0136 (0.0136)  time: 2.6981  data: 0.4353  max mem: 34254\n",
      "Valid: [epoch:78]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2231  data: 0.0312  max mem: 34254\n",
      "Valid: [epoch:78] Total time: 0:00:31 (2.2349 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_78_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:79]  [  0/127]  eta: 0:07:31  lr: 0.000100  loss: 0.0203 (0.0203)  time: 3.5534  data: 2.5726  max mem: 34254\n",
      "Train: [epoch:79]  [ 10/127]  eta: 0:02:23  lr: 0.000100  loss: 0.0190 (0.0193)  time: 1.2228  data: 0.2340  max mem: 34254\n",
      "Train: [epoch:79]  [ 20/127]  eta: 0:01:59  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0193 (0.0195)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0194 (0.0194)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0194 (0.0195)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0196 (0.0194)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0186 (0.0193)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0186 (0.0193)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:79] Total time: 0:02:08 (1.0156 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0192 (0.0193)\n",
      "Valid: [epoch:79]  [ 0/14]  eta: 0:00:35  loss: 0.0151 (0.0151)  time: 2.5392  data: 0.3878  max mem: 34254\n",
      "Valid: [epoch:79]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1522  data: 0.0278  max mem: 34254\n",
      "Valid: [epoch:79] Total time: 0:00:30 (2.1613 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_79_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:80]  [  0/127]  eta: 0:07:12  lr: 0.000100  loss: 0.0186 (0.0186)  time: 3.4088  data: 2.4347  max mem: 34254\n",
      "Train: [epoch:80]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0200 (0.0200)  time: 1.2139  data: 0.2214  max mem: 34254\n",
      "Train: [epoch:80]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0198 (0.0198)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0193 (0.0196)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0191 (0.0194)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0193 (0.0193)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0195 (0.0193)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:80] Total time: 0:02:08 (1.0153 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0189 (0.0193)\n",
      "Valid: [epoch:80]  [ 0/14]  eta: 0:00:37  loss: 0.0139 (0.0139)  time: 2.6925  data: 0.4322  max mem: 34254\n",
      "Valid: [epoch:80]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2249  data: 0.0310  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:80] Total time: 0:00:31 (2.2350 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_80_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:81]  [  0/127]  eta: 0:05:29  lr: 0.000100  loss: 0.0205 (0.0205)  time: 2.5919  data: 1.6112  max mem: 34254\n",
      "Train: [epoch:81]  [ 10/127]  eta: 0:02:13  lr: 0.000100  loss: 0.0192 (0.0192)  time: 1.1438  data: 0.1466  max mem: 34254\n",
      "Train: [epoch:81]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0200 (0.0196)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [ 40/127]  eta: 0:01:29  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [ 50/127]  eta: 0:01:18  lr: 0.000100  loss: 0.0191 (0.0194)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0196 (0.0194)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [ 70/127]  eta: 0:00:57  lr: 0.000100  loss: 0.0195 (0.0194)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9992  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9996  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:81]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0191 (0.0194)  time: 0.9995  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:81] Total time: 0:02:08 (1.0093 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0191 (0.0194)\n",
      "Valid: [epoch:81]  [ 0/14]  eta: 0:00:34  loss: 0.0140 (0.0140)  time: 2.4891  data: 0.3858  max mem: 34254\n",
      "Valid: [epoch:81]  [13/14]  eta: 0:00:02  loss: 0.0148 (0.0148)  time: 2.1506  data: 0.0276  max mem: 34254\n",
      "Valid: [epoch:81] Total time: 0:00:30 (2.1597 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_81_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:82]  [  0/127]  eta: 0:06:25  lr: 0.000100  loss: 0.0191 (0.0191)  time: 3.0351  data: 2.0645  max mem: 34254\n",
      "Train: [epoch:82]  [ 10/127]  eta: 0:02:17  lr: 0.000100  loss: 0.0191 (0.0192)  time: 1.1787  data: 0.1878  max mem: 34254\n",
      "Train: [epoch:82]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0195 (0.0194)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9921  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0191 (0.0194)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0190 (0.0191)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0190 (0.0191)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0192 (0.0191)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0190 (0.0191)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0197 (0.0192)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:82] Total time: 0:02:08 (1.0113 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0197 (0.0192)\n",
      "Valid: [epoch:82]  [ 0/14]  eta: 0:00:36  loss: 0.0150 (0.0150)  time: 2.6118  data: 0.4368  max mem: 34254\n",
      "Valid: [epoch:82]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1767  data: 0.0313  max mem: 34254\n",
      "Valid: [epoch:82] Total time: 0:00:30 (2.1883 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_82_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:83]  [  0/127]  eta: 0:06:18  lr: 0.000100  loss: 0.0191 (0.0191)  time: 2.9769  data: 1.9998  max mem: 34254\n",
      "Train: [epoch:83]  [ 10/127]  eta: 0:02:17  lr: 0.000100  loss: 0.0201 (0.0199)  time: 1.1755  data: 0.1819  max mem: 34254\n",
      "Train: [epoch:83]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0193 (0.0196)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0191 (0.0196)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0190 (0.0195)  time: 1.0024  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0187 (0.0193)  time: 1.0005  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0184 (0.0193)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0194 (0.0193)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0191 (0.0193)  time: 1.0008  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:83] Total time: 0:02:08 (1.0133 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0193 (0.0192)\n",
      "Valid: [epoch:83]  [ 0/14]  eta: 0:00:37  loss: 0.0149 (0.0149)  time: 2.6860  data: 0.4454  max mem: 34254\n",
      "Valid: [epoch:83]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2261  data: 0.0319  max mem: 34254\n",
      "Valid: [epoch:83] Total time: 0:00:31 (2.2369 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_83_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:84]  [  0/127]  eta: 0:06:47  lr: 0.000100  loss: 0.0198 (0.0198)  time: 3.2089  data: 2.1923  max mem: 34254\n",
      "Train: [epoch:84]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0195 (0.0194)  time: 1.1945  data: 0.1994  max mem: 34254\n",
      "Train: [epoch:84]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0188 (0.0192)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9926  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0194 (0.0194)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0194 (0.0193)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0185 (0.0192)  time: 0.9955  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:84]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0184 (0.0191)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9997  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9999  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0196 (0.0193)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:84] Total time: 0:02:08 (1.0144 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0196 (0.0193)\n",
      "Valid: [epoch:84]  [ 0/14]  eta: 0:00:36  loss: 0.0144 (0.0144)  time: 2.5946  data: 0.4326  max mem: 34254\n",
      "Valid: [epoch:84]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1648  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:84] Total time: 0:00:30 (2.1750 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_84_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:85]  [  0/127]  eta: 0:07:02  lr: 0.000100  loss: 0.0197 (0.0197)  time: 3.3270  data: 2.3469  max mem: 34254\n",
      "Train: [epoch:85]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0193 (0.0199)  time: 1.2053  data: 0.2135  max mem: 34254\n",
      "Train: [epoch:85]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0197 (0.0199)  time: 0.9918  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0196 (0.0196)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0189 (0.0195)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0190 (0.0195)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0184 (0.0193)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0189 (0.0194)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0185 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0184 (0.0192)  time: 0.9961  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:85]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:85] Total time: 0:02:08 (1.0138 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0189 (0.0192)\n",
      "Valid: [epoch:85]  [ 0/14]  eta: 0:00:37  loss: 0.0150 (0.0150)  time: 2.6616  data: 0.4754  max mem: 34254\n",
      "Valid: [epoch:85]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1936  data: 0.0341  max mem: 34254\n",
      "Valid: [epoch:85] Total time: 0:00:30 (2.2043 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_85_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:86]  [  0/127]  eta: 0:07:22  lr: 0.000100  loss: 0.0229 (0.0229)  time: 3.4820  data: 2.4565  max mem: 34254\n",
      "Train: [epoch:86]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0193 (0.0194)  time: 1.2164  data: 0.2234  max mem: 34254\n",
      "Train: [epoch:86]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0189 (0.0191)  time: 0.9909  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0191 (0.0191)  time: 0.9921  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0196 (0.0193)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0193 (0.0193)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0183 (0.0192)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:86] Total time: 0:02:08 (1.0150 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0183 (0.0192)\n",
      "Valid: [epoch:86]  [ 0/14]  eta: 0:00:38  loss: 0.0147 (0.0147)  time: 2.7721  data: 0.5339  max mem: 34254\n",
      "Valid: [epoch:86]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.2335  data: 0.0383  max mem: 34254\n",
      "Valid: [epoch:86] Total time: 0:00:31 (2.2438 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_86_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:87]  [  0/127]  eta: 0:05:39  lr: 0.000100  loss: 0.0209 (0.0209)  time: 2.6759  data: 1.7013  max mem: 34254\n",
      "Train: [epoch:87]  [ 10/127]  eta: 0:02:14  lr: 0.000100  loss: 0.0187 (0.0192)  time: 1.1484  data: 0.1548  max mem: 34254\n",
      "Train: [epoch:87]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0187 (0.0190)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0197 (0.0193)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0195 (0.0192)  time: 1.0003  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0192 (0.0192)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:87] Total time: 0:02:08 (1.0104 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0192 (0.0192)\n",
      "Valid: [epoch:87]  [ 0/14]  eta: 0:00:36  loss: 0.0136 (0.0136)  time: 2.5771  data: 0.4046  max mem: 34254\n",
      "Valid: [epoch:87]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1907  data: 0.0290  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:87] Total time: 0:00:30 (2.1999 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_87_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:88]  [  0/127]  eta: 0:07:12  lr: 0.000100  loss: 0.0222 (0.0222)  time: 3.4092  data: 2.4356  max mem: 34254\n",
      "Train: [epoch:88]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0194 (0.0198)  time: 1.2110  data: 0.2215  max mem: 34254\n",
      "Train: [epoch:88]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0191 (0.0195)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0189 (0.0195)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0191 (0.0194)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0194 (0.0194)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9988  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9977  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:88]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0186 (0.0193)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0186 (0.0192)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:88] Total time: 0:02:09 (1.0165 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0191 (0.0193)\n",
      "Valid: [epoch:88]  [ 0/14]  eta: 0:00:37  loss: 0.0140 (0.0140)  time: 2.6898  data: 0.4428  max mem: 34254\n",
      "Valid: [epoch:88]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1741  data: 0.0317  max mem: 34254\n",
      "Valid: [epoch:88] Total time: 0:00:30 (2.1865 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_88_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:89]  [  0/127]  eta: 0:06:28  lr: 0.000100  loss: 0.0184 (0.0184)  time: 3.0599  data: 2.0819  max mem: 34254\n",
      "Train: [epoch:89]  [ 10/127]  eta: 0:02:17  lr: 0.000100  loss: 0.0193 (0.0195)  time: 1.1790  data: 0.1894  max mem: 34254\n",
      "Train: [epoch:89]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0191 (0.0194)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0188 (0.0191)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9934  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0195 (0.0193)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0184 (0.0191)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0182 (0.0191)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0191 (0.0191)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0184 (0.0192)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0188 (0.0192)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0189 (0.0192)  time: 1.0010  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0191 (0.0192)  time: 1.0013  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:89] Total time: 0:02:08 (1.0136 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0191 (0.0192)\n",
      "Valid: [epoch:89]  [ 0/14]  eta: 0:00:36  loss: 0.0142 (0.0142)  time: 2.5752  data: 0.3946  max mem: 34254\n",
      "Valid: [epoch:89]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1942  data: 0.0283  max mem: 34254\n",
      "Valid: [epoch:89] Total time: 0:00:30 (2.2031 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_89_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:90]  [  0/127]  eta: 0:07:19  lr: 0.000100  loss: 0.0182 (0.0182)  time: 3.4633  data: 2.4690  max mem: 34254\n",
      "Train: [epoch:90]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0190 (0.0194)  time: 1.2160  data: 0.2245  max mem: 34254\n",
      "Train: [epoch:90]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0196 (0.0195)  time: 0.9921  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0195 (0.0193)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0188 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0189 (0.0191)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0197 (0.0192)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:90] Total time: 0:02:09 (1.0159 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0189 (0.0192)\n",
      "Valid: [epoch:90]  [ 0/14]  eta: 0:00:35  loss: 0.0174 (0.0174)  time: 2.5208  data: 0.3869  max mem: 34254\n",
      "Valid: [epoch:90]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2278  data: 0.0277  max mem: 34254\n",
      "Valid: [epoch:90] Total time: 0:00:31 (2.2377 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_90_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:91]  [  0/127]  eta: 0:06:45  lr: 0.000100  loss: 0.0201 (0.0201)  time: 3.1932  data: 2.2196  max mem: 34254\n",
      "Train: [epoch:91]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0192 (0.0194)  time: 1.1905  data: 0.2019  max mem: 34254\n",
      "Train: [epoch:91]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0187 (0.0195)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0186 (0.0192)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9954  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:91]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0196 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:91] Total time: 0:02:08 (1.0134 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0191 (0.0192)\n",
      "Valid: [epoch:91]  [ 0/14]  eta: 0:00:40  loss: 0.0131 (0.0131)  time: 2.8735  data: 0.3622  max mem: 34254\n",
      "Valid: [epoch:91]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.5156  data: 0.0260  max mem: 34254\n",
      "Valid: [epoch:91] Total time: 0:00:35 (2.5252 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_91_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:92]  [  0/127]  eta: 0:06:51  lr: 0.000100  loss: 0.0164 (0.0164)  time: 3.2432  data: 2.2675  max mem: 34254\n",
      "Train: [epoch:92]  [ 10/127]  eta: 0:02:19  lr: 0.000100  loss: 0.0188 (0.0186)  time: 1.1958  data: 0.2062  max mem: 34254\n",
      "Train: [epoch:92]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0190 (0.0191)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0189 (0.0189)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0184 (0.0188)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0187 (0.0190)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0191 (0.0190)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0191 (0.0191)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0194 (0.0191)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0194 (0.0191)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0192 (0.0191)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0192 (0.0192)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0196 (0.0192)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:92] Total time: 0:02:08 (1.0142 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0196 (0.0192)\n",
      "Valid: [epoch:92]  [ 0/14]  eta: 0:00:38  loss: 0.0150 (0.0150)  time: 2.7343  data: 0.4851  max mem: 34254\n",
      "Valid: [epoch:92]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1904  data: 0.0348  max mem: 34254\n",
      "Valid: [epoch:92] Total time: 0:00:30 (2.2013 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_92_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:93]  [  0/127]  eta: 0:06:56  lr: 0.000100  loss: 0.0182 (0.0182)  time: 3.2788  data: 2.3021  max mem: 34254\n",
      "Train: [epoch:93]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0190 (0.0192)  time: 1.1981  data: 0.2094  max mem: 34254\n",
      "Train: [epoch:93]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0187 (0.0189)  time: 0.9921  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0185 (0.0189)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0193 (0.0190)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0193 (0.0191)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0188 (0.0191)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0182 (0.0191)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0188 (0.0191)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:93] Total time: 0:02:08 (1.0148 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0190 (0.0192)\n",
      "Valid: [epoch:93]  [ 0/14]  eta: 0:00:34  loss: 0.0132 (0.0132)  time: 2.4651  data: 0.3819  max mem: 34254\n",
      "Valid: [epoch:93]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1276  data: 0.0274  max mem: 34254\n",
      "Valid: [epoch:93] Total time: 0:00:29 (2.1390 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_93_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:94]  [  0/127]  eta: 0:06:58  lr: 0.000100  loss: 0.0194 (0.0194)  time: 3.2961  data: 2.3218  max mem: 34254\n",
      "Train: [epoch:94]  [ 10/127]  eta: 0:02:20  lr: 0.000100  loss: 0.0199 (0.0197)  time: 1.2007  data: 0.2112  max mem: 34254\n",
      "Train: [epoch:94]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0193 (0.0196)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0189 (0.0194)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0180 (0.0192)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0189 (0.0191)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:94] Total time: 0:02:08 (1.0151 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0189 (0.0192)\n",
      "Valid: [epoch:94]  [ 0/14]  eta: 0:00:35  loss: 0.0135 (0.0135)  time: 2.5296  data: 0.3757  max mem: 34254\n",
      "Valid: [epoch:94]  [13/14]  eta: 0:00:02  loss: 0.0150 (0.0150)  time: 2.3909  data: 0.0269  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:94] Total time: 0:00:33 (2.3999 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0150)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_94_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:95]  [  0/127]  eta: 0:07:07  lr: 0.000100  loss: 0.0214 (0.0214)  time: 3.3662  data: 2.3933  max mem: 34254\n",
      "Train: [epoch:95]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0199 (0.0199)  time: 1.2057  data: 0.2177  max mem: 34254\n",
      "Train: [epoch:95]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0193 (0.0195)  time: 0.9907  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0194 (0.0195)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0195 (0.0194)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0193 (0.0194)  time: 0.9975  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9979  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0186 (0.0193)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0188 (0.0192)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:95] Total time: 0:02:08 (1.0144 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0190 (0.0192)\n",
      "Valid: [epoch:95]  [ 0/14]  eta: 0:00:37  loss: 0.0132 (0.0132)  time: 2.6603  data: 0.4590  max mem: 34254\n",
      "Valid: [epoch:95]  [13/14]  eta: 0:00:02  loss: 0.0148 (0.0148)  time: 2.2821  data: 0.0329  max mem: 34254\n",
      "Valid: [epoch:95] Total time: 0:00:32 (2.2938 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_95_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:96]  [  0/127]  eta: 0:07:15  lr: 0.000100  loss: 0.0179 (0.0179)  time: 3.4317  data: 2.4590  max mem: 34254\n",
      "Train: [epoch:96]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0198 (0.0195)  time: 1.2115  data: 0.2237  max mem: 34254\n",
      "Train: [epoch:96]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9904  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0199 (0.0195)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0201 (0.0195)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0192 (0.0195)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0190 (0.0194)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0193 (0.0193)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0194 (0.0193)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9983  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:96] Total time: 0:02:09 (1.0159 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0192 (0.0193)\n",
      "Valid: [epoch:96]  [ 0/14]  eta: 0:00:40  loss: 0.0136 (0.0136)  time: 2.8969  data: 0.4322  max mem: 34254\n",
      "Valid: [epoch:96]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.4880  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:96] Total time: 0:00:34 (2.4984 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_96_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:97]  [  0/127]  eta: 0:05:27  lr: 0.000100  loss: 0.0172 (0.0172)  time: 2.5815  data: 1.6063  max mem: 34254\n",
      "Train: [epoch:97]  [ 10/127]  eta: 0:02:12  lr: 0.000100  loss: 0.0188 (0.0191)  time: 1.1340  data: 0.1461  max mem: 34254\n",
      "Train: [epoch:97]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0192 (0.0191)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0189 (0.0189)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0189 (0.0191)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0193 (0.0191)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0188 (0.0190)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0188 (0.0190)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0192 (0.0191)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0189 (0.0191)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0188 (0.0191)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0195 (0.0191)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0191 (0.0191)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:97] Total time: 0:02:08 (1.0097 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0191 (0.0192)\n",
      "Valid: [epoch:97]  [ 0/14]  eta: 0:00:36  loss: 0.0150 (0.0150)  time: 2.5986  data: 0.4073  max mem: 34254\n",
      "Valid: [epoch:97]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1671  data: 0.0292  max mem: 34254\n",
      "Valid: [epoch:97] Total time: 0:00:30 (2.1842 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_97_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:98]  [  0/127]  eta: 0:05:46  lr: 0.000100  loss: 0.0182 (0.0182)  time: 2.7252  data: 1.7435  max mem: 34254\n",
      "Train: [epoch:98]  [ 10/127]  eta: 0:02:14  lr: 0.000100  loss: 0.0185 (0.0185)  time: 1.1471  data: 0.1586  max mem: 34254\n",
      "Train: [epoch:98]  [ 20/127]  eta: 0:01:54  lr: 0.000100  loss: 0.0189 (0.0189)  time: 0.9911  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [ 30/127]  eta: 0:01:41  lr: 0.000100  loss: 0.0191 (0.0190)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0191 (0.0191)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0190 (0.0191)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0186 (0.0191)  time: 0.9982  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:98]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0191 (0.0191)  time: 0.9994  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0195 (0.0193)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0188 (0.0192)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0188 (0.0192)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0188 (0.0191)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0194 (0.0192)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:98] Total time: 0:02:08 (1.0116 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0194 (0.0192)\n",
      "Valid: [epoch:98]  [ 0/14]  eta: 0:00:37  loss: 0.0145 (0.0145)  time: 2.6434  data: 0.4576  max mem: 34254\n",
      "Valid: [epoch:98]  [13/14]  eta: 0:00:02  loss: 0.0148 (0.0148)  time: 2.1792  data: 0.0328  max mem: 34254\n",
      "Valid: [epoch:98] Total time: 0:00:30 (2.1904 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_98_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:99]  [  0/127]  eta: 0:07:17  lr: 0.000100  loss: 0.0201 (0.0201)  time: 3.4424  data: 2.4674  max mem: 34254\n",
      "Train: [epoch:99]  [ 10/127]  eta: 0:02:23  lr: 0.000100  loss: 0.0190 (0.0192)  time: 1.2272  data: 0.2244  max mem: 34254\n",
      "Train: [epoch:99]  [ 20/127]  eta: 0:01:59  lr: 0.000100  loss: 0.0187 (0.0188)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0189 (0.0191)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [ 40/127]  eta: 0:01:32  lr: 0.000100  loss: 0.0194 (0.0191)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0189 (0.0191)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0185 (0.0191)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0188 (0.0192)  time: 0.9988  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0192 (0.0191)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0192 (0.0191)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0197 (0.0192)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0197 (0.0192)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:99] Total time: 0:02:09 (1.0179 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0197 (0.0192)\n",
      "Valid: [epoch:99]  [ 0/14]  eta: 0:00:37  loss: 0.0174 (0.0174)  time: 2.6568  data: 0.4647  max mem: 34254\n",
      "Valid: [epoch:99]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1938  data: 0.0333  max mem: 34254\n",
      "Valid: [epoch:99] Total time: 0:00:30 (2.2049 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_99_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:100]  [  0/127]  eta: 0:07:20  lr: 0.000100  loss: 0.0191 (0.0191)  time: 3.4683  data: 2.4900  max mem: 34254\n",
      "Train: [epoch:100]  [ 10/127]  eta: 0:02:22  lr: 0.000100  loss: 0.0191 (0.0192)  time: 1.2148  data: 0.2265  max mem: 34254\n",
      "Train: [epoch:100]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9908  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0192 (0.0193)  time: 1.0007  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0190 (0.0193)  time: 1.0200  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [ 70/127]  eta: 0:00:59  lr: 0.000100  loss: 0.0190 (0.0192)  time: 1.0176  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [ 90/127]  eta: 0:00:38  lr: 0.000100  loss: 0.0196 (0.0193)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0184 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0187 (0.0192)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0188 (0.0192)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:100] Total time: 0:02:09 (1.0205 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0188 (0.0192)\n",
      "Valid: [epoch:100]  [ 0/14]  eta: 0:00:37  loss: 0.0152 (0.0152)  time: 2.6431  data: 0.4289  max mem: 34254\n",
      "Valid: [epoch:100]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1956  data: 0.0308  max mem: 34254\n",
      "Valid: [epoch:100] Total time: 0:00:30 (2.2071 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_100_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:101]  [  0/127]  eta: 0:06:31  lr: 0.000100  loss: 0.0198 (0.0198)  time: 3.0790  data: 2.1030  max mem: 34254\n",
      "Train: [epoch:101]  [ 10/127]  eta: 0:02:18  lr: 0.000100  loss: 0.0198 (0.0197)  time: 1.1796  data: 0.1913  max mem: 34254\n",
      "Train: [epoch:101]  [ 20/127]  eta: 0:01:56  lr: 0.000100  loss: 0.0194 (0.0196)  time: 0.9909  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0187 (0.0193)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0192 (0.0192)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [ 60/127]  eta: 0:01:08  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [ 80/127]  eta: 0:00:47  lr: 0.000100  loss: 0.0185 (0.0193)  time: 0.9979  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9993  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9983  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0193 (0.0193)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0193 (0.0192)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:101] Total time: 0:02:08 (1.0136 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0193 (0.0192)\n",
      "Valid: [epoch:101]  [ 0/14]  eta: 0:00:37  loss: 0.0147 (0.0147)  time: 2.6641  data: 0.4340  max mem: 34254\n",
      "Valid: [epoch:101]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.4972  data: 0.0311  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:101] Total time: 0:00:35 (2.5079 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_101_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:102]  [  0/127]  eta: 0:07:03  lr: 0.000100  loss: 0.0205 (0.0205)  time: 3.3358  data: 2.3608  max mem: 34254\n",
      "Train: [epoch:102]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0194 (0.0194)  time: 1.2082  data: 0.2147  max mem: 34254\n",
      "Train: [epoch:102]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0194 (0.0196)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0186 (0.0192)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0185 (0.0191)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0191 (0.0191)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0191 (0.0191)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0193 (0.0191)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0190 (0.0192)  time: 1.0021  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0193 (0.0192)  time: 1.0035  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0190 (0.0192)  time: 0.9998  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9987  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:102] Total time: 0:02:09 (1.0171 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0191 (0.0192)\n",
      "Valid: [epoch:102]  [ 0/14]  eta: 0:00:36  loss: 0.0174 (0.0174)  time: 2.6398  data: 0.4391  max mem: 34254\n",
      "Valid: [epoch:102]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2168  data: 0.0315  max mem: 34254\n",
      "Valid: [epoch:102] Total time: 0:00:31 (2.2300 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_102_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:103]  [  0/127]  eta: 0:07:40  lr: 0.000100  loss: 0.0211 (0.0211)  time: 3.6221  data: 2.6492  max mem: 34254\n",
      "Train: [epoch:103]  [ 10/127]  eta: 0:02:23  lr: 0.000100  loss: 0.0197 (0.0195)  time: 1.2278  data: 0.2409  max mem: 34254\n",
      "Train: [epoch:103]  [ 20/127]  eta: 0:01:59  lr: 0.000100  loss: 0.0193 (0.0198)  time: 0.9895  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [ 30/127]  eta: 0:01:44  lr: 0.000100  loss: 0.0194 (0.0198)  time: 0.9916  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0195 (0.0197)  time: 0.9939  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:103]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0197 (0.0197)  time: 0.9958  data: 0.0002  max mem: 34254\n",
      "Train: [epoch:103]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0193 (0.0196)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0183 (0.0194)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0183 (0.0194)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9993  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0189 (0.0192)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:103] Total time: 0:02:09 (1.0179 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0189 (0.0192)\n",
      "Valid: [epoch:103]  [ 0/14]  eta: 0:00:37  loss: 0.0175 (0.0175)  time: 2.6764  data: 0.4467  max mem: 34254\n",
      "Valid: [epoch:103]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2247  data: 0.0321  max mem: 34254\n",
      "Valid: [epoch:103] Total time: 0:00:31 (2.2363 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_103_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:104]  [  0/127]  eta: 0:07:12  lr: 0.000100  loss: 0.0186 (0.0186)  time: 3.4031  data: 2.4290  max mem: 34254\n",
      "Train: [epoch:104]  [ 10/127]  eta: 0:02:21  lr: 0.000100  loss: 0.0191 (0.0194)  time: 1.2083  data: 0.2209  max mem: 34254\n",
      "Train: [epoch:104]  [ 20/127]  eta: 0:01:58  lr: 0.000100  loss: 0.0191 (0.0193)  time: 0.9892  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [ 30/127]  eta: 0:01:43  lr: 0.000100  loss: 0.0186 (0.0190)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [ 40/127]  eta: 0:01:31  lr: 0.000100  loss: 0.0190 (0.0190)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [ 50/127]  eta: 0:01:20  lr: 0.000100  loss: 0.0193 (0.0191)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0195 (0.0192)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0191 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0189 (0.0193)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0192 (0.0193)  time: 0.9994  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0190 (0.0193)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0189 (0.0193)  time: 1.0018  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0188 (0.0192)  time: 1.0019  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:104] Total time: 0:02:09 (1.0165 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0188 (0.0192)\n",
      "Valid: [epoch:104]  [ 0/14]  eta: 0:00:36  loss: 0.0139 (0.0139)  time: 2.5766  data: 0.3892  max mem: 34254\n",
      "Valid: [epoch:104]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1858  data: 0.0279  max mem: 34254\n",
      "Valid: [epoch:104] Total time: 0:00:30 (2.1961 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_104_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:105]  [  0/127]  eta: 0:06:40  lr: 0.000100  loss: 0.0188 (0.0188)  time: 3.1549  data: 2.1800  max mem: 34254\n",
      "Train: [epoch:105]  [ 10/127]  eta: 0:02:18  lr: 0.000100  loss: 0.0194 (0.0193)  time: 1.1873  data: 0.1983  max mem: 34254\n",
      "Train: [epoch:105]  [ 20/127]  eta: 0:01:57  lr: 0.000100  loss: 0.0194 (0.0194)  time: 0.9905  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [ 30/127]  eta: 0:01:42  lr: 0.000100  loss: 0.0192 (0.0194)  time: 0.9912  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [ 40/127]  eta: 0:01:30  lr: 0.000100  loss: 0.0188 (0.0193)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [ 50/127]  eta: 0:01:19  lr: 0.000100  loss: 0.0186 (0.0192)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [ 60/127]  eta: 0:01:09  lr: 0.000100  loss: 0.0186 (0.0191)  time: 1.0014  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:105]  [ 70/127]  eta: 0:00:58  lr: 0.000100  loss: 0.0184 (0.0190)  time: 1.0042  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [ 80/127]  eta: 0:00:48  lr: 0.000100  loss: 0.0187 (0.0191)  time: 0.9988  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [ 90/127]  eta: 0:00:37  lr: 0.000100  loss: 0.0190 (0.0191)  time: 1.0010  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [100/127]  eta: 0:00:27  lr: 0.000100  loss: 0.0189 (0.0191)  time: 1.0015  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [110/127]  eta: 0:00:17  lr: 0.000100  loss: 0.0185 (0.0192)  time: 0.9991  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [120/127]  eta: 0:00:07  lr: 0.000100  loss: 0.0196 (0.0192)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105]  [126/127]  eta: 0:00:01  lr: 0.000100  loss: 0.0198 (0.0192)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:105] Total time: 0:02:08 (1.0156 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0198 (0.0192)\n",
      "Valid: [epoch:105]  [ 0/14]  eta: 0:00:37  loss: 0.0144 (0.0144)  time: 2.6513  data: 0.4337  max mem: 34254\n",
      "Valid: [epoch:105]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.3071  data: 0.0311  max mem: 34254\n",
      "Valid: [epoch:105] Total time: 0:00:32 (2.3190 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_105_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:106]  [  0/127]  eta: 0:06:29  lr: 0.000099  loss: 0.0193 (0.0193)  time: 3.0681  data: 2.0790  max mem: 34254\n",
      "Train: [epoch:106]  [ 10/127]  eta: 0:02:18  lr: 0.000099  loss: 0.0190 (0.0188)  time: 1.1846  data: 0.1891  max mem: 34254\n",
      "Train: [epoch:106]  [ 20/127]  eta: 0:01:56  lr: 0.000099  loss: 0.0191 (0.0193)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [ 30/127]  eta: 0:01:42  lr: 0.000099  loss: 0.0196 (0.0192)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [ 40/127]  eta: 0:01:30  lr: 0.000099  loss: 0.0190 (0.0193)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [ 50/127]  eta: 0:01:19  lr: 0.000099  loss: 0.0188 (0.0191)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [ 60/127]  eta: 0:01:08  lr: 0.000099  loss: 0.0187 (0.0191)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0194 (0.0192)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [ 80/127]  eta: 0:00:47  lr: 0.000099  loss: 0.0194 (0.0192)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0190 (0.0191)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0188 (0.0191)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0191 (0.0191)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0197 (0.0192)  time: 0.9991  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0190 (0.0192)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:106] Total time: 0:02:08 (1.0143 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0190 (0.0192)\n",
      "Valid: [epoch:106]  [ 0/14]  eta: 0:00:37  loss: 0.0150 (0.0150)  time: 2.6906  data: 0.4351  max mem: 34254\n",
      "Valid: [epoch:106]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.3874  data: 0.0312  max mem: 34254\n",
      "Valid: [epoch:106] Total time: 0:00:33 (2.4003 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_106_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:107]  [  0/127]  eta: 0:07:26  lr: 0.000099  loss: 0.0217 (0.0217)  time: 3.5136  data: 2.4486  max mem: 34254\n",
      "Train: [epoch:107]  [ 10/127]  eta: 0:02:22  lr: 0.000099  loss: 0.0196 (0.0196)  time: 1.2163  data: 0.2227  max mem: 34254\n",
      "Train: [epoch:107]  [ 20/127]  eta: 0:01:58  lr: 0.000099  loss: 0.0194 (0.0195)  time: 0.9878  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [ 30/127]  eta: 0:01:43  lr: 0.000099  loss: 0.0189 (0.0193)  time: 0.9908  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [ 40/127]  eta: 0:01:31  lr: 0.000099  loss: 0.0188 (0.0191)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [ 50/127]  eta: 0:01:20  lr: 0.000099  loss: 0.0193 (0.0192)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [ 60/127]  eta: 0:01:09  lr: 0.000099  loss: 0.0194 (0.0192)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0194 (0.0193)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [ 80/127]  eta: 0:00:48  lr: 0.000099  loss: 0.0190 (0.0193)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0185 (0.0192)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0184 (0.0192)  time: 1.0012  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0187 (0.0192)  time: 1.0008  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0187 (0.0192)  time: 0.9990  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0188 (0.0192)  time: 0.9996  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:107] Total time: 0:02:09 (1.0168 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0188 (0.0192)\n",
      "Valid: [epoch:107]  [ 0/14]  eta: 0:00:37  loss: 0.0174 (0.0174)  time: 2.6613  data: 0.4189  max mem: 34254\n",
      "Valid: [epoch:107]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2567  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:107] Total time: 0:00:31 (2.2703 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_107_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:108]  [  0/127]  eta: 0:07:15  lr: 0.000099  loss: 0.0171 (0.0171)  time: 3.4271  data: 2.4406  max mem: 34254\n",
      "Train: [epoch:108]  [ 10/127]  eta: 0:02:21  lr: 0.000099  loss: 0.0190 (0.0192)  time: 1.2118  data: 0.2220  max mem: 34254\n",
      "Train: [epoch:108]  [ 20/127]  eta: 0:01:58  lr: 0.000099  loss: 0.0190 (0.0191)  time: 0.9910  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [ 30/127]  eta: 0:01:44  lr: 0.000099  loss: 0.0191 (0.0192)  time: 0.9992  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [ 40/127]  eta: 0:01:31  lr: 0.000099  loss: 0.0193 (0.0192)  time: 1.0011  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [ 50/127]  eta: 0:01:20  lr: 0.000099  loss: 0.0191 (0.0193)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [ 60/127]  eta: 0:01:09  lr: 0.000099  loss: 0.0187 (0.0193)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0187 (0.0192)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [ 80/127]  eta: 0:00:48  lr: 0.000099  loss: 0.0187 (0.0191)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0186 (0.0191)  time: 0.9983  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0187 (0.0191)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0191 (0.0191)  time: 0.9978  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0192 (0.0192)  time: 0.9981  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0198 (0.0192)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:108] Total time: 0:02:09 (1.0172 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0198 (0.0192)\n",
      "Valid: [epoch:108]  [ 0/14]  eta: 0:00:38  loss: 0.0131 (0.0131)  time: 2.7583  data: 0.4649  max mem: 34254\n",
      "Valid: [epoch:108]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.1969  data: 0.0333  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:108] Total time: 0:00:30 (2.2081 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_108_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:109]  [  0/127]  eta: 0:07:21  lr: 0.000099  loss: 0.0182 (0.0182)  time: 3.4799  data: 2.5056  max mem: 34254\n",
      "Train: [epoch:109]  [ 10/127]  eta: 0:02:22  lr: 0.000099  loss: 0.0199 (0.0195)  time: 1.2148  data: 0.2279  max mem: 34254\n",
      "Train: [epoch:109]  [ 20/127]  eta: 0:01:58  lr: 0.000099  loss: 0.0193 (0.0194)  time: 0.9884  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [ 30/127]  eta: 0:01:43  lr: 0.000099  loss: 0.0191 (0.0194)  time: 0.9896  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [ 40/127]  eta: 0:01:31  lr: 0.000099  loss: 0.0190 (0.0192)  time: 0.9911  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [ 50/127]  eta: 0:01:20  lr: 0.000099  loss: 0.0186 (0.0192)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [ 60/127]  eta: 0:01:09  lr: 0.000099  loss: 0.0186 (0.0192)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0186 (0.0192)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [ 80/127]  eta: 0:00:48  lr: 0.000099  loss: 0.0193 (0.0192)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0187 (0.0192)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0187 (0.0191)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0188 (0.0191)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0192 (0.0192)  time: 0.9977  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0187 (0.0192)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:109] Total time: 0:02:08 (1.0147 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0187 (0.0192)\n",
      "Valid: [epoch:109]  [ 0/14]  eta: 0:00:36  loss: 0.0136 (0.0136)  time: 2.5997  data: 0.4376  max mem: 34254\n",
      "Valid: [epoch:109]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2041  data: 0.0314  max mem: 34254\n",
      "Valid: [epoch:109] Total time: 0:00:30 (2.2142 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_109_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:110]  [  0/127]  eta: 0:06:09  lr: 0.000099  loss: 0.0178 (0.0178)  time: 2.9092  data: 1.9358  max mem: 34254\n",
      "Train: [epoch:110]  [ 10/127]  eta: 0:02:16  lr: 0.000099  loss: 0.0183 (0.0186)  time: 1.1679  data: 0.1761  max mem: 34254\n",
      "Train: [epoch:110]  [ 20/127]  eta: 0:01:55  lr: 0.000099  loss: 0.0191 (0.0191)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [ 30/127]  eta: 0:01:42  lr: 0.000099  loss: 0.0187 (0.0189)  time: 0.9905  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [ 40/127]  eta: 0:01:30  lr: 0.000099  loss: 0.0187 (0.0190)  time: 0.9916  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [ 50/127]  eta: 0:01:19  lr: 0.000099  loss: 0.0193 (0.0190)  time: 0.9923  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [ 60/127]  eta: 0:01:08  lr: 0.000099  loss: 0.0192 (0.0191)  time: 0.9926  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0195 (0.0192)  time: 0.9922  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [ 80/127]  eta: 0:00:47  lr: 0.000099  loss: 0.0194 (0.0192)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0185 (0.0192)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0190 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0187 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0187 (0.0192)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0199 (0.0192)  time: 0.9985  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:110] Total time: 0:02:08 (1.0104 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0199 (0.0192)\n",
      "Valid: [epoch:110]  [ 0/14]  eta: 0:00:35  loss: 0.0151 (0.0151)  time: 2.5586  data: 0.4216  max mem: 34254\n",
      "Valid: [epoch:110]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1935  data: 0.0302  max mem: 34254\n",
      "Valid: [epoch:110] Total time: 0:00:30 (2.2042 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_110_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:111]  [  0/127]  eta: 0:06:57  lr: 0.000099  loss: 0.0193 (0.0193)  time: 3.2875  data: 2.3152  max mem: 34254\n",
      "Train: [epoch:111]  [ 10/127]  eta: 0:02:20  lr: 0.000099  loss: 0.0193 (0.0192)  time: 1.1974  data: 0.2106  max mem: 34254\n",
      "Train: [epoch:111]  [ 20/127]  eta: 0:01:57  lr: 0.000099  loss: 0.0193 (0.0193)  time: 0.9894  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [ 30/127]  eta: 0:01:43  lr: 0.000099  loss: 0.0192 (0.0192)  time: 0.9924  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [ 40/127]  eta: 0:01:31  lr: 0.000099  loss: 0.0189 (0.0191)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [ 50/127]  eta: 0:01:19  lr: 0.000099  loss: 0.0191 (0.0192)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [ 60/127]  eta: 0:01:09  lr: 0.000099  loss: 0.0194 (0.0193)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0193 (0.0192)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [ 80/127]  eta: 0:00:48  lr: 0.000099  loss: 0.0189 (0.0192)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0191 (0.0192)  time: 0.9971  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0189 (0.0192)  time: 1.0009  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0190 (0.0192)  time: 1.0002  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0190 (0.0192)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0188 (0.0191)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:111] Total time: 0:02:08 (1.0147 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0188 (0.0191)\n",
      "Valid: [epoch:111]  [ 0/14]  eta: 0:00:37  loss: 0.0137 (0.0137)  time: 2.7095  data: 0.5093  max mem: 34254\n",
      "Valid: [epoch:111]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0148)  time: 2.2226  data: 0.0365  max mem: 34254\n",
      "Valid: [epoch:111] Total time: 0:00:31 (2.2326 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_111_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:112]  [  0/127]  eta: 0:06:53  lr: 0.000099  loss: 0.0219 (0.0219)  time: 3.2577  data: 2.2640  max mem: 34254\n",
      "Train: [epoch:112]  [ 10/127]  eta: 0:02:19  lr: 0.000099  loss: 0.0193 (0.0194)  time: 1.1961  data: 0.2059  max mem: 34254\n",
      "Train: [epoch:112]  [ 20/127]  eta: 0:01:57  lr: 0.000099  loss: 0.0192 (0.0194)  time: 0.9898  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [ 30/127]  eta: 0:01:43  lr: 0.000099  loss: 0.0190 (0.0193)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [ 40/127]  eta: 0:01:31  lr: 0.000099  loss: 0.0190 (0.0192)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [ 50/127]  eta: 0:01:19  lr: 0.000099  loss: 0.0190 (0.0193)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [ 60/127]  eta: 0:01:09  lr: 0.000099  loss: 0.0192 (0.0192)  time: 0.9942  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:112]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0192 (0.0192)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [ 80/127]  eta: 0:00:48  lr: 0.000099  loss: 0.0191 (0.0192)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0189 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0187 (0.0191)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0189 (0.0191)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0190 (0.0191)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0188 (0.0192)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:112] Total time: 0:02:08 (1.0137 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0188 (0.0192)\n",
      "Valid: [epoch:112]  [ 0/14]  eta: 0:00:37  loss: 0.0175 (0.0175)  time: 2.6887  data: 0.4471  max mem: 34254\n",
      "Valid: [epoch:112]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2355  data: 0.0320  max mem: 34254\n",
      "Valid: [epoch:112] Total time: 0:00:31 (2.2461 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_112_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:113]  [  0/127]  eta: 0:07:46  lr: 0.000099  loss: 0.0201 (0.0201)  time: 3.6718  data: 2.6967  max mem: 34254\n",
      "Train: [epoch:113]  [ 10/127]  eta: 0:02:24  lr: 0.000099  loss: 0.0200 (0.0198)  time: 1.2339  data: 0.2452  max mem: 34254\n",
      "Train: [epoch:113]  [ 20/127]  eta: 0:01:59  lr: 0.000099  loss: 0.0196 (0.0194)  time: 0.9900  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [ 30/127]  eta: 0:01:44  lr: 0.000099  loss: 0.0192 (0.0194)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [ 40/127]  eta: 0:01:32  lr: 0.000099  loss: 0.0194 (0.0194)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [ 50/127]  eta: 0:01:20  lr: 0.000099  loss: 0.0190 (0.0193)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [ 60/127]  eta: 0:01:09  lr: 0.000099  loss: 0.0190 (0.0193)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0189 (0.0192)  time: 0.9991  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [ 80/127]  eta: 0:00:48  lr: 0.000099  loss: 0.0192 (0.0193)  time: 0.9994  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0195 (0.0193)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0194 (0.0193)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0189 (0.0193)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0188 (0.0193)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0191 (0.0193)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:113] Total time: 0:02:09 (1.0171 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0191 (0.0193)\n",
      "Valid: [epoch:113]  [ 0/14]  eta: 0:00:35  loss: 0.0144 (0.0144)  time: 2.5598  data: 0.4003  max mem: 34254\n",
      "Valid: [epoch:113]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1479  data: 0.0287  max mem: 34254\n",
      "Valid: [epoch:113] Total time: 0:00:30 (2.1584 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_113_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:114]  [  0/127]  eta: 0:06:58  lr: 0.000099  loss: 0.0201 (0.0201)  time: 3.2958  data: 2.3182  max mem: 34254\n",
      "Train: [epoch:114]  [ 10/127]  eta: 0:02:22  lr: 0.000099  loss: 0.0194 (0.0192)  time: 1.2144  data: 0.2109  max mem: 34254\n",
      "Train: [epoch:114]  [ 20/127]  eta: 0:01:58  lr: 0.000099  loss: 0.0196 (0.0194)  time: 1.0025  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [ 30/127]  eta: 0:01:44  lr: 0.000099  loss: 0.0196 (0.0192)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [ 40/127]  eta: 0:01:31  lr: 0.000099  loss: 0.0187 (0.0191)  time: 0.9924  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [ 50/127]  eta: 0:01:20  lr: 0.000099  loss: 0.0185 (0.0191)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [ 60/127]  eta: 0:01:09  lr: 0.000099  loss: 0.0185 (0.0191)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [ 70/127]  eta: 0:00:58  lr: 0.000099  loss: 0.0186 (0.0191)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [ 80/127]  eta: 0:00:48  lr: 0.000099  loss: 0.0188 (0.0191)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [ 90/127]  eta: 0:00:37  lr: 0.000099  loss: 0.0189 (0.0191)  time: 0.9970  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [100/127]  eta: 0:00:27  lr: 0.000099  loss: 0.0189 (0.0191)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [110/127]  eta: 0:00:17  lr: 0.000099  loss: 0.0194 (0.0191)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [120/127]  eta: 0:00:07  lr: 0.000099  loss: 0.0194 (0.0191)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114]  [126/127]  eta: 0:00:01  lr: 0.000099  loss: 0.0196 (0.0192)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:114] Total time: 0:02:08 (1.0156 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.0196 (0.0192)\n",
      "Valid: [epoch:114]  [ 0/14]  eta: 0:00:42  loss: 0.0138 (0.0138)  time: 3.0349  data: 0.5451  max mem: 34254\n",
      "Valid: [epoch:114]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0146)  time: 2.5138  data: 0.0391  max mem: 34254\n",
      "Valid: [epoch:114] Total time: 0:00:35 (2.5264 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_114_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:115]  [  0/127]  eta: 0:05:52  lr: 0.000098  loss: 0.0204 (0.0204)  time: 2.7719  data: 1.7965  max mem: 34254\n",
      "Train: [epoch:115]  [ 10/127]  eta: 0:02:14  lr: 0.000098  loss: 0.0194 (0.0192)  time: 1.1525  data: 0.1634  max mem: 34254\n",
      "Train: [epoch:115]  [ 20/127]  eta: 0:01:55  lr: 0.000098  loss: 0.0191 (0.0192)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [ 30/127]  eta: 0:01:42  lr: 0.000098  loss: 0.0193 (0.0193)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [ 40/127]  eta: 0:01:30  lr: 0.000098  loss: 0.0193 (0.0193)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0193 (0.0193)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [ 60/127]  eta: 0:01:08  lr: 0.000098  loss: 0.0195 (0.0194)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0193 (0.0194)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [ 80/127]  eta: 0:00:47  lr: 0.000098  loss: 0.0186 (0.0193)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0184 (0.0193)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0190 (0.0192)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0193 (0.0192)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0187 (0.0191)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0189 (0.0192)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:115] Total time: 0:02:08 (1.0108 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0189 (0.0192)\n",
      "Valid: [epoch:115]  [ 0/14]  eta: 0:00:37  loss: 0.0131 (0.0131)  time: 2.6788  data: 0.4477  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:115]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0147)  time: 2.2373  data: 0.0321  max mem: 34254\n",
      "Valid: [epoch:115] Total time: 0:00:31 (2.2466 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_115_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:116]  [  0/127]  eta: 0:06:24  lr: 0.000098  loss: 0.0193 (0.0193)  time: 3.0278  data: 2.0490  max mem: 34254\n",
      "Train: [epoch:116]  [ 10/127]  eta: 0:02:17  lr: 0.000098  loss: 0.0194 (0.0194)  time: 1.1769  data: 0.1864  max mem: 34254\n",
      "Train: [epoch:116]  [ 20/127]  eta: 0:01:56  lr: 0.000098  loss: 0.0194 (0.0196)  time: 0.9918  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [ 30/127]  eta: 0:01:42  lr: 0.000098  loss: 0.0189 (0.0191)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [ 40/127]  eta: 0:01:30  lr: 0.000098  loss: 0.0183 (0.0190)  time: 0.9915  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0186 (0.0190)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [ 60/127]  eta: 0:01:08  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0190 (0.0192)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [ 80/127]  eta: 0:00:47  lr: 0.000098  loss: 0.0190 (0.0192)  time: 0.9936  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0187 (0.0191)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0187 (0.0190)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0192 (0.0191)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0202 (0.0192)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:116] Total time: 0:02:08 (1.0111 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0202 (0.0192)\n",
      "Valid: [epoch:116]  [ 0/14]  eta: 0:00:37  loss: 0.0150 (0.0150)  time: 2.6732  data: 0.4567  max mem: 34254\n",
      "Valid: [epoch:116]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1698  data: 0.0328  max mem: 34254\n",
      "Valid: [epoch:116] Total time: 0:00:30 (2.1827 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_116_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:117]  [  0/127]  eta: 0:06:22  lr: 0.000098  loss: 0.0195 (0.0195)  time: 3.0099  data: 2.0348  max mem: 34254\n",
      "Train: [epoch:117]  [ 10/127]  eta: 0:02:17  lr: 0.000098  loss: 0.0194 (0.0193)  time: 1.1792  data: 0.1851  max mem: 34254\n",
      "Train: [epoch:117]  [ 20/127]  eta: 0:01:56  lr: 0.000098  loss: 0.0194 (0.0197)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [ 30/127]  eta: 0:01:42  lr: 0.000098  loss: 0.0194 (0.0194)  time: 0.9896  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [ 40/127]  eta: 0:01:30  lr: 0.000098  loss: 0.0186 (0.0192)  time: 0.9906  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0191 (0.0191)  time: 0.9918  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [ 60/127]  eta: 0:01:08  lr: 0.000098  loss: 0.0192 (0.0191)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0196 (0.0192)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [ 80/127]  eta: 0:00:47  lr: 0.000098  loss: 0.0194 (0.0192)  time: 0.9951  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0191 (0.0192)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0191 (0.0192)  time: 0.9946  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0190 (0.0192)  time: 0.9945  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0194 (0.0192)  time: 0.9976  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0191 (0.0192)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:117] Total time: 0:02:08 (1.0113 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0191 (0.0192)\n",
      "Valid: [epoch:117]  [ 0/14]  eta: 0:00:37  loss: 0.0141 (0.0141)  time: 2.6884  data: 0.4319  max mem: 34254\n",
      "Valid: [epoch:117]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0146)  time: 2.2774  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:117] Total time: 0:00:32 (2.2887 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_117_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:118]  [  0/127]  eta: 0:07:00  lr: 0.000098  loss: 0.0179 (0.0179)  time: 3.3102  data: 2.3342  max mem: 34254\n",
      "Train: [epoch:118]  [ 10/127]  eta: 0:02:20  lr: 0.000098  loss: 0.0188 (0.0196)  time: 1.2008  data: 0.2123  max mem: 34254\n",
      "Train: [epoch:118]  [ 20/127]  eta: 0:01:57  lr: 0.000098  loss: 0.0188 (0.0195)  time: 0.9902  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [ 30/127]  eta: 0:01:43  lr: 0.000098  loss: 0.0194 (0.0196)  time: 0.9904  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [ 40/127]  eta: 0:01:31  lr: 0.000098  loss: 0.0194 (0.0194)  time: 0.9926  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0186 (0.0194)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [ 60/127]  eta: 0:01:09  lr: 0.000098  loss: 0.0188 (0.0194)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0192 (0.0193)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [ 80/127]  eta: 0:00:48  lr: 0.000098  loss: 0.0190 (0.0193)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0193 (0.0193)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0194 (0.0192)  time: 0.9955  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0187 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0193 (0.0192)  time: 0.9956  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0193 (0.0192)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:118] Total time: 0:02:08 (1.0138 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0193 (0.0192)\n",
      "Valid: [epoch:118]  [ 0/14]  eta: 0:00:35  loss: 0.0155 (0.0155)  time: 2.5564  data: 0.4179  max mem: 34254\n",
      "Valid: [epoch:118]  [13/14]  eta: 0:00:02  loss: 0.0150 (0.0150)  time: 2.2050  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:118] Total time: 0:00:31 (2.2163 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0150)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_118_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:119]  [  0/127]  eta: 0:06:58  lr: 0.000098  loss: 0.0172 (0.0172)  time: 3.2947  data: 2.3226  max mem: 34254\n",
      "Train: [epoch:119]  [ 10/127]  eta: 0:02:20  lr: 0.000098  loss: 0.0184 (0.0187)  time: 1.2031  data: 0.2113  max mem: 34254\n",
      "Train: [epoch:119]  [ 20/127]  eta: 0:01:57  lr: 0.000098  loss: 0.0187 (0.0188)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [ 30/127]  eta: 0:01:43  lr: 0.000098  loss: 0.0192 (0.0189)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [ 40/127]  eta: 0:01:31  lr: 0.000098  loss: 0.0194 (0.0191)  time: 0.9937  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0187 (0.0191)  time: 0.9950  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:119]  [ 60/127]  eta: 0:01:09  lr: 0.000098  loss: 0.0186 (0.0190)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0185 (0.0190)  time: 0.9966  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [ 80/127]  eta: 0:00:48  lr: 0.000098  loss: 0.0188 (0.0190)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0193 (0.0192)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0194 (0.0192)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0200 (0.0192)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:119] Total time: 0:02:08 (1.0143 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0200 (0.0192)\n",
      "Valid: [epoch:119]  [ 0/14]  eta: 0:00:37  loss: 0.0151 (0.0151)  time: 2.6504  data: 0.4271  max mem: 34254\n",
      "Valid: [epoch:119]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.3154  data: 0.0306  max mem: 34254\n",
      "Valid: [epoch:119] Total time: 0:00:32 (2.3270 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_119_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:120]  [  0/127]  eta: 0:05:54  lr: 0.000098  loss: 0.0180 (0.0180)  time: 2.7882  data: 1.8146  max mem: 34254\n",
      "Train: [epoch:120]  [ 10/127]  eta: 0:02:15  lr: 0.000098  loss: 0.0190 (0.0190)  time: 1.1539  data: 0.1651  max mem: 34254\n",
      "Train: [epoch:120]  [ 20/127]  eta: 0:01:55  lr: 0.000098  loss: 0.0192 (0.0193)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [ 30/127]  eta: 0:01:41  lr: 0.000098  loss: 0.0196 (0.0193)  time: 0.9925  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [ 40/127]  eta: 0:01:30  lr: 0.000098  loss: 0.0196 (0.0193)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0193 (0.0193)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [ 60/127]  eta: 0:01:08  lr: 0.000098  loss: 0.0186 (0.0193)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0187 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [ 80/127]  eta: 0:00:47  lr: 0.000098  loss: 0.0189 (0.0192)  time: 0.9965  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0191 (0.0192)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0189 (0.0192)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0189 (0.0192)  time: 0.9958  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0188 (0.0191)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0185 (0.0192)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:120] Total time: 0:02:08 (1.0105 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0185 (0.0192)\n",
      "Valid: [epoch:120]  [ 0/14]  eta: 0:00:37  loss: 0.0141 (0.0141)  time: 2.6662  data: 0.4260  max mem: 34254\n",
      "Valid: [epoch:120]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0146)  time: 2.2365  data: 0.0305  max mem: 34254\n",
      "Valid: [epoch:120] Total time: 0:00:31 (2.2469 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_120_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:121]  [  0/127]  eta: 0:07:07  lr: 0.000098  loss: 0.0209 (0.0209)  time: 3.3675  data: 2.3916  max mem: 34254\n",
      "Train: [epoch:121]  [ 10/127]  eta: 0:02:21  lr: 0.000098  loss: 0.0190 (0.0191)  time: 1.2077  data: 0.2175  max mem: 34254\n",
      "Train: [epoch:121]  [ 20/127]  eta: 0:01:58  lr: 0.000098  loss: 0.0190 (0.0194)  time: 0.9915  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [ 30/127]  eta: 0:01:43  lr: 0.000098  loss: 0.0197 (0.0194)  time: 0.9909  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [ 40/127]  eta: 0:01:31  lr: 0.000098  loss: 0.0197 (0.0195)  time: 0.9914  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0188 (0.0194)  time: 0.9918  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [ 60/127]  eta: 0:01:09  lr: 0.000098  loss: 0.0190 (0.0194)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0188 (0.0192)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [ 80/127]  eta: 0:00:48  lr: 0.000098  loss: 0.0178 (0.0191)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0189 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0188 (0.0192)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0188 (0.0191)  time: 0.9960  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0189 (0.0192)  time: 0.9964  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:121] Total time: 0:02:08 (1.0138 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0189 (0.0192)\n",
      "Valid: [epoch:121]  [ 0/14]  eta: 0:00:35  loss: 0.0136 (0.0136)  time: 2.5561  data: 0.4083  max mem: 34254\n",
      "Valid: [epoch:121]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2910  data: 0.0293  max mem: 34254\n",
      "Valid: [epoch:121] Total time: 0:00:32 (2.2997 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_121_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:122]  [  0/127]  eta: 0:06:03  lr: 0.000098  loss: 0.0192 (0.0192)  time: 2.8630  data: 1.8896  max mem: 34254\n",
      "Train: [epoch:122]  [ 10/127]  eta: 0:02:15  lr: 0.000098  loss: 0.0192 (0.0193)  time: 1.1616  data: 0.1719  max mem: 34254\n",
      "Train: [epoch:122]  [ 20/127]  eta: 0:01:55  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9933  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [ 30/127]  eta: 0:01:42  lr: 0.000098  loss: 0.0188 (0.0192)  time: 0.9929  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [ 40/127]  eta: 0:01:30  lr: 0.000098  loss: 0.0191 (0.0192)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0194 (0.0193)  time: 0.9973  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [ 60/127]  eta: 0:01:08  lr: 0.000098  loss: 0.0194 (0.0193)  time: 0.9980  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0190 (0.0192)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [ 80/127]  eta: 0:00:47  lr: 0.000098  loss: 0.0186 (0.0192)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0187 (0.0192)  time: 0.9944  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0193 (0.0192)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0185 (0.0191)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0188 (0.0192)  time: 0.9948  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0194 (0.0192)  time: 0.9950  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:122] Total time: 0:02:08 (1.0104 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0194 (0.0192)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:122]  [ 0/14]  eta: 0:00:37  loss: 0.0136 (0.0136)  time: 2.6791  data: 0.4332  max mem: 34254\n",
      "Valid: [epoch:122]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2500  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:122] Total time: 0:00:31 (2.2637 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_122_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:123]  [  0/127]  eta: 0:05:49  lr: 0.000098  loss: 0.0211 (0.0211)  time: 2.7527  data: 1.7700  max mem: 34254\n",
      "Train: [epoch:123]  [ 10/127]  eta: 0:02:14  lr: 0.000098  loss: 0.0194 (0.0194)  time: 1.1489  data: 0.1610  max mem: 34254\n",
      "Train: [epoch:123]  [ 20/127]  eta: 0:01:54  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9906  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [ 30/127]  eta: 0:01:41  lr: 0.000098  loss: 0.0185 (0.0190)  time: 0.9967  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [ 40/127]  eta: 0:01:30  lr: 0.000098  loss: 0.0183 (0.0188)  time: 0.9972  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [ 50/127]  eta: 0:01:19  lr: 0.000098  loss: 0.0186 (0.0188)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [ 60/127]  eta: 0:01:08  lr: 0.000098  loss: 0.0191 (0.0188)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [ 70/127]  eta: 0:00:58  lr: 0.000098  loss: 0.0186 (0.0188)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [ 80/127]  eta: 0:00:47  lr: 0.000098  loss: 0.0191 (0.0189)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [ 90/127]  eta: 0:00:37  lr: 0.000098  loss: 0.0197 (0.0190)  time: 0.9939  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [100/127]  eta: 0:00:27  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [110/127]  eta: 0:00:17  lr: 0.000098  loss: 0.0190 (0.0191)  time: 0.9940  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [120/127]  eta: 0:00:07  lr: 0.000098  loss: 0.0195 (0.0191)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123]  [126/127]  eta: 0:00:01  lr: 0.000098  loss: 0.0192 (0.0191)  time: 0.9957  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:123] Total time: 0:02:08 (1.0094 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.0192 (0.0191)\n",
      "Valid: [epoch:123]  [ 0/14]  eta: 0:00:38  loss: 0.0151 (0.0151)  time: 2.7390  data: 0.4710  max mem: 34254\n",
      "Valid: [epoch:123]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.2520  data: 0.0337  max mem: 34254\n",
      "Valid: [epoch:123] Total time: 0:00:31 (2.2636 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_123_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:124]  [  0/127]  eta: 0:06:58  lr: 0.000097  loss: 0.0200 (0.0200)  time: 3.2987  data: 2.3250  max mem: 34254\n",
      "Train: [epoch:124]  [ 10/127]  eta: 0:02:20  lr: 0.000097  loss: 0.0196 (0.0194)  time: 1.2013  data: 0.2114  max mem: 34254\n",
      "Train: [epoch:124]  [ 20/127]  eta: 0:01:57  lr: 0.000097  loss: 0.0193 (0.0194)  time: 0.9920  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [ 30/127]  eta: 0:01:43  lr: 0.000097  loss: 0.0190 (0.0193)  time: 0.9927  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [ 40/127]  eta: 0:01:31  lr: 0.000097  loss: 0.0189 (0.0193)  time: 0.9930  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [ 50/127]  eta: 0:01:19  lr: 0.000097  loss: 0.0192 (0.0193)  time: 0.9943  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [ 60/127]  eta: 0:01:09  lr: 0.000097  loss: 0.0192 (0.0193)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [ 70/127]  eta: 0:00:58  lr: 0.000097  loss: 0.0188 (0.0192)  time: 0.9974  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [ 80/127]  eta: 0:00:48  lr: 0.000097  loss: 0.0189 (0.0192)  time: 0.9982  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [ 90/127]  eta: 0:00:37  lr: 0.000097  loss: 0.0189 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [100/127]  eta: 0:00:27  lr: 0.000097  loss: 0.0191 (0.0192)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [110/127]  eta: 0:00:17  lr: 0.000097  loss: 0.0192 (0.0192)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [120/127]  eta: 0:00:07  lr: 0.000097  loss: 0.0192 (0.0193)  time: 1.0177  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124]  [126/127]  eta: 0:00:01  lr: 0.000097  loss: 0.0189 (0.0192)  time: 1.0177  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:124] Total time: 0:02:09 (1.0181 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.0189 (0.0192)\n",
      "Valid: [epoch:124]  [ 0/14]  eta: 0:00:35  loss: 0.0137 (0.0137)  time: 2.5199  data: 0.4128  max mem: 34254\n",
      "Valid: [epoch:124]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1175  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:124] Total time: 0:00:29 (2.1268 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_124_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:125]  [  0/127]  eta: 0:06:57  lr: 0.000097  loss: 0.0190 (0.0190)  time: 3.2857  data: 2.3011  max mem: 34254\n",
      "Train: [epoch:125]  [ 10/127]  eta: 0:02:21  lr: 0.000097  loss: 0.0188 (0.0187)  time: 1.2116  data: 0.2093  max mem: 34254\n",
      "Train: [epoch:125]  [ 20/127]  eta: 0:01:58  lr: 0.000097  loss: 0.0188 (0.0190)  time: 0.9984  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [ 30/127]  eta: 0:01:44  lr: 0.000097  loss: 0.0192 (0.0192)  time: 1.0064  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [ 40/127]  eta: 0:01:32  lr: 0.000097  loss: 0.0190 (0.0193)  time: 1.0157  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [ 50/127]  eta: 0:01:20  lr: 0.000097  loss: 0.0190 (0.0194)  time: 1.0028  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [ 60/127]  eta: 0:01:09  lr: 0.000097  loss: 0.0189 (0.0193)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [ 70/127]  eta: 0:00:58  lr: 0.000097  loss: 0.0187 (0.0193)  time: 0.9949  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [ 80/127]  eta: 0:00:48  lr: 0.000097  loss: 0.0188 (0.0193)  time: 0.9954  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [ 90/127]  eta: 0:00:37  lr: 0.000097  loss: 0.0189 (0.0192)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [100/127]  eta: 0:00:27  lr: 0.000097  loss: 0.0191 (0.0192)  time: 0.9963  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [110/127]  eta: 0:00:17  lr: 0.000097  loss: 0.0189 (0.0192)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [120/127]  eta: 0:00:07  lr: 0.000097  loss: 0.0188 (0.0192)  time: 0.9989  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125]  [126/127]  eta: 0:00:01  lr: 0.000097  loss: 0.0189 (0.0192)  time: 0.9986  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:125] Total time: 0:02:09 (1.0192 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.0189 (0.0192)\n",
      "Valid: [epoch:125]  [ 0/14]  eta: 0:00:35  loss: 0.0131 (0.0131)  time: 2.5342  data: 0.3804  max mem: 34254\n",
      "Valid: [epoch:125]  [13/14]  eta: 0:00:02  loss: 0.0146 (0.0146)  time: 2.1385  data: 0.0273  max mem: 34254\n",
      "Valid: [epoch:125] Total time: 0:00:30 (2.1475 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_125_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:126]  [  0/127]  eta: 0:07:12  lr: 0.000097  loss: 0.0186 (0.0186)  time: 3.4073  data: 2.4275  max mem: 34254\n",
      "Train: [epoch:126]  [ 10/127]  eta: 0:02:22  lr: 0.000097  loss: 0.0191 (0.0193)  time: 1.2147  data: 0.2208  max mem: 34254\n",
      "Train: [epoch:126]  [ 20/127]  eta: 0:01:58  lr: 0.000097  loss: 0.0191 (0.0191)  time: 0.9935  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [ 30/127]  eta: 0:01:43  lr: 0.000097  loss: 0.0190 (0.0192)  time: 0.9913  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [ 40/127]  eta: 0:01:31  lr: 0.000097  loss: 0.0188 (0.0193)  time: 0.9918  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:126]  [ 50/127]  eta: 0:01:20  lr: 0.000097  loss: 0.0188 (0.0192)  time: 0.9931  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [ 60/127]  eta: 0:01:09  lr: 0.000097  loss: 0.0189 (0.0192)  time: 0.9938  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [ 70/127]  eta: 0:00:58  lr: 0.000097  loss: 0.0191 (0.0192)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [ 80/127]  eta: 0:00:48  lr: 0.000097  loss: 0.0188 (0.0191)  time: 0.9941  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [ 90/127]  eta: 0:00:37  lr: 0.000097  loss: 0.0185 (0.0190)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [100/127]  eta: 0:00:27  lr: 0.000097  loss: 0.0186 (0.0191)  time: 0.9947  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [110/127]  eta: 0:00:17  lr: 0.000097  loss: 0.0196 (0.0191)  time: 0.9952  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [120/127]  eta: 0:00:07  lr: 0.000097  loss: 0.0199 (0.0191)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126]  [126/127]  eta: 0:00:01  lr: 0.000097  loss: 0.0195 (0.0191)  time: 0.9959  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:126] Total time: 0:02:08 (1.0144 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.0195 (0.0191)\n",
      "Valid: [epoch:126]  [ 0/14]  eta: 0:00:34  loss: 0.0147 (0.0147)  time: 2.4776  data: 0.3542  max mem: 34254\n",
      "Valid: [epoch:126]  [13/14]  eta: 0:00:02  loss: 0.0147 (0.0147)  time: 2.1347  data: 0.0254  max mem: 34254\n",
      "Valid: [epoch:126] Total time: 0:00:30 (2.1436 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_126_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Train: [epoch:127]  [  0/127]  eta: 0:06:42  lr: 0.000097  loss: 0.0194 (0.0194)  time: 3.1705  data: 2.1983  max mem: 34254\n",
      "Train: [epoch:127]  [ 10/127]  eta: 0:02:19  lr: 0.000097  loss: 0.0206 (0.0201)  time: 1.1889  data: 0.1999  max mem: 34254\n",
      "Train: [epoch:127]  [ 20/127]  eta: 0:01:57  lr: 0.000097  loss: 0.0202 (0.0200)  time: 0.9912  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127]  [ 30/127]  eta: 0:01:42  lr: 0.000097  loss: 0.0193 (0.0198)  time: 0.9919  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127]  [ 40/127]  eta: 0:01:30  lr: 0.000097  loss: 0.0192 (0.0197)  time: 0.9928  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127]  [ 50/127]  eta: 0:01:19  lr: 0.000097  loss: 0.0188 (0.0195)  time: 0.9942  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127]  [ 60/127]  eta: 0:01:08  lr: 0.000097  loss: 0.0187 (0.0194)  time: 0.9969  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127]  [ 70/127]  eta: 0:00:58  lr: 0.000097  loss: 0.0187 (0.0193)  time: 0.9968  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127]  [ 80/127]  eta: 0:00:47  lr: 0.000097  loss: 0.0188 (0.0193)  time: 0.9953  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127]  [ 90/127]  eta: 0:00:37  lr: 0.000097  loss: 0.0191 (0.0193)  time: 0.9961  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127]  [100/127]  eta: 0:00:27  lr: 0.000097  loss: 0.0188 (0.0192)  time: 0.9962  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:127]  [110/127]  eta: 0:00:17  lr: 0.000097  loss: nan (nan)  time: 0.9917  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:127]  [120/127]  eta: 0:00:07  lr: 0.000097  loss: nan (nan)  time: 0.9815  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:127]  [126/127]  eta: 0:00:01  lr: 0.000097  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:127] Total time: 0:02:08 (1.0100 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:127]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5307  data: 0.4182  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:127]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1176  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:127] Total time: 0:00:29 (2.1263 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_127_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [  0/127]  eta: 0:06:51  lr: 0.000097  loss: nan (nan)  time: 3.2436  data: 2.2853  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 10/127]  eta: 0:02:17  lr: 0.000097  loss: nan (nan)  time: 1.1791  data: 0.2078  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 20/127]  eta: 0:01:55  lr: 0.000097  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 30/127]  eta: 0:01:41  lr: 0.000097  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 40/127]  eta: 0:01:29  lr: 0.000097  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 50/127]  eta: 0:01:18  lr: 0.000097  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 60/127]  eta: 0:01:07  lr: 0.000097  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 70/127]  eta: 0:00:57  lr: 0.000097  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 80/127]  eta: 0:00:47  lr: 0.000097  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [ 90/127]  eta: 0:00:37  lr: 0.000097  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [100/127]  eta: 0:00:26  lr: 0.000097  loss: nan (nan)  time: 0.9807  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [110/127]  eta: 0:00:16  lr: 0.000097  loss: nan (nan)  time: 0.9805  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [120/127]  eta: 0:00:06  lr: 0.000097  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:128]  [126/127]  eta: 0:00:00  lr: 0.000097  loss: nan (nan)  time: 0.9783  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:128] Total time: 0:02:06 (0.9960 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:128]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6649  data: 0.4485  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:128]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1607  data: 0.0322  max mem: 34254\n",
      "Valid: [epoch:128] Total time: 0:00:30 (2.1728 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_128_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [  0/127]  eta: 0:07:36  lr: 0.000097  loss: nan (nan)  time: 3.5966  data: 2.6389  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 10/127]  eta: 0:02:21  lr: 0.000097  loss: nan (nan)  time: 1.2126  data: 0.2400  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 20/127]  eta: 0:01:57  lr: 0.000097  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 30/127]  eta: 0:01:42  lr: 0.000097  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 40/127]  eta: 0:01:30  lr: 0.000097  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 50/127]  eta: 0:01:18  lr: 0.000097  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 60/127]  eta: 0:01:08  lr: 0.000097  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 70/127]  eta: 0:00:57  lr: 0.000097  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 80/127]  eta: 0:00:47  lr: 0.000097  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [ 90/127]  eta: 0:00:37  lr: 0.000097  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [100/127]  eta: 0:00:26  lr: 0.000097  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [110/127]  eta: 0:00:16  lr: 0.000097  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [120/127]  eta: 0:00:06  lr: 0.000097  loss: nan (nan)  time: 0.9781  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:129]  [126/127]  eta: 0:00:00  lr: 0.000097  loss: nan (nan)  time: 0.9780  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:129] Total time: 0:02:06 (0.9967 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:129]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6439  data: 0.4335  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:129]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1844  data: 0.0311  max mem: 34254\n",
      "Valid: [epoch:129] Total time: 0:00:30 (2.1960 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_129_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [  0/127]  eta: 0:07:32  lr: 0.000097  loss: nan (nan)  time: 3.5611  data: 2.4922  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 10/127]  eta: 0:02:21  lr: 0.000097  loss: nan (nan)  time: 1.2056  data: 0.2267  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 20/127]  eta: 0:01:57  lr: 0.000097  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 30/127]  eta: 0:01:42  lr: 0.000097  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 40/127]  eta: 0:01:30  lr: 0.000097  loss: nan (nan)  time: 0.9780  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 50/127]  eta: 0:01:18  lr: 0.000097  loss: nan (nan)  time: 0.9783  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 60/127]  eta: 0:01:08  lr: 0.000097  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 70/127]  eta: 0:00:57  lr: 0.000097  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 80/127]  eta: 0:00:47  lr: 0.000097  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [ 90/127]  eta: 0:00:37  lr: 0.000097  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [100/127]  eta: 0:00:27  lr: 0.000097  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [110/127]  eta: 0:00:16  lr: 0.000097  loss: nan (nan)  time: 0.9779  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [120/127]  eta: 0:00:06  lr: 0.000097  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:130]  [126/127]  eta: 0:00:00  lr: 0.000097  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:130] Total time: 0:02:06 (0.9974 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:130]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.5972  data: 0.4240  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:130]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1808  data: 0.0304  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:130] Total time: 0:00:30 (2.1922 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_130_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [  0/127]  eta: 0:06:07  lr: 0.000097  loss: nan (nan)  time: 2.8928  data: 1.9283  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 10/127]  eta: 0:02:13  lr: 0.000097  loss: nan (nan)  time: 1.1442  data: 0.1754  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 20/127]  eta: 0:01:53  lr: 0.000097  loss: nan (nan)  time: 0.9700  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 30/127]  eta: 0:01:40  lr: 0.000097  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 40/127]  eta: 0:01:28  lr: 0.000097  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 50/127]  eta: 0:01:17  lr: 0.000097  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 60/127]  eta: 0:01:07  lr: 0.000097  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 70/127]  eta: 0:00:57  lr: 0.000097  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 80/127]  eta: 0:00:46  lr: 0.000097  loss: nan (nan)  time: 0.9825  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [ 90/127]  eta: 0:00:36  lr: 0.000097  loss: nan (nan)  time: 0.9832  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [100/127]  eta: 0:00:26  lr: 0.000097  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [110/127]  eta: 0:00:16  lr: 0.000097  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [120/127]  eta: 0:00:06  lr: 0.000097  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:131]  [126/127]  eta: 0:00:00  lr: 0.000097  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:131] Total time: 0:02:05 (0.9919 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:131]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6132  data: 0.4109  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:131]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2076  data: 0.0294  max mem: 34254\n",
      "Valid: [epoch:131] Total time: 0:00:31 (2.2174 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_131_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [  0/127]  eta: 0:06:22  lr: 0.000097  loss: nan (nan)  time: 3.0102  data: 2.0427  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 10/127]  eta: 0:02:15  lr: 0.000097  loss: nan (nan)  time: 1.1552  data: 0.1858  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 20/127]  eta: 0:01:54  lr: 0.000097  loss: nan (nan)  time: 0.9710  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 30/127]  eta: 0:01:40  lr: 0.000097  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 40/127]  eta: 0:01:28  lr: 0.000097  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 50/127]  eta: 0:01:17  lr: 0.000097  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 60/127]  eta: 0:01:07  lr: 0.000097  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 70/127]  eta: 0:00:57  lr: 0.000097  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 80/127]  eta: 0:00:46  lr: 0.000097  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [ 90/127]  eta: 0:00:36  lr: 0.000097  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [100/127]  eta: 0:00:26  lr: 0.000097  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [110/127]  eta: 0:00:16  lr: 0.000097  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [120/127]  eta: 0:00:06  lr: 0.000097  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:132]  [126/127]  eta: 0:00:00  lr: 0.000097  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:132] Total time: 0:02:05 (0.9917 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:132]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6483  data: 0.4461  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:132]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2071  data: 0.0320  max mem: 34254\n",
      "Valid: [epoch:132] Total time: 0:00:31 (2.2193 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_132_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [  0/127]  eta: 0:06:26  lr: 0.000096  loss: nan (nan)  time: 3.0408  data: 2.0859  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 10/127]  eta: 0:02:15  lr: 0.000096  loss: nan (nan)  time: 1.1599  data: 0.1897  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 20/127]  eta: 0:01:54  lr: 0.000096  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 30/127]  eta: 0:01:40  lr: 0.000096  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 40/127]  eta: 0:01:29  lr: 0.000096  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 50/127]  eta: 0:01:18  lr: 0.000096  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 60/127]  eta: 0:01:07  lr: 0.000096  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 80/127]  eta: 0:00:47  lr: 0.000096  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [ 90/127]  eta: 0:00:36  lr: 0.000096  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [100/127]  eta: 0:00:26  lr: 0.000096  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:133]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:133] Total time: 0:02:06 (0.9934 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:133]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7089  data: 0.5219  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:133]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2243  data: 0.0374  max mem: 34254\n",
      "Valid: [epoch:133] Total time: 0:00:31 (2.2354 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_133_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [  0/127]  eta: 0:06:08  lr: 0.000096  loss: nan (nan)  time: 2.8983  data: 1.9355  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 10/127]  eta: 0:02:14  lr: 0.000096  loss: nan (nan)  time: 1.1474  data: 0.1761  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 20/127]  eta: 0:01:53  lr: 0.000096  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 30/127]  eta: 0:01:40  lr: 0.000096  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 40/127]  eta: 0:01:28  lr: 0.000096  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 50/127]  eta: 0:01:17  lr: 0.000096  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 60/127]  eta: 0:01:07  lr: 0.000096  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 80/127]  eta: 0:00:46  lr: 0.000096  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [ 90/127]  eta: 0:00:36  lr: 0.000096  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [100/127]  eta: 0:00:26  lr: 0.000096  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:134]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:134] Total time: 0:02:05 (0.9920 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:134]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6478  data: 0.4059  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:134]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2194  data: 0.0291  max mem: 34254\n",
      "Valid: [epoch:134] Total time: 0:00:31 (2.2296 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_134_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [  0/127]  eta: 0:06:25  lr: 0.000096  loss: nan (nan)  time: 3.0358  data: 2.0768  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 10/127]  eta: 0:02:16  lr: 0.000096  loss: nan (nan)  time: 1.1636  data: 0.1889  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 20/127]  eta: 0:01:54  lr: 0.000096  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 30/127]  eta: 0:01:40  lr: 0.000096  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 40/127]  eta: 0:01:29  lr: 0.000096  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 50/127]  eta: 0:01:18  lr: 0.000096  loss: nan (nan)  time: 0.9785  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 60/127]  eta: 0:01:07  lr: 0.000096  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 80/127]  eta: 0:00:47  lr: 0.000096  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [ 90/127]  eta: 0:00:36  lr: 0.000096  loss: nan (nan)  time: 0.9780  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [100/127]  eta: 0:00:26  lr: 0.000096  loss: nan (nan)  time: 0.9782  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9792  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:135]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:135] Total time: 0:02:06 (0.9941 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:135]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6214  data: 0.4373  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:135]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1968  data: 0.0313  max mem: 34254\n",
      "Valid: [epoch:135] Total time: 0:00:30 (2.2088 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_135_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [  0/127]  eta: 0:07:37  lr: 0.000096  loss: nan (nan)  time: 3.6030  data: 2.5821  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 10/127]  eta: 0:02:21  lr: 0.000096  loss: nan (nan)  time: 1.2107  data: 0.2348  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 20/127]  eta: 0:01:57  lr: 0.000096  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 30/127]  eta: 0:01:42  lr: 0.000096  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 40/127]  eta: 0:01:30  lr: 0.000096  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 50/127]  eta: 0:01:18  lr: 0.000096  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 60/127]  eta: 0:01:08  lr: 0.000096  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 80/127]  eta: 0:00:47  lr: 0.000096  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [ 90/127]  eta: 0:00:37  lr: 0.000096  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [100/127]  eta: 0:00:27  lr: 0.000096  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:136]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:136] Total time: 0:02:06 (0.9970 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:136]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7112  data: 0.4569  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:136]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1755  data: 0.0327  max mem: 34254\n",
      "Valid: [epoch:136] Total time: 0:00:30 (2.1861 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_136_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [  0/127]  eta: 0:06:30  lr: 0.000096  loss: nan (nan)  time: 3.0768  data: 2.1161  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 10/127]  eta: 0:02:15  lr: 0.000096  loss: nan (nan)  time: 1.1618  data: 0.1925  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 20/127]  eta: 0:01:54  lr: 0.000096  loss: nan (nan)  time: 0.9707  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 30/127]  eta: 0:01:40  lr: 0.000096  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 40/127]  eta: 0:01:29  lr: 0.000096  loss: nan (nan)  time: 0.9789  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 50/127]  eta: 0:01:18  lr: 0.000096  loss: nan (nan)  time: 0.9809  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 60/127]  eta: 0:01:07  lr: 0.000096  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 80/127]  eta: 0:00:47  lr: 0.000096  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [ 90/127]  eta: 0:00:36  lr: 0.000096  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [100/127]  eta: 0:00:26  lr: 0.000096  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:137]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:137] Total time: 0:02:06 (0.9939 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:137]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7156  data: 0.4534  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:137]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2254  data: 0.0325  max mem: 34254\n",
      "Valid: [epoch:137] Total time: 0:00:31 (2.2371 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_137_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [  0/127]  eta: 0:06:00  lr: 0.000096  loss: nan (nan)  time: 2.8411  data: 1.8815  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 10/127]  eta: 0:02:13  lr: 0.000096  loss: nan (nan)  time: 1.1405  data: 0.1711  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 20/127]  eta: 0:01:53  lr: 0.000096  loss: nan (nan)  time: 0.9715  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 30/127]  eta: 0:01:40  lr: 0.000096  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 40/127]  eta: 0:01:28  lr: 0.000096  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 50/127]  eta: 0:01:17  lr: 0.000096  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 60/127]  eta: 0:01:07  lr: 0.000096  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 80/127]  eta: 0:00:46  lr: 0.000096  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [ 90/127]  eta: 0:00:36  lr: 0.000096  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [100/127]  eta: 0:00:26  lr: 0.000096  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:138]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:138] Total time: 0:02:05 (0.9915 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:138]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5281  data: 0.3898  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:138]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1391  data: 0.0279  max mem: 34254\n",
      "Valid: [epoch:138] Total time: 0:00:30 (2.1507 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_138_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [  0/127]  eta: 0:07:01  lr: 0.000096  loss: nan (nan)  time: 3.3179  data: 2.3602  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 10/127]  eta: 0:02:18  lr: 0.000096  loss: nan (nan)  time: 1.1853  data: 0.2147  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 20/127]  eta: 0:01:55  lr: 0.000096  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 30/127]  eta: 0:01:41  lr: 0.000096  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 40/127]  eta: 0:01:29  lr: 0.000096  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 50/127]  eta: 0:01:18  lr: 0.000096  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 60/127]  eta: 0:01:07  lr: 0.000096  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 80/127]  eta: 0:00:47  lr: 0.000096  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [ 90/127]  eta: 0:00:37  lr: 0.000096  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [100/127]  eta: 0:00:26  lr: 0.000096  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9779  data: 0.0002  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:139]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9783  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:139] Total time: 0:02:06 (0.9956 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:139]  [ 0/14]  eta: 0:00:39  loss: nan (nan)  time: 2.8481  data: 0.5248  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:139]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.4681  data: 0.0376  max mem: 34254\n",
      "Valid: [epoch:139] Total time: 0:00:34 (2.4793 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_139_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [  0/127]  eta: 0:07:20  lr: 0.000096  loss: nan (nan)  time: 3.4709  data: 2.5141  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 10/127]  eta: 0:02:20  lr: 0.000096  loss: nan (nan)  time: 1.2045  data: 0.2287  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 20/127]  eta: 0:01:57  lr: 0.000096  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 30/127]  eta: 0:01:42  lr: 0.000096  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 40/127]  eta: 0:01:30  lr: 0.000096  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 50/127]  eta: 0:01:18  lr: 0.000096  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 60/127]  eta: 0:01:08  lr: 0.000096  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 80/127]  eta: 0:00:47  lr: 0.000096  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [ 90/127]  eta: 0:00:37  lr: 0.000096  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [100/127]  eta: 0:00:27  lr: 0.000096  loss: nan (nan)  time: 0.9785  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9785  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:140]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:140] Total time: 0:02:06 (0.9971 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:140]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6726  data: 0.4328  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:140]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.3354  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:140] Total time: 0:00:32 (2.3470 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_140_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [  0/127]  eta: 0:06:53  lr: 0.000096  loss: nan (nan)  time: 3.2593  data: 2.2939  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 10/127]  eta: 0:02:17  lr: 0.000096  loss: nan (nan)  time: 1.1792  data: 0.2086  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 20/127]  eta: 0:01:55  lr: 0.000096  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 30/127]  eta: 0:01:41  lr: 0.000096  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 40/127]  eta: 0:01:29  lr: 0.000096  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 50/127]  eta: 0:01:18  lr: 0.000096  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 60/127]  eta: 0:01:07  lr: 0.000096  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 70/127]  eta: 0:00:57  lr: 0.000096  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 80/127]  eta: 0:00:47  lr: 0.000096  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [ 90/127]  eta: 0:00:36  lr: 0.000096  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [100/127]  eta: 0:00:26  lr: 0.000096  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [110/127]  eta: 0:00:16  lr: 0.000096  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [120/127]  eta: 0:00:06  lr: 0.000096  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:141]  [126/127]  eta: 0:00:00  lr: 0.000096  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:141] Total time: 0:02:06 (0.9937 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:141]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6167  data: 0.4094  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:141]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1742  data: 0.0294  max mem: 34254\n",
      "Valid: [epoch:141] Total time: 0:00:30 (2.1841 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_141_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [  0/127]  eta: 0:06:39  lr: 0.000095  loss: nan (nan)  time: 3.1464  data: 2.1927  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 10/127]  eta: 0:02:16  lr: 0.000095  loss: nan (nan)  time: 1.1699  data: 0.1994  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 20/127]  eta: 0:01:55  lr: 0.000095  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 30/127]  eta: 0:01:41  lr: 0.000095  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 40/127]  eta: 0:01:29  lr: 0.000095  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 50/127]  eta: 0:01:18  lr: 0.000095  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 70/127]  eta: 0:00:57  lr: 0.000095  loss: nan (nan)  time: 0.9817  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 80/127]  eta: 0:00:47  lr: 0.000095  loss: nan (nan)  time: 0.9821  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [ 90/127]  eta: 0:00:36  lr: 0.000095  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:142]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:142] Total time: 0:02:06 (0.9942 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:142]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5325  data: 0.3866  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:142]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1145  data: 0.0277  max mem: 34254\n",
      "Valid: [epoch:142] Total time: 0:00:29 (2.1235 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_142_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [  0/127]  eta: 0:06:58  lr: 0.000095  loss: nan (nan)  time: 3.2968  data: 2.3351  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 10/127]  eta: 0:02:18  lr: 0.000095  loss: nan (nan)  time: 1.1827  data: 0.2124  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 20/127]  eta: 0:01:55  lr: 0.000095  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 30/127]  eta: 0:01:41  lr: 0.000095  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 40/127]  eta: 0:01:29  lr: 0.000095  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 50/127]  eta: 0:01:18  lr: 0.000095  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 70/127]  eta: 0:00:57  lr: 0.000095  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 80/127]  eta: 0:00:47  lr: 0.000095  loss: nan (nan)  time: 0.9800  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [ 90/127]  eta: 0:00:37  lr: 0.000095  loss: nan (nan)  time: 0.9805  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9793  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9823  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:143]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9822  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:143] Total time: 0:02:06 (0.9965 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:143]  [ 0/14]  eta: 0:00:34  loss: nan (nan)  time: 2.4976  data: 0.3713  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:143]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1105  data: 0.0266  max mem: 34254\n",
      "Valid: [epoch:143] Total time: 0:00:29 (2.1185 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_143_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [  0/127]  eta: 0:06:51  lr: 0.000095  loss: nan (nan)  time: 3.2409  data: 2.2309  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 10/127]  eta: 0:02:17  lr: 0.000095  loss: nan (nan)  time: 1.1779  data: 0.2029  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 20/127]  eta: 0:01:55  lr: 0.000095  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 30/127]  eta: 0:01:41  lr: 0.000095  loss: nan (nan)  time: 0.9716  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 40/127]  eta: 0:01:29  lr: 0.000095  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 50/127]  eta: 0:01:18  lr: 0.000095  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 70/127]  eta: 0:00:57  lr: 0.000095  loss: nan (nan)  time: 0.9795  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 80/127]  eta: 0:00:47  lr: 0.000095  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [ 90/127]  eta: 0:00:36  lr: 0.000095  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:144]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:144] Total time: 0:02:06 (0.9942 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:144]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5321  data: 0.3746  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:144]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1642  data: 0.0269  max mem: 34254\n",
      "Valid: [epoch:144] Total time: 0:00:30 (2.1731 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_144_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [  0/127]  eta: 0:06:44  lr: 0.000095  loss: nan (nan)  time: 3.1882  data: 2.2106  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 10/127]  eta: 0:02:17  lr: 0.000095  loss: nan (nan)  time: 1.1732  data: 0.2010  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 20/127]  eta: 0:01:55  lr: 0.000095  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 30/127]  eta: 0:01:41  lr: 0.000095  loss: nan (nan)  time: 0.9803  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 40/127]  eta: 0:01:29  lr: 0.000095  loss: nan (nan)  time: 0.9815  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 50/127]  eta: 0:01:18  lr: 0.000095  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 70/127]  eta: 0:00:57  lr: 0.000095  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 80/127]  eta: 0:00:47  lr: 0.000095  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [ 90/127]  eta: 0:00:37  lr: 0.000095  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:145]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:145] Total time: 0:02:06 (0.9950 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:145]  [ 0/14]  eta: 0:00:34  loss: nan (nan)  time: 2.4727  data: 0.3718  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:145]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1118  data: 0.0266  max mem: 34254\n",
      "Valid: [epoch:145] Total time: 0:00:29 (2.1207 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_145_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [  0/127]  eta: 0:05:22  lr: 0.000095  loss: nan (nan)  time: 2.5431  data: 1.5802  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 10/127]  eta: 0:02:11  lr: 0.000095  loss: nan (nan)  time: 1.1237  data: 0.1438  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 20/127]  eta: 0:01:52  lr: 0.000095  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 30/127]  eta: 0:01:39  lr: 0.000095  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 40/127]  eta: 0:01:28  lr: 0.000095  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 50/127]  eta: 0:01:17  lr: 0.000095  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 70/127]  eta: 0:00:56  lr: 0.000095  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 80/127]  eta: 0:00:46  lr: 0.000095  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [ 90/127]  eta: 0:00:36  lr: 0.000095  loss: nan (nan)  time: 0.9789  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9793  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:146]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:146] Total time: 0:02:05 (0.9887 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:146]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6054  data: 0.4446  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:146]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2104  data: 0.0319  max mem: 34254\n",
      "Valid: [epoch:146] Total time: 0:00:31 (2.2223 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_146_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [  0/127]  eta: 0:05:25  lr: 0.000095  loss: nan (nan)  time: 2.5604  data: 1.5998  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 10/127]  eta: 0:02:10  lr: 0.000095  loss: nan (nan)  time: 1.1147  data: 0.1456  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 20/127]  eta: 0:01:52  lr: 0.000095  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 30/127]  eta: 0:01:39  lr: 0.000095  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 40/127]  eta: 0:01:28  lr: 0.000095  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 50/127]  eta: 0:01:17  lr: 0.000095  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 70/127]  eta: 0:00:56  lr: 0.000095  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 80/127]  eta: 0:00:46  lr: 0.000095  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [ 90/127]  eta: 0:00:36  lr: 0.000095  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:147]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:147] Total time: 0:02:05 (0.9888 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:147]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5251  data: 0.3890  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:147]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1627  data: 0.0279  max mem: 34254\n",
      "Valid: [epoch:147] Total time: 0:00:30 (2.1740 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_147_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [  0/127]  eta: 0:06:26  lr: 0.000095  loss: nan (nan)  time: 3.0425  data: 2.0401  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 10/127]  eta: 0:02:15  lr: 0.000095  loss: nan (nan)  time: 1.1592  data: 0.1856  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 20/127]  eta: 0:01:54  lr: 0.000095  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 30/127]  eta: 0:01:40  lr: 0.000095  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 40/127]  eta: 0:01:29  lr: 0.000095  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 50/127]  eta: 0:01:18  lr: 0.000095  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 70/127]  eta: 0:00:57  lr: 0.000095  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 80/127]  eta: 0:00:46  lr: 0.000095  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [ 90/127]  eta: 0:00:36  lr: 0.000095  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:148]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:148] Total time: 0:02:06 (0.9922 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:148]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.5745  data: 0.4211  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:148]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1874  data: 0.0302  max mem: 34254\n",
      "Valid: [epoch:148] Total time: 0:00:30 (2.1997 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_148_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [  0/127]  eta: 0:05:50  lr: 0.000095  loss: nan (nan)  time: 2.7615  data: 1.8029  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 10/127]  eta: 0:02:12  lr: 0.000095  loss: nan (nan)  time: 1.1341  data: 0.1640  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 20/127]  eta: 0:01:53  lr: 0.000095  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 30/127]  eta: 0:01:39  lr: 0.000095  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 40/127]  eta: 0:01:28  lr: 0.000095  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 50/127]  eta: 0:01:17  lr: 0.000095  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 70/127]  eta: 0:00:56  lr: 0.000095  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 80/127]  eta: 0:00:46  lr: 0.000095  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [ 90/127]  eta: 0:00:36  lr: 0.000095  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:149]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:149] Total time: 0:02:05 (0.9896 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:149]  [ 0/14]  eta: 0:00:40  loss: nan (nan)  time: 2.8830  data: 0.3836  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:149]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.4917  data: 0.0275  max mem: 34254\n",
      "Valid: [epoch:149] Total time: 0:00:35 (2.5011 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_149_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [  0/127]  eta: 0:05:48  lr: 0.000095  loss: nan (nan)  time: 2.7405  data: 1.7807  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 10/127]  eta: 0:02:12  lr: 0.000095  loss: nan (nan)  time: 1.1332  data: 0.1620  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 20/127]  eta: 0:01:53  lr: 0.000095  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 30/127]  eta: 0:01:40  lr: 0.000095  loss: nan (nan)  time: 0.9796  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 40/127]  eta: 0:01:28  lr: 0.000095  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 50/127]  eta: 0:01:17  lr: 0.000095  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 60/127]  eta: 0:01:07  lr: 0.000095  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 70/127]  eta: 0:00:57  lr: 0.000095  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 80/127]  eta: 0:00:46  lr: 0.000095  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [ 90/127]  eta: 0:00:36  lr: 0.000095  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [100/127]  eta: 0:00:26  lr: 0.000095  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [110/127]  eta: 0:00:16  lr: 0.000095  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [120/127]  eta: 0:00:06  lr: 0.000095  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:150]  [126/127]  eta: 0:00:00  lr: 0.000095  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:150] Total time: 0:02:05 (0.9912 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:150]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5065  data: 0.3769  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:150]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.3539  data: 0.0270  max mem: 34254\n",
      "Valid: [epoch:150] Total time: 0:00:33 (2.3622 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_150_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [  0/127]  eta: 0:06:11  lr: 0.000094  loss: nan (nan)  time: 2.9248  data: 1.9720  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 10/127]  eta: 0:02:14  lr: 0.000094  loss: nan (nan)  time: 1.1483  data: 0.1794  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 20/127]  eta: 0:01:53  lr: 0.000094  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 30/127]  eta: 0:01:40  lr: 0.000094  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 40/127]  eta: 0:01:28  lr: 0.000094  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 50/127]  eta: 0:01:17  lr: 0.000094  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 60/127]  eta: 0:01:07  lr: 0.000094  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 70/127]  eta: 0:00:57  lr: 0.000094  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 80/127]  eta: 0:00:46  lr: 0.000094  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [ 90/127]  eta: 0:00:36  lr: 0.000094  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [100/127]  eta: 0:00:26  lr: 0.000094  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [110/127]  eta: 0:00:16  lr: 0.000094  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:151]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:151] Total time: 0:02:05 (0.9918 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:151]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6663  data: 0.3871  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:151]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2858  data: 0.0278  max mem: 34254\n",
      "Valid: [epoch:151] Total time: 0:00:32 (2.2984 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_151_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [  0/127]  eta: 0:05:49  lr: 0.000094  loss: nan (nan)  time: 2.7496  data: 1.7911  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 10/127]  eta: 0:02:12  lr: 0.000094  loss: nan (nan)  time: 1.1346  data: 0.1629  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 20/127]  eta: 0:01:53  lr: 0.000094  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 30/127]  eta: 0:01:40  lr: 0.000094  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 40/127]  eta: 0:01:28  lr: 0.000094  loss: nan (nan)  time: 0.9780  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 50/127]  eta: 0:01:17  lr: 0.000094  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 60/127]  eta: 0:01:07  lr: 0.000094  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 70/127]  eta: 0:00:57  lr: 0.000094  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 80/127]  eta: 0:00:46  lr: 0.000094  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [ 90/127]  eta: 0:00:36  lr: 0.000094  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [100/127]  eta: 0:00:26  lr: 0.000094  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [110/127]  eta: 0:00:16  lr: 0.000094  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:152]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:152] Total time: 0:02:05 (0.9908 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:152]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5296  data: 0.3851  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:152]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1508  data: 0.0276  max mem: 34254\n",
      "Valid: [epoch:152] Total time: 0:00:30 (2.1600 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_152_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [  0/127]  eta: 0:07:10  lr: 0.000094  loss: nan (nan)  time: 3.3911  data: 2.4323  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 10/127]  eta: 0:02:19  lr: 0.000094  loss: nan (nan)  time: 1.1915  data: 0.2212  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 20/127]  eta: 0:01:56  lr: 0.000094  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 30/127]  eta: 0:01:41  lr: 0.000094  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 40/127]  eta: 0:01:29  lr: 0.000094  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 50/127]  eta: 0:01:18  lr: 0.000094  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 60/127]  eta: 0:01:07  lr: 0.000094  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 70/127]  eta: 0:00:57  lr: 0.000094  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 80/127]  eta: 0:00:47  lr: 0.000094  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [ 90/127]  eta: 0:00:37  lr: 0.000094  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [100/127]  eta: 0:00:26  lr: 0.000094  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [110/127]  eta: 0:00:16  lr: 0.000094  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:153]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:153] Total time: 0:02:06 (0.9954 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:153]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5512  data: 0.3876  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:153]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1661  data: 0.0278  max mem: 34254\n",
      "Valid: [epoch:153] Total time: 0:00:30 (2.1756 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_153_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [  0/127]  eta: 0:06:08  lr: 0.000094  loss: nan (nan)  time: 2.9011  data: 1.9432  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 10/127]  eta: 0:02:14  lr: 0.000094  loss: nan (nan)  time: 1.1476  data: 0.1767  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 20/127]  eta: 0:01:53  lr: 0.000094  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 30/127]  eta: 0:01:40  lr: 0.000094  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 40/127]  eta: 0:01:28  lr: 0.000094  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 50/127]  eta: 0:01:17  lr: 0.000094  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 60/127]  eta: 0:01:07  lr: 0.000094  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 70/127]  eta: 0:00:57  lr: 0.000094  loss: nan (nan)  time: 1.0020  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 80/127]  eta: 0:00:47  lr: 0.000094  loss: nan (nan)  time: 1.0272  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [ 90/127]  eta: 0:00:37  lr: 0.000094  loss: nan (nan)  time: 1.0007  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [100/127]  eta: 0:00:27  lr: 0.000094  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [110/127]  eta: 0:00:17  lr: 0.000094  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:154]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:154] Total time: 0:02:06 (0.9996 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:154]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.5722  data: 0.4409  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:154]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1552  data: 0.0316  max mem: 34254\n",
      "Valid: [epoch:154] Total time: 0:00:30 (2.1644 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_154_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [  0/127]  eta: 0:06:20  lr: 0.000094  loss: nan (nan)  time: 2.9957  data: 2.0368  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 10/127]  eta: 0:02:15  lr: 0.000094  loss: nan (nan)  time: 1.1561  data: 0.1853  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 20/127]  eta: 0:01:54  lr: 0.000094  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 30/127]  eta: 0:01:40  lr: 0.000094  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 40/127]  eta: 0:01:29  lr: 0.000094  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 50/127]  eta: 0:01:18  lr: 0.000094  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 60/127]  eta: 0:01:07  lr: 0.000094  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 70/127]  eta: 0:00:57  lr: 0.000094  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 80/127]  eta: 0:00:46  lr: 0.000094  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [ 90/127]  eta: 0:00:36  lr: 0.000094  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [100/127]  eta: 0:00:26  lr: 0.000094  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [110/127]  eta: 0:00:16  lr: 0.000094  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:155]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:155] Total time: 0:02:06 (0.9923 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:155]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5423  data: 0.3777  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:155]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.4263  data: 0.0271  max mem: 34254\n",
      "Valid: [epoch:155] Total time: 0:00:34 (2.4379 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_155_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [  0/127]  eta: 0:05:34  lr: 0.000094  loss: nan (nan)  time: 2.6337  data: 1.6754  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 10/127]  eta: 0:02:12  lr: 0.000094  loss: nan (nan)  time: 1.1352  data: 0.1524  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 20/127]  eta: 0:01:53  lr: 0.000094  loss: nan (nan)  time: 0.9803  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 30/127]  eta: 0:01:40  lr: 0.000094  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 40/127]  eta: 0:01:28  lr: 0.000094  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 50/127]  eta: 0:01:17  lr: 0.000094  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 60/127]  eta: 0:01:07  lr: 0.000094  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 70/127]  eta: 0:00:56  lr: 0.000094  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 80/127]  eta: 0:00:46  lr: 0.000094  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [ 90/127]  eta: 0:00:36  lr: 0.000094  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [100/127]  eta: 0:00:26  lr: 0.000094  loss: nan (nan)  time: 0.9799  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [110/127]  eta: 0:00:16  lr: 0.000094  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:156]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:156] Total time: 0:02:05 (0.9914 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:156]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5227  data: 0.3756  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:156]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2177  data: 0.0269  max mem: 34254\n",
      "Valid: [epoch:156] Total time: 0:00:31 (2.2296 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_156_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [  0/127]  eta: 0:06:43  lr: 0.000094  loss: nan (nan)  time: 3.1741  data: 2.2185  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 10/127]  eta: 0:02:17  lr: 0.000094  loss: nan (nan)  time: 1.1783  data: 0.2018  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 20/127]  eta: 0:01:55  lr: 0.000094  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 30/127]  eta: 0:01:41  lr: 0.000094  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 40/127]  eta: 0:01:29  lr: 0.000094  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 50/127]  eta: 0:01:18  lr: 0.000094  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 60/127]  eta: 0:01:07  lr: 0.000094  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 70/127]  eta: 0:00:57  lr: 0.000094  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 80/127]  eta: 0:00:47  lr: 0.000094  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [ 90/127]  eta: 0:00:36  lr: 0.000094  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [100/127]  eta: 0:00:26  lr: 0.000094  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [110/127]  eta: 0:00:16  lr: 0.000094  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:157]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:157] Total time: 0:02:06 (0.9943 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:157]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7248  data: 0.5049  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:157]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.4470  data: 0.0362  max mem: 34254\n",
      "Valid: [epoch:157] Total time: 0:00:34 (2.4567 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_157_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [  0/127]  eta: 0:05:32  lr: 0.000094  loss: nan (nan)  time: 2.6153  data: 1.6594  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 10/127]  eta: 0:02:11  lr: 0.000094  loss: nan (nan)  time: 1.1198  data: 0.1509  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 20/127]  eta: 0:01:52  lr: 0.000094  loss: nan (nan)  time: 0.9708  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 30/127]  eta: 0:01:39  lr: 0.000094  loss: nan (nan)  time: 0.9715  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 40/127]  eta: 0:01:28  lr: 0.000094  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 50/127]  eta: 0:01:17  lr: 0.000094  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 60/127]  eta: 0:01:06  lr: 0.000094  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 70/127]  eta: 0:00:56  lr: 0.000094  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 80/127]  eta: 0:00:46  lr: 0.000094  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [ 90/127]  eta: 0:00:36  lr: 0.000094  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [100/127]  eta: 0:00:26  lr: 0.000094  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [110/127]  eta: 0:00:16  lr: 0.000094  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:158]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:158] Total time: 0:02:05 (0.9889 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:158]  [ 0/14]  eta: 0:00:40  loss: nan (nan)  time: 2.8935  data: 0.4213  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:158]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.3386  data: 0.0302  max mem: 34254\n",
      "Valid: [epoch:158] Total time: 0:00:32 (2.3512 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_158_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [  0/127]  eta: 0:06:44  lr: 0.000094  loss: nan (nan)  time: 3.1832  data: 2.2221  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 10/127]  eta: 0:02:17  lr: 0.000094  loss: nan (nan)  time: 1.1723  data: 0.2021  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 20/127]  eta: 0:01:55  lr: 0.000094  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 30/127]  eta: 0:01:41  lr: 0.000094  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 40/127]  eta: 0:01:29  lr: 0.000094  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 50/127]  eta: 0:01:18  lr: 0.000094  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 60/127]  eta: 0:01:07  lr: 0.000094  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 70/127]  eta: 0:00:57  lr: 0.000094  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 80/127]  eta: 0:00:47  lr: 0.000094  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [ 90/127]  eta: 0:00:36  lr: 0.000094  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [100/127]  eta: 0:00:26  lr: 0.000094  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [110/127]  eta: 0:00:16  lr: 0.000094  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [120/127]  eta: 0:00:06  lr: 0.000094  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:159]  [126/127]  eta: 0:00:00  lr: 0.000094  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:159] Total time: 0:02:06 (0.9939 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:159]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7441  data: 0.4709  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:159]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2770  data: 0.0337  max mem: 34254\n",
      "Valid: [epoch:159] Total time: 0:00:32 (2.2871 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_159_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [  0/127]  eta: 0:06:15  lr: 0.000093  loss: nan (nan)  time: 2.9595  data: 2.0039  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 10/127]  eta: 0:02:14  lr: 0.000093  loss: nan (nan)  time: 1.1516  data: 0.1823  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 20/127]  eta: 0:01:54  lr: 0.000093  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 30/127]  eta: 0:01:40  lr: 0.000093  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 40/127]  eta: 0:01:28  lr: 0.000093  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 50/127]  eta: 0:01:17  lr: 0.000093  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 70/127]  eta: 0:00:57  lr: 0.000093  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 80/127]  eta: 0:00:46  lr: 0.000093  loss: nan (nan)  time: 0.9754  data: 0.0002  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [ 90/127]  eta: 0:00:36  lr: 0.000093  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:160]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:160]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:160] Total time: 0:02:06 (0.9924 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:160]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5260  data: 0.3903  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:160]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2466  data: 0.0280  max mem: 34254\n",
      "Valid: [epoch:160] Total time: 0:00:31 (2.2568 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_160_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [  0/127]  eta: 0:06:45  lr: 0.000093  loss: nan (nan)  time: 3.1926  data: 2.2332  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 10/127]  eta: 0:02:17  lr: 0.000093  loss: nan (nan)  time: 1.1733  data: 0.2031  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 20/127]  eta: 0:01:55  lr: 0.000093  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 30/127]  eta: 0:01:41  lr: 0.000093  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 40/127]  eta: 0:01:29  lr: 0.000093  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 50/127]  eta: 0:01:18  lr: 0.000093  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9798  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 70/127]  eta: 0:00:57  lr: 0.000093  loss: nan (nan)  time: 0.9806  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 80/127]  eta: 0:00:47  lr: 0.000093  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [ 90/127]  eta: 0:00:36  lr: 0.000093  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:161]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:161] Total time: 0:02:06 (0.9944 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:161]  [ 0/14]  eta: 0:00:40  loss: nan (nan)  time: 2.8673  data: 0.3770  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:161]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.3567  data: 0.0270  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:161] Total time: 0:00:33 (2.3668 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_161_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [  0/127]  eta: 0:06:40  lr: 0.000093  loss: nan (nan)  time: 3.1540  data: 2.1925  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 10/127]  eta: 0:02:16  lr: 0.000093  loss: nan (nan)  time: 1.1699  data: 0.1994  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 20/127]  eta: 0:01:55  lr: 0.000093  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 30/127]  eta: 0:01:41  lr: 0.000093  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 40/127]  eta: 0:01:29  lr: 0.000093  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 50/127]  eta: 0:01:18  lr: 0.000093  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 70/127]  eta: 0:00:57  lr: 0.000093  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 80/127]  eta: 0:00:47  lr: 0.000093  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [ 90/127]  eta: 0:00:36  lr: 0.000093  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:162]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:162] Total time: 0:02:06 (0.9941 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:162]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6369  data: 0.4239  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:162]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2227  data: 0.0304  max mem: 34254\n",
      "Valid: [epoch:162] Total time: 0:00:31 (2.2343 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_162_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [  0/127]  eta: 0:07:22  lr: 0.000093  loss: nan (nan)  time: 3.4843  data: 2.4448  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 10/127]  eta: 0:02:20  lr: 0.000093  loss: nan (nan)  time: 1.1999  data: 0.2224  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 20/127]  eta: 0:01:56  lr: 0.000093  loss: nan (nan)  time: 0.9709  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 30/127]  eta: 0:01:42  lr: 0.000093  loss: nan (nan)  time: 0.9708  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 40/127]  eta: 0:01:29  lr: 0.000093  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 50/127]  eta: 0:01:18  lr: 0.000093  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 70/127]  eta: 0:00:57  lr: 0.000093  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 80/127]  eta: 0:00:47  lr: 0.000093  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [ 90/127]  eta: 0:00:37  lr: 0.000093  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:163]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:163] Total time: 0:02:06 (0.9964 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:163]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5233  data: 0.3730  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:163]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2867  data: 0.0267  max mem: 34254\n",
      "Valid: [epoch:163] Total time: 0:00:32 (2.2979 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_163_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [  0/127]  eta: 0:05:35  lr: 0.000093  loss: nan (nan)  time: 2.6438  data: 1.6896  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 10/127]  eta: 0:02:11  lr: 0.000093  loss: nan (nan)  time: 1.1244  data: 0.1537  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 20/127]  eta: 0:01:52  lr: 0.000093  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 30/127]  eta: 0:01:39  lr: 0.000093  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 40/127]  eta: 0:01:28  lr: 0.000093  loss: nan (nan)  time: 0.9798  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 50/127]  eta: 0:01:17  lr: 0.000093  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 70/127]  eta: 0:00:56  lr: 0.000093  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 80/127]  eta: 0:00:46  lr: 0.000093  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [ 90/127]  eta: 0:00:36  lr: 0.000093  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:164]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9779  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:164] Total time: 0:02:05 (0.9911 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:164]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5344  data: 0.3721  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:164]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2096  data: 0.0267  max mem: 34254\n",
      "Valid: [epoch:164] Total time: 0:00:31 (2.2229 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_164_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [  0/127]  eta: 0:07:16  lr: 0.000093  loss: nan (nan)  time: 3.4357  data: 2.4811  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 10/127]  eta: 0:02:19  lr: 0.000093  loss: nan (nan)  time: 1.1953  data: 0.2257  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 20/127]  eta: 0:01:56  lr: 0.000093  loss: nan (nan)  time: 0.9715  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 30/127]  eta: 0:01:41  lr: 0.000093  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 40/127]  eta: 0:01:29  lr: 0.000093  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 50/127]  eta: 0:01:18  lr: 0.000093  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 70/127]  eta: 0:00:57  lr: 0.000093  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 80/127]  eta: 0:00:47  lr: 0.000093  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [ 90/127]  eta: 0:00:37  lr: 0.000093  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:165]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:165] Total time: 0:02:06 (0.9952 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:165]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6252  data: 0.4269  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:165]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1922  data: 0.0306  max mem: 34254\n",
      "Valid: [epoch:165] Total time: 0:00:30 (2.2039 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_165_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [  0/127]  eta: 0:06:36  lr: 0.000093  loss: nan (nan)  time: 3.1245  data: 2.1676  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 10/127]  eta: 0:02:16  lr: 0.000093  loss: nan (nan)  time: 1.1672  data: 0.1972  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 20/127]  eta: 0:01:54  lr: 0.000093  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 30/127]  eta: 0:01:41  lr: 0.000093  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 40/127]  eta: 0:01:29  lr: 0.000093  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 50/127]  eta: 0:01:18  lr: 0.000093  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 70/127]  eta: 0:00:57  lr: 0.000093  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 80/127]  eta: 0:00:47  lr: 0.000093  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [ 90/127]  eta: 0:00:36  lr: 0.000093  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:166]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:166] Total time: 0:02:06 (0.9930 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:166]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.5792  data: 0.4195  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:166]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1615  data: 0.0301  max mem: 34254\n",
      "Valid: [epoch:166] Total time: 0:00:30 (2.1710 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_166_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [  0/127]  eta: 0:06:38  lr: 0.000093  loss: nan (nan)  time: 3.1395  data: 2.1868  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 10/127]  eta: 0:02:16  lr: 0.000093  loss: nan (nan)  time: 1.1666  data: 0.1989  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 20/127]  eta: 0:01:54  lr: 0.000093  loss: nan (nan)  time: 0.9712  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 30/127]  eta: 0:01:41  lr: 0.000093  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 40/127]  eta: 0:01:29  lr: 0.000093  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 50/127]  eta: 0:01:18  lr: 0.000093  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 70/127]  eta: 0:00:57  lr: 0.000093  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 80/127]  eta: 0:00:47  lr: 0.000093  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [ 90/127]  eta: 0:00:36  lr: 0.000093  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:167]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:167] Total time: 0:02:06 (0.9935 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:167]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6173  data: 0.4153  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:167]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1717  data: 0.0298  max mem: 34254\n",
      "Valid: [epoch:167] Total time: 0:00:30 (2.1817 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_167_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [  0/127]  eta: 0:06:14  lr: 0.000093  loss: nan (nan)  time: 2.9481  data: 1.9901  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 10/127]  eta: 0:02:14  lr: 0.000093  loss: nan (nan)  time: 1.1515  data: 0.1810  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 20/127]  eta: 0:01:54  lr: 0.000093  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 30/127]  eta: 0:01:40  lr: 0.000093  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 40/127]  eta: 0:01:28  lr: 0.000093  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 50/127]  eta: 0:01:17  lr: 0.000093  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 60/127]  eta: 0:01:07  lr: 0.000093  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 70/127]  eta: 0:00:57  lr: 0.000093  loss: nan (nan)  time: 0.9810  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 80/127]  eta: 0:00:46  lr: 0.000093  loss: nan (nan)  time: 0.9811  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [ 90/127]  eta: 0:00:36  lr: 0.000093  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [100/127]  eta: 0:00:26  lr: 0.000093  loss: nan (nan)  time: 0.9817  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [110/127]  eta: 0:00:16  lr: 0.000093  loss: nan (nan)  time: 0.9810  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [120/127]  eta: 0:00:06  lr: 0.000093  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:168]  [126/127]  eta: 0:00:00  lr: 0.000093  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:168] Total time: 0:02:06 (0.9935 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:168]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5299  data: 0.4346  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:168]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1558  data: 0.0311  max mem: 34254\n",
      "Valid: [epoch:168] Total time: 0:00:30 (2.1650 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_168_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [  0/127]  eta: 0:07:08  lr: 0.000092  loss: nan (nan)  time: 3.3707  data: 2.4114  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 10/127]  eta: 0:02:19  lr: 0.000092  loss: nan (nan)  time: 1.1897  data: 0.2193  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 20/127]  eta: 0:01:56  lr: 0.000092  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 30/127]  eta: 0:01:42  lr: 0.000092  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 40/127]  eta: 0:01:29  lr: 0.000092  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 50/127]  eta: 0:01:18  lr: 0.000092  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 70/127]  eta: 0:00:57  lr: 0.000092  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 80/127]  eta: 0:00:47  lr: 0.000092  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [ 90/127]  eta: 0:00:37  lr: 0.000092  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:169]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:169] Total time: 0:02:06 (0.9943 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:169]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7500  data: 0.4422  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:169]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2807  data: 0.0317  max mem: 34254\n",
      "Valid: [epoch:169] Total time: 0:00:32 (2.2913 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_169_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [  0/127]  eta: 0:06:20  lr: 0.000092  loss: nan (nan)  time: 2.9985  data: 2.0462  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 10/127]  eta: 0:02:15  lr: 0.000092  loss: nan (nan)  time: 1.1556  data: 0.1861  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 20/127]  eta: 0:01:54  lr: 0.000092  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 30/127]  eta: 0:01:41  lr: 0.000092  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 40/127]  eta: 0:01:29  lr: 0.000092  loss: nan (nan)  time: 0.9796  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 50/127]  eta: 0:01:18  lr: 0.000092  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 70/127]  eta: 0:00:57  lr: 0.000092  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 80/127]  eta: 0:00:47  lr: 0.000092  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [ 90/127]  eta: 0:00:36  lr: 0.000092  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9827  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:170]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9830  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:170] Total time: 0:02:06 (0.9937 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:170]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6351  data: 0.4134  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:170]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2267  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:170] Total time: 0:00:31 (2.2377 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_170_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [  0/127]  eta: 0:07:20  lr: 0.000092  loss: nan (nan)  time: 3.4649  data: 2.5019  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 10/127]  eta: 0:02:20  lr: 0.000092  loss: nan (nan)  time: 1.1988  data: 0.2275  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 20/127]  eta: 0:01:56  lr: 0.000092  loss: nan (nan)  time: 0.9717  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 30/127]  eta: 0:01:42  lr: 0.000092  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 40/127]  eta: 0:01:29  lr: 0.000092  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 50/127]  eta: 0:01:18  lr: 0.000092  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 70/127]  eta: 0:00:57  lr: 0.000092  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 80/127]  eta: 0:00:47  lr: 0.000092  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [ 90/127]  eta: 0:00:37  lr: 0.000092  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:171]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:171] Total time: 0:02:06 (0.9955 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:171]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6678  data: 0.3761  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:171]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2372  data: 0.0270  max mem: 34254\n",
      "Valid: [epoch:171] Total time: 0:00:31 (2.2491 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_171_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [  0/127]  eta: 0:06:09  lr: 0.000092  loss: nan (nan)  time: 2.9068  data: 1.9486  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 10/127]  eta: 0:02:14  lr: 0.000092  loss: nan (nan)  time: 1.1462  data: 0.1772  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 20/127]  eta: 0:01:53  lr: 0.000092  loss: nan (nan)  time: 0.9703  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 30/127]  eta: 0:01:40  lr: 0.000092  loss: nan (nan)  time: 0.9707  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 40/127]  eta: 0:01:28  lr: 0.000092  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 50/127]  eta: 0:01:17  lr: 0.000092  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 70/127]  eta: 0:00:56  lr: 0.000092  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 80/127]  eta: 0:00:46  lr: 0.000092  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [ 90/127]  eta: 0:00:36  lr: 0.000092  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:172]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:172] Total time: 0:02:05 (0.9908 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:172]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5249  data: 0.3793  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:172]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2396  data: 0.0272  max mem: 34254\n",
      "Valid: [epoch:172] Total time: 0:00:31 (2.2519 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_172_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [  0/127]  eta: 0:06:31  lr: 0.000092  loss: nan (nan)  time: 3.0791  data: 2.1151  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 10/127]  eta: 0:02:16  lr: 0.000092  loss: nan (nan)  time: 1.1627  data: 0.1924  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 20/127]  eta: 0:01:54  lr: 0.000092  loss: nan (nan)  time: 0.9712  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 30/127]  eta: 0:01:40  lr: 0.000092  loss: nan (nan)  time: 0.9714  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 40/127]  eta: 0:01:29  lr: 0.000092  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 50/127]  eta: 0:01:18  lr: 0.000092  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 70/127]  eta: 0:00:57  lr: 0.000092  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 80/127]  eta: 0:00:46  lr: 0.000092  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [ 90/127]  eta: 0:00:36  lr: 0.000092  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:173]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:173] Total time: 0:02:06 (0.9925 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:173]  [ 0/14]  eta: 0:00:41  loss: nan (nan)  time: 2.9625  data: 0.4265  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:173]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2399  data: 0.0306  max mem: 34254\n",
      "Valid: [epoch:173] Total time: 0:00:31 (2.2516 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_173_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [  0/127]  eta: 0:06:17  lr: 0.000092  loss: nan (nan)  time: 2.9748  data: 2.0176  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 10/127]  eta: 0:02:14  lr: 0.000092  loss: nan (nan)  time: 1.1520  data: 0.1835  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 20/127]  eta: 0:01:54  lr: 0.000092  loss: nan (nan)  time: 0.9705  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 30/127]  eta: 0:01:40  lr: 0.000092  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 40/127]  eta: 0:01:28  lr: 0.000092  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 50/127]  eta: 0:01:17  lr: 0.000092  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 70/127]  eta: 0:00:57  lr: 0.000092  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 80/127]  eta: 0:00:46  lr: 0.000092  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [ 90/127]  eta: 0:00:36  lr: 0.000092  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:174]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:174] Total time: 0:02:05 (0.9912 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:174]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6549  data: 0.4452  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:174]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2598  data: 0.0319  max mem: 34254\n",
      "Valid: [epoch:174] Total time: 0:00:31 (2.2717 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_174_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [  0/127]  eta: 0:06:19  lr: 0.000092  loss: nan (nan)  time: 2.9853  data: 2.0306  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 10/127]  eta: 0:02:15  lr: 0.000092  loss: nan (nan)  time: 1.1542  data: 0.1847  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 20/127]  eta: 0:01:54  lr: 0.000092  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 30/127]  eta: 0:01:40  lr: 0.000092  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 40/127]  eta: 0:01:28  lr: 0.000092  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 50/127]  eta: 0:01:17  lr: 0.000092  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 70/127]  eta: 0:00:57  lr: 0.000092  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 80/127]  eta: 0:00:46  lr: 0.000092  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [ 90/127]  eta: 0:00:36  lr: 0.000092  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:175]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:175] Total time: 0:02:05 (0.9919 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:175]  [ 0/14]  eta: 0:00:41  loss: nan (nan)  time: 2.9484  data: 0.4437  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:175]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.4784  data: 0.0318  max mem: 34254\n",
      "Valid: [epoch:175] Total time: 0:00:34 (2.4914 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_175_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [  0/127]  eta: 0:06:15  lr: 0.000092  loss: nan (nan)  time: 2.9549  data: 1.9998  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 10/127]  eta: 0:02:14  lr: 0.000092  loss: nan (nan)  time: 1.1536  data: 0.1819  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 20/127]  eta: 0:01:54  lr: 0.000092  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 30/127]  eta: 0:01:40  lr: 0.000092  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 40/127]  eta: 0:01:28  lr: 0.000092  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 50/127]  eta: 0:01:17  lr: 0.000092  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 70/127]  eta: 0:00:57  lr: 0.000092  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 80/127]  eta: 0:00:46  lr: 0.000092  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [ 90/127]  eta: 0:00:36  lr: 0.000092  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:176]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:176] Total time: 0:02:05 (0.9917 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:176]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5363  data: 0.4035  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:176]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1122  data: 0.0289  max mem: 34254\n",
      "Valid: [epoch:176] Total time: 0:00:29 (2.1211 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_176_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [  0/127]  eta: 0:06:36  lr: 0.000092  loss: nan (nan)  time: 3.1256  data: 2.1680  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 10/127]  eta: 0:02:16  lr: 0.000092  loss: nan (nan)  time: 1.1668  data: 0.1972  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 20/127]  eta: 0:01:54  lr: 0.000092  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 30/127]  eta: 0:01:40  lr: 0.000092  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 40/127]  eta: 0:01:29  lr: 0.000092  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 50/127]  eta: 0:01:18  lr: 0.000092  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 60/127]  eta: 0:01:07  lr: 0.000092  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 70/127]  eta: 0:00:57  lr: 0.000092  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 80/127]  eta: 0:00:47  lr: 0.000092  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [ 90/127]  eta: 0:00:36  lr: 0.000092  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [100/127]  eta: 0:00:26  lr: 0.000092  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [110/127]  eta: 0:00:16  lr: 0.000092  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [120/127]  eta: 0:00:06  lr: 0.000092  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:177]  [126/127]  eta: 0:00:00  lr: 0.000092  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:177] Total time: 0:02:06 (0.9928 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:177]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.5910  data: 0.4322  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:177]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1691  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:177] Total time: 0:00:30 (2.1781 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_177_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [  0/127]  eta: 0:06:33  lr: 0.000091  loss: nan (nan)  time: 3.0971  data: 2.1375  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 10/127]  eta: 0:02:16  lr: 0.000091  loss: nan (nan)  time: 1.1664  data: 0.1944  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 20/127]  eta: 0:01:55  lr: 0.000091  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 30/127]  eta: 0:01:41  lr: 0.000091  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 40/127]  eta: 0:01:29  lr: 0.000091  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 50/127]  eta: 0:01:18  lr: 0.000091  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 60/127]  eta: 0:01:07  lr: 0.000091  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 80/127]  eta: 0:00:47  lr: 0.000091  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [ 90/127]  eta: 0:00:36  lr: 0.000091  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:178]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:178] Total time: 0:02:06 (0.9938 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:178]  [ 0/14]  eta: 0:00:40  loss: nan (nan)  time: 2.8686  data: 0.3709  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:178]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.5012  data: 0.0266  max mem: 34254\n",
      "Valid: [epoch:178] Total time: 0:00:35 (2.5111 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_178_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [  0/127]  eta: 0:06:36  lr: 0.000091  loss: nan (nan)  time: 3.1251  data: 2.1673  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 10/127]  eta: 0:02:16  lr: 0.000091  loss: nan (nan)  time: 1.1680  data: 0.1971  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 20/127]  eta: 0:01:55  lr: 0.000091  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 30/127]  eta: 0:01:41  lr: 0.000091  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 40/127]  eta: 0:01:29  lr: 0.000091  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 50/127]  eta: 0:01:18  lr: 0.000091  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 60/127]  eta: 0:01:07  lr: 0.000091  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 80/127]  eta: 0:00:47  lr: 0.000091  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [ 90/127]  eta: 0:00:36  lr: 0.000091  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:179]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:179] Total time: 0:02:06 (0.9938 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:179]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5328  data: 0.3793  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:179]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1312  data: 0.0272  max mem: 34254\n",
      "Valid: [epoch:179] Total time: 0:00:29 (2.1405 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_179_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [  0/127]  eta: 0:06:16  lr: 0.000091  loss: nan (nan)  time: 2.9667  data: 2.0099  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 10/127]  eta: 0:02:15  lr: 0.000091  loss: nan (nan)  time: 1.1543  data: 0.1828  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 20/127]  eta: 0:01:54  lr: 0.000091  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 30/127]  eta: 0:01:40  lr: 0.000091  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 40/127]  eta: 0:01:28  lr: 0.000091  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 50/127]  eta: 0:01:18  lr: 0.000091  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 60/127]  eta: 0:01:08  lr: 0.000091  loss: nan (nan)  time: 1.0007  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 1.0030  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 80/127]  eta: 0:00:47  lr: 0.000091  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [ 90/127]  eta: 0:00:37  lr: 0.000091  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:180]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:180] Total time: 0:02:06 (0.9965 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:180]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5575  data: 0.4262  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:180]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1495  data: 0.0305  max mem: 34254\n",
      "Valid: [epoch:180] Total time: 0:00:30 (2.1594 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_180_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [  0/127]  eta: 0:06:02  lr: 0.000091  loss: nan (nan)  time: 2.8527  data: 1.8908  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 10/127]  eta: 0:02:13  lr: 0.000091  loss: nan (nan)  time: 1.1443  data: 0.1720  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 20/127]  eta: 0:01:53  lr: 0.000091  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 30/127]  eta: 0:01:40  lr: 0.000091  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 40/127]  eta: 0:01:28  lr: 0.000091  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 50/127]  eta: 0:01:17  lr: 0.000091  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 60/127]  eta: 0:01:07  lr: 0.000091  loss: nan (nan)  time: 0.9782  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 0.9783  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 80/127]  eta: 0:00:46  lr: 0.000091  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [ 90/127]  eta: 0:00:36  lr: 0.000091  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:181]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:181] Total time: 0:02:06 (0.9926 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:181]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5539  data: 0.4029  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:181]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1564  data: 0.0289  max mem: 34254\n",
      "Valid: [epoch:181] Total time: 0:00:30 (2.1662 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_181_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [  0/127]  eta: 0:05:48  lr: 0.000091  loss: nan (nan)  time: 2.7406  data: 1.7533  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 10/127]  eta: 0:02:12  lr: 0.000091  loss: nan (nan)  time: 1.1339  data: 0.1595  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 20/127]  eta: 0:01:53  lr: 0.000091  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 30/127]  eta: 0:01:40  lr: 0.000091  loss: nan (nan)  time: 0.9797  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 40/127]  eta: 0:01:28  lr: 0.000091  loss: nan (nan)  time: 0.9801  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 50/127]  eta: 0:01:17  lr: 0.000091  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 60/127]  eta: 0:01:07  lr: 0.000091  loss: nan (nan)  time: 0.9783  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 80/127]  eta: 0:00:46  lr: 0.000091  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [ 90/127]  eta: 0:00:36  lr: 0.000091  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9871  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9902  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9796  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:182]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:182] Total time: 0:02:06 (0.9942 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:182]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5076  data: 0.4126  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:182]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1103  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:182] Total time: 0:00:29 (2.1193 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_182_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [  0/127]  eta: 0:06:31  lr: 0.000091  loss: nan (nan)  time: 3.0861  data: 2.1299  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 10/127]  eta: 0:02:16  lr: 0.000091  loss: nan (nan)  time: 1.1639  data: 0.1937  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 20/127]  eta: 0:01:54  lr: 0.000091  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 30/127]  eta: 0:01:40  lr: 0.000091  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 40/127]  eta: 0:01:29  lr: 0.000091  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 50/127]  eta: 0:01:18  lr: 0.000091  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 60/127]  eta: 0:01:07  lr: 0.000091  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 0.9810  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 80/127]  eta: 0:00:47  lr: 0.000091  loss: nan (nan)  time: 0.9794  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [ 90/127]  eta: 0:00:36  lr: 0.000091  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:183]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:183] Total time: 0:02:06 (0.9934 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:183]  [ 0/14]  eta: 0:00:40  loss: nan (nan)  time: 2.8900  data: 0.4032  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:183]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.4874  data: 0.0289  max mem: 34254\n",
      "Valid: [epoch:183] Total time: 0:00:34 (2.4968 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_183_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [  0/127]  eta: 0:06:24  lr: 0.000091  loss: nan (nan)  time: 3.0310  data: 2.0724  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 10/127]  eta: 0:02:15  lr: 0.000091  loss: nan (nan)  time: 1.1596  data: 0.1885  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 20/127]  eta: 0:01:54  lr: 0.000091  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 30/127]  eta: 0:01:41  lr: 0.000091  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 40/127]  eta: 0:01:29  lr: 0.000091  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 50/127]  eta: 0:01:18  lr: 0.000091  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 60/127]  eta: 0:01:07  lr: 0.000091  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 80/127]  eta: 0:00:47  lr: 0.000091  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [ 90/127]  eta: 0:00:36  lr: 0.000091  loss: nan (nan)  time: 0.9800  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9809  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:184]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:184] Total time: 0:02:06 (0.9946 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:184]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5071  data: 0.3783  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:184]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1199  data: 0.0271  max mem: 34254\n",
      "Valid: [epoch:184] Total time: 0:00:29 (2.1298 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_184_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [  0/127]  eta: 0:06:35  lr: 0.000091  loss: nan (nan)  time: 3.1119  data: 2.1548  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 10/127]  eta: 0:02:16  lr: 0.000091  loss: nan (nan)  time: 1.1670  data: 0.1960  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 20/127]  eta: 0:01:55  lr: 0.000091  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 30/127]  eta: 0:01:41  lr: 0.000091  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 40/127]  eta: 0:01:29  lr: 0.000091  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 50/127]  eta: 0:01:18  lr: 0.000091  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 60/127]  eta: 0:01:07  lr: 0.000091  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 80/127]  eta: 0:00:46  lr: 0.000091  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [ 90/127]  eta: 0:00:36  lr: 0.000091  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9798  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9800  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:185]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:185] Total time: 0:02:06 (0.9933 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:185]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5220  data: 0.3839  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:185]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1689  data: 0.0275  max mem: 34254\n",
      "Valid: [epoch:185] Total time: 0:00:30 (2.1777 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_185_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [  0/127]  eta: 0:06:53  lr: 0.000091  loss: nan (nan)  time: 3.2575  data: 2.2205  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 10/127]  eta: 0:02:17  lr: 0.000091  loss: nan (nan)  time: 1.1776  data: 0.2019  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 20/127]  eta: 0:01:55  lr: 0.000091  loss: nan (nan)  time: 0.9704  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 30/127]  eta: 0:01:41  lr: 0.000091  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 40/127]  eta: 0:01:29  lr: 0.000091  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 50/127]  eta: 0:01:18  lr: 0.000091  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 60/127]  eta: 0:01:07  lr: 0.000091  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 70/127]  eta: 0:00:57  lr: 0.000091  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 80/127]  eta: 0:00:47  lr: 0.000091  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [ 90/127]  eta: 0:00:37  lr: 0.000091  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [100/127]  eta: 0:00:26  lr: 0.000091  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [110/127]  eta: 0:00:16  lr: 0.000091  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [120/127]  eta: 0:00:06  lr: 0.000091  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:186]  [126/127]  eta: 0:00:00  lr: 0.000091  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:186] Total time: 0:02:06 (0.9947 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:186]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6878  data: 0.4404  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:186]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2938  data: 0.0316  max mem: 34254\n",
      "Valid: [epoch:186] Total time: 0:00:32 (2.3052 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_186_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [  0/127]  eta: 0:06:47  lr: 0.000090  loss: nan (nan)  time: 3.2082  data: 2.2495  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 10/127]  eta: 0:02:17  lr: 0.000090  loss: nan (nan)  time: 1.1757  data: 0.2046  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 20/127]  eta: 0:01:55  lr: 0.000090  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 30/127]  eta: 0:01:41  lr: 0.000090  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 40/127]  eta: 0:01:29  lr: 0.000090  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 50/127]  eta: 0:01:18  lr: 0.000090  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 60/127]  eta: 0:01:07  lr: 0.000090  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 70/127]  eta: 0:00:57  lr: 0.000090  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 80/127]  eta: 0:00:47  lr: 0.000090  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [ 90/127]  eta: 0:00:36  lr: 0.000090  loss: nan (nan)  time: 0.9785  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9791  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:187]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9795  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:187] Total time: 0:02:06 (0.9946 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:187]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6918  data: 0.4770  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:187]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2256  data: 0.0342  max mem: 34254\n",
      "Valid: [epoch:187] Total time: 0:00:31 (2.2393 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_187_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [  0/127]  eta: 0:05:51  lr: 0.000090  loss: nan (nan)  time: 2.7679  data: 1.7991  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 10/127]  eta: 0:02:12  lr: 0.000090  loss: nan (nan)  time: 1.1341  data: 0.1637  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 20/127]  eta: 0:01:53  lr: 0.000090  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 30/127]  eta: 0:01:39  lr: 0.000090  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 40/127]  eta: 0:01:28  lr: 0.000090  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 50/127]  eta: 0:01:17  lr: 0.000090  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 60/127]  eta: 0:01:07  lr: 0.000090  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 70/127]  eta: 0:00:56  lr: 0.000090  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 80/127]  eta: 0:00:46  lr: 0.000090  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [ 90/127]  eta: 0:00:36  lr: 0.000090  loss: nan (nan)  time: 0.9815  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9802  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:188]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:188] Total time: 0:02:05 (0.9913 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:188]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7253  data: 0.4258  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:188]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2139  data: 0.0305  max mem: 34254\n",
      "Valid: [epoch:188] Total time: 0:00:31 (2.2235 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_188_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [  0/127]  eta: 0:06:20  lr: 0.000090  loss: nan (nan)  time: 2.9949  data: 2.0409  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 10/127]  eta: 0:02:15  lr: 0.000090  loss: nan (nan)  time: 1.1547  data: 0.1856  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 20/127]  eta: 0:01:54  lr: 0.000090  loss: nan (nan)  time: 0.9709  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 30/127]  eta: 0:01:40  lr: 0.000090  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 40/127]  eta: 0:01:28  lr: 0.000090  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 50/127]  eta: 0:01:17  lr: 0.000090  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 60/127]  eta: 0:01:07  lr: 0.000090  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 70/127]  eta: 0:00:57  lr: 0.000090  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 80/127]  eta: 0:00:46  lr: 0.000090  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [ 90/127]  eta: 0:00:36  lr: 0.000090  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:189]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:189] Total time: 0:02:05 (0.9920 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:189]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5309  data: 0.3801  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:189]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2211  data: 0.0273  max mem: 34254\n",
      "Valid: [epoch:189] Total time: 0:00:31 (2.2332 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_189_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [  0/127]  eta: 0:06:00  lr: 0.000090  loss: nan (nan)  time: 2.8368  data: 1.8804  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 10/127]  eta: 0:02:13  lr: 0.000090  loss: nan (nan)  time: 1.1404  data: 0.1710  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 20/127]  eta: 0:01:53  lr: 0.000090  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 30/127]  eta: 0:01:40  lr: 0.000090  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 40/127]  eta: 0:01:28  lr: 0.000090  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 50/127]  eta: 0:01:17  lr: 0.000090  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 60/127]  eta: 0:01:07  lr: 0.000090  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 70/127]  eta: 0:00:57  lr: 0.000090  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 80/127]  eta: 0:00:46  lr: 0.000090  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [ 90/127]  eta: 0:00:36  lr: 0.000090  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:190]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:190] Total time: 0:02:05 (0.9916 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:190]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5097  data: 0.3734  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:190]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2422  data: 0.0268  max mem: 34254\n",
      "Valid: [epoch:190] Total time: 0:00:31 (2.2538 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_190_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [  0/127]  eta: 0:06:50  lr: 0.000090  loss: nan (nan)  time: 3.2358  data: 2.2785  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 10/127]  eta: 0:02:17  lr: 0.000090  loss: nan (nan)  time: 1.1784  data: 0.2072  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 20/127]  eta: 0:01:55  lr: 0.000090  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 30/127]  eta: 0:01:41  lr: 0.000090  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 40/127]  eta: 0:01:29  lr: 0.000090  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 50/127]  eta: 0:01:18  lr: 0.000090  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 60/127]  eta: 0:01:07  lr: 0.000090  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 70/127]  eta: 0:00:57  lr: 0.000090  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 80/127]  eta: 0:00:47  lr: 0.000090  loss: nan (nan)  time: 0.9783  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [ 90/127]  eta: 0:00:37  lr: 0.000090  loss: nan (nan)  time: 0.9780  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:191]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:191]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:191] Total time: 0:02:06 (0.9950 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:191]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5062  data: 0.3976  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:191]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1220  data: 0.0285  max mem: 34254\n",
      "Valid: [epoch:191] Total time: 0:00:29 (2.1329 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_191_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [  0/127]  eta: 0:06:08  lr: 0.000090  loss: nan (nan)  time: 2.8995  data: 1.9102  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 10/127]  eta: 0:02:14  lr: 0.000090  loss: nan (nan)  time: 1.1456  data: 0.1737  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 20/127]  eta: 0:01:53  lr: 0.000090  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 30/127]  eta: 0:01:40  lr: 0.000090  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 40/127]  eta: 0:01:28  lr: 0.000090  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 50/127]  eta: 0:01:17  lr: 0.000090  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 60/127]  eta: 0:01:07  lr: 0.000090  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 70/127]  eta: 0:00:57  lr: 0.000090  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 80/127]  eta: 0:00:46  lr: 0.000090  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [ 90/127]  eta: 0:00:36  lr: 0.000090  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9821  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9823  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:192]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:192] Total time: 0:02:06 (0.9933 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:192]  [ 0/14]  eta: 0:00:40  loss: nan (nan)  time: 2.8637  data: 0.4271  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:192]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.5024  data: 0.0306  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:192] Total time: 0:00:35 (2.5157 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_192_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [  0/127]  eta: 0:06:05  lr: 0.000090  loss: nan (nan)  time: 2.8797  data: 1.9208  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 10/127]  eta: 0:02:14  lr: 0.000090  loss: nan (nan)  time: 1.1454  data: 0.1747  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 20/127]  eta: 0:01:53  lr: 0.000090  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 30/127]  eta: 0:01:40  lr: 0.000090  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 40/127]  eta: 0:01:28  lr: 0.000090  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 50/127]  eta: 0:01:17  lr: 0.000090  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 60/127]  eta: 0:01:07  lr: 0.000090  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 70/127]  eta: 0:00:57  lr: 0.000090  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 80/127]  eta: 0:00:46  lr: 0.000090  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [ 90/127]  eta: 0:00:36  lr: 0.000090  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9779  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:193]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9782  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:193] Total time: 0:02:05 (0.9921 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:193]  [ 0/14]  eta: 0:00:40  loss: nan (nan)  time: 2.8898  data: 0.3930  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:193]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.4977  data: 0.0282  max mem: 34254\n",
      "Valid: [epoch:193] Total time: 0:00:35 (2.5094 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_193_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [  0/127]  eta: 0:05:46  lr: 0.000090  loss: nan (nan)  time: 2.7257  data: 1.7648  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 10/127]  eta: 0:02:12  lr: 0.000090  loss: nan (nan)  time: 1.1314  data: 0.1605  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 20/127]  eta: 0:01:53  lr: 0.000090  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 30/127]  eta: 0:01:40  lr: 0.000090  loss: nan (nan)  time: 0.9782  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 40/127]  eta: 0:01:28  lr: 0.000090  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 50/127]  eta: 0:01:17  lr: 0.000090  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 60/127]  eta: 0:01:07  lr: 0.000090  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 70/127]  eta: 0:00:57  lr: 0.000090  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 80/127]  eta: 0:00:46  lr: 0.000090  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [ 90/127]  eta: 0:00:36  lr: 0.000090  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:194]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:194] Total time: 0:02:05 (0.9913 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:194]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6772  data: 0.4321  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:194]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2339  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:194] Total time: 0:00:31 (2.2452 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_194_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [  0/127]  eta: 0:05:10  lr: 0.000090  loss: nan (nan)  time: 2.4425  data: 1.4810  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 10/127]  eta: 0:02:09  lr: 0.000090  loss: nan (nan)  time: 1.1041  data: 0.1347  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 20/127]  eta: 0:01:51  lr: 0.000090  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 30/127]  eta: 0:01:38  lr: 0.000090  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 40/127]  eta: 0:01:27  lr: 0.000090  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 50/127]  eta: 0:01:17  lr: 0.000090  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 60/127]  eta: 0:01:06  lr: 0.000090  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 70/127]  eta: 0:00:56  lr: 0.000090  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 80/127]  eta: 0:00:46  lr: 0.000090  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [ 90/127]  eta: 0:00:36  lr: 0.000090  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [100/127]  eta: 0:00:26  lr: 0.000090  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [110/127]  eta: 0:00:16  lr: 0.000090  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [120/127]  eta: 0:00:06  lr: 0.000090  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:195]  [126/127]  eta: 0:00:00  lr: 0.000090  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:195] Total time: 0:02:05 (0.9861 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:195]  [ 0/14]  eta: 0:00:41  loss: nan (nan)  time: 2.9427  data: 0.6428  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:195]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2400  data: 0.0460  max mem: 34254\n",
      "Valid: [epoch:195] Total time: 0:00:31 (2.2516 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_195_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [  0/127]  eta: 0:05:12  lr: 0.000089  loss: nan (nan)  time: 2.4625  data: 1.4803  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 10/127]  eta: 0:02:09  lr: 0.000089  loss: nan (nan)  time: 1.1075  data: 0.1347  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 20/127]  eta: 0:01:51  lr: 0.000089  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 30/127]  eta: 0:01:38  lr: 0.000089  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 40/127]  eta: 0:01:27  lr: 0.000089  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 50/127]  eta: 0:01:17  lr: 0.000089  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 60/127]  eta: 0:01:06  lr: 0.000089  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 70/127]  eta: 0:00:56  lr: 0.000089  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 80/127]  eta: 0:00:46  lr: 0.000089  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [ 90/127]  eta: 0:00:36  lr: 0.000089  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [100/127]  eta: 0:00:26  lr: 0.000089  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9789  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:196]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:196] Total time: 0:02:05 (0.9875 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:196]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6868  data: 0.4547  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:196]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2330  data: 0.0326  max mem: 34254\n",
      "Valid: [epoch:196] Total time: 0:00:31 (2.2467 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_196_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [  0/127]  eta: 0:07:03  lr: 0.000089  loss: nan (nan)  time: 3.3350  data: 2.3641  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 10/127]  eta: 0:02:18  lr: 0.000089  loss: nan (nan)  time: 1.1869  data: 0.2150  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 20/127]  eta: 0:01:55  lr: 0.000089  loss: nan (nan)  time: 0.9712  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 30/127]  eta: 0:01:41  lr: 0.000089  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 40/127]  eta: 0:01:29  lr: 0.000089  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 50/127]  eta: 0:01:18  lr: 0.000089  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 60/127]  eta: 0:01:07  lr: 0.000089  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 70/127]  eta: 0:00:57  lr: 0.000089  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 80/127]  eta: 0:00:47  lr: 0.000089  loss: nan (nan)  time: 0.9843  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [ 90/127]  eta: 0:00:37  lr: 0.000089  loss: nan (nan)  time: 0.9891  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [100/127]  eta: 0:00:27  lr: 0.000089  loss: nan (nan)  time: 0.9887  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9805  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:197]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:197] Total time: 0:02:06 (0.9974 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:197]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6795  data: 0.4695  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:197]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2206  data: 0.0336  max mem: 34254\n",
      "Valid: [epoch:197] Total time: 0:00:31 (2.2316 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_197_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [  0/127]  eta: 0:05:14  lr: 0.000089  loss: nan (nan)  time: 2.4755  data: 1.5065  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 10/127]  eta: 0:02:09  lr: 0.000089  loss: nan (nan)  time: 1.1062  data: 0.1370  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 20/127]  eta: 0:01:51  lr: 0.000089  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 30/127]  eta: 0:01:39  lr: 0.000089  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 40/127]  eta: 0:01:27  lr: 0.000089  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 50/127]  eta: 0:01:17  lr: 0.000089  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 60/127]  eta: 0:01:06  lr: 0.000089  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 70/127]  eta: 0:00:56  lr: 0.000089  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 80/127]  eta: 0:00:46  lr: 0.000089  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [ 90/127]  eta: 0:00:36  lr: 0.000089  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [100/127]  eta: 0:00:26  lr: 0.000089  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:198]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:198] Total time: 0:02:05 (0.9880 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:198]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6114  data: 0.4328  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:198]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1750  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:198] Total time: 0:00:30 (2.1846 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_198_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [  0/127]  eta: 0:05:37  lr: 0.000089  loss: nan (nan)  time: 2.6587  data: 1.7040  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 10/127]  eta: 0:02:12  lr: 0.000089  loss: nan (nan)  time: 1.1310  data: 0.1550  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 20/127]  eta: 0:01:52  lr: 0.000089  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 30/127]  eta: 0:01:39  lr: 0.000089  loss: nan (nan)  time: 0.9710  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 40/127]  eta: 0:01:28  lr: 0.000089  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 50/127]  eta: 0:01:17  lr: 0.000089  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 60/127]  eta: 0:01:07  lr: 0.000089  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 70/127]  eta: 0:00:56  lr: 0.000089  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 80/127]  eta: 0:00:46  lr: 0.000089  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [ 90/127]  eta: 0:00:36  lr: 0.000089  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [100/127]  eta: 0:00:26  lr: 0.000089  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:199]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:199] Total time: 0:02:05 (0.9888 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:199]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6378  data: 0.4419  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:199]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1884  data: 0.0317  max mem: 34254\n",
      "Valid: [epoch:199] Total time: 0:00:30 (2.1997 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_199_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [  0/127]  eta: 0:06:22  lr: 0.000089  loss: nan (nan)  time: 3.0099  data: 2.0561  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 10/127]  eta: 0:02:15  lr: 0.000089  loss: nan (nan)  time: 1.1556  data: 0.1870  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 20/127]  eta: 0:01:54  lr: 0.000089  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 30/127]  eta: 0:01:40  lr: 0.000089  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 40/127]  eta: 0:01:28  lr: 0.000089  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 50/127]  eta: 0:01:17  lr: 0.000089  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 60/127]  eta: 0:01:07  lr: 0.000089  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 70/127]  eta: 0:00:57  lr: 0.000089  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 80/127]  eta: 0:00:46  lr: 0.000089  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [ 90/127]  eta: 0:00:36  lr: 0.000089  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [100/127]  eta: 0:00:26  lr: 0.000089  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:200]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:200] Total time: 0:02:05 (0.9916 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:200]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6574  data: 0.4303  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:200]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2342  data: 0.0308  max mem: 34254\n",
      "Valid: [epoch:200] Total time: 0:00:31 (2.2452 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_200_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [  0/127]  eta: 0:05:31  lr: 0.000089  loss: nan (nan)  time: 2.6072  data: 1.6528  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 10/127]  eta: 0:02:10  lr: 0.000089  loss: nan (nan)  time: 1.1179  data: 0.1503  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 20/127]  eta: 0:01:52  lr: 0.000089  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 30/127]  eta: 0:01:39  lr: 0.000089  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 40/127]  eta: 0:01:28  lr: 0.000089  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 50/127]  eta: 0:01:17  lr: 0.000089  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 60/127]  eta: 0:01:07  lr: 0.000089  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 70/127]  eta: 0:00:56  lr: 0.000089  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 80/127]  eta: 0:00:46  lr: 0.000089  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [ 90/127]  eta: 0:00:36  lr: 0.000089  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [100/127]  eta: 0:00:26  lr: 0.000089  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:201]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:201] Total time: 0:02:05 (0.9887 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:201]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6717  data: 0.4234  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:201]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2468  data: 0.0303  max mem: 34254\n",
      "Valid: [epoch:201] Total time: 0:00:31 (2.2581 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_201_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [  0/127]  eta: 0:07:00  lr: 0.000089  loss: nan (nan)  time: 3.3123  data: 2.3540  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 10/127]  eta: 0:02:18  lr: 0.000089  loss: nan (nan)  time: 1.1821  data: 0.2141  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 20/127]  eta: 0:01:55  lr: 0.000089  loss: nan (nan)  time: 0.9703  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 30/127]  eta: 0:01:41  lr: 0.000089  loss: nan (nan)  time: 0.9710  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 40/127]  eta: 0:01:29  lr: 0.000089  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 50/127]  eta: 0:01:18  lr: 0.000089  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 60/127]  eta: 0:01:07  lr: 0.000089  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 70/127]  eta: 0:00:57  lr: 0.000089  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 80/127]  eta: 0:00:47  lr: 0.000089  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [ 90/127]  eta: 0:00:36  lr: 0.000089  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [100/127]  eta: 0:00:26  lr: 0.000089  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9799  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:202]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:202] Total time: 0:02:06 (0.9940 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:202]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6756  data: 0.4494  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:202]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2300  data: 0.0322  max mem: 34254\n",
      "Valid: [epoch:202] Total time: 0:00:31 (2.2416 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_202_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [  0/127]  eta: 0:07:09  lr: 0.000089  loss: nan (nan)  time: 3.3788  data: 2.3981  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 10/127]  eta: 0:02:19  lr: 0.000089  loss: nan (nan)  time: 1.1892  data: 0.2181  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 20/127]  eta: 0:01:56  lr: 0.000089  loss: nan (nan)  time: 0.9708  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 30/127]  eta: 0:01:41  lr: 0.000089  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 40/127]  eta: 0:01:29  lr: 0.000089  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 50/127]  eta: 0:01:18  lr: 0.000089  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 60/127]  eta: 0:01:07  lr: 0.000089  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 70/127]  eta: 0:00:57  lr: 0.000089  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 80/127]  eta: 0:00:47  lr: 0.000089  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [ 90/127]  eta: 0:00:37  lr: 0.000089  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [100/127]  eta: 0:00:26  lr: 0.000089  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:203]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:203] Total time: 0:02:06 (0.9943 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:203]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6938  data: 0.4574  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:203]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2342  data: 0.0328  max mem: 34254\n",
      "Valid: [epoch:203] Total time: 0:00:31 (2.2432 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_203_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [  0/127]  eta: 0:06:47  lr: 0.000089  loss: nan (nan)  time: 3.2106  data: 2.2547  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 10/127]  eta: 0:02:17  lr: 0.000089  loss: nan (nan)  time: 1.1738  data: 0.2051  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 20/127]  eta: 0:01:55  lr: 0.000089  loss: nan (nan)  time: 0.9707  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 30/127]  eta: 0:01:41  lr: 0.000089  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 40/127]  eta: 0:01:29  lr: 0.000089  loss: nan (nan)  time: 0.9804  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 50/127]  eta: 0:01:18  lr: 0.000089  loss: nan (nan)  time: 0.9800  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 60/127]  eta: 0:01:07  lr: 0.000089  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 70/127]  eta: 0:00:57  lr: 0.000089  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 80/127]  eta: 0:00:47  lr: 0.000089  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [ 90/127]  eta: 0:00:36  lr: 0.000089  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [100/127]  eta: 0:00:26  lr: 0.000089  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [110/127]  eta: 0:00:16  lr: 0.000089  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [120/127]  eta: 0:00:06  lr: 0.000089  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:204]  [126/127]  eta: 0:00:00  lr: 0.000089  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:204] Total time: 0:02:06 (0.9942 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:204]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6479  data: 0.4172  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:204]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2429  data: 0.0299  max mem: 34254\n",
      "Valid: [epoch:204] Total time: 0:00:31 (2.2518 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_204_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [  0/127]  eta: 0:07:17  lr: 0.000088  loss: nan (nan)  time: 3.4461  data: 2.4883  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 10/127]  eta: 0:02:19  lr: 0.000088  loss: nan (nan)  time: 1.1954  data: 0.2263  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 20/127]  eta: 0:01:56  lr: 0.000088  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 30/127]  eta: 0:01:41  lr: 0.000088  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 40/127]  eta: 0:01:29  lr: 0.000088  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 50/127]  eta: 0:01:18  lr: 0.000088  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 80/127]  eta: 0:00:47  lr: 0.000088  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [ 90/127]  eta: 0:00:37  lr: 0.000088  loss: nan (nan)  time: 0.9781  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:205]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:205] Total time: 0:02:06 (0.9957 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:205]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.5970  data: 0.4315  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:205]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1985  data: 0.0309  max mem: 34254\n",
      "Valid: [epoch:205] Total time: 0:00:30 (2.2102 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_205_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [  0/127]  eta: 0:06:08  lr: 0.000088  loss: nan (nan)  time: 2.9041  data: 1.9303  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 10/127]  eta: 0:02:14  lr: 0.000088  loss: nan (nan)  time: 1.1469  data: 0.1756  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 20/127]  eta: 0:01:53  lr: 0.000088  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 30/127]  eta: 0:01:40  lr: 0.000088  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 40/127]  eta: 0:01:28  lr: 0.000088  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 50/127]  eta: 0:01:17  lr: 0.000088  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 80/127]  eta: 0:00:46  lr: 0.000088  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [ 90/127]  eta: 0:00:36  lr: 0.000088  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:206]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:206] Total time: 0:02:05 (0.9909 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:206]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6676  data: 0.4367  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:206]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2260  data: 0.0313  max mem: 34254\n",
      "Valid: [epoch:206] Total time: 0:00:31 (2.2371 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_206_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [  0/127]  eta: 0:06:48  lr: 0.000088  loss: nan (nan)  time: 3.2195  data: 2.2641  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 10/127]  eta: 0:02:17  lr: 0.000088  loss: nan (nan)  time: 1.1754  data: 0.2060  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 20/127]  eta: 0:01:55  lr: 0.000088  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 30/127]  eta: 0:01:41  lr: 0.000088  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 40/127]  eta: 0:01:29  lr: 0.000088  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 50/127]  eta: 0:01:18  lr: 0.000088  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9782  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 80/127]  eta: 0:00:47  lr: 0.000088  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [ 90/127]  eta: 0:00:37  lr: 0.000088  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:207]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:207] Total time: 0:02:06 (0.9949 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:207]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6456  data: 0.4210  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:207]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2133  data: 0.0302  max mem: 34254\n",
      "Valid: [epoch:207] Total time: 0:00:31 (2.2223 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_207_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [  0/127]  eta: 0:07:02  lr: 0.000088  loss: nan (nan)  time: 3.3261  data: 2.3697  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 10/127]  eta: 0:02:18  lr: 0.000088  loss: nan (nan)  time: 1.1858  data: 0.2155  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 20/127]  eta: 0:01:55  lr: 0.000088  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 30/127]  eta: 0:01:41  lr: 0.000088  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 40/127]  eta: 0:01:29  lr: 0.000088  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 50/127]  eta: 0:01:18  lr: 0.000088  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 80/127]  eta: 0:00:47  lr: 0.000088  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [ 90/127]  eta: 0:00:37  lr: 0.000088  loss: nan (nan)  time: 0.9799  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9808  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:208]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:208] Total time: 0:02:06 (0.9949 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:208]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6724  data: 0.4577  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:208]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2143  data: 0.0328  max mem: 34254\n",
      "Valid: [epoch:208] Total time: 0:00:31 (2.2258 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_208_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [  0/127]  eta: 0:06:42  lr: 0.000088  loss: nan (nan)  time: 3.1660  data: 2.2111  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 10/127]  eta: 0:02:16  lr: 0.000088  loss: nan (nan)  time: 1.1707  data: 0.2011  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 20/127]  eta: 0:01:55  lr: 0.000088  loss: nan (nan)  time: 0.9710  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 30/127]  eta: 0:01:41  lr: 0.000088  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 40/127]  eta: 0:01:29  lr: 0.000088  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 50/127]  eta: 0:01:18  lr: 0.000088  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 80/127]  eta: 0:00:47  lr: 0.000088  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [ 90/127]  eta: 0:00:36  lr: 0.000088  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:209]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:209] Total time: 0:02:06 (0.9922 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:209]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7172  data: 0.4156  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:209]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2859  data: 0.0298  max mem: 34254\n",
      "Valid: [epoch:209] Total time: 0:00:32 (2.2974 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_209_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [  0/127]  eta: 0:06:21  lr: 0.000088  loss: nan (nan)  time: 3.0045  data: 2.0341  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 10/127]  eta: 0:02:15  lr: 0.000088  loss: nan (nan)  time: 1.1583  data: 0.1850  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 20/127]  eta: 0:01:54  lr: 0.000088  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 30/127]  eta: 0:01:40  lr: 0.000088  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 40/127]  eta: 0:01:29  lr: 0.000088  loss: nan (nan)  time: 0.9835  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 50/127]  eta: 0:01:18  lr: 0.000088  loss: nan (nan)  time: 0.9853  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 80/127]  eta: 0:00:47  lr: 0.000088  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [ 90/127]  eta: 0:00:36  lr: 0.000088  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:210]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:210] Total time: 0:02:06 (0.9931 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:210]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6912  data: 0.4620  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:210]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2815  data: 0.0331  max mem: 34254\n",
      "Valid: [epoch:210] Total time: 0:00:32 (2.2929 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_210_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [  0/127]  eta: 0:07:31  lr: 0.000088  loss: nan (nan)  time: 3.5518  data: 2.5967  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 10/127]  eta: 0:02:20  lr: 0.000088  loss: nan (nan)  time: 1.2047  data: 0.2362  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 20/127]  eta: 0:01:57  lr: 0.000088  loss: nan (nan)  time: 0.9712  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 30/127]  eta: 0:01:42  lr: 0.000088  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 40/127]  eta: 0:01:30  lr: 0.000088  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 50/127]  eta: 0:01:18  lr: 0.000088  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 80/127]  eta: 0:00:47  lr: 0.000088  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [ 90/127]  eta: 0:00:37  lr: 0.000088  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9791  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:211]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:211] Total time: 0:02:06 (0.9961 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:211]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.5992  data: 0.4136  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:211]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1735  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:211] Total time: 0:00:30 (2.1839 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_211_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [  0/127]  eta: 0:06:45  lr: 0.000088  loss: nan (nan)  time: 3.1925  data: 2.2129  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 10/127]  eta: 0:02:17  lr: 0.000088  loss: nan (nan)  time: 1.1755  data: 0.2013  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 20/127]  eta: 0:01:55  lr: 0.000088  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 30/127]  eta: 0:01:41  lr: 0.000088  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 40/127]  eta: 0:01:29  lr: 0.000088  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 50/127]  eta: 0:01:18  lr: 0.000088  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9779  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 80/127]  eta: 0:00:47  lr: 0.000088  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [ 90/127]  eta: 0:00:36  lr: 0.000088  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:212]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:212] Total time: 0:02:06 (0.9940 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:212]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7350  data: 0.4321  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:212]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2883  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:212] Total time: 0:00:32 (2.3000 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_212_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [  0/127]  eta: 0:05:54  lr: 0.000088  loss: nan (nan)  time: 2.7944  data: 1.8401  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 10/127]  eta: 0:02:13  lr: 0.000088  loss: nan (nan)  time: 1.1370  data: 0.1674  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 20/127]  eta: 0:01:53  lr: 0.000088  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 30/127]  eta: 0:01:39  lr: 0.000088  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 40/127]  eta: 0:01:28  lr: 0.000088  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 50/127]  eta: 0:01:17  lr: 0.000088  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 60/127]  eta: 0:01:07  lr: 0.000088  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 70/127]  eta: 0:00:57  lr: 0.000088  loss: nan (nan)  time: 0.9811  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 80/127]  eta: 0:00:46  lr: 0.000088  loss: nan (nan)  time: 0.9810  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [ 90/127]  eta: 0:00:36  lr: 0.000088  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [100/127]  eta: 0:00:26  lr: 0.000088  loss: nan (nan)  time: 0.9792  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [110/127]  eta: 0:00:16  lr: 0.000088  loss: nan (nan)  time: 0.9795  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [120/127]  eta: 0:00:06  lr: 0.000088  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:213]  [126/127]  eta: 0:00:00  lr: 0.000088  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:213] Total time: 0:02:05 (0.9912 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:213]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6570  data: 0.4376  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:213]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2170  data: 0.0314  max mem: 34254\n",
      "Valid: [epoch:213] Total time: 0:00:31 (2.2283 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_213_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [  0/127]  eta: 0:07:04  lr: 0.000087  loss: nan (nan)  time: 3.3429  data: 2.3860  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 10/127]  eta: 0:02:18  lr: 0.000087  loss: nan (nan)  time: 1.1866  data: 0.2170  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 20/127]  eta: 0:01:56  lr: 0.000087  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 30/127]  eta: 0:01:42  lr: 0.000087  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 40/127]  eta: 0:01:29  lr: 0.000087  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 50/127]  eta: 0:01:18  lr: 0.000087  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 60/127]  eta: 0:01:07  lr: 0.000087  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 70/127]  eta: 0:00:57  lr: 0.000087  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 80/127]  eta: 0:00:47  lr: 0.000087  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [ 90/127]  eta: 0:00:37  lr: 0.000087  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [100/127]  eta: 0:00:26  lr: 0.000087  loss: nan (nan)  time: 0.9826  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9827  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:214]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:214] Total time: 0:02:06 (0.9956 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:214]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7669  data: 0.4564  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:214]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2297  data: 0.0327  max mem: 34254\n",
      "Valid: [epoch:214] Total time: 0:00:31 (2.2413 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_214_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [  0/127]  eta: 0:05:39  lr: 0.000087  loss: nan (nan)  time: 2.6749  data: 1.7169  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 10/127]  eta: 0:02:11  lr: 0.000087  loss: nan (nan)  time: 1.1255  data: 0.1562  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 20/127]  eta: 0:01:52  lr: 0.000087  loss: nan (nan)  time: 0.9714  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 30/127]  eta: 0:01:39  lr: 0.000087  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 40/127]  eta: 0:01:28  lr: 0.000087  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 50/127]  eta: 0:01:17  lr: 0.000087  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 60/127]  eta: 0:01:07  lr: 0.000087  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 70/127]  eta: 0:00:56  lr: 0.000087  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 80/127]  eta: 0:00:46  lr: 0.000087  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [ 90/127]  eta: 0:00:36  lr: 0.000087  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [100/127]  eta: 0:00:26  lr: 0.000087  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:215]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:215] Total time: 0:02:05 (0.9897 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:215]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6192  data: 0.4314  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:215]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1953  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:215] Total time: 0:00:30 (2.2055 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_215_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [  0/127]  eta: 0:05:17  lr: 0.000087  loss: nan (nan)  time: 2.5002  data: 1.5431  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 10/127]  eta: 0:02:10  lr: 0.000087  loss: nan (nan)  time: 1.1153  data: 0.1404  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 20/127]  eta: 0:01:52  lr: 0.000087  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 30/127]  eta: 0:01:39  lr: 0.000087  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 40/127]  eta: 0:01:27  lr: 0.000087  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 50/127]  eta: 0:01:17  lr: 0.000087  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 60/127]  eta: 0:01:07  lr: 0.000087  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 70/127]  eta: 0:00:56  lr: 0.000087  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 80/127]  eta: 0:00:46  lr: 0.000087  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [ 90/127]  eta: 0:00:36  lr: 0.000087  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [100/127]  eta: 0:00:26  lr: 0.000087  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:216]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:216] Total time: 0:02:05 (0.9886 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:216]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7290  data: 0.4144  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:216]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2167  data: 0.0297  max mem: 34254\n",
      "Valid: [epoch:216] Total time: 0:00:31 (2.2278 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_216_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [  0/127]  eta: 0:06:38  lr: 0.000087  loss: nan (nan)  time: 3.1340  data: 2.1758  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 10/127]  eta: 0:02:16  lr: 0.000087  loss: nan (nan)  time: 1.1670  data: 0.1979  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 20/127]  eta: 0:01:54  lr: 0.000087  loss: nan (nan)  time: 0.9712  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 30/127]  eta: 0:01:41  lr: 0.000087  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 40/127]  eta: 0:01:29  lr: 0.000087  loss: nan (nan)  time: 0.9716  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 50/127]  eta: 0:01:18  lr: 0.000087  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 60/127]  eta: 0:01:07  lr: 0.000087  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 70/127]  eta: 0:00:57  lr: 0.000087  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 80/127]  eta: 0:00:46  lr: 0.000087  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [ 90/127]  eta: 0:00:36  lr: 0.000087  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [100/127]  eta: 0:00:26  lr: 0.000087  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:217]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:217] Total time: 0:02:05 (0.9918 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:217]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6741  data: 0.4702  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:217]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2011  data: 0.0337  max mem: 34254\n",
      "Valid: [epoch:217] Total time: 0:00:30 (2.2112 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_217_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [  0/127]  eta: 0:05:36  lr: 0.000087  loss: nan (nan)  time: 2.6520  data: 1.6924  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 10/127]  eta: 0:02:11  lr: 0.000087  loss: nan (nan)  time: 1.1235  data: 0.1539  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 20/127]  eta: 0:01:52  lr: 0.000087  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 30/127]  eta: 0:01:39  lr: 0.000087  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 40/127]  eta: 0:01:28  lr: 0.000087  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 50/127]  eta: 0:01:17  lr: 0.000087  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 60/127]  eta: 0:01:07  lr: 0.000087  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 70/127]  eta: 0:00:56  lr: 0.000087  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 80/127]  eta: 0:00:46  lr: 0.000087  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [ 90/127]  eta: 0:00:36  lr: 0.000087  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [100/127]  eta: 0:00:26  lr: 0.000087  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:218]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:218] Total time: 0:02:05 (0.9900 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:218]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6048  data: 0.4187  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:218]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2132  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:218] Total time: 0:00:31 (2.2235 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_218_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [  0/127]  eta: 0:07:01  lr: 0.000087  loss: nan (nan)  time: 3.3160  data: 2.3605  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 10/127]  eta: 0:02:18  lr: 0.000087  loss: nan (nan)  time: 1.1833  data: 0.2147  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 20/127]  eta: 0:01:55  lr: 0.000087  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 30/127]  eta: 0:01:41  lr: 0.000087  loss: nan (nan)  time: 0.9717  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 40/127]  eta: 0:01:29  lr: 0.000087  loss: nan (nan)  time: 0.9716  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 50/127]  eta: 0:01:18  lr: 0.000087  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 60/127]  eta: 0:01:07  lr: 0.000087  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 70/127]  eta: 0:00:57  lr: 0.000087  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 80/127]  eta: 0:00:47  lr: 0.000087  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [ 90/127]  eta: 0:00:36  lr: 0.000087  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [100/127]  eta: 0:00:26  lr: 0.000087  loss: nan (nan)  time: 0.9785  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:219]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:219] Total time: 0:02:06 (0.9940 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:219]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6091  data: 0.4468  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:219]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2608  data: 0.0320  max mem: 34254\n",
      "Valid: [epoch:219] Total time: 0:00:31 (2.2719 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_219_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [  0/127]  eta: 0:07:19  lr: 0.000087  loss: nan (nan)  time: 3.4583  data: 2.4888  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 10/127]  eta: 0:02:20  lr: 0.000087  loss: nan (nan)  time: 1.1973  data: 0.2263  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 20/127]  eta: 0:01:57  lr: 0.000087  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 30/127]  eta: 0:01:42  lr: 0.000087  loss: nan (nan)  time: 0.9794  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 40/127]  eta: 0:01:30  lr: 0.000087  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 50/127]  eta: 0:01:18  lr: 0.000087  loss: nan (nan)  time: 0.9779  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 60/127]  eta: 0:01:08  lr: 0.000087  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 70/127]  eta: 0:00:57  lr: 0.000087  loss: nan (nan)  time: 0.9781  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 80/127]  eta: 0:00:47  lr: 0.000087  loss: nan (nan)  time: 0.9781  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [ 90/127]  eta: 0:00:37  lr: 0.000087  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [100/127]  eta: 0:00:27  lr: 0.000087  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:220]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:220] Total time: 0:02:06 (0.9976 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:220]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6297  data: 0.4109  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:220]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2250  data: 0.0294  max mem: 34254\n",
      "Valid: [epoch:220] Total time: 0:00:31 (2.2355 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_220_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [  0/127]  eta: 0:06:32  lr: 0.000087  loss: nan (nan)  time: 3.0929  data: 2.1285  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 10/127]  eta: 0:02:16  lr: 0.000087  loss: nan (nan)  time: 1.1662  data: 0.1936  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 20/127]  eta: 0:01:55  lr: 0.000087  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 30/127]  eta: 0:01:41  lr: 0.000087  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 40/127]  eta: 0:01:29  lr: 0.000087  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 50/127]  eta: 0:01:18  lr: 0.000087  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 60/127]  eta: 0:01:07  lr: 0.000087  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 70/127]  eta: 0:00:57  lr: 0.000087  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 80/127]  eta: 0:00:47  lr: 0.000087  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [ 90/127]  eta: 0:00:36  lr: 0.000087  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [100/127]  eta: 0:00:26  lr: 0.000087  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:221]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:221] Total time: 0:02:06 (0.9929 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:221]  [ 0/14]  eta: 0:00:39  loss: nan (nan)  time: 2.7926  data: 0.4350  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:221]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.3564  data: 0.0312  max mem: 34254\n",
      "Valid: [epoch:221] Total time: 0:00:33 (2.3670 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_221_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [  0/127]  eta: 0:07:09  lr: 0.000087  loss: nan (nan)  time: 3.3789  data: 2.3573  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 10/127]  eta: 0:02:19  lr: 0.000087  loss: nan (nan)  time: 1.1910  data: 0.2144  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 20/127]  eta: 0:01:56  lr: 0.000087  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 30/127]  eta: 0:01:41  lr: 0.000087  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 40/127]  eta: 0:01:29  lr: 0.000087  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 50/127]  eta: 0:01:18  lr: 0.000087  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 60/127]  eta: 0:01:08  lr: 0.000087  loss: nan (nan)  time: 0.9796  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 70/127]  eta: 0:00:57  lr: 0.000087  loss: nan (nan)  time: 0.9781  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 80/127]  eta: 0:00:47  lr: 0.000087  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [ 90/127]  eta: 0:00:37  lr: 0.000087  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [100/127]  eta: 0:00:26  lr: 0.000087  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:222]  [110/127]  eta: 0:00:16  lr: 0.000087  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [120/127]  eta: 0:00:06  lr: 0.000087  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:222]  [126/127]  eta: 0:00:00  lr: 0.000087  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:222] Total time: 0:02:06 (0.9956 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:222]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7364  data: 0.4203  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:222]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.3355  data: 0.0301  max mem: 34254\n",
      "Valid: [epoch:222] Total time: 0:00:32 (2.3471 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_222_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [  0/127]  eta: 0:06:54  lr: 0.000086  loss: nan (nan)  time: 3.2664  data: 2.3076  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 10/127]  eta: 0:02:18  lr: 0.000086  loss: nan (nan)  time: 1.1815  data: 0.2099  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 20/127]  eta: 0:01:55  lr: 0.000086  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 30/127]  eta: 0:01:41  lr: 0.000086  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 40/127]  eta: 0:01:29  lr: 0.000086  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 50/127]  eta: 0:01:18  lr: 0.000086  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 60/127]  eta: 0:01:07  lr: 0.000086  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 70/127]  eta: 0:00:57  lr: 0.000086  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 80/127]  eta: 0:00:47  lr: 0.000086  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [ 90/127]  eta: 0:00:36  lr: 0.000086  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [100/127]  eta: 0:00:26  lr: 0.000086  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:223]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:223] Total time: 0:02:06 (0.9934 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:223]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6112  data: 0.4076  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:223]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.3888  data: 0.0292  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:223] Total time: 0:00:33 (2.4004 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_223_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [  0/127]  eta: 0:06:24  lr: 0.000086  loss: nan (nan)  time: 3.0301  data: 2.0709  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 10/127]  eta: 0:02:15  lr: 0.000086  loss: nan (nan)  time: 1.1587  data: 0.1884  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 20/127]  eta: 0:01:54  lr: 0.000086  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 30/127]  eta: 0:01:40  lr: 0.000086  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 40/127]  eta: 0:01:29  lr: 0.000086  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 50/127]  eta: 0:01:18  lr: 0.000086  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 60/127]  eta: 0:01:07  lr: 0.000086  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 70/127]  eta: 0:00:57  lr: 0.000086  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 80/127]  eta: 0:00:46  lr: 0.000086  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [ 90/127]  eta: 0:00:36  lr: 0.000086  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [100/127]  eta: 0:00:26  lr: 0.000086  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:224]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:224] Total time: 0:02:06 (0.9924 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:224]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7078  data: 0.3967  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:224]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2827  data: 0.0285  max mem: 34254\n",
      "Valid: [epoch:224] Total time: 0:00:32 (2.2952 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_224_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [  0/127]  eta: 0:07:22  lr: 0.000086  loss: nan (nan)  time: 3.4877  data: 2.5316  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 10/127]  eta: 0:02:20  lr: 0.000086  loss: nan (nan)  time: 1.2001  data: 0.2302  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 20/127]  eta: 0:01:57  lr: 0.000086  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 30/127]  eta: 0:01:42  lr: 0.000086  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 40/127]  eta: 0:01:30  lr: 0.000086  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 50/127]  eta: 0:01:18  lr: 0.000086  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 60/127]  eta: 0:01:07  lr: 0.000086  loss: nan (nan)  time: 0.9717  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 70/127]  eta: 0:00:57  lr: 0.000086  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 80/127]  eta: 0:00:47  lr: 0.000086  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [ 90/127]  eta: 0:00:37  lr: 0.000086  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [100/127]  eta: 0:00:26  lr: 0.000086  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:225]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:225] Total time: 0:02:06 (0.9950 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:225]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7050  data: 0.4932  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:225]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2335  data: 0.0353  max mem: 34254\n",
      "Valid: [epoch:225] Total time: 0:00:31 (2.2452 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_225_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [  0/127]  eta: 0:07:03  lr: 0.000086  loss: nan (nan)  time: 3.3373  data: 2.3620  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 10/127]  eta: 0:02:18  lr: 0.000086  loss: nan (nan)  time: 1.1865  data: 0.2148  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 20/127]  eta: 0:01:56  lr: 0.000086  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 30/127]  eta: 0:01:41  lr: 0.000086  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 40/127]  eta: 0:01:29  lr: 0.000086  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 50/127]  eta: 0:01:18  lr: 0.000086  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 60/127]  eta: 0:01:07  lr: 0.000086  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 70/127]  eta: 0:00:57  lr: 0.000086  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 80/127]  eta: 0:00:47  lr: 0.000086  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [ 90/127]  eta: 0:00:37  lr: 0.000086  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [100/127]  eta: 0:00:26  lr: 0.000086  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9781  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9832  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:226]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9791  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:226] Total time: 0:02:06 (0.9958 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:226]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7077  data: 0.4776  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:226]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2431  data: 0.0342  max mem: 34254\n",
      "Valid: [epoch:226] Total time: 0:00:31 (2.2538 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_226_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [  0/127]  eta: 0:05:13  lr: 0.000086  loss: nan (nan)  time: 2.4719  data: 1.5113  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 10/127]  eta: 0:02:09  lr: 0.000086  loss: nan (nan)  time: 1.1062  data: 0.1375  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 20/127]  eta: 0:01:51  lr: 0.000086  loss: nan (nan)  time: 0.9706  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 30/127]  eta: 0:01:38  lr: 0.000086  loss: nan (nan)  time: 0.9717  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 40/127]  eta: 0:01:27  lr: 0.000086  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 50/127]  eta: 0:01:17  lr: 0.000086  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 60/127]  eta: 0:01:06  lr: 0.000086  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 70/127]  eta: 0:00:56  lr: 0.000086  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 80/127]  eta: 0:00:46  lr: 0.000086  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [ 90/127]  eta: 0:00:36  lr: 0.000086  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [100/127]  eta: 0:00:26  lr: 0.000086  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:227]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:227] Total time: 0:02:05 (0.9869 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:227]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6337  data: 0.4129  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:227]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2336  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:227] Total time: 0:00:31 (2.2446 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_227_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [  0/127]  eta: 0:06:09  lr: 0.000086  loss: nan (nan)  time: 2.9115  data: 1.9558  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 10/127]  eta: 0:02:14  lr: 0.000086  loss: nan (nan)  time: 1.1465  data: 0.1779  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 20/127]  eta: 0:01:53  lr: 0.000086  loss: nan (nan)  time: 0.9706  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 30/127]  eta: 0:01:40  lr: 0.000086  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 40/127]  eta: 0:01:28  lr: 0.000086  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 50/127]  eta: 0:01:17  lr: 0.000086  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 60/127]  eta: 0:01:07  lr: 0.000086  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 70/127]  eta: 0:00:56  lr: 0.000086  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 80/127]  eta: 0:00:46  lr: 0.000086  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [ 90/127]  eta: 0:00:36  lr: 0.000086  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [100/127]  eta: 0:00:26  lr: 0.000086  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:228]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:228] Total time: 0:02:05 (0.9897 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:228]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6710  data: 0.4391  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:228]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2256  data: 0.0315  max mem: 34254\n",
      "Valid: [epoch:228] Total time: 0:00:31 (2.2363 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_228_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [  0/127]  eta: 0:06:52  lr: 0.000086  loss: nan (nan)  time: 3.2512  data: 2.2882  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 10/127]  eta: 0:02:17  lr: 0.000086  loss: nan (nan)  time: 1.1776  data: 0.2081  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 20/127]  eta: 0:01:55  lr: 0.000086  loss: nan (nan)  time: 0.9705  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 30/127]  eta: 0:01:41  lr: 0.000086  loss: nan (nan)  time: 0.9714  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 40/127]  eta: 0:01:29  lr: 0.000086  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 50/127]  eta: 0:01:18  lr: 0.000086  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 60/127]  eta: 0:01:07  lr: 0.000086  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 70/127]  eta: 0:00:57  lr: 0.000086  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 80/127]  eta: 0:00:47  lr: 0.000086  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [ 90/127]  eta: 0:00:36  lr: 0.000086  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [100/127]  eta: 0:00:26  lr: 0.000086  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:229]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:229] Total time: 0:02:06 (0.9928 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:229]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7195  data: 0.4068  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:229]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2425  data: 0.0292  max mem: 34254\n",
      "Valid: [epoch:229] Total time: 0:00:31 (2.2542 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_229_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [  0/127]  eta: 0:06:22  lr: 0.000086  loss: nan (nan)  time: 3.0089  data: 2.0516  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 10/127]  eta: 0:02:15  lr: 0.000086  loss: nan (nan)  time: 1.1575  data: 0.1866  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 20/127]  eta: 0:01:54  lr: 0.000086  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 30/127]  eta: 0:01:40  lr: 0.000086  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 40/127]  eta: 0:01:28  lr: 0.000086  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 50/127]  eta: 0:01:18  lr: 0.000086  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 60/127]  eta: 0:01:07  lr: 0.000086  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 70/127]  eta: 0:00:57  lr: 0.000086  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 80/127]  eta: 0:00:46  lr: 0.000086  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [ 90/127]  eta: 0:00:36  lr: 0.000086  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [100/127]  eta: 0:00:26  lr: 0.000086  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9806  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:230]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9807  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:230] Total time: 0:02:06 (0.9924 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:230]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7233  data: 0.4822  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:230]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2346  data: 0.0345  max mem: 34254\n",
      "Valid: [epoch:230] Total time: 0:00:31 (2.2447 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_230_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [  0/127]  eta: 0:07:24  lr: 0.000086  loss: nan (nan)  time: 3.4993  data: 2.5422  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 10/127]  eta: 0:02:20  lr: 0.000086  loss: nan (nan)  time: 1.2006  data: 0.2312  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 20/127]  eta: 0:01:56  lr: 0.000086  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 30/127]  eta: 0:01:42  lr: 0.000086  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 40/127]  eta: 0:01:30  lr: 0.000086  loss: nan (nan)  time: 0.9791  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 50/127]  eta: 0:01:18  lr: 0.000086  loss: nan (nan)  time: 0.9796  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 60/127]  eta: 0:01:08  lr: 0.000086  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 70/127]  eta: 0:00:57  lr: 0.000086  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 80/127]  eta: 0:00:47  lr: 0.000086  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [ 90/127]  eta: 0:00:37  lr: 0.000086  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [100/127]  eta: 0:00:27  lr: 0.000086  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [110/127]  eta: 0:00:16  lr: 0.000086  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [120/127]  eta: 0:00:06  lr: 0.000086  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:231]  [126/127]  eta: 0:00:00  lr: 0.000086  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:231] Total time: 0:02:06 (0.9965 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:231]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6370  data: 0.4533  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:231]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1778  data: 0.0325  max mem: 34254\n",
      "Valid: [epoch:231] Total time: 0:00:30 (2.1893 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_231_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [  0/127]  eta: 0:06:18  lr: 0.000085  loss: nan (nan)  time: 2.9804  data: 2.0239  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 10/127]  eta: 0:02:15  lr: 0.000085  loss: nan (nan)  time: 1.1580  data: 0.1841  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 20/127]  eta: 0:01:54  lr: 0.000085  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 30/127]  eta: 0:01:40  lr: 0.000085  loss: nan (nan)  time: 0.9710  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 40/127]  eta: 0:01:28  lr: 0.000085  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 50/127]  eta: 0:01:17  lr: 0.000085  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 60/127]  eta: 0:01:07  lr: 0.000085  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 80/127]  eta: 0:00:46  lr: 0.000085  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [ 90/127]  eta: 0:00:36  lr: 0.000085  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9789  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:232]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:232] Total time: 0:02:05 (0.9918 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:232]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6367  data: 0.4218  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:232]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2141  data: 0.0302  max mem: 34254\n",
      "Valid: [epoch:232] Total time: 0:00:31 (2.2246 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_232_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [  0/127]  eta: 0:07:22  lr: 0.000085  loss: nan (nan)  time: 3.4849  data: 2.4834  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 10/127]  eta: 0:02:20  lr: 0.000085  loss: nan (nan)  time: 1.1997  data: 0.2259  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 20/127]  eta: 0:01:56  lr: 0.000085  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 30/127]  eta: 0:01:42  lr: 0.000085  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 40/127]  eta: 0:01:29  lr: 0.000085  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 50/127]  eta: 0:01:18  lr: 0.000085  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 60/127]  eta: 0:01:08  lr: 0.000085  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 80/127]  eta: 0:00:47  lr: 0.000085  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [ 90/127]  eta: 0:00:37  lr: 0.000085  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:233]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:233] Total time: 0:02:06 (0.9965 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:233]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6944  data: 0.4435  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:233]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2691  data: 0.0318  max mem: 34254\n",
      "Valid: [epoch:233] Total time: 0:00:31 (2.2793 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_233_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [  0/127]  eta: 0:06:56  lr: 0.000085  loss: nan (nan)  time: 3.2813  data: 2.3213  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 10/127]  eta: 0:02:18  lr: 0.000085  loss: nan (nan)  time: 1.1864  data: 0.2111  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 20/127]  eta: 0:01:55  lr: 0.000085  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 30/127]  eta: 0:01:41  lr: 0.000085  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 40/127]  eta: 0:01:29  lr: 0.000085  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 50/127]  eta: 0:01:18  lr: 0.000085  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 60/127]  eta: 0:01:07  lr: 0.000085  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 80/127]  eta: 0:00:47  lr: 0.000085  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [ 90/127]  eta: 0:00:36  lr: 0.000085  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:234]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:234] Total time: 0:02:06 (0.9946 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:234]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6344  data: 0.4048  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:234]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2104  data: 0.0290  max mem: 34254\n",
      "Valid: [epoch:234] Total time: 0:00:31 (2.2219 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_234_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [  0/127]  eta: 0:06:10  lr: 0.000085  loss: nan (nan)  time: 2.9194  data: 1.9323  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 10/127]  eta: 0:02:14  lr: 0.000085  loss: nan (nan)  time: 1.1480  data: 0.1758  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 20/127]  eta: 0:01:53  lr: 0.000085  loss: nan (nan)  time: 0.9712  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 30/127]  eta: 0:01:40  lr: 0.000085  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 40/127]  eta: 0:01:28  lr: 0.000085  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 50/127]  eta: 0:01:17  lr: 0.000085  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 60/127]  eta: 0:01:07  lr: 0.000085  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 80/127]  eta: 0:00:46  lr: 0.000085  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [ 90/127]  eta: 0:00:36  lr: 0.000085  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:235]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:235] Total time: 0:02:06 (0.9922 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:235]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6516  data: 0.4293  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:235]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2151  data: 0.0308  max mem: 34254\n",
      "Valid: [epoch:235] Total time: 0:00:31 (2.2261 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_235_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [  0/127]  eta: 0:05:59  lr: 0.000085  loss: nan (nan)  time: 2.8334  data: 1.8734  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 10/127]  eta: 0:02:13  lr: 0.000085  loss: nan (nan)  time: 1.1397  data: 0.1704  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 20/127]  eta: 0:01:53  lr: 0.000085  loss: nan (nan)  time: 0.9705  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 30/127]  eta: 0:01:40  lr: 0.000085  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 40/127]  eta: 0:01:28  lr: 0.000085  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 50/127]  eta: 0:01:17  lr: 0.000085  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 60/127]  eta: 0:01:07  lr: 0.000085  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 80/127]  eta: 0:00:46  lr: 0.000085  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [ 90/127]  eta: 0:00:36  lr: 0.000085  loss: nan (nan)  time: 0.9793  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9802  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:236]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:236] Total time: 0:02:05 (0.9913 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:236]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6546  data: 0.4210  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:236]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2215  data: 0.0302  max mem: 34254\n",
      "Valid: [epoch:236] Total time: 0:00:31 (2.2320 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_236_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [  0/127]  eta: 0:06:57  lr: 0.000085  loss: nan (nan)  time: 3.2863  data: 2.3248  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 10/127]  eta: 0:02:18  lr: 0.000085  loss: nan (nan)  time: 1.1827  data: 0.2114  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 20/127]  eta: 0:01:55  lr: 0.000085  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 30/127]  eta: 0:01:41  lr: 0.000085  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 40/127]  eta: 0:01:29  lr: 0.000085  loss: nan (nan)  time: 0.9791  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 50/127]  eta: 0:01:18  lr: 0.000085  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 60/127]  eta: 0:01:07  lr: 0.000085  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9781  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 80/127]  eta: 0:00:47  lr: 0.000085  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [ 90/127]  eta: 0:00:37  lr: 0.000085  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:237]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:237] Total time: 0:02:06 (0.9952 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:237]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6274  data: 0.4399  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:237]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2555  data: 0.0315  max mem: 34254\n",
      "Valid: [epoch:237] Total time: 0:00:31 (2.2654 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_237_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [  0/127]  eta: 0:06:18  lr: 0.000085  loss: nan (nan)  time: 2.9819  data: 2.0267  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 10/127]  eta: 0:02:14  lr: 0.000085  loss: nan (nan)  time: 1.1535  data: 0.1843  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 20/127]  eta: 0:01:54  lr: 0.000085  loss: nan (nan)  time: 0.9709  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 30/127]  eta: 0:01:40  lr: 0.000085  loss: nan (nan)  time: 0.9716  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 40/127]  eta: 0:01:28  lr: 0.000085  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 50/127]  eta: 0:01:17  lr: 0.000085  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 60/127]  eta: 0:01:07  lr: 0.000085  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 80/127]  eta: 0:00:46  lr: 0.000085  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [ 90/127]  eta: 0:00:36  lr: 0.000085  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:238]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:238] Total time: 0:02:05 (0.9915 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:238]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7677  data: 0.4526  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:238]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2572  data: 0.0324  max mem: 34254\n",
      "Valid: [epoch:238] Total time: 0:00:31 (2.2688 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_238_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [  0/127]  eta: 0:06:55  lr: 0.000085  loss: nan (nan)  time: 3.2754  data: 2.3181  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 10/127]  eta: 0:02:17  lr: 0.000085  loss: nan (nan)  time: 1.1792  data: 0.2108  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 20/127]  eta: 0:01:55  lr: 0.000085  loss: nan (nan)  time: 0.9702  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 30/127]  eta: 0:01:41  lr: 0.000085  loss: nan (nan)  time: 0.9710  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 40/127]  eta: 0:01:29  lr: 0.000085  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 50/127]  eta: 0:01:18  lr: 0.000085  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 60/127]  eta: 0:01:07  lr: 0.000085  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 80/127]  eta: 0:00:47  lr: 0.000085  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [ 90/127]  eta: 0:00:36  lr: 0.000085  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:239]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:239] Total time: 0:02:06 (0.9928 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:239]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6734  data: 0.4480  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:239]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2363  data: 0.0321  max mem: 34254\n",
      "Valid: [epoch:239] Total time: 0:00:31 (2.2478 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_239_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [  0/127]  eta: 0:06:09  lr: 0.000085  loss: nan (nan)  time: 2.9111  data: 1.9563  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 10/127]  eta: 0:02:14  lr: 0.000085  loss: nan (nan)  time: 1.1475  data: 0.1779  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 20/127]  eta: 0:01:53  lr: 0.000085  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 30/127]  eta: 0:01:40  lr: 0.000085  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 40/127]  eta: 0:01:28  lr: 0.000085  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 50/127]  eta: 0:01:17  lr: 0.000085  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 60/127]  eta: 0:01:07  lr: 0.000085  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 70/127]  eta: 0:00:57  lr: 0.000085  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 80/127]  eta: 0:00:46  lr: 0.000085  loss: nan (nan)  time: 0.9806  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [ 90/127]  eta: 0:00:36  lr: 0.000085  loss: nan (nan)  time: 0.9810  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [100/127]  eta: 0:00:26  lr: 0.000085  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [110/127]  eta: 0:00:16  lr: 0.000085  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [120/127]  eta: 0:00:06  lr: 0.000085  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:240]  [126/127]  eta: 0:00:00  lr: 0.000085  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:240] Total time: 0:02:06 (0.9923 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:240]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6478  data: 0.4169  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:240]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2121  data: 0.0299  max mem: 34254\n",
      "Valid: [epoch:240] Total time: 0:00:31 (2.2221 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_240_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [  0/127]  eta: 0:06:17  lr: 0.000084  loss: nan (nan)  time: 2.9736  data: 2.0180  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 10/127]  eta: 0:02:14  lr: 0.000084  loss: nan (nan)  time: 1.1532  data: 0.1835  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 20/127]  eta: 0:01:54  lr: 0.000084  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 30/127]  eta: 0:01:40  lr: 0.000084  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 40/127]  eta: 0:01:28  lr: 0.000084  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 50/127]  eta: 0:01:17  lr: 0.000084  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 60/127]  eta: 0:01:07  lr: 0.000084  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 70/127]  eta: 0:00:57  lr: 0.000084  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 80/127]  eta: 0:00:46  lr: 0.000084  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [ 90/127]  eta: 0:00:36  lr: 0.000084  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [100/127]  eta: 0:00:26  lr: 0.000084  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:241]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9805  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:241] Total time: 0:02:05 (0.9919 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:241]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6807  data: 0.4057  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:241]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2491  data: 0.0291  max mem: 34254\n",
      "Valid: [epoch:241] Total time: 0:00:31 (2.2600 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_241_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [  0/127]  eta: 0:06:00  lr: 0.000084  loss: nan (nan)  time: 2.8371  data: 1.7835  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 10/127]  eta: 0:02:13  lr: 0.000084  loss: nan (nan)  time: 1.1385  data: 0.1622  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 20/127]  eta: 0:01:53  lr: 0.000084  loss: nan (nan)  time: 0.9701  data: 0.0003  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 30/127]  eta: 0:01:40  lr: 0.000084  loss: nan (nan)  time: 0.9749  data: 0.0003  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 40/127]  eta: 0:01:28  lr: 0.000084  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 50/127]  eta: 0:01:17  lr: 0.000084  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 60/127]  eta: 0:01:07  lr: 0.000084  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 70/127]  eta: 0:00:57  lr: 0.000084  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 80/127]  eta: 0:00:46  lr: 0.000084  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [ 90/127]  eta: 0:00:36  lr: 0.000084  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [100/127]  eta: 0:00:26  lr: 0.000084  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:242]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:242] Total time: 0:02:05 (0.9907 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:242]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6735  data: 0.4184  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:242]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2507  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:242] Total time: 0:00:31 (2.2625 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_242_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [  0/127]  eta: 0:05:59  lr: 0.000084  loss: nan (nan)  time: 2.8284  data: 1.8698  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 10/127]  eta: 0:02:13  lr: 0.000084  loss: nan (nan)  time: 1.1445  data: 0.1701  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 20/127]  eta: 0:01:53  lr: 0.000084  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 30/127]  eta: 0:01:40  lr: 0.000084  loss: nan (nan)  time: 0.9714  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 40/127]  eta: 0:01:28  lr: 0.000084  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 50/127]  eta: 0:01:17  lr: 0.000084  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 60/127]  eta: 0:01:07  lr: 0.000084  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 70/127]  eta: 0:00:56  lr: 0.000084  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 80/127]  eta: 0:00:46  lr: 0.000084  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [ 90/127]  eta: 0:00:36  lr: 0.000084  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [100/127]  eta: 0:00:26  lr: 0.000084  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:243]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:243] Total time: 0:02:05 (0.9905 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:243]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6122  data: 0.4119  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:243]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2881  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:243] Total time: 0:00:32 (2.2990 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_243_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [  0/127]  eta: 0:07:01  lr: 0.000084  loss: nan (nan)  time: 3.3205  data: 2.3637  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 10/127]  eta: 0:02:18  lr: 0.000084  loss: nan (nan)  time: 1.1842  data: 0.2150  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 20/127]  eta: 0:01:55  lr: 0.000084  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 30/127]  eta: 0:01:41  lr: 0.000084  loss: nan (nan)  time: 0.9716  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 40/127]  eta: 0:01:29  lr: 0.000084  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 50/127]  eta: 0:01:18  lr: 0.000084  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 60/127]  eta: 0:01:07  lr: 0.000084  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 70/127]  eta: 0:00:57  lr: 0.000084  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 80/127]  eta: 0:00:47  lr: 0.000084  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [ 90/127]  eta: 0:00:36  lr: 0.000084  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [100/127]  eta: 0:00:26  lr: 0.000084  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9785  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:244]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:244] Total time: 0:02:06 (0.9942 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:244]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6278  data: 0.4188  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:244]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2065  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:244] Total time: 0:00:31 (2.2166 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_244_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [  0/127]  eta: 0:06:43  lr: 0.000084  loss: nan (nan)  time: 3.1751  data: 2.2182  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 10/127]  eta: 0:02:17  lr: 0.000084  loss: nan (nan)  time: 1.1715  data: 0.2017  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 20/127]  eta: 0:01:55  lr: 0.000084  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 30/127]  eta: 0:01:41  lr: 0.000084  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 40/127]  eta: 0:01:29  lr: 0.000084  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 50/127]  eta: 0:01:18  lr: 0.000084  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 60/127]  eta: 0:01:07  lr: 0.000084  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 70/127]  eta: 0:00:57  lr: 0.000084  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 80/127]  eta: 0:00:47  lr: 0.000084  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [ 90/127]  eta: 0:00:36  lr: 0.000084  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [100/127]  eta: 0:00:26  lr: 0.000084  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:245]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:245] Total time: 0:02:06 (0.9928 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:245]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6886  data: 0.4665  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:245]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2076  data: 0.0334  max mem: 34254\n",
      "Valid: [epoch:245] Total time: 0:00:31 (2.2193 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_245_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [  0/127]  eta: 0:07:51  lr: 0.000084  loss: nan (nan)  time: 3.7125  data: 2.7113  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 10/127]  eta: 0:02:22  lr: 0.000084  loss: nan (nan)  time: 1.2201  data: 0.2466  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 20/127]  eta: 0:01:57  lr: 0.000084  loss: nan (nan)  time: 0.9714  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 30/127]  eta: 0:01:42  lr: 0.000084  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 40/127]  eta: 0:01:30  lr: 0.000084  loss: nan (nan)  time: 0.9767  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 50/127]  eta: 0:01:19  lr: 0.000084  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 60/127]  eta: 0:01:08  lr: 0.000084  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 70/127]  eta: 0:00:57  lr: 0.000084  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 80/127]  eta: 0:00:47  lr: 0.000084  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [ 90/127]  eta: 0:00:37  lr: 0.000084  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [100/127]  eta: 0:00:27  lr: 0.000084  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:246]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:246] Total time: 0:02:06 (0.9973 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:246]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6502  data: 0.4552  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:246]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1946  data: 0.0326  max mem: 34254\n",
      "Valid: [epoch:246] Total time: 0:00:30 (2.2046 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_246_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [  0/127]  eta: 0:06:55  lr: 0.000084  loss: nan (nan)  time: 3.2725  data: 2.2676  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 10/127]  eta: 0:02:18  lr: 0.000084  loss: nan (nan)  time: 1.1805  data: 0.2062  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 20/127]  eta: 0:01:55  lr: 0.000084  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 30/127]  eta: 0:01:41  lr: 0.000084  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 40/127]  eta: 0:01:29  lr: 0.000084  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 50/127]  eta: 0:01:18  lr: 0.000084  loss: nan (nan)  time: 0.9827  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 60/127]  eta: 0:01:08  lr: 0.000084  loss: nan (nan)  time: 0.9856  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 70/127]  eta: 0:00:57  lr: 0.000084  loss: nan (nan)  time: 0.9868  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 80/127]  eta: 0:00:47  lr: 0.000084  loss: nan (nan)  time: 0.9830  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [ 90/127]  eta: 0:00:37  lr: 0.000084  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [100/127]  eta: 0:00:27  lr: 0.000084  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:247]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:247] Total time: 0:02:06 (0.9976 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:247]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7017  data: 0.4198  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:247]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2655  data: 0.0301  max mem: 34254\n",
      "Valid: [epoch:247] Total time: 0:00:31 (2.2756 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_247_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [  0/127]  eta: 0:06:41  lr: 0.000084  loss: nan (nan)  time: 3.1586  data: 2.2018  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 10/127]  eta: 0:02:17  lr: 0.000084  loss: nan (nan)  time: 1.1715  data: 0.2003  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 20/127]  eta: 0:01:55  lr: 0.000084  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 30/127]  eta: 0:01:41  lr: 0.000084  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 40/127]  eta: 0:01:29  lr: 0.000084  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 50/127]  eta: 0:01:18  lr: 0.000084  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 60/127]  eta: 0:01:07  lr: 0.000084  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 70/127]  eta: 0:00:57  lr: 0.000084  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 80/127]  eta: 0:00:47  lr: 0.000084  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [ 90/127]  eta: 0:00:36  lr: 0.000084  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [100/127]  eta: 0:00:26  lr: 0.000084  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:248]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:248] Total time: 0:02:06 (0.9936 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:248]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6552  data: 0.4562  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:248]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2095  data: 0.0327  max mem: 34254\n",
      "Valid: [epoch:248] Total time: 0:00:31 (2.2211 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_248_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [  0/127]  eta: 0:07:04  lr: 0.000084  loss: nan (nan)  time: 3.3408  data: 2.3851  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 10/127]  eta: 0:02:18  lr: 0.000084  loss: nan (nan)  time: 1.1863  data: 0.2169  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 20/127]  eta: 0:01:56  lr: 0.000084  loss: nan (nan)  time: 0.9716  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 30/127]  eta: 0:01:41  lr: 0.000084  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 40/127]  eta: 0:01:29  lr: 0.000084  loss: nan (nan)  time: 0.9793  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 50/127]  eta: 0:01:18  lr: 0.000084  loss: nan (nan)  time: 0.9796  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 60/127]  eta: 0:01:08  lr: 0.000084  loss: nan (nan)  time: 0.9843  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 70/127]  eta: 0:00:57  lr: 0.000084  loss: nan (nan)  time: 0.9846  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 80/127]  eta: 0:00:47  lr: 0.000084  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [ 90/127]  eta: 0:00:37  lr: 0.000084  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [100/127]  eta: 0:00:27  lr: 0.000084  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [110/127]  eta: 0:00:16  lr: 0.000084  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [120/127]  eta: 0:00:06  lr: 0.000084  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:249]  [126/127]  eta: 0:00:00  lr: 0.000084  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:249] Total time: 0:02:06 (0.9972 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:249]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6447  data: 0.4201  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:249]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2174  data: 0.0301  max mem: 34254\n",
      "Valid: [epoch:249] Total time: 0:00:31 (2.2279 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_249_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [  0/127]  eta: 0:07:06  lr: 0.000083  loss: nan (nan)  time: 3.3562  data: 2.3913  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 10/127]  eta: 0:02:19  lr: 0.000083  loss: nan (nan)  time: 1.1883  data: 0.2175  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 20/127]  eta: 0:01:56  lr: 0.000083  loss: nan (nan)  time: 0.9717  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 30/127]  eta: 0:01:41  lr: 0.000083  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 40/127]  eta: 0:01:29  lr: 0.000083  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 50/127]  eta: 0:01:18  lr: 0.000083  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 60/127]  eta: 0:01:07  lr: 0.000083  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 70/127]  eta: 0:00:57  lr: 0.000083  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 80/127]  eta: 0:00:47  lr: 0.000083  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [ 90/127]  eta: 0:00:37  lr: 0.000083  loss: nan (nan)  time: 0.9793  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [100/127]  eta: 0:00:26  lr: 0.000083  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:250]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9738  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:250] Total time: 0:02:06 (0.9949 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:250]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6850  data: 0.4523  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:250]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2093  data: 0.0324  max mem: 34254\n",
      "Valid: [epoch:250] Total time: 0:00:31 (2.2196 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_250_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [  0/127]  eta: 0:07:30  lr: 0.000083  loss: nan (nan)  time: 3.5434  data: 2.5667  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 10/127]  eta: 0:02:20  lr: 0.000083  loss: nan (nan)  time: 1.2050  data: 0.2334  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 20/127]  eta: 0:01:57  lr: 0.000083  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 30/127]  eta: 0:01:42  lr: 0.000083  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 40/127]  eta: 0:01:30  lr: 0.000083  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 50/127]  eta: 0:01:18  lr: 0.000083  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 60/127]  eta: 0:01:08  lr: 0.000083  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 70/127]  eta: 0:00:57  lr: 0.000083  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 80/127]  eta: 0:00:47  lr: 0.000083  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [ 90/127]  eta: 0:00:37  lr: 0.000083  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [100/127]  eta: 0:00:27  lr: 0.000083  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:251]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:251] Total time: 0:02:06 (0.9964 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:251]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7246  data: 0.4544  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:251]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2465  data: 0.0326  max mem: 34254\n",
      "Valid: [epoch:251] Total time: 0:00:31 (2.2566 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_251_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [  0/127]  eta: 0:06:44  lr: 0.000083  loss: nan (nan)  time: 3.1853  data: 2.2260  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 10/127]  eta: 0:02:17  lr: 0.000083  loss: nan (nan)  time: 1.1725  data: 0.2025  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 20/127]  eta: 0:01:55  lr: 0.000083  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 30/127]  eta: 0:01:41  lr: 0.000083  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 40/127]  eta: 0:01:29  lr: 0.000083  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 50/127]  eta: 0:01:18  lr: 0.000083  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 60/127]  eta: 0:01:07  lr: 0.000083  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 70/127]  eta: 0:00:57  lr: 0.000083  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 80/127]  eta: 0:00:47  lr: 0.000083  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [ 90/127]  eta: 0:00:36  lr: 0.000083  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [100/127]  eta: 0:00:26  lr: 0.000083  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:252]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:252] Total time: 0:02:06 (0.9938 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:252]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6200  data: 0.4323  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:252]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1966  data: 0.0310  max mem: 34254\n",
      "Valid: [epoch:252] Total time: 0:00:30 (2.2066 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_252_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [  0/127]  eta: 0:05:52  lr: 0.000083  loss: nan (nan)  time: 2.7745  data: 1.7388  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 10/127]  eta: 0:02:12  lr: 0.000083  loss: nan (nan)  time: 1.1309  data: 0.1582  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 20/127]  eta: 0:01:52  lr: 0.000083  loss: nan (nan)  time: 0.9692  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 30/127]  eta: 0:01:39  lr: 0.000083  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 40/127]  eta: 0:01:28  lr: 0.000083  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 50/127]  eta: 0:01:17  lr: 0.000083  loss: nan (nan)  time: 0.9781  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 60/127]  eta: 0:01:07  lr: 0.000083  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 70/127]  eta: 0:00:56  lr: 0.000083  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 80/127]  eta: 0:00:46  lr: 0.000083  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [ 90/127]  eta: 0:00:36  lr: 0.000083  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [100/127]  eta: 0:00:26  lr: 0.000083  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:253]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:253]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:253] Total time: 0:02:05 (0.9898 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:253]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7220  data: 0.4734  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:253]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2210  data: 0.0339  max mem: 34254\n",
      "Valid: [epoch:253] Total time: 0:00:31 (2.2323 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_253_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [  0/127]  eta: 0:06:42  lr: 0.000083  loss: nan (nan)  time: 3.1690  data: 2.2107  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 10/127]  eta: 0:02:16  lr: 0.000083  loss: nan (nan)  time: 1.1703  data: 0.2011  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 20/127]  eta: 0:01:55  lr: 0.000083  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 30/127]  eta: 0:01:41  lr: 0.000083  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 40/127]  eta: 0:01:29  lr: 0.000083  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 50/127]  eta: 0:01:18  lr: 0.000083  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 60/127]  eta: 0:01:07  lr: 0.000083  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 70/127]  eta: 0:00:57  lr: 0.000083  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 80/127]  eta: 0:00:46  lr: 0.000083  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [ 90/127]  eta: 0:00:36  lr: 0.000083  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [100/127]  eta: 0:00:26  lr: 0.000083  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:254]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:254] Total time: 0:02:05 (0.9916 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:254]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6660  data: 0.4536  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:254]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1992  data: 0.0325  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:254] Total time: 0:00:30 (2.2103 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_254_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [  0/127]  eta: 0:05:17  lr: 0.000083  loss: nan (nan)  time: 2.5017  data: 1.4711  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 10/127]  eta: 0:02:11  lr: 0.000083  loss: nan (nan)  time: 1.1248  data: 0.1338  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 20/127]  eta: 0:01:52  lr: 0.000083  loss: nan (nan)  time: 0.9789  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 30/127]  eta: 0:01:39  lr: 0.000083  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 40/127]  eta: 0:01:28  lr: 0.000083  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 50/127]  eta: 0:01:17  lr: 0.000083  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 60/127]  eta: 0:01:06  lr: 0.000083  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 70/127]  eta: 0:00:56  lr: 0.000083  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 80/127]  eta: 0:00:46  lr: 0.000083  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [ 90/127]  eta: 0:00:36  lr: 0.000083  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [100/127]  eta: 0:00:26  lr: 0.000083  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:255]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:255] Total time: 0:02:05 (0.9888 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:255]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7065  data: 0.4459  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:255]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2723  data: 0.0319  max mem: 34254\n",
      "Valid: [epoch:255] Total time: 0:00:31 (2.2841 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_255_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [  0/127]  eta: 0:05:57  lr: 0.000083  loss: nan (nan)  time: 2.8132  data: 1.8314  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 10/127]  eta: 0:02:13  lr: 0.000083  loss: nan (nan)  time: 1.1387  data: 0.1666  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 20/127]  eta: 0:01:53  lr: 0.000083  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 30/127]  eta: 0:01:40  lr: 0.000083  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 40/127]  eta: 0:01:28  lr: 0.000083  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 50/127]  eta: 0:01:17  lr: 0.000083  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 60/127]  eta: 0:01:07  lr: 0.000083  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 70/127]  eta: 0:00:57  lr: 0.000083  loss: nan (nan)  time: 0.9794  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 80/127]  eta: 0:00:46  lr: 0.000083  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [ 90/127]  eta: 0:00:36  lr: 0.000083  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [100/127]  eta: 0:00:26  lr: 0.000083  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:256]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:256] Total time: 0:02:05 (0.9907 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:256]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6611  data: 0.4066  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:256]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2474  data: 0.0291  max mem: 34254\n",
      "Valid: [epoch:256] Total time: 0:00:31 (2.2594 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_256_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [  0/127]  eta: 0:05:47  lr: 0.000083  loss: nan (nan)  time: 2.7355  data: 1.7740  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 10/127]  eta: 0:02:13  lr: 0.000083  loss: nan (nan)  time: 1.1386  data: 0.1614  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 20/127]  eta: 0:01:53  lr: 0.000083  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 30/127]  eta: 0:01:39  lr: 0.000083  loss: nan (nan)  time: 0.9714  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 40/127]  eta: 0:01:28  lr: 0.000083  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 50/127]  eta: 0:01:17  lr: 0.000083  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 60/127]  eta: 0:01:07  lr: 0.000083  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 70/127]  eta: 0:00:56  lr: 0.000083  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 80/127]  eta: 0:00:46  lr: 0.000083  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [ 90/127]  eta: 0:00:36  lr: 0.000083  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [100/127]  eta: 0:00:26  lr: 0.000083  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:257]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:257] Total time: 0:02:05 (0.9896 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:257]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6945  data: 0.4342  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:257]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2429  data: 0.0311  max mem: 34254\n",
      "Valid: [epoch:257] Total time: 0:00:31 (2.2542 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_257_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [  0/127]  eta: 0:05:07  lr: 0.000083  loss: nan (nan)  time: 2.4245  data: 1.4624  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 10/127]  eta: 0:02:08  lr: 0.000083  loss: nan (nan)  time: 1.1024  data: 0.1330  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 20/127]  eta: 0:01:51  lr: 0.000083  loss: nan (nan)  time: 0.9717  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 30/127]  eta: 0:01:38  lr: 0.000083  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 40/127]  eta: 0:01:27  lr: 0.000083  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 50/127]  eta: 0:01:17  lr: 0.000083  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 60/127]  eta: 0:01:06  lr: 0.000083  loss: nan (nan)  time: 0.9789  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 70/127]  eta: 0:00:56  lr: 0.000083  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 80/127]  eta: 0:00:46  lr: 0.000083  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [ 90/127]  eta: 0:00:36  lr: 0.000083  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [100/127]  eta: 0:00:26  lr: 0.000083  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [110/127]  eta: 0:00:16  lr: 0.000083  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [120/127]  eta: 0:00:06  lr: 0.000083  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:258]  [126/127]  eta: 0:00:00  lr: 0.000083  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:258] Total time: 0:02:05 (0.9878 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:258]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6288  data: 0.3999  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:258]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2132  data: 0.0287  max mem: 34254\n",
      "Valid: [epoch:258] Total time: 0:00:31 (2.2255 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_258_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [  0/127]  eta: 0:06:46  lr: 0.000082  loss: nan (nan)  time: 3.2039  data: 2.2310  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 10/127]  eta: 0:02:18  lr: 0.000082  loss: nan (nan)  time: 1.1801  data: 0.2029  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 20/127]  eta: 0:01:55  lr: 0.000082  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 30/127]  eta: 0:01:41  lr: 0.000082  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 40/127]  eta: 0:01:29  lr: 0.000082  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 50/127]  eta: 0:01:18  lr: 0.000082  loss: nan (nan)  time: 0.9789  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 60/127]  eta: 0:01:07  lr: 0.000082  loss: nan (nan)  time: 0.9791  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 70/127]  eta: 0:00:57  lr: 0.000082  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 80/127]  eta: 0:00:47  lr: 0.000082  loss: nan (nan)  time: 0.9768  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [ 90/127]  eta: 0:00:37  lr: 0.000082  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:259]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:259] Total time: 0:02:06 (0.9954 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:259]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6281  data: 0.4299  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:259]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1932  data: 0.0308  max mem: 34254\n",
      "Valid: [epoch:259] Total time: 0:00:30 (2.2034 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_259_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [  0/127]  eta: 0:06:46  lr: 0.000082  loss: nan (nan)  time: 3.2021  data: 2.2431  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 10/127]  eta: 0:02:17  lr: 0.000082  loss: nan (nan)  time: 1.1749  data: 0.2040  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 20/127]  eta: 0:01:55  lr: 0.000082  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 30/127]  eta: 0:01:41  lr: 0.000082  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 40/127]  eta: 0:01:29  lr: 0.000082  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 50/127]  eta: 0:01:18  lr: 0.000082  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 60/127]  eta: 0:01:07  lr: 0.000082  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 70/127]  eta: 0:00:57  lr: 0.000082  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 80/127]  eta: 0:00:47  lr: 0.000082  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [ 90/127]  eta: 0:00:36  lr: 0.000082  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:260]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:260] Total time: 0:02:06 (0.9935 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:260]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6346  data: 0.4444  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:260]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2253  data: 0.0319  max mem: 34254\n",
      "Valid: [epoch:260] Total time: 0:00:31 (2.2353 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_260_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [  0/127]  eta: 0:05:42  lr: 0.000082  loss: nan (nan)  time: 2.6976  data: 1.7080  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 10/127]  eta: 0:02:12  lr: 0.000082  loss: nan (nan)  time: 1.1325  data: 0.1554  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 20/127]  eta: 0:01:53  lr: 0.000082  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 30/127]  eta: 0:01:39  lr: 0.000082  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 40/127]  eta: 0:01:28  lr: 0.000082  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 50/127]  eta: 0:01:17  lr: 0.000082  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 60/127]  eta: 0:01:07  lr: 0.000082  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 70/127]  eta: 0:00:56  lr: 0.000082  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 80/127]  eta: 0:00:46  lr: 0.000082  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [ 90/127]  eta: 0:00:36  lr: 0.000082  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9793  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9786  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:261]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:261] Total time: 0:02:05 (0.9902 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:261]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6852  data: 0.4758  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:261]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2107  data: 0.0341  max mem: 34254\n",
      "Valid: [epoch:261] Total time: 0:00:31 (2.2218 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_261_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [  0/127]  eta: 0:07:28  lr: 0.000082  loss: nan (nan)  time: 3.5280  data: 2.5626  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 10/127]  eta: 0:02:20  lr: 0.000082  loss: nan (nan)  time: 1.2023  data: 0.2331  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 20/127]  eta: 0:01:56  lr: 0.000082  loss: nan (nan)  time: 0.9706  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 30/127]  eta: 0:01:42  lr: 0.000082  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 40/127]  eta: 0:01:29  lr: 0.000082  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 50/127]  eta: 0:01:18  lr: 0.000082  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 60/127]  eta: 0:01:08  lr: 0.000082  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 70/127]  eta: 0:00:57  lr: 0.000082  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 80/127]  eta: 0:00:47  lr: 0.000082  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [ 90/127]  eta: 0:00:37  lr: 0.000082  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:262]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:262] Total time: 0:02:06 (0.9957 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:262]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7481  data: 0.4349  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:262]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2149  data: 0.0312  max mem: 34254\n",
      "Valid: [epoch:262] Total time: 0:00:31 (2.2263 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_262_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [  0/127]  eta: 0:05:15  lr: 0.000082  loss: nan (nan)  time: 2.4856  data: 1.5168  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 10/127]  eta: 0:02:09  lr: 0.000082  loss: nan (nan)  time: 1.1085  data: 0.1380  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 20/127]  eta: 0:01:51  lr: 0.000082  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 30/127]  eta: 0:01:39  lr: 0.000082  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 40/127]  eta: 0:01:27  lr: 0.000082  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 50/127]  eta: 0:01:17  lr: 0.000082  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 60/127]  eta: 0:01:06  lr: 0.000082  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 70/127]  eta: 0:00:56  lr: 0.000082  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 80/127]  eta: 0:00:46  lr: 0.000082  loss: nan (nan)  time: 0.9779  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [ 90/127]  eta: 0:00:36  lr: 0.000082  loss: nan (nan)  time: 0.9780  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9780  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9807  data: 0.0002  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:263]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9799  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:263] Total time: 0:02:05 (0.9894 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:263]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7033  data: 0.4141  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:263]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2386  data: 0.0297  max mem: 34254\n",
      "Valid: [epoch:263] Total time: 0:00:31 (2.2497 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_263_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [  0/127]  eta: 0:06:24  lr: 0.000082  loss: nan (nan)  time: 3.0257  data: 2.0680  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 10/127]  eta: 0:02:16  lr: 0.000082  loss: nan (nan)  time: 1.1634  data: 0.1881  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 20/127]  eta: 0:01:54  lr: 0.000082  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 30/127]  eta: 0:01:40  lr: 0.000082  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 40/127]  eta: 0:01:29  lr: 0.000082  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 50/127]  eta: 0:01:18  lr: 0.000082  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 60/127]  eta: 0:01:07  lr: 0.000082  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 70/127]  eta: 0:00:57  lr: 0.000082  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 80/127]  eta: 0:00:47  lr: 0.000082  loss: nan (nan)  time: 0.9801  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [ 90/127]  eta: 0:00:36  lr: 0.000082  loss: nan (nan)  time: 0.9806  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:264]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:264] Total time: 0:02:06 (0.9931 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:264]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7247  data: 0.4369  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:264]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2721  data: 0.0313  max mem: 34254\n",
      "Valid: [epoch:264] Total time: 0:00:31 (2.2829 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_264_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [  0/127]  eta: 0:07:01  lr: 0.000082  loss: nan (nan)  time: 3.3193  data: 2.3646  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 10/127]  eta: 0:02:19  lr: 0.000082  loss: nan (nan)  time: 1.1881  data: 0.2151  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 20/127]  eta: 0:01:56  lr: 0.000082  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 30/127]  eta: 0:01:41  lr: 0.000082  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 40/127]  eta: 0:01:29  lr: 0.000082  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 50/127]  eta: 0:01:18  lr: 0.000082  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 60/127]  eta: 0:01:07  lr: 0.000082  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 70/127]  eta: 0:00:57  lr: 0.000082  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 80/127]  eta: 0:00:47  lr: 0.000082  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [ 90/127]  eta: 0:00:37  lr: 0.000082  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:265]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9777  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:265] Total time: 0:02:06 (0.9949 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:265]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6561  data: 0.4260  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:265]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2237  data: 0.0305  max mem: 34254\n",
      "Valid: [epoch:265] Total time: 0:00:31 (2.2349 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_265_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [  0/127]  eta: 0:04:50  lr: 0.000082  loss: nan (nan)  time: 2.2877  data: 1.3247  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 10/127]  eta: 0:02:07  lr: 0.000082  loss: nan (nan)  time: 1.0902  data: 0.1205  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 20/127]  eta: 0:01:50  lr: 0.000082  loss: nan (nan)  time: 0.9704  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 30/127]  eta: 0:01:38  lr: 0.000082  loss: nan (nan)  time: 0.9717  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 40/127]  eta: 0:01:27  lr: 0.000082  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 50/127]  eta: 0:01:16  lr: 0.000082  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 60/127]  eta: 0:01:06  lr: 0.000082  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 70/127]  eta: 0:00:56  lr: 0.000082  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 80/127]  eta: 0:00:46  lr: 0.000082  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [ 90/127]  eta: 0:00:36  lr: 0.000082  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:266]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:266] Total time: 0:02:05 (0.9856 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:266]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6229  data: 0.4004  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:266]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1828  data: 0.0287  max mem: 34254\n",
      "Valid: [epoch:266] Total time: 0:00:30 (2.1932 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_266_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [  0/127]  eta: 0:05:50  lr: 0.000082  loss: nan (nan)  time: 2.7564  data: 1.7876  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 10/127]  eta: 0:02:12  lr: 0.000082  loss: nan (nan)  time: 1.1317  data: 0.1626  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 20/127]  eta: 0:01:52  lr: 0.000082  loss: nan (nan)  time: 0.9696  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 30/127]  eta: 0:01:39  lr: 0.000082  loss: nan (nan)  time: 0.9711  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 40/127]  eta: 0:01:28  lr: 0.000082  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 50/127]  eta: 0:01:17  lr: 0.000082  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 60/127]  eta: 0:01:07  lr: 0.000082  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 70/127]  eta: 0:00:56  lr: 0.000082  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 80/127]  eta: 0:00:46  lr: 0.000082  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [ 90/127]  eta: 0:00:36  lr: 0.000082  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [100/127]  eta: 0:00:26  lr: 0.000082  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [110/127]  eta: 0:00:16  lr: 0.000082  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [120/127]  eta: 0:00:06  lr: 0.000082  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:267]  [126/127]  eta: 0:00:00  lr: 0.000082  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:267] Total time: 0:02:05 (0.9890 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:267]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6790  data: 0.4628  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:267]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2277  data: 0.0331  max mem: 34254\n",
      "Valid: [epoch:267] Total time: 0:00:31 (2.2391 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_267_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [  0/127]  eta: 0:06:39  lr: 0.000081  loss: nan (nan)  time: 3.1437  data: 2.1159  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 10/127]  eta: 0:02:16  lr: 0.000081  loss: nan (nan)  time: 1.1682  data: 0.1924  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 20/127]  eta: 0:01:54  lr: 0.000081  loss: nan (nan)  time: 0.9709  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 30/127]  eta: 0:01:41  lr: 0.000081  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 40/127]  eta: 0:01:29  lr: 0.000081  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 50/127]  eta: 0:01:18  lr: 0.000081  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 60/127]  eta: 0:01:07  lr: 0.000081  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 70/127]  eta: 0:00:57  lr: 0.000081  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 80/127]  eta: 0:00:47  lr: 0.000081  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [ 90/127]  eta: 0:00:36  lr: 0.000081  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [100/127]  eta: 0:00:26  lr: 0.000081  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [110/127]  eta: 0:00:16  lr: 0.000081  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:268]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9766  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:268] Total time: 0:02:06 (0.9929 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:268]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7144  data: 0.4127  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:268]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2665  data: 0.0296  max mem: 34254\n",
      "Valid: [epoch:268] Total time: 0:00:31 (2.2786 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_268_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [  0/127]  eta: 0:06:55  lr: 0.000081  loss: nan (nan)  time: 3.2751  data: 2.3109  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 10/127]  eta: 0:02:18  lr: 0.000081  loss: nan (nan)  time: 1.1807  data: 0.2102  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 20/127]  eta: 0:01:55  lr: 0.000081  loss: nan (nan)  time: 0.9715  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 30/127]  eta: 0:01:41  lr: 0.000081  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 40/127]  eta: 0:01:29  lr: 0.000081  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 50/127]  eta: 0:01:18  lr: 0.000081  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 60/127]  eta: 0:01:07  lr: 0.000081  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 70/127]  eta: 0:00:57  lr: 0.000081  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 80/127]  eta: 0:00:47  lr: 0.000081  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [ 90/127]  eta: 0:00:36  lr: 0.000081  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [100/127]  eta: 0:00:26  lr: 0.000081  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [110/127]  eta: 0:00:16  lr: 0.000081  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:269]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:269] Total time: 0:02:06 (0.9937 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:269]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7413  data: 0.4405  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:269]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2865  data: 0.0316  max mem: 34254\n",
      "Valid: [epoch:269] Total time: 0:00:32 (2.2984 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_269_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [  0/127]  eta: 0:06:05  lr: 0.000081  loss: nan (nan)  time: 2.8792  data: 1.8703  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 10/127]  eta: 0:02:14  lr: 0.000081  loss: nan (nan)  time: 1.1524  data: 0.1701  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 20/127]  eta: 0:01:54  lr: 0.000081  loss: nan (nan)  time: 0.9752  data: 0.0002  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 30/127]  eta: 0:01:40  lr: 0.000081  loss: nan (nan)  time: 0.9716  data: 0.0002  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 40/127]  eta: 0:01:28  lr: 0.000081  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 50/127]  eta: 0:01:17  lr: 0.000081  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 60/127]  eta: 0:01:07  lr: 0.000081  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 70/127]  eta: 0:00:57  lr: 0.000081  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 80/127]  eta: 0:00:46  lr: 0.000081  loss: nan (nan)  time: 0.9773  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [ 90/127]  eta: 0:00:36  lr: 0.000081  loss: nan (nan)  time: 0.9771  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [100/127]  eta: 0:00:26  lr: 0.000081  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [110/127]  eta: 0:00:16  lr: 0.000081  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:270]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9780  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:270] Total time: 0:02:05 (0.9921 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:270]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7322  data: 0.4190  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:270]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2831  data: 0.0300  max mem: 34254\n",
      "Valid: [epoch:270] Total time: 0:00:32 (2.2951 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_270_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [  0/127]  eta: 0:06:44  lr: 0.000081  loss: nan (nan)  time: 3.1820  data: 2.2033  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 10/127]  eta: 0:02:17  lr: 0.000081  loss: nan (nan)  time: 1.1719  data: 0.2004  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 20/127]  eta: 0:01:55  lr: 0.000081  loss: nan (nan)  time: 0.9714  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 30/127]  eta: 0:01:41  lr: 0.000081  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 40/127]  eta: 0:01:29  lr: 0.000081  loss: nan (nan)  time: 0.9724  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 50/127]  eta: 0:01:18  lr: 0.000081  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 60/127]  eta: 0:01:07  lr: 0.000081  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 70/127]  eta: 0:00:57  lr: 0.000081  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 80/127]  eta: 0:00:47  lr: 0.000081  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [ 90/127]  eta: 0:00:36  lr: 0.000081  loss: nan (nan)  time: 0.9836  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [100/127]  eta: 0:00:26  lr: 0.000081  loss: nan (nan)  time: 0.9851  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [110/127]  eta: 0:00:16  lr: 0.000081  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9783  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:271]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:271] Total time: 0:02:06 (0.9948 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:271]  [ 0/14]  eta: 0:00:38  loss: nan (nan)  time: 2.7414  data: 0.4531  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:271]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2466  data: 0.0325  max mem: 34254\n",
      "Valid: [epoch:271] Total time: 0:00:31 (2.2571 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_271_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [  0/127]  eta: 0:07:02  lr: 0.000081  loss: nan (nan)  time: 3.3258  data: 2.3688  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 10/127]  eta: 0:02:18  lr: 0.000081  loss: nan (nan)  time: 1.1846  data: 0.2154  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 20/127]  eta: 0:01:55  lr: 0.000081  loss: nan (nan)  time: 0.9709  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 30/127]  eta: 0:01:41  lr: 0.000081  loss: nan (nan)  time: 0.9714  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 40/127]  eta: 0:01:29  lr: 0.000081  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 50/127]  eta: 0:01:18  lr: 0.000081  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 60/127]  eta: 0:01:07  lr: 0.000081  loss: nan (nan)  time: 0.9736  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 70/127]  eta: 0:00:57  lr: 0.000081  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 80/127]  eta: 0:00:47  lr: 0.000081  loss: nan (nan)  time: 0.9794  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [ 90/127]  eta: 0:00:37  lr: 0.000081  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [100/127]  eta: 0:00:26  lr: 0.000081  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [110/127]  eta: 0:00:16  lr: 0.000081  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:272]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:272] Total time: 0:02:06 (0.9945 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:272]  [ 0/14]  eta: 0:00:42  loss: nan (nan)  time: 3.0328  data: 0.4721  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:272]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2486  data: 0.0338  max mem: 34254\n",
      "Valid: [epoch:272] Total time: 0:00:31 (2.2615 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_272_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [  0/127]  eta: 0:07:42  lr: 0.000081  loss: nan (nan)  time: 3.6415  data: 2.6831  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 10/127]  eta: 0:02:23  lr: 0.000081  loss: nan (nan)  time: 1.2249  data: 0.2440  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 20/127]  eta: 0:01:58  lr: 0.000081  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 30/127]  eta: 0:01:42  lr: 0.000081  loss: nan (nan)  time: 0.9715  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 40/127]  eta: 0:01:30  lr: 0.000081  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 50/127]  eta: 0:01:19  lr: 0.000081  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 60/127]  eta: 0:01:08  lr: 0.000081  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 70/127]  eta: 0:00:57  lr: 0.000081  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 80/127]  eta: 0:00:47  lr: 0.000081  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [ 90/127]  eta: 0:00:37  lr: 0.000081  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [100/127]  eta: 0:00:27  lr: 0.000081  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [110/127]  eta: 0:00:16  lr: 0.000081  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:273]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:273] Total time: 0:02:06 (0.9974 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:273]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6071  data: 0.3924  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:273]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1636  data: 0.0281  max mem: 34254\n",
      "Valid: [epoch:273] Total time: 0:00:30 (2.1733 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_273_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [  0/127]  eta: 0:07:37  lr: 0.000081  loss: nan (nan)  time: 3.6025  data: 2.6453  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 10/127]  eta: 0:02:21  lr: 0.000081  loss: nan (nan)  time: 1.2099  data: 0.2406  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 20/127]  eta: 0:01:57  lr: 0.000081  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 30/127]  eta: 0:01:42  lr: 0.000081  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 40/127]  eta: 0:01:30  lr: 0.000081  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 50/127]  eta: 0:01:18  lr: 0.000081  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 60/127]  eta: 0:01:08  lr: 0.000081  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 70/127]  eta: 0:00:57  lr: 0.000081  loss: nan (nan)  time: 0.9807  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 80/127]  eta: 0:00:47  lr: 0.000081  loss: nan (nan)  time: 0.9830  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [ 90/127]  eta: 0:00:37  lr: 0.000081  loss: nan (nan)  time: 0.9809  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [100/127]  eta: 0:00:27  lr: 0.000081  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [110/127]  eta: 0:00:17  lr: 0.000081  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9796  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:274]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9787  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:274] Total time: 0:02:06 (0.9989 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:274]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6750  data: 0.4365  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:274]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2773  data: 0.0313  max mem: 34254\n",
      "Valid: [epoch:274] Total time: 0:00:32 (2.2882 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_274_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [  0/127]  eta: 0:05:20  lr: 0.000081  loss: nan (nan)  time: 2.5255  data: 1.5665  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 10/127]  eta: 0:02:10  lr: 0.000081  loss: nan (nan)  time: 1.1170  data: 0.1425  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 20/127]  eta: 0:01:52  lr: 0.000081  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 30/127]  eta: 0:01:39  lr: 0.000081  loss: nan (nan)  time: 0.9718  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 40/127]  eta: 0:01:27  lr: 0.000081  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 50/127]  eta: 0:01:17  lr: 0.000081  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 60/127]  eta: 0:01:06  lr: 0.000081  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 70/127]  eta: 0:00:56  lr: 0.000081  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 80/127]  eta: 0:00:46  lr: 0.000081  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [ 90/127]  eta: 0:00:36  lr: 0.000081  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [100/127]  eta: 0:00:26  lr: 0.000081  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [110/127]  eta: 0:00:16  lr: 0.000081  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:275]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:275] Total time: 0:02:05 (0.9873 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:275]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6759  data: 0.4290  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:275]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2634  data: 0.0307  max mem: 34254\n",
      "Valid: [epoch:275] Total time: 0:00:31 (2.2743 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_275_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [  0/127]  eta: 0:06:25  lr: 0.000081  loss: nan (nan)  time: 3.0334  data: 2.0778  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 10/127]  eta: 0:02:15  lr: 0.000081  loss: nan (nan)  time: 1.1582  data: 0.1890  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 20/127]  eta: 0:01:54  lr: 0.000081  loss: nan (nan)  time: 0.9712  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 30/127]  eta: 0:01:40  lr: 0.000081  loss: nan (nan)  time: 0.9715  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 40/127]  eta: 0:01:28  lr: 0.000081  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 50/127]  eta: 0:01:18  lr: 0.000081  loss: nan (nan)  time: 0.9774  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 60/127]  eta: 0:01:07  lr: 0.000081  loss: nan (nan)  time: 0.9790  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 70/127]  eta: 0:00:57  lr: 0.000081  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 80/127]  eta: 0:00:46  lr: 0.000081  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [ 90/127]  eta: 0:00:36  lr: 0.000081  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [100/127]  eta: 0:00:26  lr: 0.000081  loss: nan (nan)  time: 0.9757  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [110/127]  eta: 0:00:16  lr: 0.000081  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [120/127]  eta: 0:00:06  lr: 0.000081  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:276]  [126/127]  eta: 0:00:00  lr: 0.000081  loss: nan (nan)  time: 0.9759  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:276] Total time: 0:02:06 (0.9922 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:276]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6372  data: 0.4203  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:276]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2313  data: 0.0301  max mem: 34254\n",
      "Valid: [epoch:276] Total time: 0:00:31 (2.2434 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_276_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [  0/127]  eta: 0:06:27  lr: 0.000080  loss: nan (nan)  time: 3.0493  data: 2.0929  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 10/127]  eta: 0:02:16  lr: 0.000080  loss: nan (nan)  time: 1.1629  data: 0.1904  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 20/127]  eta: 0:01:54  lr: 0.000080  loss: nan (nan)  time: 0.9731  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 30/127]  eta: 0:01:40  lr: 0.000080  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 40/127]  eta: 0:01:29  lr: 0.000080  loss: nan (nan)  time: 0.9732  data: 0.0001  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 50/127]  eta: 0:01:18  lr: 0.000080  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 60/127]  eta: 0:01:07  lr: 0.000080  loss: nan (nan)  time: 0.9741  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 70/127]  eta: 0:00:57  lr: 0.000080  loss: nan (nan)  time: 0.9735  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 80/127]  eta: 0:00:46  lr: 0.000080  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [ 90/127]  eta: 0:00:36  lr: 0.000080  loss: nan (nan)  time: 0.9729  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9717  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:277]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:277] Total time: 0:02:05 (0.9912 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:277]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6086  data: 0.4292  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:277]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.1827  data: 0.0307  max mem: 34254\n",
      "Valid: [epoch:277] Total time: 0:00:30 (2.1923 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_277_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [  0/127]  eta: 0:07:06  lr: 0.000080  loss: nan (nan)  time: 3.3607  data: 2.4033  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 10/127]  eta: 0:02:19  lr: 0.000080  loss: nan (nan)  time: 1.1893  data: 0.2186  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 20/127]  eta: 0:01:56  lr: 0.000080  loss: nan (nan)  time: 0.9721  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 30/127]  eta: 0:01:41  lr: 0.000080  loss: nan (nan)  time: 0.9726  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 40/127]  eta: 0:01:29  lr: 0.000080  loss: nan (nan)  time: 0.9749  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 50/127]  eta: 0:01:18  lr: 0.000080  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 60/127]  eta: 0:01:07  lr: 0.000080  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 70/127]  eta: 0:00:57  lr: 0.000080  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 80/127]  eta: 0:00:47  lr: 0.000080  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [ 90/127]  eta: 0:00:37  lr: 0.000080  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9778  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9734  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:278]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:278] Total time: 0:02:06 (0.9951 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:278]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6637  data: 0.4493  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:278]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2455  data: 0.0322  max mem: 34254\n",
      "Valid: [epoch:278] Total time: 0:00:31 (2.2546 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_278_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [  0/127]  eta: 0:07:20  lr: 0.000080  loss: nan (nan)  time: 3.4663  data: 2.5075  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 10/127]  eta: 0:02:20  lr: 0.000080  loss: nan (nan)  time: 1.1998  data: 0.2280  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 20/127]  eta: 0:01:57  lr: 0.000080  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 30/127]  eta: 0:01:42  lr: 0.000080  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 40/127]  eta: 0:01:30  lr: 0.000080  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 50/127]  eta: 0:01:18  lr: 0.000080  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 60/127]  eta: 0:01:08  lr: 0.000080  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 70/127]  eta: 0:00:57  lr: 0.000080  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 80/127]  eta: 0:00:47  lr: 0.000080  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [ 90/127]  eta: 0:00:37  lr: 0.000080  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:279]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:279] Total time: 0:02:06 (0.9956 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:279]  [ 0/14]  eta: 0:00:36  loss: nan (nan)  time: 2.6174  data: 0.4049  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:279]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2199  data: 0.0290  max mem: 34254\n",
      "Valid: [epoch:279] Total time: 0:00:31 (2.2349 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_279_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [  0/127]  eta: 0:06:58  lr: 0.000080  loss: nan (nan)  time: 3.2944  data: 2.3371  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 10/127]  eta: 0:02:18  lr: 0.000080  loss: nan (nan)  time: 1.1827  data: 0.2126  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 20/127]  eta: 0:01:56  lr: 0.000080  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 30/127]  eta: 0:01:41  lr: 0.000080  loss: nan (nan)  time: 0.9788  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 40/127]  eta: 0:01:30  lr: 0.000080  loss: nan (nan)  time: 0.9784  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 50/127]  eta: 0:01:18  lr: 0.000080  loss: nan (nan)  time: 0.9789  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 60/127]  eta: 0:01:08  lr: 0.000080  loss: nan (nan)  time: 0.9746  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 70/127]  eta: 0:00:57  lr: 0.000080  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 80/127]  eta: 0:00:47  lr: 0.000080  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [ 90/127]  eta: 0:00:37  lr: 0.000080  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9769  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:280]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9739  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:280] Total time: 0:02:06 (0.9956 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:280]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6451  data: 0.4082  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:280]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2440  data: 0.0293  max mem: 34254\n",
      "Valid: [epoch:280] Total time: 0:00:31 (2.2533 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_280_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [  0/127]  eta: 0:07:21  lr: 0.000080  loss: nan (nan)  time: 3.4794  data: 2.5124  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 10/127]  eta: 0:02:20  lr: 0.000080  loss: nan (nan)  time: 1.1986  data: 0.2285  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 20/127]  eta: 0:01:56  lr: 0.000080  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 30/127]  eta: 0:01:42  lr: 0.000080  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 40/127]  eta: 0:01:29  lr: 0.000080  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 50/127]  eta: 0:01:18  lr: 0.000080  loss: nan (nan)  time: 0.9740  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 60/127]  eta: 0:01:07  lr: 0.000080  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 70/127]  eta: 0:00:57  lr: 0.000080  loss: nan (nan)  time: 0.9763  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 80/127]  eta: 0:00:47  lr: 0.000080  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [ 90/127]  eta: 0:00:37  lr: 0.000080  loss: nan (nan)  time: 0.9760  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:281]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:281] Total time: 0:02:06 (0.9956 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:281]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6763  data: 0.4414  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:281]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2213  data: 0.0316  max mem: 34254\n",
      "Valid: [epoch:281] Total time: 0:00:31 (2.2330 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_281_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [  0/127]  eta: 0:06:38  lr: 0.000080  loss: nan (nan)  time: 3.1385  data: 2.1618  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 10/127]  eta: 0:02:16  lr: 0.000080  loss: nan (nan)  time: 1.1687  data: 0.1966  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 20/127]  eta: 0:01:55  lr: 0.000080  loss: nan (nan)  time: 0.9722  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 30/127]  eta: 0:01:41  lr: 0.000080  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 40/127]  eta: 0:01:29  lr: 0.000080  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 50/127]  eta: 0:01:18  lr: 0.000080  loss: nan (nan)  time: 0.9733  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 60/127]  eta: 0:01:07  lr: 0.000080  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 70/127]  eta: 0:00:57  lr: 0.000080  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 80/127]  eta: 0:00:47  lr: 0.000080  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [ 90/127]  eta: 0:00:36  lr: 0.000080  loss: nan (nan)  time: 0.9770  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9762  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:282]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:282] Total time: 0:02:06 (0.9935 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:282]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6561  data: 0.4206  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:282]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2433  data: 0.0302  max mem: 34254\n",
      "Valid: [epoch:282] Total time: 0:00:31 (2.2539 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_282_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [  0/127]  eta: 0:05:41  lr: 0.000080  loss: nan (nan)  time: 2.6908  data: 1.7327  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 10/127]  eta: 0:02:11  lr: 0.000080  loss: nan (nan)  time: 1.1271  data: 0.1576  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 20/127]  eta: 0:01:52  lr: 0.000080  loss: nan (nan)  time: 0.9720  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 30/127]  eta: 0:01:39  lr: 0.000080  loss: nan (nan)  time: 0.9728  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 40/127]  eta: 0:01:28  lr: 0.000080  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 50/127]  eta: 0:01:17  lr: 0.000080  loss: nan (nan)  time: 0.9747  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 60/127]  eta: 0:01:07  lr: 0.000080  loss: nan (nan)  time: 0.9754  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 70/127]  eta: 0:00:56  lr: 0.000080  loss: nan (nan)  time: 0.9745  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 80/127]  eta: 0:00:46  lr: 0.000080  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [ 90/127]  eta: 0:00:36  lr: 0.000080  loss: nan (nan)  time: 0.9750  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9752  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9753  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9751  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:283]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9748  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:283] Total time: 0:02:05 (0.9891 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:283]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.6803  data: 0.4204  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:283]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2497  data: 0.0301  max mem: 34254\n",
      "Valid: [epoch:283] Total time: 0:00:31 (2.2646 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_283_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [  0/127]  eta: 0:06:25  lr: 0.000080  loss: nan (nan)  time: 3.0372  data: 2.0818  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 10/127]  eta: 0:02:16  lr: 0.000080  loss: nan (nan)  time: 1.1669  data: 0.1894  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 20/127]  eta: 0:01:54  lr: 0.000080  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 30/127]  eta: 0:01:40  lr: 0.000080  loss: nan (nan)  time: 0.9719  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 40/127]  eta: 0:01:29  lr: 0.000080  loss: nan (nan)  time: 0.9725  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 50/127]  eta: 0:01:18  lr: 0.000080  loss: nan (nan)  time: 0.9737  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 60/127]  eta: 0:01:07  lr: 0.000080  loss: nan (nan)  time: 0.9772  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 70/127]  eta: 0:00:57  lr: 0.000080  loss: nan (nan)  time: 0.9775  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 80/127]  eta: 0:00:47  lr: 0.000080  loss: nan (nan)  time: 0.9755  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [ 90/127]  eta: 0:00:36  lr: 0.000080  loss: nan (nan)  time: 0.9808  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9809  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:284]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9758  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:284]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:284] Total time: 0:02:06 (0.9939 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:284]  [ 0/14]  eta: 0:00:35  loss: nan (nan)  time: 2.5460  data: 0.3920  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:284]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.4053  data: 0.0281  max mem: 34254\n",
      "Valid: [epoch:284] Total time: 0:00:33 (2.4190 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_284_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [  0/127]  eta: 0:06:51  lr: 0.000080  loss: nan (nan)  time: 3.2400  data: 2.2722  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 10/127]  eta: 0:02:17  lr: 0.000080  loss: nan (nan)  time: 1.1779  data: 0.2067  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 20/127]  eta: 0:01:55  lr: 0.000080  loss: nan (nan)  time: 0.9723  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 30/127]  eta: 0:01:41  lr: 0.000080  loss: nan (nan)  time: 0.9727  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 40/127]  eta: 0:01:29  lr: 0.000080  loss: nan (nan)  time: 0.9730  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 50/127]  eta: 0:01:18  lr: 0.000080  loss: nan (nan)  time: 0.9764  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 60/127]  eta: 0:01:07  lr: 0.000080  loss: nan (nan)  time: 0.9776  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 70/127]  eta: 0:00:57  lr: 0.000080  loss: nan (nan)  time: 0.9761  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 80/127]  eta: 0:00:47  lr: 0.000080  loss: nan (nan)  time: 0.9765  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [ 90/127]  eta: 0:00:36  lr: 0.000080  loss: nan (nan)  time: 0.9756  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [100/127]  eta: 0:00:26  lr: 0.000080  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [110/127]  eta: 0:00:16  lr: 0.000080  loss: nan (nan)  time: 0.9743  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [120/127]  eta: 0:00:06  lr: 0.000080  loss: nan (nan)  time: 0.9744  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:285]  [126/127]  eta: 0:00:00  lr: 0.000080  loss: nan (nan)  time: 0.9742  data: 0.0001  max mem: 34254\n",
      "Train: [epoch:285] Total time: 0:02:06 (0.9939 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: nan (nan)\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:285]  [ 0/14]  eta: 0:00:37  loss: nan (nan)  time: 2.7037  data: 0.4393  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Valid: [epoch:285]  [13/14]  eta: 0:00:02  loss: nan (nan)  time: 2.2412  data: 0.0315  max mem: 34254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:285] Total time: 0:00:31 (2.2516 s / it)\n",
      "Averaged stats: loss: nan (nan)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/epoch_285_input_n_20.png\n",
      "loss of the network on the 14 valid images: nan%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 114.000\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:286]  [  0/127]  eta: 0:06:21  lr: 0.000079  loss: nan (nan)  time: 3.0075  data: 2.0529  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:286]  [ 10/127]  eta: 0:02:15  lr: 0.000079  loss: nan (nan)  time: 1.1559  data: 0.1867  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Loss is nan, stopping training\n",
      "Train: [epoch:286]  [ 20/127]  eta: 0:01:54  lr: 0.000079  loss: nan (nan)  time: 0.9713  data: 0.0001  max mem: 34254\n",
      "Loss is nan, stopping training\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 336, in <module>\n",
      "    main(args)\n",
      "  File \"train.py\", line 196, in main\n",
      "    train_stats = train_TED_Net_Previous(model, criterion, data_loader_train, optimizer, device, epoch, args.patch_training)\n",
      "  File \"/workspace/sunggu/4.Dose_img2img/scripts study/engine.py\", line 1375, in train_TED_Net_Previous\n",
      "    optimizer.step()\n",
      "  File \"/home/sunggu/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/sunggu/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/sunggu/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/sunggu/.local/lib/python3.8/site-packages/torch/optim/adamw.py\", line 137, in step\n",
      "    F.adamw(params_with_grad,\n",
      "  File \"/home/sunggu/.local/lib/python3.8/site-packages/torch/optim/_functional.py\", line 131, in adamw\n",
      "    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--batch-size 54 \\\n",
    "--epochs 1000 \\\n",
    "--lr_scheduler \"lambda\" \\\n",
    "--lr 1e-4 \\\n",
    "--data-set 'Sinogram_DCM' \\\n",
    "--model-name 'TED_Net' \\\n",
    "--criterion 'Perceptual+L1 Loss' \\\n",
    "--output_dir '/workspace/sunggu/4.Dose_img2img/model/[Previous]TED_Net_Mixed' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]TED_Net_Mixed/low2high/' \\\n",
    "--validate-every 2 \\\n",
    "--num_workers 4 \\\n",
    "--criterion_mode 'not balance' \\\n",
    "--multiple_GT \"False\" \\\n",
    "--patch_training \"True\" \\\n",
    "--multi-gpu-mode 'Single' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "import functools\n",
    "import pydicom\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore') \n",
    "\n",
    "\n",
    "def list_sort_nicely(l):   \n",
    "    def tryint(s):        \n",
    "        try:            \n",
    "            return int(s)        \n",
    "        except:            \n",
    "            return s\n",
    "        \n",
    "    def alphanum_key(s):\n",
    "        return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "    l.sort(key=alphanum_key)    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/20/*/*/*.dcm')) + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/20/*/*/*.dcm'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/X/*/*/*.dcm'))  + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/X/*/*/*.dcm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_hu(path):\n",
    "    # pydicom version...!\n",
    "    # referred from https://www.kaggle.com/gzuidhof/full-preprocessing-tutorial\n",
    "    # ref: pydicom.pixel_data_handlers.util.apply_modality_lut\n",
    "    # '''\n",
    "    # Awesome pydicom lut fuction...!\n",
    "    # ds  = pydicom.dcmread(fname)\n",
    "    # arr = ds.pixel_array\n",
    "    # hu  = apply_modality_lut(arr, ds)\n",
    "    # '''\n",
    "    dcm_image = pydicom.read_file(path)\n",
    "    image = dcm_image.pixel_array\n",
    "    image = image.astype(np.int16)\n",
    "    image[image == -2000] = 0\n",
    "\n",
    "    intercept = dcm_image.RescaleIntercept\n",
    "    slope     = dcm_image.RescaleSlope\n",
    "\n",
    "    if slope != 1:\n",
    "        image = slope * image.astype(np.float64)\n",
    "        image = image.astype(np.int16)\n",
    "\n",
    "    image += np.int16(intercept)\n",
    "    # print(image.shape) # (512, 512)\n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "def dicom_normalize(image, MIN_HU=-1024.0, MAX_HU=3071.0):   # I already check the max value is 3071.0\n",
    "   image = (image - MIN_HU) / (MAX_HU - MIN_HU)   # Range  0.0 ~ 1.0\n",
    "#    image = (image - 0.5) / 0.5                  # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n",
    "   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.transforms import *\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [Total]  number =  6899\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/20/*/*/*.dcm')) + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/20/*/*/*.dcm'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/X/*/*/*.dcm'))  + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/X/*/*/*.dcm'))\n",
    "\n",
    "files = [{\"n_20\": n_20, \"n_100\": n_100} for n_20, n_100 in zip(n_20_imgs, n_100_imgs)]            \n",
    "print(\"Train [Total]  number = \", len(n_20_imgs))\n",
    "\n",
    "# CT에 맞는 Augmentation\n",
    "\n",
    "transforms = Compose(\n",
    "    [\n",
    "        Lambdad(keys=[\"n_20\", \"n_100\"], func=get_pixels_hu),\n",
    "        Lambdad(keys=[\"n_20\", \"n_100\"], func=dicom_normalize),\n",
    "        AddChanneld(keys=[\"n_20\", \"n_100\"]),                 \n",
    "\n",
    "        # Crop  \n",
    "        # RandWeightedCropd(keys=[\"image\"], w_key=[\"image\"], spatial_size=(512,512,1), num_samples=1),\n",
    "        # RandSpatialCropd(keys=[\"image\"], roi_size=(512, 512), random_size=False, random_center=True),\n",
    "        # RandSpatialCropd(keys=[\"image\"], roi_size=(512,512,3), random_size=False, random_center=True),\n",
    "#         RandSpatialCropSamplesd(keys=[\"n_20\", \"n_100\"], roi_size=(64, 64), num_samples=8, random_center=True, random_size=False, meta_keys=None, allow_missing_keys=False), \n",
    "            # patch training, next(iter(loader)) output : list로 sample 만큼,,, 그 List 안에 (B, C, H, W)\n",
    "\n",
    "        # (45 degree rotation, vertical & horizontal flip & scaling)\n",
    "#         RandFlipd(keys=[\"n_20\", \"n_100\"], prob=0.1, spatial_axis=[0, 1], allow_missing_keys=False),\n",
    "#         RandRotated(keys=[\"n_20\", \"n_100\"], prob=0.1, range_x=np.pi/4, range_y=np.pi/4, range_z=0.0, keep_size=True, align_corners=False, allow_missing_keys=False),\n",
    "#         RandZoomd(keys=[\"n_20\", \"n_100\"], prob=0.1, min_zoom=0.5, max_zoom=2.0, align_corners=None, keep_size=True, allow_missing_keys=False),\n",
    "        ToTensord(keys=[\"n_20\", \"n_100\"]),\n",
    "    ]\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dataset(data=files, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_denormalize(image, MIN_HU=-1024.0, MAX_HU=3071.0):\n",
    "    # image = (image - 0.5) / 0.5           # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n",
    "    image = (MAX_HU - MIN_HU)*image + MIN_HU\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f68522fcc18>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dicom_denormalize(t[470]['n_20'].squeeze()), 'gray', vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f68520e9668>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dicom_denormalize(t[470]['n_100'].squeeze()), 'gray', vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0016, dtype=torch.float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "#         return torch.mean(torch.log(torch.cosh(torch.pow(ey_t, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LogCoshLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1081e-06, dtype=torch.float64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.1081e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(t[470]['n_100'], t[470]['n_100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019999999999999996"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000*4.1081e-06 - 1000*2.1081e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list = read_log(path = '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/log.txt')\n",
    "\n",
    "train_lr   = [ log_list[i]['train_lr'] for i in range(len(log_list)) ]\n",
    "train_loss = [ log_list[i]['train_loss'] for i in range(len(log_list)) ]\n",
    "valid_loss = [ log_list[i]['valid_loss'] for i in range(len(log_list)) ]\n",
    "epoch      = [ log_list[i]['epoch'] for i in range(len(log_list)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(train_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(valid_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(np.argsort(valid_loss)[:10]) & set(np.argsort(train_loss)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py \\\n",
    "--training-mode 'sinogram' \\\n",
    "--data-set 'TEST_Sinogram_DCM' \\\n",
    "--model-name 'ED_CNN' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Test/png/[Privious]ED_CNN/epoch_999/' \\\n",
    "--num_workers 4 \\\n",
    "--pin-mem \\\n",
    "--range-minus1-plus1 'False' \\\n",
    "--teacher_forcing \"False\" \\\n",
    "--resume '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/epoch_999_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 978 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Original === \n",
    "PSNR avg: 54.4628 \n",
    "SSIM avg: 0.9956 \n",
    "RMSE avg: 7.9607\n",
    "\n",
    "\n",
    "Predictions === \n",
    "PSNR avg: 57.6190 \n",
    "SSIM avg: 0.9980 \n",
    "RMSE avg: 5.5423\n",
    "***********************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
