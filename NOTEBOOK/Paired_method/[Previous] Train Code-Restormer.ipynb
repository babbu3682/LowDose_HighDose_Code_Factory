{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels==0.7.4\n",
    "# !pip install efficientnet-pytorch==0.6.3\n",
    "# !pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CUDA 11.1\n",
    "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE -> MAE Loss 꿀팁!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 12 03:59:22 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:18:00.0 Off |                  Off |\n",
      "| 46%   73C    P2   294W / 300W |  45498MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:3B:00.0 Off |                  Off |\n",
      "| 30%   30C    P8    18W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:86:00.0 Off |                  Off |\n",
      "| 30%   34C    P8    69W / 300W |    580MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:AF:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    16W / 300W |      1MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/4.Dose_img2img/scripts study\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/4.Dose_img2img/scripts study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 갯수 =  64\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"CPU 갯수 = \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "Dataset Name:  Sinogram_DCM\n",
      "---------- Model ----------\n",
      "Resume From:  /workspace/sunggu/4.Dose_img2img/model/[Previous]Restormer/epoch_780_checkpoint.pth\n",
      "Output To:  /workspace/sunggu/4.Dose_img2img/model/[Previous]Restormer\n",
      "Save   To:  /workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  1e-06\n",
      "Batchsize:  7\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6899\n",
      "Valid [Total]  number =  14\n",
      "Creating criterion: Perceptual+L1 Loss\n",
      "Creating model: Restormer\n",
      "Number of Learnable Params: 26109076\n",
      "Restormer(\n",
      "  (patch_embed): OverlapPatchEmbed(\n",
      "    (proj): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (encoder_level1): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(48, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=254, bias=False)\n",
      "        (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(48, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=254, bias=False)\n",
      "        (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(48, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=254, bias=False)\n",
      "        (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(48, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=254, bias=False)\n",
      "        (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down1_2): Downsample(\n",
      "    (body): Sequential(\n",
      "      (0): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): PixelUnshuffle(downscale_factor=2)\n",
      "    )\n",
      "  )\n",
      "  (encoder_level2): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down2_3): Downsample(\n",
      "    (body): Sequential(\n",
      "      (0): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): PixelUnshuffle(downscale_factor=2)\n",
      "    )\n",
      "  )\n",
      "  (encoder_level3): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down3_4): Downsample(\n",
      "    (body): Sequential(\n",
      "      (0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): PixelUnshuffle(downscale_factor=2)\n",
      "    )\n",
      "  )\n",
      "  (latent): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)\n",
      "        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)\n",
      "        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)\n",
      "        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)\n",
      "        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)\n",
      "        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)\n",
      "        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)\n",
      "        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(384, 2042, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(2042, 2042, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2042, bias=False)\n",
      "        (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up4_3): Upsample(\n",
      "    (body): Sequential(\n",
      "      (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): PixelShuffle(upscale_factor=2)\n",
      "    )\n",
      "  )\n",
      "  (reduce_chan_level3): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (decoder_level3): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(192, 1020, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(1020, 1020, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1020, bias=False)\n",
      "        (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up3_2): Upsample(\n",
      "    (body): Sequential(\n",
      "      (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): PixelShuffle(upscale_factor=2)\n",
      "    )\n",
      "  )\n",
      "  (reduce_chan_level2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (decoder_level2): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2_1): Upsample(\n",
      "    (body): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): PixelShuffle(upscale_factor=2)\n",
      "    )\n",
      "  )\n",
      "  (decoder_level1): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (refinement): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "        (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm(\n",
      "        (body): BiasFree_LayerNorm()\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (project_in): Conv2d(96, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=510, bias=False)\n",
      "        (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      ")\n",
      "Loading pre-trained Weight...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 1000 epochs\n",
      "Train: [epoch:781]  [  0/985]  eta: 0:45:47  lr: 0.000004  loss: 0.0307 (0.0307)  time: 2.7891  data: 1.2298  max mem: 41800\n",
      "Train: [epoch:781]  [ 10/985]  eta: 0:18:02  lr: 0.000004  loss: 0.0195 (0.0214)  time: 1.1107  data: 0.1119  max mem: 41892\n",
      "Train: [epoch:781]  [ 20/985]  eta: 0:16:26  lr: 0.000004  loss: 0.0185 (0.0197)  time: 0.9344  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [ 30/985]  eta: 0:15:46  lr: 0.000004  loss: 0.0187 (0.0199)  time: 0.9257  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [ 40/985]  eta: 0:15:22  lr: 0.000004  loss: 0.0192 (0.0193)  time: 0.9269  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [ 50/985]  eta: 0:15:04  lr: 0.000004  loss: 0.0165 (0.0188)  time: 0.9291  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [ 60/985]  eta: 0:14:49  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9314  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [ 70/985]  eta: 0:14:36  lr: 0.000004  loss: 0.0197 (0.0192)  time: 0.9343  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [ 80/985]  eta: 0:14:24  lr: 0.000004  loss: 0.0172 (0.0191)  time: 0.9360  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [ 90/985]  eta: 0:14:13  lr: 0.000004  loss: 0.0172 (0.0191)  time: 0.9372  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [100/985]  eta: 0:14:02  lr: 0.000004  loss: 0.0187 (0.0192)  time: 0.9389  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [110/985]  eta: 0:13:52  lr: 0.000004  loss: 0.0192 (0.0193)  time: 0.9414  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [120/985]  eta: 0:13:41  lr: 0.000004  loss: 0.0199 (0.0194)  time: 0.9421  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [130/985]  eta: 0:13:31  lr: 0.000004  loss: 0.0192 (0.0194)  time: 0.9415  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [140/985]  eta: 0:13:22  lr: 0.000004  loss: 0.0182 (0.0194)  time: 0.9424  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [150/985]  eta: 0:13:12  lr: 0.000004  loss: 0.0184 (0.0194)  time: 0.9431  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [160/985]  eta: 0:13:02  lr: 0.000004  loss: 0.0189 (0.0195)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [170/985]  eta: 0:12:53  lr: 0.000004  loss: 0.0189 (0.0194)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [180/985]  eta: 0:12:43  lr: 0.000004  loss: 0.0170 (0.0194)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [190/985]  eta: 0:12:34  lr: 0.000004  loss: 0.0184 (0.0195)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [200/985]  eta: 0:12:24  lr: 0.000004  loss: 0.0189 (0.0195)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [210/985]  eta: 0:12:14  lr: 0.000004  loss: 0.0178 (0.0194)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [220/985]  eta: 0:12:05  lr: 0.000004  loss: 0.0168 (0.0194)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [230/985]  eta: 0:11:55  lr: 0.000004  loss: 0.0172 (0.0193)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [240/985]  eta: 0:11:46  lr: 0.000004  loss: 0.0174 (0.0193)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [250/985]  eta: 0:11:37  lr: 0.000004  loss: 0.0184 (0.0193)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [260/985]  eta: 0:11:27  lr: 0.000004  loss: 0.0176 (0.0192)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [270/985]  eta: 0:11:18  lr: 0.000004  loss: 0.0167 (0.0192)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [280/985]  eta: 0:11:08  lr: 0.000004  loss: 0.0191 (0.0193)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [290/985]  eta: 0:10:59  lr: 0.000004  loss: 0.0198 (0.0193)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [300/985]  eta: 0:10:49  lr: 0.000004  loss: 0.0198 (0.0194)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [310/985]  eta: 0:10:40  lr: 0.000004  loss: 0.0183 (0.0193)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [320/985]  eta: 0:10:30  lr: 0.000004  loss: 0.0166 (0.0193)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [330/985]  eta: 0:10:21  lr: 0.000004  loss: 0.0172 (0.0193)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [340/985]  eta: 0:10:11  lr: 0.000004  loss: 0.0184 (0.0193)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [350/985]  eta: 0:10:02  lr: 0.000004  loss: 0.0180 (0.0193)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [360/985]  eta: 0:09:52  lr: 0.000004  loss: 0.0180 (0.0193)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [370/985]  eta: 0:09:43  lr: 0.000004  loss: 0.0185 (0.0193)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [380/985]  eta: 0:09:33  lr: 0.000004  loss: 0.0187 (0.0193)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [390/985]  eta: 0:09:24  lr: 0.000004  loss: 0.0191 (0.0193)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [400/985]  eta: 0:09:14  lr: 0.000004  loss: 0.0192 (0.0193)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [410/985]  eta: 0:09:05  lr: 0.000004  loss: 0.0200 (0.0193)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [420/985]  eta: 0:08:56  lr: 0.000004  loss: 0.0187 (0.0193)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [430/985]  eta: 0:08:46  lr: 0.000004  loss: 0.0171 (0.0192)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [440/985]  eta: 0:08:37  lr: 0.000004  loss: 0.0171 (0.0192)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [450/985]  eta: 0:08:27  lr: 0.000004  loss: 0.0178 (0.0192)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [460/985]  eta: 0:08:18  lr: 0.000004  loss: 0.0177 (0.0192)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [470/985]  eta: 0:08:08  lr: 0.000004  loss: 0.0177 (0.0192)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [480/985]  eta: 0:07:59  lr: 0.000004  loss: 0.0179 (0.0192)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [490/985]  eta: 0:07:49  lr: 0.000004  loss: 0.0187 (0.0192)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [500/985]  eta: 0:07:40  lr: 0.000004  loss: 0.0191 (0.0193)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [510/985]  eta: 0:07:30  lr: 0.000004  loss: 0.0191 (0.0193)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [520/985]  eta: 0:07:21  lr: 0.000004  loss: 0.0184 (0.0193)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [530/985]  eta: 0:07:11  lr: 0.000004  loss: 0.0187 (0.0193)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [540/985]  eta: 0:07:02  lr: 0.000004  loss: 0.0190 (0.0193)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [550/985]  eta: 0:06:52  lr: 0.000004  loss: 0.0176 (0.0193)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [560/985]  eta: 0:06:43  lr: 0.000004  loss: 0.0176 (0.0193)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [570/985]  eta: 0:06:33  lr: 0.000004  loss: 0.0173 (0.0192)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [580/985]  eta: 0:06:24  lr: 0.000004  loss: 0.0171 (0.0192)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [590/985]  eta: 0:06:14  lr: 0.000004  loss: 0.0177 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [600/985]  eta: 0:06:05  lr: 0.000004  loss: 0.0180 (0.0192)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [610/985]  eta: 0:05:55  lr: 0.000004  loss: 0.0175 (0.0192)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [620/985]  eta: 0:05:46  lr: 0.000004  loss: 0.0177 (0.0192)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [630/985]  eta: 0:05:36  lr: 0.000004  loss: 0.0173 (0.0192)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [640/985]  eta: 0:05:27  lr: 0.000004  loss: 0.0163 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [650/985]  eta: 0:05:17  lr: 0.000004  loss: 0.0180 (0.0192)  time: 0.9489  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:781]  [660/985]  eta: 0:05:08  lr: 0.000004  loss: 0.0186 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [670/985]  eta: 0:04:58  lr: 0.000004  loss: 0.0174 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [680/985]  eta: 0:04:49  lr: 0.000004  loss: 0.0167 (0.0191)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [690/985]  eta: 0:04:39  lr: 0.000004  loss: 0.0160 (0.0190)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [700/985]  eta: 0:04:30  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [710/985]  eta: 0:04:20  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [720/985]  eta: 0:04:11  lr: 0.000004  loss: 0.0180 (0.0190)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [730/985]  eta: 0:04:01  lr: 0.000004  loss: 0.0170 (0.0190)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [740/985]  eta: 0:03:52  lr: 0.000004  loss: 0.0170 (0.0190)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [750/985]  eta: 0:03:42  lr: 0.000004  loss: 0.0179 (0.0190)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [760/985]  eta: 0:03:33  lr: 0.000004  loss: 0.0180 (0.0190)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [770/985]  eta: 0:03:24  lr: 0.000004  loss: 0.0180 (0.0190)  time: 0.9462  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [780/985]  eta: 0:03:14  lr: 0.000004  loss: 0.0198 (0.0190)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [790/985]  eta: 0:03:05  lr: 0.000004  loss: 0.0176 (0.0190)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [800/985]  eta: 0:02:55  lr: 0.000004  loss: 0.0176 (0.0191)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [810/985]  eta: 0:02:46  lr: 0.000004  loss: 0.0217 (0.0191)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [820/985]  eta: 0:02:36  lr: 0.000004  loss: 0.0189 (0.0191)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [830/985]  eta: 0:02:27  lr: 0.000004  loss: 0.0186 (0.0191)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [840/985]  eta: 0:02:17  lr: 0.000004  loss: 0.0193 (0.0191)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [850/985]  eta: 0:02:08  lr: 0.000004  loss: 0.0167 (0.0191)  time: 0.9464  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [860/985]  eta: 0:01:58  lr: 0.000004  loss: 0.0177 (0.0191)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [870/985]  eta: 0:01:49  lr: 0.000004  loss: 0.0174 (0.0191)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [880/985]  eta: 0:01:39  lr: 0.000004  loss: 0.0177 (0.0191)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [890/985]  eta: 0:01:30  lr: 0.000004  loss: 0.0180 (0.0191)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [900/985]  eta: 0:01:20  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9464  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [910/985]  eta: 0:01:11  lr: 0.000004  loss: 0.0188 (0.0191)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [920/985]  eta: 0:01:01  lr: 0.000004  loss: 0.0188 (0.0191)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [930/985]  eta: 0:00:52  lr: 0.000004  loss: 0.0177 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [940/985]  eta: 0:00:42  lr: 0.000004  loss: 0.0175 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [950/985]  eta: 0:00:33  lr: 0.000004  loss: 0.0182 (0.0191)  time: 0.9461  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [960/985]  eta: 0:00:23  lr: 0.000004  loss: 0.0191 (0.0191)  time: 0.9462  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [970/985]  eta: 0:00:14  lr: 0.000004  loss: 0.0202 (0.0191)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [980/985]  eta: 0:00:04  lr: 0.000004  loss: 0.0202 (0.0191)  time: 0.9446  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781]  [984/985]  eta: 0:00:00  lr: 0.000004  loss: 0.0188 (0.0191)  time: 0.9446  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:781] Total time: 0:15:34 (0.9486 s / it)\n",
      "Averaged stats: lr: 0.000004  loss: 0.0188 (0.0191)\n",
      "Valid: [epoch:781]  [ 0/14]  eta: 0:02:35  loss: 0.0151 (0.0151)  time: 11.1350  data: 0.4630  max mem: 41892\n",
      "Valid: [epoch:781]  [13/14]  eta: 0:00:10  loss: 0.0146 (0.0145)  time: 10.9707  data: 0.0331  max mem: 41892\n",
      "Valid: [epoch:781] Total time: 0:02:33 (10.9763 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_781_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 781.000\n",
      "Train: [epoch:782]  [  0/985]  eta: 0:49:21  lr: 0.000004  loss: 0.0250 (0.0250)  time: 3.0062  data: 2.0477  max mem: 41892\n",
      "Train: [epoch:782]  [ 10/985]  eta: 0:18:21  lr: 0.000004  loss: 0.0218 (0.0211)  time: 1.1302  data: 0.1863  max mem: 41892\n",
      "Train: [epoch:782]  [ 20/985]  eta: 0:16:41  lr: 0.000004  loss: 0.0179 (0.0201)  time: 0.9390  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [ 30/985]  eta: 0:15:58  lr: 0.000004  loss: 0.0177 (0.0198)  time: 0.9347  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [ 40/985]  eta: 0:15:34  lr: 0.000004  loss: 0.0176 (0.0193)  time: 0.9375  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [ 50/985]  eta: 0:15:14  lr: 0.000004  loss: 0.0170 (0.0189)  time: 0.9384  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [ 60/985]  eta: 0:14:58  lr: 0.000004  loss: 0.0183 (0.0194)  time: 0.9369  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [ 70/985]  eta: 0:14:45  lr: 0.000004  loss: 0.0183 (0.0192)  time: 0.9389  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [ 80/985]  eta: 0:14:32  lr: 0.000004  loss: 0.0159 (0.0188)  time: 0.9401  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [ 90/985]  eta: 0:14:20  lr: 0.000004  loss: 0.0165 (0.0189)  time: 0.9419  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [100/985]  eta: 0:14:09  lr: 0.000004  loss: 0.0197 (0.0190)  time: 0.9434  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [110/985]  eta: 0:13:58  lr: 0.000004  loss: 0.0189 (0.0190)  time: 0.9454  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [120/985]  eta: 0:13:48  lr: 0.000004  loss: 0.0184 (0.0190)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [130/985]  eta: 0:13:37  lr: 0.000004  loss: 0.0184 (0.0190)  time: 0.9440  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [140/985]  eta: 0:13:27  lr: 0.000004  loss: 0.0186 (0.0190)  time: 0.9446  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [150/985]  eta: 0:13:17  lr: 0.000004  loss: 0.0179 (0.0190)  time: 0.9451  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [160/985]  eta: 0:13:07  lr: 0.000004  loss: 0.0178 (0.0190)  time: 0.9457  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [170/985]  eta: 0:12:57  lr: 0.000004  loss: 0.0171 (0.0189)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [180/985]  eta: 0:12:47  lr: 0.000004  loss: 0.0173 (0.0189)  time: 0.9452  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [190/985]  eta: 0:12:37  lr: 0.000004  loss: 0.0178 (0.0189)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [200/985]  eta: 0:12:27  lr: 0.000004  loss: 0.0191 (0.0190)  time: 0.9454  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [210/985]  eta: 0:12:18  lr: 0.000004  loss: 0.0177 (0.0190)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [220/985]  eta: 0:12:08  lr: 0.000004  loss: 0.0171 (0.0190)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [230/985]  eta: 0:11:58  lr: 0.000004  loss: 0.0177 (0.0191)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [240/985]  eta: 0:11:48  lr: 0.000004  loss: 0.0188 (0.0190)  time: 0.9461  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [250/985]  eta: 0:11:39  lr: 0.000004  loss: 0.0174 (0.0189)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [260/985]  eta: 0:11:29  lr: 0.000004  loss: 0.0175 (0.0190)  time: 0.9485  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:782]  [270/985]  eta: 0:11:20  lr: 0.000004  loss: 0.0187 (0.0190)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [280/985]  eta: 0:11:10  lr: 0.000004  loss: 0.0183 (0.0190)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [290/985]  eta: 0:11:00  lr: 0.000004  loss: 0.0186 (0.0191)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [300/985]  eta: 0:10:51  lr: 0.000004  loss: 0.0194 (0.0191)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [310/985]  eta: 0:10:41  lr: 0.000004  loss: 0.0185 (0.0191)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [320/985]  eta: 0:10:32  lr: 0.000004  loss: 0.0178 (0.0190)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [330/985]  eta: 0:10:22  lr: 0.000004  loss: 0.0179 (0.0190)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [340/985]  eta: 0:10:12  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [350/985]  eta: 0:10:03  lr: 0.000004  loss: 0.0178 (0.0190)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [360/985]  eta: 0:09:53  lr: 0.000004  loss: 0.0179 (0.0190)  time: 0.9465  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [370/985]  eta: 0:09:44  lr: 0.000004  loss: 0.0182 (0.0190)  time: 0.9464  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [380/985]  eta: 0:09:34  lr: 0.000004  loss: 0.0189 (0.0190)  time: 0.9465  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [390/985]  eta: 0:09:25  lr: 0.000004  loss: 0.0193 (0.0190)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [400/985]  eta: 0:09:15  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [410/985]  eta: 0:09:06  lr: 0.000004  loss: 0.0197 (0.0191)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [420/985]  eta: 0:08:56  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [430/985]  eta: 0:08:47  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9464  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [440/985]  eta: 0:08:37  lr: 0.000004  loss: 0.0200 (0.0192)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [450/985]  eta: 0:08:28  lr: 0.000004  loss: 0.0184 (0.0192)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [460/985]  eta: 0:08:18  lr: 0.000004  loss: 0.0186 (0.0192)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [470/985]  eta: 0:08:08  lr: 0.000004  loss: 0.0186 (0.0192)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [480/985]  eta: 0:07:59  lr: 0.000004  loss: 0.0182 (0.0192)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [490/985]  eta: 0:07:49  lr: 0.000004  loss: 0.0194 (0.0192)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [500/985]  eta: 0:07:40  lr: 0.000004  loss: 0.0194 (0.0192)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [510/985]  eta: 0:07:30  lr: 0.000004  loss: 0.0189 (0.0193)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [520/985]  eta: 0:07:21  lr: 0.000004  loss: 0.0189 (0.0193)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [530/985]  eta: 0:07:11  lr: 0.000004  loss: 0.0183 (0.0193)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [540/985]  eta: 0:07:02  lr: 0.000004  loss: 0.0180 (0.0193)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [550/985]  eta: 0:06:52  lr: 0.000004  loss: 0.0205 (0.0193)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [560/985]  eta: 0:06:43  lr: 0.000004  loss: 0.0207 (0.0193)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [570/985]  eta: 0:06:33  lr: 0.000004  loss: 0.0179 (0.0193)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [580/985]  eta: 0:06:24  lr: 0.000004  loss: 0.0195 (0.0193)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [590/985]  eta: 0:06:14  lr: 0.000004  loss: 0.0195 (0.0193)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [600/985]  eta: 0:06:05  lr: 0.000004  loss: 0.0179 (0.0193)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [610/985]  eta: 0:05:55  lr: 0.000004  loss: 0.0174 (0.0193)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [620/985]  eta: 0:05:46  lr: 0.000004  loss: 0.0183 (0.0193)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [630/985]  eta: 0:05:36  lr: 0.000004  loss: 0.0175 (0.0193)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [640/985]  eta: 0:05:27  lr: 0.000004  loss: 0.0171 (0.0193)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [650/985]  eta: 0:05:17  lr: 0.000004  loss: 0.0178 (0.0192)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [660/985]  eta: 0:05:08  lr: 0.000004  loss: 0.0171 (0.0192)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [670/985]  eta: 0:04:58  lr: 0.000004  loss: 0.0164 (0.0192)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [680/985]  eta: 0:04:49  lr: 0.000004  loss: 0.0168 (0.0192)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [690/985]  eta: 0:04:39  lr: 0.000004  loss: 0.0171 (0.0192)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [700/985]  eta: 0:04:30  lr: 0.000004  loss: 0.0186 (0.0192)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [710/985]  eta: 0:04:20  lr: 0.000004  loss: 0.0197 (0.0192)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [720/985]  eta: 0:04:11  lr: 0.000004  loss: 0.0193 (0.0192)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [730/985]  eta: 0:04:01  lr: 0.000004  loss: 0.0186 (0.0192)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [740/985]  eta: 0:03:52  lr: 0.000004  loss: 0.0185 (0.0192)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [750/985]  eta: 0:03:43  lr: 0.000004  loss: 0.0188 (0.0192)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [760/985]  eta: 0:03:33  lr: 0.000004  loss: 0.0199 (0.0192)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [770/985]  eta: 0:03:24  lr: 0.000004  loss: 0.0178 (0.0192)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [780/985]  eta: 0:03:14  lr: 0.000004  loss: 0.0174 (0.0192)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [790/985]  eta: 0:03:05  lr: 0.000004  loss: 0.0174 (0.0192)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [800/985]  eta: 0:02:55  lr: 0.000004  loss: 0.0170 (0.0192)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [810/985]  eta: 0:02:46  lr: 0.000004  loss: 0.0169 (0.0192)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [820/985]  eta: 0:02:36  lr: 0.000004  loss: 0.0176 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [830/985]  eta: 0:02:27  lr: 0.000004  loss: 0.0173 (0.0192)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [840/985]  eta: 0:02:17  lr: 0.000004  loss: 0.0171 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [850/985]  eta: 0:02:08  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [860/985]  eta: 0:01:58  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [870/985]  eta: 0:01:49  lr: 0.000004  loss: 0.0174 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [880/985]  eta: 0:01:39  lr: 0.000004  loss: 0.0168 (0.0191)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [890/985]  eta: 0:01:30  lr: 0.000004  loss: 0.0165 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [900/985]  eta: 0:01:20  lr: 0.000004  loss: 0.0182 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [910/985]  eta: 0:01:11  lr: 0.000004  loss: 0.0194 (0.0191)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [920/985]  eta: 0:01:01  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:782]  [930/985]  eta: 0:00:52  lr: 0.000004  loss: 0.0169 (0.0191)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [940/985]  eta: 0:00:42  lr: 0.000004  loss: 0.0176 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [950/985]  eta: 0:00:33  lr: 0.000004  loss: 0.0187 (0.0191)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [960/985]  eta: 0:00:23  lr: 0.000004  loss: 0.0195 (0.0191)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [970/985]  eta: 0:00:14  lr: 0.000004  loss: 0.0190 (0.0191)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [980/985]  eta: 0:00:04  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782]  [984/985]  eta: 0:00:00  lr: 0.000004  loss: 0.0190 (0.0191)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:782] Total time: 0:15:35 (0.9496 s / it)\n",
      "Averaged stats: lr: 0.000004  loss: 0.0190 (0.0191)\n",
      "Valid: [epoch:782]  [ 0/14]  eta: 0:02:46  loss: 0.0136 (0.0136)  time: 11.9123  data: 0.4624  max mem: 41892\n",
      "Valid: [epoch:782]  [13/14]  eta: 0:00:10  loss: 0.0148 (0.0147)  time: 10.8978  data: 0.0331  max mem: 41892\n",
      "Valid: [epoch:782] Total time: 0:02:32 (10.9038 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_782_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 781.000\n",
      "Train: [epoch:783]  [  0/985]  eta: 1:01:48  lr: 0.000004  loss: 0.0290 (0.0290)  time: 3.7648  data: 2.8073  max mem: 41892\n",
      "Train: [epoch:783]  [ 10/985]  eta: 0:19:21  lr: 0.000004  loss: 0.0193 (0.0200)  time: 1.1910  data: 0.2553  max mem: 41892\n",
      "Train: [epoch:783]  [ 20/985]  eta: 0:17:11  lr: 0.000004  loss: 0.0192 (0.0200)  time: 0.9341  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [ 30/985]  eta: 0:16:20  lr: 0.000004  loss: 0.0185 (0.0196)  time: 0.9366  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [ 40/985]  eta: 0:15:50  lr: 0.000004  loss: 0.0168 (0.0188)  time: 0.9394  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [ 50/985]  eta: 0:15:29  lr: 0.000004  loss: 0.0165 (0.0187)  time: 0.9420  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [ 60/985]  eta: 0:15:12  lr: 0.000004  loss: 0.0186 (0.0190)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [ 70/985]  eta: 0:14:57  lr: 0.000004  loss: 0.0186 (0.0188)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [ 80/985]  eta: 0:14:44  lr: 0.000004  loss: 0.0169 (0.0186)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [ 90/985]  eta: 0:14:31  lr: 0.000004  loss: 0.0180 (0.0185)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [100/985]  eta: 0:14:19  lr: 0.000004  loss: 0.0180 (0.0186)  time: 0.9462  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [110/985]  eta: 0:14:07  lr: 0.000004  loss: 0.0182 (0.0187)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [120/985]  eta: 0:13:56  lr: 0.000004  loss: 0.0197 (0.0189)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [130/985]  eta: 0:13:45  lr: 0.000004  loss: 0.0200 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [140/985]  eta: 0:13:35  lr: 0.000004  loss: 0.0192 (0.0191)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [150/985]  eta: 0:13:24  lr: 0.000004  loss: 0.0177 (0.0191)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [160/985]  eta: 0:13:14  lr: 0.000004  loss: 0.0177 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [170/985]  eta: 0:13:03  lr: 0.000004  loss: 0.0191 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [180/985]  eta: 0:12:54  lr: 0.000004  loss: 0.0191 (0.0192)  time: 0.9564  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [190/985]  eta: 0:12:44  lr: 0.000004  loss: 0.0185 (0.0193)  time: 0.9581  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [200/985]  eta: 0:12:34  lr: 0.000004  loss: 0.0183 (0.0193)  time: 0.9532  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [210/985]  eta: 0:12:24  lr: 0.000004  loss: 0.0182 (0.0193)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [220/985]  eta: 0:12:15  lr: 0.000004  loss: 0.0175 (0.0192)  time: 0.9604  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [230/985]  eta: 0:12:05  lr: 0.000004  loss: 0.0171 (0.0191)  time: 0.9594  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [240/985]  eta: 0:11:55  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9573  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [250/985]  eta: 0:11:46  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9610  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [260/985]  eta: 0:11:36  lr: 0.000004  loss: 0.0175 (0.0191)  time: 0.9629  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [270/985]  eta: 0:11:27  lr: 0.000004  loss: 0.0172 (0.0191)  time: 0.9616  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [280/985]  eta: 0:11:17  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9567  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [290/985]  eta: 0:11:07  lr: 0.000004  loss: 0.0185 (0.0191)  time: 0.9502  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [300/985]  eta: 0:10:57  lr: 0.000004  loss: 0.0185 (0.0191)  time: 0.9544  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:783]  [310/985]  eta: 0:10:47  lr: 0.000004  loss: 0.0185 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [320/985]  eta: 0:10:38  lr: 0.000004  loss: 0.0185 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [330/985]  eta: 0:10:28  lr: 0.000004  loss: 0.0178 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [340/985]  eta: 0:10:19  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [350/985]  eta: 0:10:09  lr: 0.000004  loss: 0.0180 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [360/985]  eta: 0:09:59  lr: 0.000004  loss: 0.0174 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [370/985]  eta: 0:09:50  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [380/985]  eta: 0:09:40  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [390/985]  eta: 0:09:31  lr: 0.000004  loss: 0.0180 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [400/985]  eta: 0:09:21  lr: 0.000004  loss: 0.0182 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [410/985]  eta: 0:09:11  lr: 0.000004  loss: 0.0188 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [420/985]  eta: 0:09:01  lr: 0.000004  loss: 0.0198 (0.0192)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [430/985]  eta: 0:08:52  lr: 0.000004  loss: 0.0191 (0.0192)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [440/985]  eta: 0:08:42  lr: 0.000004  loss: 0.0185 (0.0192)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [450/985]  eta: 0:08:32  lr: 0.000004  loss: 0.0182 (0.0192)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [460/985]  eta: 0:08:23  lr: 0.000004  loss: 0.0183 (0.0192)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [470/985]  eta: 0:08:13  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [480/985]  eta: 0:08:04  lr: 0.000004  loss: 0.0169 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [490/985]  eta: 0:07:54  lr: 0.000004  loss: 0.0185 (0.0192)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [500/985]  eta: 0:07:45  lr: 0.000004  loss: 0.0192 (0.0192)  time: 0.9630  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [510/985]  eta: 0:07:35  lr: 0.000004  loss: 0.0171 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [520/985]  eta: 0:07:25  lr: 0.000004  loss: 0.0178 (0.0192)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [530/985]  eta: 0:07:16  lr: 0.000004  loss: 0.0196 (0.0192)  time: 0.9606  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:783]  [540/985]  eta: 0:07:06  lr: 0.000004  loss: 0.0171 (0.0192)  time: 0.9667  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [550/985]  eta: 0:06:57  lr: 0.000004  loss: 0.0172 (0.0192)  time: 0.9627  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [560/985]  eta: 0:06:47  lr: 0.000004  loss: 0.0191 (0.0192)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [570/985]  eta: 0:06:37  lr: 0.000004  loss: 0.0182 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [580/985]  eta: 0:06:28  lr: 0.000004  loss: 0.0172 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [590/985]  eta: 0:06:18  lr: 0.000004  loss: 0.0187 (0.0192)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [600/985]  eta: 0:06:09  lr: 0.000004  loss: 0.0174 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [610/985]  eta: 0:05:59  lr: 0.000004  loss: 0.0174 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [620/985]  eta: 0:05:49  lr: 0.000004  loss: 0.0180 (0.0191)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [630/985]  eta: 0:05:40  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [640/985]  eta: 0:05:30  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [650/985]  eta: 0:05:20  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [660/985]  eta: 0:05:11  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [670/985]  eta: 0:05:01  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [680/985]  eta: 0:04:52  lr: 0.000004  loss: 0.0185 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [690/985]  eta: 0:04:42  lr: 0.000004  loss: 0.0187 (0.0191)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [700/985]  eta: 0:04:32  lr: 0.000004  loss: 0.0182 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [710/985]  eta: 0:04:23  lr: 0.000004  loss: 0.0190 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [720/985]  eta: 0:04:13  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [730/985]  eta: 0:04:04  lr: 0.000004  loss: 0.0166 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [740/985]  eta: 0:03:54  lr: 0.000004  loss: 0.0166 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [750/985]  eta: 0:03:44  lr: 0.000004  loss: 0.0187 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [760/985]  eta: 0:03:35  lr: 0.000004  loss: 0.0178 (0.0191)  time: 0.9633  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [770/985]  eta: 0:03:25  lr: 0.000004  loss: 0.0178 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [780/985]  eta: 0:03:16  lr: 0.000004  loss: 0.0180 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [790/985]  eta: 0:03:06  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [800/985]  eta: 0:02:57  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [810/985]  eta: 0:02:47  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [820/985]  eta: 0:02:37  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [830/985]  eta: 0:02:28  lr: 0.000004  loss: 0.0190 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [840/985]  eta: 0:02:18  lr: 0.000004  loss: 0.0178 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [850/985]  eta: 0:02:09  lr: 0.000004  loss: 0.0177 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [860/985]  eta: 0:01:59  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [870/985]  eta: 0:01:50  lr: 0.000004  loss: 0.0178 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [880/985]  eta: 0:01:40  lr: 0.000004  loss: 0.0180 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [890/985]  eta: 0:01:30  lr: 0.000004  loss: 0.0182 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [900/985]  eta: 0:01:21  lr: 0.000004  loss: 0.0188 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [910/985]  eta: 0:01:11  lr: 0.000004  loss: 0.0190 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [920/985]  eta: 0:01:02  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [930/985]  eta: 0:00:52  lr: 0.000004  loss: 0.0184 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [940/985]  eta: 0:00:43  lr: 0.000004  loss: 0.0186 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [950/985]  eta: 0:00:33  lr: 0.000004  loss: 0.0197 (0.0192)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [960/985]  eta: 0:00:23  lr: 0.000004  loss: 0.0197 (0.0192)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [970/985]  eta: 0:00:14  lr: 0.000004  loss: 0.0188 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [980/985]  eta: 0:00:04  lr: 0.000004  loss: 0.0181 (0.0192)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783]  [984/985]  eta: 0:00:00  lr: 0.000004  loss: 0.0184 (0.0192)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:783] Total time: 0:15:42 (0.9565 s / it)\n",
      "Averaged stats: lr: 0.000004  loss: 0.0184 (0.0192)\n",
      "Valid: [epoch:783]  [ 0/14]  eta: 0:02:40  loss: 0.0150 (0.0150)  time: 11.4729  data: 0.5534  max mem: 41892\n",
      "Valid: [epoch:783]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0146)  time: 11.1807  data: 0.0396  max mem: 41892\n",
      "Valid: [epoch:783] Total time: 0:02:36 (11.1868 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_783_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 781.000\n",
      "Train: [epoch:784]  [  0/985]  eta: 0:44:30  lr: 0.000003  loss: 0.0181 (0.0181)  time: 2.7113  data: 1.7348  max mem: 41892\n",
      "Train: [epoch:784]  [ 10/985]  eta: 0:17:50  lr: 0.000003  loss: 0.0197 (0.0199)  time: 1.0981  data: 0.1578  max mem: 41892\n",
      "Train: [epoch:784]  [ 20/985]  eta: 0:16:26  lr: 0.000003  loss: 0.0171 (0.0192)  time: 0.9377  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [ 30/985]  eta: 0:15:49  lr: 0.000003  loss: 0.0172 (0.0189)  time: 0.9367  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [ 40/985]  eta: 0:15:26  lr: 0.000003  loss: 0.0161 (0.0181)  time: 0.9371  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [ 50/985]  eta: 0:15:09  lr: 0.000003  loss: 0.0157 (0.0178)  time: 0.9391  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [ 60/985]  eta: 0:14:55  lr: 0.000003  loss: 0.0179 (0.0185)  time: 0.9436  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [ 70/985]  eta: 0:14:43  lr: 0.000003  loss: 0.0187 (0.0184)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [ 80/985]  eta: 0:14:32  lr: 0.000003  loss: 0.0168 (0.0185)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [ 90/985]  eta: 0:14:21  lr: 0.000003  loss: 0.0178 (0.0185)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [100/985]  eta: 0:14:11  lr: 0.000003  loss: 0.0174 (0.0184)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [110/985]  eta: 0:14:01  lr: 0.000003  loss: 0.0186 (0.0186)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [120/985]  eta: 0:13:51  lr: 0.000003  loss: 0.0186 (0.0188)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [130/985]  eta: 0:13:42  lr: 0.000003  loss: 0.0191 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [140/985]  eta: 0:13:32  lr: 0.000003  loss: 0.0192 (0.0192)  time: 0.9619  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:784]  [150/985]  eta: 0:13:22  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [160/985]  eta: 0:13:13  lr: 0.000003  loss: 0.0177 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [170/985]  eta: 0:13:03  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [180/985]  eta: 0:12:53  lr: 0.000003  loss: 0.0180 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [190/985]  eta: 0:12:43  lr: 0.000003  loss: 0.0187 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [200/985]  eta: 0:12:33  lr: 0.000003  loss: 0.0204 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [210/985]  eta: 0:12:24  lr: 0.000003  loss: 0.0201 (0.0193)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [220/985]  eta: 0:12:14  lr: 0.000003  loss: 0.0178 (0.0193)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [230/985]  eta: 0:12:04  lr: 0.000003  loss: 0.0178 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [240/985]  eta: 0:11:54  lr: 0.000003  loss: 0.0177 (0.0192)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [250/985]  eta: 0:11:44  lr: 0.000003  loss: 0.0177 (0.0192)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [260/985]  eta: 0:11:34  lr: 0.000003  loss: 0.0166 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [270/985]  eta: 0:11:25  lr: 0.000003  loss: 0.0168 (0.0192)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [280/985]  eta: 0:11:15  lr: 0.000003  loss: 0.0178 (0.0192)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [290/985]  eta: 0:11:06  lr: 0.000003  loss: 0.0172 (0.0192)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [300/985]  eta: 0:10:56  lr: 0.000003  loss: 0.0186 (0.0193)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [310/985]  eta: 0:10:46  lr: 0.000003  loss: 0.0186 (0.0192)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [320/985]  eta: 0:10:36  lr: 0.000003  loss: 0.0173 (0.0192)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [330/985]  eta: 0:10:27  lr: 0.000003  loss: 0.0179 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [340/985]  eta: 0:10:17  lr: 0.000003  loss: 0.0198 (0.0192)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [350/985]  eta: 0:10:07  lr: 0.000003  loss: 0.0188 (0.0193)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [360/985]  eta: 0:09:58  lr: 0.000003  loss: 0.0173 (0.0192)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [370/985]  eta: 0:09:48  lr: 0.000003  loss: 0.0185 (0.0193)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [380/985]  eta: 0:09:38  lr: 0.000003  loss: 0.0195 (0.0193)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [390/985]  eta: 0:09:29  lr: 0.000003  loss: 0.0195 (0.0193)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [400/985]  eta: 0:09:19  lr: 0.000003  loss: 0.0187 (0.0193)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [410/985]  eta: 0:09:09  lr: 0.000003  loss: 0.0181 (0.0193)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [420/985]  eta: 0:09:00  lr: 0.000003  loss: 0.0182 (0.0193)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [430/985]  eta: 0:08:50  lr: 0.000003  loss: 0.0184 (0.0193)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [440/985]  eta: 0:08:41  lr: 0.000003  loss: 0.0195 (0.0193)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [450/985]  eta: 0:08:31  lr: 0.000003  loss: 0.0187 (0.0193)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [460/985]  eta: 0:08:21  lr: 0.000003  loss: 0.0181 (0.0193)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [470/985]  eta: 0:08:12  lr: 0.000003  loss: 0.0179 (0.0193)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [480/985]  eta: 0:08:02  lr: 0.000003  loss: 0.0178 (0.0193)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [490/985]  eta: 0:07:53  lr: 0.000003  loss: 0.0191 (0.0193)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [500/985]  eta: 0:07:43  lr: 0.000003  loss: 0.0203 (0.0193)  time: 0.9640  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [510/985]  eta: 0:07:34  lr: 0.000003  loss: 0.0174 (0.0193)  time: 0.9643  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [520/985]  eta: 0:07:24  lr: 0.000003  loss: 0.0174 (0.0193)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [530/985]  eta: 0:07:15  lr: 0.000003  loss: 0.0178 (0.0193)  time: 0.9654  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [540/985]  eta: 0:07:05  lr: 0.000003  loss: 0.0176 (0.0193)  time: 0.9646  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [550/985]  eta: 0:06:56  lr: 0.000003  loss: 0.0168 (0.0192)  time: 0.9631  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [560/985]  eta: 0:06:46  lr: 0.000003  loss: 0.0174 (0.0192)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [570/985]  eta: 0:06:37  lr: 0.000003  loss: 0.0184 (0.0192)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [580/985]  eta: 0:06:27  lr: 0.000003  loss: 0.0182 (0.0192)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [590/985]  eta: 0:06:18  lr: 0.000003  loss: 0.0183 (0.0193)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [600/985]  eta: 0:06:08  lr: 0.000003  loss: 0.0185 (0.0193)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [610/985]  eta: 0:05:58  lr: 0.000003  loss: 0.0175 (0.0192)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [620/985]  eta: 0:05:49  lr: 0.000003  loss: 0.0180 (0.0192)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [630/985]  eta: 0:05:39  lr: 0.000003  loss: 0.0174 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [640/985]  eta: 0:05:30  lr: 0.000003  loss: 0.0172 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [650/985]  eta: 0:05:20  lr: 0.000003  loss: 0.0176 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [660/985]  eta: 0:05:10  lr: 0.000003  loss: 0.0176 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [670/985]  eta: 0:05:01  lr: 0.000003  loss: 0.0177 (0.0192)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [680/985]  eta: 0:04:51  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [690/985]  eta: 0:04:42  lr: 0.000003  loss: 0.0177 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [700/985]  eta: 0:04:32  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [710/985]  eta: 0:04:22  lr: 0.000003  loss: 0.0195 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [720/985]  eta: 0:04:13  lr: 0.000003  loss: 0.0195 (0.0192)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [730/985]  eta: 0:04:03  lr: 0.000003  loss: 0.0178 (0.0192)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [740/985]  eta: 0:03:54  lr: 0.000003  loss: 0.0201 (0.0192)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [750/985]  eta: 0:03:44  lr: 0.000003  loss: 0.0191 (0.0192)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [760/985]  eta: 0:03:35  lr: 0.000003  loss: 0.0182 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [770/985]  eta: 0:03:25  lr: 0.000003  loss: 0.0171 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [780/985]  eta: 0:03:15  lr: 0.000003  loss: 0.0175 (0.0192)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [790/985]  eta: 0:03:06  lr: 0.000003  loss: 0.0174 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [800/985]  eta: 0:02:56  lr: 0.000003  loss: 0.0170 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:784]  [810/985]  eta: 0:02:47  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [820/985]  eta: 0:02:37  lr: 0.000003  loss: 0.0177 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [830/985]  eta: 0:02:28  lr: 0.000003  loss: 0.0191 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [840/985]  eta: 0:02:18  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [850/985]  eta: 0:02:09  lr: 0.000003  loss: 0.0164 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [860/985]  eta: 0:01:59  lr: 0.000003  loss: 0.0179 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [870/985]  eta: 0:01:49  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [880/985]  eta: 0:01:40  lr: 0.000003  loss: 0.0172 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [890/985]  eta: 0:01:30  lr: 0.000003  loss: 0.0184 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [900/985]  eta: 0:01:21  lr: 0.000003  loss: 0.0185 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [910/985]  eta: 0:01:11  lr: 0.000003  loss: 0.0193 (0.0192)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [920/985]  eta: 0:01:02  lr: 0.000003  loss: 0.0194 (0.0192)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [930/985]  eta: 0:00:52  lr: 0.000003  loss: 0.0181 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [940/985]  eta: 0:00:42  lr: 0.000003  loss: 0.0183 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [950/985]  eta: 0:00:33  lr: 0.000003  loss: 0.0190 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [960/985]  eta: 0:00:23  lr: 0.000003  loss: 0.0196 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [970/985]  eta: 0:00:14  lr: 0.000003  loss: 0.0183 (0.0192)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [980/985]  eta: 0:00:04  lr: 0.000003  loss: 0.0174 (0.0192)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784]  [984/985]  eta: 0:00:00  lr: 0.000003  loss: 0.0174 (0.0192)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:784] Total time: 0:15:40 (0.9553 s / it)\n",
      "Averaged stats: lr: 0.000003  loss: 0.0174 (0.0192)\n",
      "Valid: [epoch:784]  [ 0/14]  eta: 0:02:41  loss: 0.0141 (0.0141)  time: 11.5481  data: 0.5282  max mem: 41892\n",
      "Valid: [epoch:784]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0146)  time: 11.1840  data: 0.0378  max mem: 41892\n",
      "Valid: [epoch:784] Total time: 0:02:36 (11.1896 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_784_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.015\n",
      "Best Epoch: 781.000\n",
      "Train: [epoch:785]  [  0/985]  eta: 0:47:52  lr: 0.000003  loss: 0.0199 (0.0199)  time: 2.9160  data: 1.9580  max mem: 41892\n",
      "Train: [epoch:785]  [ 10/985]  eta: 0:18:06  lr: 0.000003  loss: 0.0199 (0.0191)  time: 1.1145  data: 0.1781  max mem: 41892\n",
      "Train: [epoch:785]  [ 20/985]  eta: 0:16:31  lr: 0.000003  loss: 0.0190 (0.0189)  time: 0.9331  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [ 30/985]  eta: 0:15:52  lr: 0.000003  loss: 0.0179 (0.0188)  time: 0.9331  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [ 40/985]  eta: 0:15:28  lr: 0.000003  loss: 0.0173 (0.0185)  time: 0.9356  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [ 50/985]  eta: 0:15:10  lr: 0.000003  loss: 0.0166 (0.0182)  time: 0.9376  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [ 60/985]  eta: 0:14:56  lr: 0.000003  loss: 0.0180 (0.0184)  time: 0.9429  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [ 70/985]  eta: 0:14:43  lr: 0.000003  loss: 0.0180 (0.0183)  time: 0.9447  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [ 80/985]  eta: 0:14:31  lr: 0.000003  loss: 0.0164 (0.0180)  time: 0.9442  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [ 90/985]  eta: 0:14:20  lr: 0.000003  loss: 0.0173 (0.0182)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [100/985]  eta: 0:14:09  lr: 0.000003  loss: 0.0182 (0.0184)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [110/985]  eta: 0:13:59  lr: 0.000003  loss: 0.0177 (0.0185)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [120/985]  eta: 0:13:49  lr: 0.000003  loss: 0.0185 (0.0187)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [130/985]  eta: 0:13:38  lr: 0.000003  loss: 0.0202 (0.0187)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [140/985]  eta: 0:13:28  lr: 0.000003  loss: 0.0202 (0.0188)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [150/985]  eta: 0:13:18  lr: 0.000003  loss: 0.0186 (0.0188)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [160/985]  eta: 0:13:08  lr: 0.000003  loss: 0.0180 (0.0188)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [170/985]  eta: 0:12:59  lr: 0.000003  loss: 0.0180 (0.0188)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [180/985]  eta: 0:12:49  lr: 0.000003  loss: 0.0197 (0.0190)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [190/985]  eta: 0:12:39  lr: 0.000003  loss: 0.0186 (0.0190)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [200/985]  eta: 0:12:29  lr: 0.000003  loss: 0.0180 (0.0190)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [210/985]  eta: 0:12:20  lr: 0.000003  loss: 0.0189 (0.0190)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [220/985]  eta: 0:12:10  lr: 0.000003  loss: 0.0175 (0.0189)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [230/985]  eta: 0:12:00  lr: 0.000003  loss: 0.0175 (0.0189)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [240/985]  eta: 0:11:51  lr: 0.000003  loss: 0.0174 (0.0189)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [250/985]  eta: 0:11:41  lr: 0.000003  loss: 0.0167 (0.0188)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [260/985]  eta: 0:11:31  lr: 0.000003  loss: 0.0167 (0.0188)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [270/985]  eta: 0:11:22  lr: 0.000003  loss: 0.0170 (0.0188)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [280/985]  eta: 0:11:12  lr: 0.000003  loss: 0.0172 (0.0187)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [290/985]  eta: 0:11:02  lr: 0.000003  loss: 0.0188 (0.0188)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [300/985]  eta: 0:10:53  lr: 0.000003  loss: 0.0192 (0.0189)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [310/985]  eta: 0:10:43  lr: 0.000003  loss: 0.0179 (0.0189)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [320/985]  eta: 0:10:33  lr: 0.000003  loss: 0.0173 (0.0189)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [330/985]  eta: 0:10:24  lr: 0.000003  loss: 0.0173 (0.0189)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [340/985]  eta: 0:10:14  lr: 0.000003  loss: 0.0181 (0.0190)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [350/985]  eta: 0:10:05  lr: 0.000003  loss: 0.0179 (0.0189)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [360/985]  eta: 0:09:55  lr: 0.000003  loss: 0.0182 (0.0190)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [370/985]  eta: 0:09:46  lr: 0.000003  loss: 0.0192 (0.0190)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [380/985]  eta: 0:09:36  lr: 0.000003  loss: 0.0182 (0.0190)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [390/985]  eta: 0:09:26  lr: 0.000003  loss: 0.0225 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [400/985]  eta: 0:09:17  lr: 0.000003  loss: 0.0204 (0.0192)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [410/985]  eta: 0:09:07  lr: 0.000003  loss: 0.0195 (0.0192)  time: 0.9503  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:785]  [420/985]  eta: 0:08:58  lr: 0.000003  loss: 0.0170 (0.0192)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [430/985]  eta: 0:08:48  lr: 0.000003  loss: 0.0170 (0.0192)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [440/985]  eta: 0:08:39  lr: 0.000003  loss: 0.0186 (0.0192)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [450/985]  eta: 0:08:29  lr: 0.000003  loss: 0.0179 (0.0192)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [460/985]  eta: 0:08:19  lr: 0.000003  loss: 0.0176 (0.0192)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [470/985]  eta: 0:08:10  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [480/985]  eta: 0:08:00  lr: 0.000003  loss: 0.0188 (0.0192)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [490/985]  eta: 0:07:51  lr: 0.000003  loss: 0.0199 (0.0192)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [500/985]  eta: 0:07:41  lr: 0.000003  loss: 0.0196 (0.0192)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [510/985]  eta: 0:07:32  lr: 0.000003  loss: 0.0183 (0.0192)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [520/985]  eta: 0:07:22  lr: 0.000003  loss: 0.0179 (0.0192)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [530/985]  eta: 0:07:13  lr: 0.000003  loss: 0.0200 (0.0192)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [540/985]  eta: 0:07:03  lr: 0.000003  loss: 0.0184 (0.0192)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [550/985]  eta: 0:06:54  lr: 0.000003  loss: 0.0177 (0.0192)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [560/985]  eta: 0:06:44  lr: 0.000003  loss: 0.0171 (0.0192)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [570/985]  eta: 0:06:34  lr: 0.000003  loss: 0.0170 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [580/985]  eta: 0:06:25  lr: 0.000003  loss: 0.0173 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [590/985]  eta: 0:06:15  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [600/985]  eta: 0:06:06  lr: 0.000003  loss: 0.0173 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [610/985]  eta: 0:05:56  lr: 0.000003  loss: 0.0172 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [620/985]  eta: 0:05:47  lr: 0.000003  loss: 0.0185 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [630/985]  eta: 0:05:37  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [640/985]  eta: 0:05:28  lr: 0.000003  loss: 0.0179 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [650/985]  eta: 0:05:18  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [660/985]  eta: 0:05:09  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [670/985]  eta: 0:04:59  lr: 0.000003  loss: 0.0163 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [680/985]  eta: 0:04:50  lr: 0.000003  loss: 0.0163 (0.0190)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [690/985]  eta: 0:04:40  lr: 0.000003  loss: 0.0173 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [700/985]  eta: 0:04:31  lr: 0.000003  loss: 0.0190 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [710/985]  eta: 0:04:21  lr: 0.000003  loss: 0.0196 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [720/985]  eta: 0:04:12  lr: 0.000003  loss: 0.0193 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [730/985]  eta: 0:04:02  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [740/985]  eta: 0:03:53  lr: 0.000003  loss: 0.0174 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [750/985]  eta: 0:03:43  lr: 0.000003  loss: 0.0179 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [760/985]  eta: 0:03:34  lr: 0.000003  loss: 0.0174 (0.0190)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [770/985]  eta: 0:03:24  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [780/985]  eta: 0:03:15  lr: 0.000003  loss: 0.0198 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [790/985]  eta: 0:03:05  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [800/985]  eta: 0:02:56  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [810/985]  eta: 0:02:46  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [820/985]  eta: 0:02:36  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [830/985]  eta: 0:02:27  lr: 0.000003  loss: 0.0172 (0.0191)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [840/985]  eta: 0:02:17  lr: 0.000003  loss: 0.0174 (0.0191)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [850/985]  eta: 0:02:08  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [860/985]  eta: 0:01:58  lr: 0.000003  loss: 0.0174 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [870/985]  eta: 0:01:49  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [880/985]  eta: 0:01:39  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [890/985]  eta: 0:01:30  lr: 0.000003  loss: 0.0170 (0.0191)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [900/985]  eta: 0:01:20  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [910/985]  eta: 0:01:11  lr: 0.000003  loss: 0.0189 (0.0191)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [920/985]  eta: 0:01:01  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [930/985]  eta: 0:00:52  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [940/985]  eta: 0:00:42  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [950/985]  eta: 0:00:33  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [960/985]  eta: 0:00:23  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [970/985]  eta: 0:00:14  lr: 0.000003  loss: 0.0189 (0.0191)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [980/985]  eta: 0:00:04  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785]  [984/985]  eta: 0:00:00  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:785] Total time: 0:15:36 (0.9512 s / it)\n",
      "Averaged stats: lr: 0.000003  loss: 0.0187 (0.0191)\n",
      "Valid: [epoch:785]  [ 0/14]  eta: 0:02:44  loss: 0.0143 (0.0143)  time: 11.7317  data: 0.4938  max mem: 41892\n",
      "Valid: [epoch:785]  [13/14]  eta: 0:00:11  loss: 0.0145 (0.0145)  time: 11.0796  data: 0.0353  max mem: 41892\n",
      "Valid: [epoch:785] Total time: 0:02:35 (11.0853 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_785_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 785.000\n",
      "Train: [epoch:786]  [  0/985]  eta: 1:03:22  lr: 0.000003  loss: 0.0164 (0.0164)  time: 3.8605  data: 2.8963  max mem: 41892\n",
      "Train: [epoch:786]  [ 10/985]  eta: 0:19:24  lr: 0.000003  loss: 0.0197 (0.0204)  time: 1.1948  data: 0.2634  max mem: 41892\n",
      "Train: [epoch:786]  [ 20/985]  eta: 0:17:12  lr: 0.000003  loss: 0.0197 (0.0202)  time: 0.9301  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:786]  [ 30/985]  eta: 0:16:19  lr: 0.000003  loss: 0.0176 (0.0199)  time: 0.9331  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [ 40/985]  eta: 0:15:48  lr: 0.000003  loss: 0.0173 (0.0188)  time: 0.9354  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [ 50/985]  eta: 0:15:26  lr: 0.000003  loss: 0.0179 (0.0189)  time: 0.9371  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [ 60/985]  eta: 0:15:10  lr: 0.000003  loss: 0.0189 (0.0193)  time: 0.9422  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [ 70/985]  eta: 0:14:54  lr: 0.000003  loss: 0.0176 (0.0190)  time: 0.9444  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [ 80/985]  eta: 0:14:41  lr: 0.000003  loss: 0.0171 (0.0189)  time: 0.9443  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [ 90/985]  eta: 0:14:28  lr: 0.000003  loss: 0.0174 (0.0189)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [100/985]  eta: 0:14:17  lr: 0.000003  loss: 0.0186 (0.0189)  time: 0.9457  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [110/985]  eta: 0:14:05  lr: 0.000003  loss: 0.0180 (0.0189)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [120/985]  eta: 0:13:54  lr: 0.000003  loss: 0.0180 (0.0190)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [130/985]  eta: 0:13:43  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [140/985]  eta: 0:13:33  lr: 0.000003  loss: 0.0181 (0.0192)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [150/985]  eta: 0:13:23  lr: 0.000003  loss: 0.0183 (0.0192)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [160/985]  eta: 0:13:13  lr: 0.000003  loss: 0.0201 (0.0193)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [170/985]  eta: 0:13:03  lr: 0.000003  loss: 0.0181 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [180/985]  eta: 0:12:53  lr: 0.000003  loss: 0.0181 (0.0192)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [190/985]  eta: 0:12:43  lr: 0.000003  loss: 0.0193 (0.0193)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [200/985]  eta: 0:12:33  lr: 0.000003  loss: 0.0189 (0.0193)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [210/985]  eta: 0:12:23  lr: 0.000003  loss: 0.0187 (0.0194)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [220/985]  eta: 0:12:13  lr: 0.000003  loss: 0.0192 (0.0193)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [230/985]  eta: 0:12:03  lr: 0.000003  loss: 0.0187 (0.0194)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [240/985]  eta: 0:11:53  lr: 0.000003  loss: 0.0187 (0.0194)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [250/985]  eta: 0:11:44  lr: 0.000003  loss: 0.0182 (0.0193)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [260/985]  eta: 0:11:34  lr: 0.000003  loss: 0.0173 (0.0193)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [270/985]  eta: 0:11:24  lr: 0.000003  loss: 0.0174 (0.0193)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [280/985]  eta: 0:11:14  lr: 0.000003  loss: 0.0186 (0.0193)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [290/985]  eta: 0:11:04  lr: 0.000003  loss: 0.0190 (0.0193)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [300/985]  eta: 0:10:55  lr: 0.000003  loss: 0.0183 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [310/985]  eta: 0:10:45  lr: 0.000003  loss: 0.0183 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [320/985]  eta: 0:10:35  lr: 0.000003  loss: 0.0177 (0.0192)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [330/985]  eta: 0:10:26  lr: 0.000003  loss: 0.0176 (0.0192)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [340/985]  eta: 0:10:16  lr: 0.000003  loss: 0.0182 (0.0192)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [350/985]  eta: 0:10:06  lr: 0.000003  loss: 0.0165 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [360/985]  eta: 0:09:57  lr: 0.000003  loss: 0.0168 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [370/985]  eta: 0:09:47  lr: 0.000003  loss: 0.0185 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [380/985]  eta: 0:09:37  lr: 0.000003  loss: 0.0179 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [390/985]  eta: 0:09:28  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [400/985]  eta: 0:09:18  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [410/985]  eta: 0:09:08  lr: 0.000003  loss: 0.0195 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [420/985]  eta: 0:08:59  lr: 0.000003  loss: 0.0204 (0.0192)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [430/985]  eta: 0:08:49  lr: 0.000003  loss: 0.0182 (0.0192)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [440/985]  eta: 0:08:40  lr: 0.000003  loss: 0.0168 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [450/985]  eta: 0:08:30  lr: 0.000003  loss: 0.0167 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [460/985]  eta: 0:08:20  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [470/985]  eta: 0:08:11  lr: 0.000003  loss: 0.0185 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [480/985]  eta: 0:08:01  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [490/985]  eta: 0:07:52  lr: 0.000003  loss: 0.0173 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [500/985]  eta: 0:07:42  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [510/985]  eta: 0:07:33  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [520/985]  eta: 0:07:23  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [530/985]  eta: 0:07:13  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [540/985]  eta: 0:07:04  lr: 0.000003  loss: 0.0184 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [550/985]  eta: 0:06:54  lr: 0.000003  loss: 0.0174 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [560/985]  eta: 0:06:45  lr: 0.000003  loss: 0.0171 (0.0191)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [570/985]  eta: 0:06:35  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [580/985]  eta: 0:06:26  lr: 0.000003  loss: 0.0192 (0.0191)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [590/985]  eta: 0:06:16  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [600/985]  eta: 0:06:06  lr: 0.000003  loss: 0.0177 (0.0191)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [610/985]  eta: 0:05:57  lr: 0.000003  loss: 0.0174 (0.0191)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [620/985]  eta: 0:05:47  lr: 0.000003  loss: 0.0184 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [630/985]  eta: 0:05:38  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [640/985]  eta: 0:05:28  lr: 0.000003  loss: 0.0184 (0.0191)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [650/985]  eta: 0:05:19  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [660/985]  eta: 0:05:09  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [670/985]  eta: 0:05:00  lr: 0.000003  loss: 0.0168 (0.0190)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [680/985]  eta: 0:04:50  lr: 0.000003  loss: 0.0174 (0.0190)  time: 0.9500  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:786]  [690/985]  eta: 0:04:41  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [700/985]  eta: 0:04:31  lr: 0.000003  loss: 0.0178 (0.0190)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [710/985]  eta: 0:04:21  lr: 0.000003  loss: 0.0176 (0.0190)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [720/985]  eta: 0:04:12  lr: 0.000003  loss: 0.0182 (0.0190)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [730/985]  eta: 0:04:02  lr: 0.000003  loss: 0.0182 (0.0190)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [740/985]  eta: 0:03:53  lr: 0.000003  loss: 0.0171 (0.0190)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [750/985]  eta: 0:03:43  lr: 0.000003  loss: 0.0175 (0.0190)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [760/985]  eta: 0:03:34  lr: 0.000003  loss: 0.0186 (0.0190)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [770/985]  eta: 0:03:24  lr: 0.000003  loss: 0.0182 (0.0190)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [780/985]  eta: 0:03:15  lr: 0.000003  loss: 0.0183 (0.0190)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [790/985]  eta: 0:03:05  lr: 0.000003  loss: 0.0181 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [800/985]  eta: 0:02:56  lr: 0.000003  loss: 0.0168 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [810/985]  eta: 0:02:46  lr: 0.000003  loss: 0.0168 (0.0190)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [820/985]  eta: 0:02:37  lr: 0.000003  loss: 0.0191 (0.0190)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [830/985]  eta: 0:02:27  lr: 0.000003  loss: 0.0196 (0.0190)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [840/985]  eta: 0:02:18  lr: 0.000003  loss: 0.0195 (0.0190)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [850/985]  eta: 0:02:08  lr: 0.000003  loss: 0.0199 (0.0190)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [860/985]  eta: 0:01:59  lr: 0.000003  loss: 0.0192 (0.0190)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [870/985]  eta: 0:01:49  lr: 0.000003  loss: 0.0185 (0.0190)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [880/985]  eta: 0:01:39  lr: 0.000003  loss: 0.0175 (0.0190)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [890/985]  eta: 0:01:30  lr: 0.000003  loss: 0.0175 (0.0190)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [900/985]  eta: 0:01:20  lr: 0.000003  loss: 0.0185 (0.0190)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [910/985]  eta: 0:01:11  lr: 0.000003  loss: 0.0186 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [920/985]  eta: 0:01:01  lr: 0.000003  loss: 0.0186 (0.0190)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [930/985]  eta: 0:00:52  lr: 0.000003  loss: 0.0185 (0.0190)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [940/985]  eta: 0:00:42  lr: 0.000003  loss: 0.0203 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [950/985]  eta: 0:00:33  lr: 0.000003  loss: 0.0223 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [960/985]  eta: 0:00:23  lr: 0.000003  loss: 0.0191 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [970/985]  eta: 0:00:14  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [980/985]  eta: 0:00:04  lr: 0.000003  loss: 0.0185 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786]  [984/985]  eta: 0:00:00  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:786] Total time: 0:15:37 (0.9521 s / it)\n",
      "Averaged stats: lr: 0.000003  loss: 0.0188 (0.0191)\n",
      "Valid: [epoch:786]  [ 0/14]  eta: 0:02:51  loss: 0.0172 (0.0172)  time: 12.2465  data: 0.4901  max mem: 41892\n",
      "Valid: [epoch:786]  [13/14]  eta: 0:00:11  loss: 0.0147 (0.0146)  time: 11.2000  data: 0.0351  max mem: 41892\n",
      "Valid: [epoch:786] Total time: 0:02:36 (11.2070 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_786_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 785.000\n",
      "Train: [epoch:787]  [  0/985]  eta: 1:05:31  lr: 0.000003  loss: 0.0322 (0.0322)  time: 3.9911  data: 3.0221  max mem: 41892\n",
      "Train: [epoch:787]  [ 10/985]  eta: 0:19:41  lr: 0.000003  loss: 0.0189 (0.0200)  time: 1.2118  data: 0.2748  max mem: 41892\n",
      "Train: [epoch:787]  [ 20/985]  eta: 0:17:21  lr: 0.000003  loss: 0.0178 (0.0195)  time: 0.9334  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [ 30/985]  eta: 0:16:26  lr: 0.000003  loss: 0.0168 (0.0189)  time: 0.9340  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [ 40/985]  eta: 0:15:55  lr: 0.000003  loss: 0.0164 (0.0188)  time: 0.9390  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [ 50/985]  eta: 0:15:31  lr: 0.000003  loss: 0.0161 (0.0188)  time: 0.9411  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [ 60/985]  eta: 0:15:13  lr: 0.000003  loss: 0.0186 (0.0192)  time: 0.9409  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [ 70/985]  eta: 0:14:58  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9446  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [ 80/985]  eta: 0:14:45  lr: 0.000003  loss: 0.0162 (0.0189)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [ 90/985]  eta: 0:14:34  lr: 0.000003  loss: 0.0167 (0.0190)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [100/985]  eta: 0:14:22  lr: 0.000003  loss: 0.0179 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [110/985]  eta: 0:14:11  lr: 0.000003  loss: 0.0178 (0.0188)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [120/985]  eta: 0:14:00  lr: 0.000003  loss: 0.0178 (0.0190)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [130/985]  eta: 0:13:50  lr: 0.000003  loss: 0.0187 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [140/985]  eta: 0:13:39  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [150/985]  eta: 0:13:29  lr: 0.000003  loss: 0.0189 (0.0192)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [160/985]  eta: 0:13:19  lr: 0.000003  loss: 0.0177 (0.0193)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [170/985]  eta: 0:13:09  lr: 0.000003  loss: 0.0174 (0.0193)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [180/985]  eta: 0:12:59  lr: 0.000003  loss: 0.0181 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [190/985]  eta: 0:12:49  lr: 0.000003  loss: 0.0185 (0.0193)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [200/985]  eta: 0:12:39  lr: 0.000003  loss: 0.0179 (0.0192)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [210/985]  eta: 0:12:29  lr: 0.000003  loss: 0.0174 (0.0192)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [220/985]  eta: 0:12:19  lr: 0.000003  loss: 0.0177 (0.0192)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [230/985]  eta: 0:12:09  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [240/985]  eta: 0:11:59  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [250/985]  eta: 0:11:49  lr: 0.000003  loss: 0.0179 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [260/985]  eta: 0:11:39  lr: 0.000003  loss: 0.0171 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [270/985]  eta: 0:11:29  lr: 0.000003  loss: 0.0170 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [280/985]  eta: 0:11:19  lr: 0.000003  loss: 0.0172 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [290/985]  eta: 0:11:09  lr: 0.000003  loss: 0.0183 (0.0190)  time: 0.9499  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:787]  [300/985]  eta: 0:10:59  lr: 0.000003  loss: 0.0191 (0.0190)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [310/985]  eta: 0:10:49  lr: 0.000003  loss: 0.0182 (0.0190)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [320/985]  eta: 0:10:40  lr: 0.000003  loss: 0.0171 (0.0190)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [330/985]  eta: 0:10:30  lr: 0.000003  loss: 0.0178 (0.0190)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [340/985]  eta: 0:10:20  lr: 0.000003  loss: 0.0178 (0.0190)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [350/985]  eta: 0:10:10  lr: 0.000003  loss: 0.0172 (0.0189)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [360/985]  eta: 0:10:00  lr: 0.000003  loss: 0.0177 (0.0189)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [370/985]  eta: 0:09:50  lr: 0.000003  loss: 0.0179 (0.0189)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [380/985]  eta: 0:09:41  lr: 0.000003  loss: 0.0178 (0.0189)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [390/985]  eta: 0:09:31  lr: 0.000003  loss: 0.0187 (0.0189)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [400/985]  eta: 0:09:21  lr: 0.000003  loss: 0.0186 (0.0189)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [410/985]  eta: 0:09:11  lr: 0.000003  loss: 0.0173 (0.0189)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [420/985]  eta: 0:09:02  lr: 0.000003  loss: 0.0179 (0.0189)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [430/985]  eta: 0:08:52  lr: 0.000003  loss: 0.0193 (0.0190)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [440/985]  eta: 0:08:42  lr: 0.000003  loss: 0.0193 (0.0190)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [450/985]  eta: 0:08:33  lr: 0.000003  loss: 0.0182 (0.0190)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [460/985]  eta: 0:08:23  lr: 0.000003  loss: 0.0181 (0.0190)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [470/985]  eta: 0:08:13  lr: 0.000003  loss: 0.0193 (0.0190)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [480/985]  eta: 0:08:04  lr: 0.000003  loss: 0.0180 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [490/985]  eta: 0:07:54  lr: 0.000003  loss: 0.0176 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [500/985]  eta: 0:07:44  lr: 0.000003  loss: 0.0178 (0.0190)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [510/985]  eta: 0:07:35  lr: 0.000003  loss: 0.0206 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [520/985]  eta: 0:07:25  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [530/985]  eta: 0:07:15  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [540/985]  eta: 0:07:06  lr: 0.000003  loss: 0.0172 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [550/985]  eta: 0:06:56  lr: 0.000003  loss: 0.0172 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [560/985]  eta: 0:06:47  lr: 0.000003  loss: 0.0183 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [570/985]  eta: 0:06:37  lr: 0.000003  loss: 0.0196 (0.0191)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [580/985]  eta: 0:06:28  lr: 0.000003  loss: 0.0192 (0.0191)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [590/985]  eta: 0:06:18  lr: 0.000003  loss: 0.0189 (0.0191)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [600/985]  eta: 0:06:08  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [610/985]  eta: 0:05:59  lr: 0.000003  loss: 0.0167 (0.0191)  time: 0.9670  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [620/985]  eta: 0:05:49  lr: 0.000003  loss: 0.0194 (0.0191)  time: 0.9656  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [630/985]  eta: 0:05:40  lr: 0.000003  loss: 0.0190 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [640/985]  eta: 0:05:30  lr: 0.000003  loss: 0.0173 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [650/985]  eta: 0:05:21  lr: 0.000003  loss: 0.0186 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [660/985]  eta: 0:05:11  lr: 0.000003  loss: 0.0167 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [670/985]  eta: 0:05:01  lr: 0.000003  loss: 0.0171 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [680/985]  eta: 0:04:52  lr: 0.000003  loss: 0.0173 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [690/985]  eta: 0:04:42  lr: 0.000003  loss: 0.0189 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [700/985]  eta: 0:04:33  lr: 0.000003  loss: 0.0189 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [710/985]  eta: 0:04:23  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [720/985]  eta: 0:04:13  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [730/985]  eta: 0:04:04  lr: 0.000003  loss: 0.0176 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [740/985]  eta: 0:03:54  lr: 0.000003  loss: 0.0176 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [750/985]  eta: 0:03:45  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [760/985]  eta: 0:03:35  lr: 0.000003  loss: 0.0186 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [770/985]  eta: 0:03:25  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [780/985]  eta: 0:03:16  lr: 0.000003  loss: 0.0171 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [790/985]  eta: 0:03:06  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [800/985]  eta: 0:02:57  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [810/985]  eta: 0:02:47  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [820/985]  eta: 0:02:37  lr: 0.000003  loss: 0.0185 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [830/985]  eta: 0:02:28  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [840/985]  eta: 0:02:18  lr: 0.000003  loss: 0.0176 (0.0190)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [850/985]  eta: 0:02:09  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [860/985]  eta: 0:01:59  lr: 0.000003  loss: 0.0175 (0.0190)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [870/985]  eta: 0:01:50  lr: 0.000003  loss: 0.0168 (0.0190)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [880/985]  eta: 0:01:40  lr: 0.000003  loss: 0.0179 (0.0190)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [890/985]  eta: 0:01:30  lr: 0.000003  loss: 0.0185 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [900/985]  eta: 0:01:21  lr: 0.000003  loss: 0.0209 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [910/985]  eta: 0:01:11  lr: 0.000003  loss: 0.0212 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [920/985]  eta: 0:01:02  lr: 0.000003  loss: 0.0197 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [930/985]  eta: 0:00:52  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [940/985]  eta: 0:00:43  lr: 0.000003  loss: 0.0182 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [950/985]  eta: 0:00:33  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:787]  [960/985]  eta: 0:00:23  lr: 0.000003  loss: 0.0202 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [970/985]  eta: 0:00:14  lr: 0.000003  loss: 0.0184 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [980/985]  eta: 0:00:04  lr: 0.000003  loss: 0.0172 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787]  [984/985]  eta: 0:00:00  lr: 0.000003  loss: 0.0184 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:787] Total time: 0:15:41 (0.9563 s / it)\n",
      "Averaged stats: lr: 0.000003  loss: 0.0184 (0.0191)\n",
      "Valid: [epoch:787]  [ 0/14]  eta: 0:02:46  loss: 0.0136 (0.0136)  time: 11.9090  data: 0.5104  max mem: 41892\n",
      "Valid: [epoch:787]  [13/14]  eta: 0:00:11  loss: 0.0147 (0.0146)  time: 11.1984  data: 0.0365  max mem: 41892\n",
      "Valid: [epoch:787] Total time: 0:02:36 (11.2045 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_787_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 785.000\n",
      "Train: [epoch:788]  [  0/985]  eta: 0:57:35  lr: 0.000002  loss: 0.0264 (0.0264)  time: 3.5077  data: 2.5479  max mem: 41892\n",
      "Train: [epoch:788]  [ 10/985]  eta: 0:19:04  lr: 0.000002  loss: 0.0190 (0.0202)  time: 1.1738  data: 0.2317  max mem: 41892\n",
      "Train: [epoch:788]  [ 20/985]  eta: 0:17:04  lr: 0.000002  loss: 0.0180 (0.0190)  time: 0.9390  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [ 30/985]  eta: 0:16:14  lr: 0.000002  loss: 0.0183 (0.0195)  time: 0.9362  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [ 40/985]  eta: 0:15:45  lr: 0.000002  loss: 0.0173 (0.0188)  time: 0.9357  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [ 50/985]  eta: 0:15:24  lr: 0.000002  loss: 0.0161 (0.0184)  time: 0.9407  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [ 60/985]  eta: 0:15:07  lr: 0.000002  loss: 0.0187 (0.0189)  time: 0.9437  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [ 70/985]  eta: 0:14:52  lr: 0.000002  loss: 0.0188 (0.0187)  time: 0.9421  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [ 80/985]  eta: 0:14:39  lr: 0.000002  loss: 0.0165 (0.0185)  time: 0.9421  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [ 90/985]  eta: 0:14:27  lr: 0.000002  loss: 0.0173 (0.0188)  time: 0.9431  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [100/985]  eta: 0:14:15  lr: 0.000002  loss: 0.0188 (0.0189)  time: 0.9445  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [110/985]  eta: 0:14:04  lr: 0.000002  loss: 0.0184 (0.0190)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [120/985]  eta: 0:13:53  lr: 0.000002  loss: 0.0193 (0.0192)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [130/985]  eta: 0:13:42  lr: 0.000002  loss: 0.0195 (0.0191)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [140/985]  eta: 0:13:32  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [150/985]  eta: 0:13:21  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [160/985]  eta: 0:13:11  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [170/985]  eta: 0:13:01  lr: 0.000002  loss: 0.0195 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [180/985]  eta: 0:12:51  lr: 0.000002  loss: 0.0181 (0.0192)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [190/985]  eta: 0:12:41  lr: 0.000002  loss: 0.0180 (0.0192)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [200/985]  eta: 0:12:31  lr: 0.000002  loss: 0.0193 (0.0192)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [210/985]  eta: 0:12:21  lr: 0.000002  loss: 0.0196 (0.0192)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [220/985]  eta: 0:12:12  lr: 0.000002  loss: 0.0172 (0.0192)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [230/985]  eta: 0:12:02  lr: 0.000002  loss: 0.0172 (0.0192)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [240/985]  eta: 0:11:52  lr: 0.000002  loss: 0.0186 (0.0192)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [250/985]  eta: 0:11:42  lr: 0.000002  loss: 0.0186 (0.0192)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [260/985]  eta: 0:11:32  lr: 0.000002  loss: 0.0182 (0.0192)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [270/985]  eta: 0:11:23  lr: 0.000002  loss: 0.0187 (0.0192)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [280/985]  eta: 0:11:13  lr: 0.000002  loss: 0.0194 (0.0193)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [290/985]  eta: 0:11:04  lr: 0.000002  loss: 0.0176 (0.0192)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [300/985]  eta: 0:10:54  lr: 0.000002  loss: 0.0173 (0.0192)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [310/985]  eta: 0:10:44  lr: 0.000002  loss: 0.0178 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [320/985]  eta: 0:10:35  lr: 0.000002  loss: 0.0172 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [330/985]  eta: 0:10:25  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [340/985]  eta: 0:10:16  lr: 0.000002  loss: 0.0191 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [350/985]  eta: 0:10:06  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [360/985]  eta: 0:09:56  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [370/985]  eta: 0:09:47  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [380/985]  eta: 0:09:37  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [390/985]  eta: 0:09:27  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [400/985]  eta: 0:09:18  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [410/985]  eta: 0:09:08  lr: 0.000002  loss: 0.0192 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [420/985]  eta: 0:08:59  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [430/985]  eta: 0:08:49  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [440/985]  eta: 0:08:39  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [450/985]  eta: 0:08:30  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [460/985]  eta: 0:08:20  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [470/985]  eta: 0:08:11  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [480/985]  eta: 0:08:01  lr: 0.000002  loss: 0.0190 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [490/985]  eta: 0:07:52  lr: 0.000002  loss: 0.0195 (0.0192)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [500/985]  eta: 0:07:42  lr: 0.000002  loss: 0.0195 (0.0192)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [510/985]  eta: 0:07:33  lr: 0.000002  loss: 0.0186 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [520/985]  eta: 0:07:23  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [530/985]  eta: 0:07:13  lr: 0.000002  loss: 0.0181 (0.0192)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [540/985]  eta: 0:07:04  lr: 0.000002  loss: 0.0176 (0.0192)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [550/985]  eta: 0:06:54  lr: 0.000002  loss: 0.0191 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [560/985]  eta: 0:06:45  lr: 0.000002  loss: 0.0196 (0.0192)  time: 0.9489  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:788]  [570/985]  eta: 0:06:35  lr: 0.000002  loss: 0.0178 (0.0192)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [580/985]  eta: 0:06:26  lr: 0.000002  loss: 0.0174 (0.0192)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [590/985]  eta: 0:06:16  lr: 0.000002  loss: 0.0187 (0.0192)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [600/985]  eta: 0:06:06  lr: 0.000002  loss: 0.0172 (0.0192)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [610/985]  eta: 0:05:57  lr: 0.000002  loss: 0.0166 (0.0191)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [620/985]  eta: 0:05:47  lr: 0.000002  loss: 0.0188 (0.0192)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [630/985]  eta: 0:05:38  lr: 0.000002  loss: 0.0186 (0.0192)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [640/985]  eta: 0:05:28  lr: 0.000002  loss: 0.0180 (0.0192)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [650/985]  eta: 0:05:19  lr: 0.000002  loss: 0.0180 (0.0192)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [660/985]  eta: 0:05:09  lr: 0.000002  loss: 0.0179 (0.0192)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [670/985]  eta: 0:05:00  lr: 0.000002  loss: 0.0172 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [680/985]  eta: 0:04:50  lr: 0.000002  loss: 0.0164 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [690/985]  eta: 0:04:41  lr: 0.000002  loss: 0.0166 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [700/985]  eta: 0:04:31  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [710/985]  eta: 0:04:21  lr: 0.000002  loss: 0.0199 (0.0191)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [720/985]  eta: 0:04:12  lr: 0.000002  loss: 0.0192 (0.0192)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [730/985]  eta: 0:04:02  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [740/985]  eta: 0:03:53  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [750/985]  eta: 0:03:43  lr: 0.000002  loss: 0.0178 (0.0191)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [760/985]  eta: 0:03:34  lr: 0.000002  loss: 0.0193 (0.0191)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [770/985]  eta: 0:03:24  lr: 0.000002  loss: 0.0193 (0.0192)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [780/985]  eta: 0:03:15  lr: 0.000002  loss: 0.0193 (0.0192)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [790/985]  eta: 0:03:05  lr: 0.000002  loss: 0.0166 (0.0191)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [800/985]  eta: 0:02:56  lr: 0.000002  loss: 0.0166 (0.0191)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [810/985]  eta: 0:02:46  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [820/985]  eta: 0:02:37  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [830/985]  eta: 0:02:27  lr: 0.000002  loss: 0.0185 (0.0192)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [840/985]  eta: 0:02:18  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [850/985]  eta: 0:02:08  lr: 0.000002  loss: 0.0162 (0.0191)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [860/985]  eta: 0:01:58  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [870/985]  eta: 0:01:49  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [880/985]  eta: 0:01:39  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [890/985]  eta: 0:01:30  lr: 0.000002  loss: 0.0191 (0.0191)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [900/985]  eta: 0:01:20  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [910/985]  eta: 0:01:11  lr: 0.000002  loss: 0.0172 (0.0191)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [920/985]  eta: 0:01:01  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [930/985]  eta: 0:00:52  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [940/985]  eta: 0:00:42  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [950/985]  eta: 0:00:33  lr: 0.000002  loss: 0.0193 (0.0191)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [960/985]  eta: 0:00:23  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9452  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [970/985]  eta: 0:00:14  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9449  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [980/985]  eta: 0:00:04  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9452  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788]  [984/985]  eta: 0:00:00  lr: 0.000002  loss: 0.0194 (0.0191)  time: 0.9452  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:788] Total time: 0:15:37 (0.9513 s / it)\n",
      "Averaged stats: lr: 0.000002  loss: 0.0194 (0.0191)\n",
      "Valid: [epoch:788]  [ 0/14]  eta: 0:02:39  loss: 0.0151 (0.0151)  time: 11.3927  data: 0.5051  max mem: 41892\n",
      "Valid: [epoch:788]  [13/14]  eta: 0:00:10  loss: 0.0147 (0.0146)  time: 10.8886  data: 0.0361  max mem: 41892\n",
      "Valid: [epoch:788] Total time: 0:02:32 (10.8947 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_788_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 785.000\n",
      "Train: [epoch:789]  [  0/985]  eta: 1:08:06  lr: 0.000002  loss: 0.0167 (0.0167)  time: 4.1490  data: 3.1448  max mem: 41892\n",
      "Train: [epoch:789]  [ 10/985]  eta: 0:19:51  lr: 0.000002  loss: 0.0181 (0.0191)  time: 1.2223  data: 0.2860  max mem: 41892\n",
      "Train: [epoch:789]  [ 20/985]  eta: 0:17:27  lr: 0.000002  loss: 0.0190 (0.0193)  time: 0.9324  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [ 30/985]  eta: 0:16:29  lr: 0.000002  loss: 0.0190 (0.0192)  time: 0.9337  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [ 40/985]  eta: 0:15:56  lr: 0.000002  loss: 0.0169 (0.0185)  time: 0.9353  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [ 50/985]  eta: 0:15:32  lr: 0.000002  loss: 0.0166 (0.0184)  time: 0.9371  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [ 60/985]  eta: 0:15:13  lr: 0.000002  loss: 0.0180 (0.0186)  time: 0.9369  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [ 70/985]  eta: 0:14:57  lr: 0.000002  loss: 0.0180 (0.0186)  time: 0.9394  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [ 80/985]  eta: 0:14:43  lr: 0.000002  loss: 0.0160 (0.0182)  time: 0.9411  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [ 90/985]  eta: 0:14:30  lr: 0.000002  loss: 0.0161 (0.0184)  time: 0.9425  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [100/985]  eta: 0:14:17  lr: 0.000002  loss: 0.0182 (0.0184)  time: 0.9427  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [110/985]  eta: 0:14:06  lr: 0.000002  loss: 0.0172 (0.0184)  time: 0.9429  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [120/985]  eta: 0:13:55  lr: 0.000002  loss: 0.0171 (0.0184)  time: 0.9444  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [130/985]  eta: 0:13:44  lr: 0.000002  loss: 0.0174 (0.0186)  time: 0.9452  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [140/985]  eta: 0:13:33  lr: 0.000002  loss: 0.0202 (0.0187)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [150/985]  eta: 0:13:22  lr: 0.000002  loss: 0.0202 (0.0189)  time: 0.9462  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [160/985]  eta: 0:13:12  lr: 0.000002  loss: 0.0188 (0.0189)  time: 0.9461  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [170/985]  eta: 0:13:02  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9461  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:789]  [180/985]  eta: 0:12:51  lr: 0.000002  loss: 0.0185 (0.0189)  time: 0.9464  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [190/985]  eta: 0:12:41  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [200/985]  eta: 0:12:31  lr: 0.000002  loss: 0.0186 (0.0192)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [210/985]  eta: 0:12:22  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [220/985]  eta: 0:12:12  lr: 0.000002  loss: 0.0167 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [230/985]  eta: 0:12:02  lr: 0.000002  loss: 0.0179 (0.0190)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [240/985]  eta: 0:11:52  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [250/985]  eta: 0:11:42  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [260/985]  eta: 0:11:33  lr: 0.000002  loss: 0.0181 (0.0190)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [270/985]  eta: 0:11:23  lr: 0.000002  loss: 0.0172 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [280/985]  eta: 0:11:13  lr: 0.000002  loss: 0.0179 (0.0190)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [290/985]  eta: 0:11:04  lr: 0.000002  loss: 0.0192 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [300/985]  eta: 0:10:54  lr: 0.000002  loss: 0.0212 (0.0192)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [310/985]  eta: 0:10:44  lr: 0.000002  loss: 0.0201 (0.0192)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [320/985]  eta: 0:10:34  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [330/985]  eta: 0:10:25  lr: 0.000002  loss: 0.0190 (0.0192)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [340/985]  eta: 0:10:15  lr: 0.000002  loss: 0.0190 (0.0191)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [350/985]  eta: 0:10:05  lr: 0.000002  loss: 0.0192 (0.0192)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [360/985]  eta: 0:09:56  lr: 0.000002  loss: 0.0192 (0.0192)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [370/985]  eta: 0:09:46  lr: 0.000002  loss: 0.0180 (0.0193)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [380/985]  eta: 0:09:36  lr: 0.000002  loss: 0.0185 (0.0193)  time: 0.9464  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [390/985]  eta: 0:09:27  lr: 0.000002  loss: 0.0182 (0.0193)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [400/985]  eta: 0:09:17  lr: 0.000002  loss: 0.0169 (0.0192)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [410/985]  eta: 0:09:08  lr: 0.000002  loss: 0.0182 (0.0192)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [420/985]  eta: 0:08:58  lr: 0.000002  loss: 0.0184 (0.0193)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [430/985]  eta: 0:08:48  lr: 0.000002  loss: 0.0188 (0.0193)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [440/985]  eta: 0:08:39  lr: 0.000002  loss: 0.0180 (0.0193)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [450/985]  eta: 0:08:29  lr: 0.000002  loss: 0.0180 (0.0193)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [460/985]  eta: 0:08:20  lr: 0.000002  loss: 0.0193 (0.0193)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [470/985]  eta: 0:08:10  lr: 0.000002  loss: 0.0182 (0.0193)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [480/985]  eta: 0:08:01  lr: 0.000002  loss: 0.0182 (0.0193)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [490/985]  eta: 0:07:51  lr: 0.000002  loss: 0.0180 (0.0193)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [500/985]  eta: 0:07:42  lr: 0.000002  loss: 0.0179 (0.0193)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [510/985]  eta: 0:07:32  lr: 0.000002  loss: 0.0189 (0.0193)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [520/985]  eta: 0:07:22  lr: 0.000002  loss: 0.0179 (0.0192)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [530/985]  eta: 0:07:13  lr: 0.000002  loss: 0.0179 (0.0192)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [540/985]  eta: 0:07:03  lr: 0.000002  loss: 0.0175 (0.0192)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [550/985]  eta: 0:06:54  lr: 0.000002  loss: 0.0167 (0.0192)  time: 0.9461  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [560/985]  eta: 0:06:44  lr: 0.000002  loss: 0.0176 (0.0192)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [570/985]  eta: 0:06:35  lr: 0.000002  loss: 0.0186 (0.0192)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [580/985]  eta: 0:06:25  lr: 0.000002  loss: 0.0186 (0.0192)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [590/985]  eta: 0:06:16  lr: 0.000002  loss: 0.0193 (0.0192)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [600/985]  eta: 0:06:06  lr: 0.000002  loss: 0.0189 (0.0192)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [610/985]  eta: 0:05:56  lr: 0.000002  loss: 0.0175 (0.0192)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [620/985]  eta: 0:05:47  lr: 0.000002  loss: 0.0190 (0.0192)  time: 0.9464  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [630/985]  eta: 0:05:37  lr: 0.000002  loss: 0.0180 (0.0192)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [640/985]  eta: 0:05:28  lr: 0.000002  loss: 0.0171 (0.0192)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [650/985]  eta: 0:05:18  lr: 0.000002  loss: 0.0194 (0.0192)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [660/985]  eta: 0:05:09  lr: 0.000002  loss: 0.0181 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [670/985]  eta: 0:04:59  lr: 0.000002  loss: 0.0180 (0.0192)  time: 0.9563  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [680/985]  eta: 0:04:50  lr: 0.000002  loss: 0.0169 (0.0192)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [690/985]  eta: 0:04:40  lr: 0.000002  loss: 0.0172 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [700/985]  eta: 0:04:31  lr: 0.000002  loss: 0.0172 (0.0192)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [710/985]  eta: 0:04:21  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9518  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [720/985]  eta: 0:04:12  lr: 0.000002  loss: 0.0196 (0.0192)  time: 0.9523  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [730/985]  eta: 0:04:02  lr: 0.000002  loss: 0.0192 (0.0192)  time: 0.9504  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [740/985]  eta: 0:03:53  lr: 0.000002  loss: 0.0174 (0.0192)  time: 0.9542  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [750/985]  eta: 0:03:43  lr: 0.000002  loss: 0.0176 (0.0192)  time: 0.9553  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [760/985]  eta: 0:03:34  lr: 0.000002  loss: 0.0183 (0.0192)  time: 0.9538  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [770/985]  eta: 0:03:24  lr: 0.000002  loss: 0.0189 (0.0192)  time: 0.9533  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [780/985]  eta: 0:03:15  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [790/985]  eta: 0:03:05  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9570  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [800/985]  eta: 0:02:56  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [810/985]  eta: 0:02:46  lr: 0.000002  loss: 0.0186 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [820/985]  eta: 0:02:37  lr: 0.000002  loss: 0.0190 (0.0192)  time: 0.9585  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [830/985]  eta: 0:02:27  lr: 0.000002  loss: 0.0195 (0.0192)  time: 0.9526  data: 0.0002  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:789]  [840/985]  eta: 0:02:18  lr: 0.000002  loss: 0.0183 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [850/985]  eta: 0:02:08  lr: 0.000002  loss: 0.0181 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [860/985]  eta: 0:01:59  lr: 0.000002  loss: 0.0179 (0.0192)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [870/985]  eta: 0:01:49  lr: 0.000002  loss: 0.0179 (0.0192)  time: 0.9534  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [880/985]  eta: 0:01:39  lr: 0.000002  loss: 0.0175 (0.0192)  time: 0.9527  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:789]  [890/985]  eta: 0:01:30  lr: 0.000002  loss: 0.0201 (0.0192)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [900/985]  eta: 0:01:20  lr: 0.000002  loss: 0.0198 (0.0192)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [910/985]  eta: 0:01:11  lr: 0.000002  loss: 0.0181 (0.0192)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [920/985]  eta: 0:01:01  lr: 0.000002  loss: 0.0177 (0.0192)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [930/985]  eta: 0:00:52  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [940/985]  eta: 0:00:42  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [950/985]  eta: 0:00:33  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [960/985]  eta: 0:00:23  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [970/985]  eta: 0:00:14  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [980/985]  eta: 0:00:04  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789]  [984/985]  eta: 0:00:00  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:789] Total time: 0:15:37 (0.9522 s / it)\n",
      "Averaged stats: lr: 0.000002  loss: 0.0182 (0.0191)\n",
      "Valid: [epoch:789]  [ 0/14]  eta: 0:02:46  loss: 0.0152 (0.0152)  time: 11.8604  data: 0.5489  max mem: 41892\n",
      "Valid: [epoch:789]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0146)  time: 11.2558  data: 0.0393  max mem: 41892\n",
      "Valid: [epoch:789] Total time: 0:02:37 (11.2627 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_789_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 785.000\n",
      "Train: [epoch:790]  [  0/985]  eta: 0:59:48  lr: 0.000002  loss: 0.0215 (0.0215)  time: 3.6429  data: 2.6840  max mem: 41892\n",
      "Train: [epoch:790]  [ 10/985]  eta: 0:19:05  lr: 0.000002  loss: 0.0179 (0.0186)  time: 1.1754  data: 0.2441  max mem: 41892\n",
      "Train: [epoch:790]  [ 20/985]  eta: 0:17:04  lr: 0.000002  loss: 0.0185 (0.0187)  time: 0.9322  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [ 30/985]  eta: 0:16:14  lr: 0.000002  loss: 0.0187 (0.0188)  time: 0.9352  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [ 40/985]  eta: 0:15:45  lr: 0.000002  loss: 0.0173 (0.0184)  time: 0.9357  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [ 50/985]  eta: 0:15:24  lr: 0.000002  loss: 0.0170 (0.0183)  time: 0.9407  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [ 60/985]  eta: 0:15:07  lr: 0.000002  loss: 0.0187 (0.0187)  time: 0.9433  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [ 70/985]  eta: 0:14:53  lr: 0.000002  loss: 0.0186 (0.0188)  time: 0.9428  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [ 80/985]  eta: 0:14:39  lr: 0.000002  loss: 0.0174 (0.0187)  time: 0.9444  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [ 90/985]  eta: 0:14:27  lr: 0.000002  loss: 0.0188 (0.0191)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [100/985]  eta: 0:14:16  lr: 0.000002  loss: 0.0189 (0.0189)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [110/985]  eta: 0:14:05  lr: 0.000002  loss: 0.0180 (0.0190)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [120/985]  eta: 0:13:54  lr: 0.000002  loss: 0.0185 (0.0190)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [130/985]  eta: 0:13:43  lr: 0.000002  loss: 0.0193 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [140/985]  eta: 0:13:33  lr: 0.000002  loss: 0.0192 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [150/985]  eta: 0:13:22  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [160/985]  eta: 0:13:12  lr: 0.000002  loss: 0.0176 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [170/985]  eta: 0:13:02  lr: 0.000002  loss: 0.0178 (0.0190)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [180/985]  eta: 0:12:52  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [190/985]  eta: 0:12:42  lr: 0.000002  loss: 0.0188 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [200/985]  eta: 0:12:32  lr: 0.000002  loss: 0.0184 (0.0192)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [210/985]  eta: 0:12:22  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [220/985]  eta: 0:12:13  lr: 0.000002  loss: 0.0167 (0.0190)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [230/985]  eta: 0:12:03  lr: 0.000002  loss: 0.0170 (0.0190)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [240/985]  eta: 0:11:53  lr: 0.000002  loss: 0.0172 (0.0189)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [250/985]  eta: 0:11:43  lr: 0.000002  loss: 0.0173 (0.0189)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [260/985]  eta: 0:11:33  lr: 0.000002  loss: 0.0177 (0.0189)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [270/985]  eta: 0:11:24  lr: 0.000002  loss: 0.0174 (0.0188)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [280/985]  eta: 0:11:14  lr: 0.000002  loss: 0.0168 (0.0188)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [290/985]  eta: 0:11:04  lr: 0.000002  loss: 0.0179 (0.0188)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [300/985]  eta: 0:10:55  lr: 0.000002  loss: 0.0184 (0.0188)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [310/985]  eta: 0:10:45  lr: 0.000002  loss: 0.0182 (0.0188)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [320/985]  eta: 0:10:35  lr: 0.000002  loss: 0.0176 (0.0188)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [330/985]  eta: 0:10:25  lr: 0.000002  loss: 0.0209 (0.0189)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [340/985]  eta: 0:10:16  lr: 0.000002  loss: 0.0207 (0.0189)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [350/985]  eta: 0:10:06  lr: 0.000002  loss: 0.0184 (0.0189)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [360/985]  eta: 0:09:57  lr: 0.000002  loss: 0.0182 (0.0189)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [370/985]  eta: 0:09:47  lr: 0.000002  loss: 0.0182 (0.0189)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [380/985]  eta: 0:09:37  lr: 0.000002  loss: 0.0181 (0.0189)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [390/985]  eta: 0:09:28  lr: 0.000002  loss: 0.0181 (0.0189)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [400/985]  eta: 0:09:18  lr: 0.000002  loss: 0.0191 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [410/985]  eta: 0:09:09  lr: 0.000002  loss: 0.0191 (0.0190)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [420/985]  eta: 0:08:59  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [430/985]  eta: 0:08:49  lr: 0.000002  loss: 0.0181 (0.0190)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [440/985]  eta: 0:08:40  lr: 0.000002  loss: 0.0178 (0.0190)  time: 0.9517  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:790]  [450/985]  eta: 0:08:30  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [460/985]  eta: 0:08:21  lr: 0.000002  loss: 0.0187 (0.0190)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [470/985]  eta: 0:08:11  lr: 0.000002  loss: 0.0195 (0.0190)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [480/985]  eta: 0:08:01  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [490/985]  eta: 0:07:52  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [500/985]  eta: 0:07:42  lr: 0.000002  loss: 0.0198 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [510/985]  eta: 0:07:33  lr: 0.000002  loss: 0.0190 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [520/985]  eta: 0:07:23  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [530/985]  eta: 0:07:14  lr: 0.000002  loss: 0.0176 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [540/985]  eta: 0:07:04  lr: 0.000002  loss: 0.0174 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [550/985]  eta: 0:06:54  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [560/985]  eta: 0:06:45  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [570/985]  eta: 0:06:35  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [580/985]  eta: 0:06:26  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [590/985]  eta: 0:06:16  lr: 0.000002  loss: 0.0190 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [600/985]  eta: 0:06:07  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [610/985]  eta: 0:05:57  lr: 0.000002  loss: 0.0170 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [620/985]  eta: 0:05:48  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [630/985]  eta: 0:05:38  lr: 0.000002  loss: 0.0186 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [640/985]  eta: 0:05:29  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [650/985]  eta: 0:05:19  lr: 0.000002  loss: 0.0198 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [660/985]  eta: 0:05:10  lr: 0.000002  loss: 0.0225 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [670/985]  eta: 0:05:00  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [680/985]  eta: 0:04:50  lr: 0.000002  loss: 0.0158 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [690/985]  eta: 0:04:41  lr: 0.000002  loss: 0.0174 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [700/985]  eta: 0:04:31  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [710/985]  eta: 0:04:22  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [720/985]  eta: 0:04:12  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [730/985]  eta: 0:04:03  lr: 0.000002  loss: 0.0186 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [740/985]  eta: 0:03:53  lr: 0.000002  loss: 0.0169 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [750/985]  eta: 0:03:44  lr: 0.000002  loss: 0.0169 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [760/985]  eta: 0:03:34  lr: 0.000002  loss: 0.0169 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [770/985]  eta: 0:03:25  lr: 0.000002  loss: 0.0166 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [780/985]  eta: 0:03:15  lr: 0.000002  loss: 0.0166 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [790/985]  eta: 0:03:06  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [800/985]  eta: 0:02:56  lr: 0.000002  loss: 0.0193 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [810/985]  eta: 0:02:47  lr: 0.000002  loss: 0.0195 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [820/985]  eta: 0:02:37  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [830/985]  eta: 0:02:27  lr: 0.000002  loss: 0.0178 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [840/985]  eta: 0:02:18  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [850/985]  eta: 0:02:08  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [860/985]  eta: 0:01:59  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [870/985]  eta: 0:01:49  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [880/985]  eta: 0:01:40  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [890/985]  eta: 0:01:30  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [900/985]  eta: 0:01:21  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [910/985]  eta: 0:01:11  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [920/985]  eta: 0:01:02  lr: 0.000002  loss: 0.0174 (0.0191)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [930/985]  eta: 0:00:52  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [940/985]  eta: 0:00:42  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [950/985]  eta: 0:00:33  lr: 0.000002  loss: 0.0188 (0.0191)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [960/985]  eta: 0:00:23  lr: 0.000002  loss: 0.0186 (0.0191)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [970/985]  eta: 0:00:14  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [980/985]  eta: 0:00:04  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790]  [984/985]  eta: 0:00:00  lr: 0.000002  loss: 0.0190 (0.0191)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:790] Total time: 0:15:39 (0.9540 s / it)\n",
      "Averaged stats: lr: 0.000002  loss: 0.0190 (0.0191)\n",
      "Valid: [epoch:790]  [ 0/14]  eta: 0:02:43  loss: 0.0130 (0.0130)  time: 11.7126  data: 0.5086  max mem: 41892\n",
      "Valid: [epoch:790]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0145)  time: 11.1349  data: 0.0364  max mem: 41892\n",
      "Valid: [epoch:790] Total time: 0:02:35 (11.1407 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_790_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 785.000\n",
      "Train: [epoch:791]  [  0/985]  eta: 0:59:01  lr: 0.000002  loss: 0.0185 (0.0185)  time: 3.5955  data: 2.6351  max mem: 41892\n",
      "Train: [epoch:791]  [ 10/985]  eta: 0:19:06  lr: 0.000002  loss: 0.0183 (0.0185)  time: 1.1757  data: 0.2396  max mem: 41892\n",
      "Train: [epoch:791]  [ 20/985]  eta: 0:17:03  lr: 0.000002  loss: 0.0175 (0.0187)  time: 0.9340  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [ 30/985]  eta: 0:16:14  lr: 0.000002  loss: 0.0177 (0.0185)  time: 0.9347  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [ 40/985]  eta: 0:15:45  lr: 0.000002  loss: 0.0164 (0.0182)  time: 0.9365  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [ 50/985]  eta: 0:15:24  lr: 0.000002  loss: 0.0161 (0.0183)  time: 0.9389  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:791]  [ 60/985]  eta: 0:15:07  lr: 0.000002  loss: 0.0187 (0.0184)  time: 0.9411  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [ 70/985]  eta: 0:14:53  lr: 0.000002  loss: 0.0177 (0.0184)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [ 80/985]  eta: 0:14:40  lr: 0.000002  loss: 0.0162 (0.0181)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [ 90/985]  eta: 0:14:28  lr: 0.000002  loss: 0.0176 (0.0183)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [100/985]  eta: 0:14:16  lr: 0.000002  loss: 0.0181 (0.0182)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [110/985]  eta: 0:14:05  lr: 0.000002  loss: 0.0177 (0.0182)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [120/985]  eta: 0:13:54  lr: 0.000002  loss: 0.0186 (0.0185)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [130/985]  eta: 0:13:43  lr: 0.000002  loss: 0.0188 (0.0185)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [140/985]  eta: 0:13:33  lr: 0.000002  loss: 0.0187 (0.0186)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [150/985]  eta: 0:13:22  lr: 0.000002  loss: 0.0189 (0.0187)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [160/985]  eta: 0:13:12  lr: 0.000002  loss: 0.0189 (0.0187)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [170/985]  eta: 0:13:03  lr: 0.000002  loss: 0.0173 (0.0187)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [180/985]  eta: 0:12:52  lr: 0.000002  loss: 0.0182 (0.0188)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [190/985]  eta: 0:12:43  lr: 0.000002  loss: 0.0190 (0.0188)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [200/985]  eta: 0:12:33  lr: 0.000002  loss: 0.0179 (0.0188)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [210/985]  eta: 0:12:23  lr: 0.000002  loss: 0.0180 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [220/985]  eta: 0:12:13  lr: 0.000002  loss: 0.0177 (0.0188)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [230/985]  eta: 0:12:04  lr: 0.000002  loss: 0.0177 (0.0188)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [240/985]  eta: 0:11:54  lr: 0.000002  loss: 0.0200 (0.0189)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [250/985]  eta: 0:11:44  lr: 0.000002  loss: 0.0188 (0.0188)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [260/985]  eta: 0:11:34  lr: 0.000002  loss: 0.0174 (0.0188)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [270/985]  eta: 0:11:24  lr: 0.000002  loss: 0.0174 (0.0188)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [280/985]  eta: 0:11:15  lr: 0.000002  loss: 0.0184 (0.0189)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [290/985]  eta: 0:11:05  lr: 0.000002  loss: 0.0193 (0.0189)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [300/985]  eta: 0:10:55  lr: 0.000002  loss: 0.0189 (0.0190)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [310/985]  eta: 0:10:46  lr: 0.000002  loss: 0.0189 (0.0190)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [320/985]  eta: 0:10:36  lr: 0.000002  loss: 0.0175 (0.0190)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [330/985]  eta: 0:10:26  lr: 0.000002  loss: 0.0174 (0.0190)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [340/985]  eta: 0:10:17  lr: 0.000002  loss: 0.0184 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [350/985]  eta: 0:10:07  lr: 0.000002  loss: 0.0176 (0.0190)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [360/985]  eta: 0:09:57  lr: 0.000002  loss: 0.0174 (0.0190)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [370/985]  eta: 0:09:48  lr: 0.000002  loss: 0.0174 (0.0190)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [380/985]  eta: 0:09:38  lr: 0.000002  loss: 0.0172 (0.0190)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [390/985]  eta: 0:09:28  lr: 0.000002  loss: 0.0173 (0.0189)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [400/985]  eta: 0:09:19  lr: 0.000002  loss: 0.0179 (0.0190)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [410/985]  eta: 0:09:09  lr: 0.000002  loss: 0.0190 (0.0190)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [420/985]  eta: 0:08:59  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [430/985]  eta: 0:08:50  lr: 0.000002  loss: 0.0185 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [440/985]  eta: 0:08:40  lr: 0.000002  loss: 0.0183 (0.0190)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [450/985]  eta: 0:08:31  lr: 0.000002  loss: 0.0181 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [460/985]  eta: 0:08:21  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [470/985]  eta: 0:08:11  lr: 0.000002  loss: 0.0193 (0.0190)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [480/985]  eta: 0:08:02  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [490/985]  eta: 0:07:52  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [500/985]  eta: 0:07:43  lr: 0.000002  loss: 0.0191 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [510/985]  eta: 0:07:33  lr: 0.000002  loss: 0.0194 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [520/985]  eta: 0:07:24  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [530/985]  eta: 0:07:14  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [540/985]  eta: 0:07:04  lr: 0.000002  loss: 0.0170 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [550/985]  eta: 0:06:55  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [560/985]  eta: 0:06:45  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [570/985]  eta: 0:06:36  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [580/985]  eta: 0:06:26  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [590/985]  eta: 0:06:17  lr: 0.000002  loss: 0.0198 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [600/985]  eta: 0:06:07  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [610/985]  eta: 0:05:57  lr: 0.000002  loss: 0.0164 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [620/985]  eta: 0:05:48  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [630/985]  eta: 0:05:38  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [640/985]  eta: 0:05:29  lr: 0.000002  loss: 0.0158 (0.0190)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [650/985]  eta: 0:05:19  lr: 0.000002  loss: 0.0170 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [660/985]  eta: 0:05:10  lr: 0.000002  loss: 0.0192 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [670/985]  eta: 0:05:00  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [680/985]  eta: 0:04:51  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [690/985]  eta: 0:04:41  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [700/985]  eta: 0:04:31  lr: 0.000002  loss: 0.0178 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [710/985]  eta: 0:04:22  lr: 0.000002  loss: 0.0178 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:791]  [720/985]  eta: 0:04:12  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [730/985]  eta: 0:04:03  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [740/985]  eta: 0:03:53  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [750/985]  eta: 0:03:44  lr: 0.000002  loss: 0.0195 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [760/985]  eta: 0:03:34  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [770/985]  eta: 0:03:25  lr: 0.000002  loss: 0.0167 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [780/985]  eta: 0:03:15  lr: 0.000002  loss: 0.0166 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [790/985]  eta: 0:03:05  lr: 0.000002  loss: 0.0174 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [800/985]  eta: 0:02:56  lr: 0.000002  loss: 0.0188 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [810/985]  eta: 0:02:46  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [820/985]  eta: 0:02:37  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [830/985]  eta: 0:02:27  lr: 0.000002  loss: 0.0176 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [840/985]  eta: 0:02:18  lr: 0.000002  loss: 0.0174 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [850/985]  eta: 0:02:08  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [860/985]  eta: 0:01:59  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [870/985]  eta: 0:01:49  lr: 0.000002  loss: 0.0176 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [880/985]  eta: 0:01:40  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [890/985]  eta: 0:01:30  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [900/985]  eta: 0:01:21  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [910/985]  eta: 0:01:11  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [920/985]  eta: 0:01:01  lr: 0.000002  loss: 0.0188 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [930/985]  eta: 0:00:52  lr: 0.000002  loss: 0.0201 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [940/985]  eta: 0:00:42  lr: 0.000002  loss: 0.0209 (0.0191)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [950/985]  eta: 0:00:33  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [960/985]  eta: 0:00:23  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [970/985]  eta: 0:00:14  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [980/985]  eta: 0:00:04  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791]  [984/985]  eta: 0:00:00  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:791] Total time: 0:15:39 (0.9535 s / it)\n",
      "Averaged stats: lr: 0.000002  loss: 0.0189 (0.0191)\n",
      "Valid: [epoch:791]  [ 0/14]  eta: 0:02:48  loss: 0.0140 (0.0140)  time: 12.0474  data: 0.6223  max mem: 41892\n",
      "Valid: [epoch:791]  [13/14]  eta: 0:00:11  loss: 0.0145 (0.0145)  time: 11.3336  data: 0.0445  max mem: 41892\n",
      "Valid: [epoch:791] Total time: 0:02:38 (11.3397 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_791_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:792]  [  0/985]  eta: 0:59:13  lr: 0.000002  loss: 0.0223 (0.0223)  time: 3.6074  data: 2.6300  max mem: 41892\n",
      "Train: [epoch:792]  [ 10/985]  eta: 0:19:10  lr: 0.000002  loss: 0.0192 (0.0195)  time: 1.1800  data: 0.2392  max mem: 41892\n",
      "Train: [epoch:792]  [ 20/985]  eta: 0:17:04  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9347  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [ 30/985]  eta: 0:16:15  lr: 0.000002  loss: 0.0180 (0.0187)  time: 0.9343  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [ 40/985]  eta: 0:15:45  lr: 0.000002  loss: 0.0175 (0.0185)  time: 0.9367  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [ 50/985]  eta: 0:15:24  lr: 0.000002  loss: 0.0164 (0.0187)  time: 0.9372  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [ 60/985]  eta: 0:15:07  lr: 0.000002  loss: 0.0176 (0.0186)  time: 0.9412  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [ 70/985]  eta: 0:14:53  lr: 0.000002  loss: 0.0176 (0.0184)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [ 80/985]  eta: 0:14:40  lr: 0.000002  loss: 0.0159 (0.0184)  time: 0.9453  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [ 90/985]  eta: 0:14:27  lr: 0.000002  loss: 0.0186 (0.0186)  time: 0.9452  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [100/985]  eta: 0:14:15  lr: 0.000002  loss: 0.0186 (0.0186)  time: 0.9459  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [110/985]  eta: 0:14:04  lr: 0.000002  loss: 0.0179 (0.0188)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [120/985]  eta: 0:13:54  lr: 0.000002  loss: 0.0194 (0.0190)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [130/985]  eta: 0:13:43  lr: 0.000002  loss: 0.0188 (0.0189)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [140/985]  eta: 0:13:33  lr: 0.000002  loss: 0.0176 (0.0189)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [150/985]  eta: 0:13:22  lr: 0.000002  loss: 0.0179 (0.0190)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [160/985]  eta: 0:13:13  lr: 0.000002  loss: 0.0184 (0.0190)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [170/985]  eta: 0:13:02  lr: 0.000002  loss: 0.0180 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [180/985]  eta: 0:12:52  lr: 0.000002  loss: 0.0175 (0.0190)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [190/985]  eta: 0:12:42  lr: 0.000002  loss: 0.0188 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [200/985]  eta: 0:12:33  lr: 0.000002  loss: 0.0186 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [210/985]  eta: 0:12:23  lr: 0.000002  loss: 0.0178 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [220/985]  eta: 0:12:13  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [230/985]  eta: 0:12:03  lr: 0.000002  loss: 0.0175 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [240/985]  eta: 0:11:53  lr: 0.000002  loss: 0.0186 (0.0192)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [250/985]  eta: 0:11:43  lr: 0.000002  loss: 0.0178 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [260/985]  eta: 0:11:34  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [270/985]  eta: 0:11:24  lr: 0.000002  loss: 0.0173 (0.0190)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [280/985]  eta: 0:11:14  lr: 0.000002  loss: 0.0177 (0.0190)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [290/985]  eta: 0:11:04  lr: 0.000002  loss: 0.0182 (0.0190)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [300/985]  eta: 0:10:55  lr: 0.000002  loss: 0.0182 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [310/985]  eta: 0:10:45  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [320/985]  eta: 0:10:35  lr: 0.000002  loss: 0.0179 (0.0190)  time: 0.9500  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:792]  [330/985]  eta: 0:10:26  lr: 0.000002  loss: 0.0179 (0.0190)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [340/985]  eta: 0:10:16  lr: 0.000002  loss: 0.0184 (0.0190)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [350/985]  eta: 0:10:06  lr: 0.000002  loss: 0.0181 (0.0190)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [360/985]  eta: 0:09:57  lr: 0.000002  loss: 0.0177 (0.0190)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [370/985]  eta: 0:09:47  lr: 0.000002  loss: 0.0177 (0.0190)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [380/985]  eta: 0:09:37  lr: 0.000002  loss: 0.0175 (0.0190)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [390/985]  eta: 0:09:28  lr: 0.000002  loss: 0.0188 (0.0190)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [400/985]  eta: 0:09:18  lr: 0.000002  loss: 0.0188 (0.0190)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [410/985]  eta: 0:09:09  lr: 0.000002  loss: 0.0185 (0.0190)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [420/985]  eta: 0:08:59  lr: 0.000002  loss: 0.0196 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [430/985]  eta: 0:08:49  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [440/985]  eta: 0:08:40  lr: 0.000002  loss: 0.0190 (0.0190)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [450/985]  eta: 0:08:30  lr: 0.000002  loss: 0.0190 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [460/985]  eta: 0:08:21  lr: 0.000002  loss: 0.0187 (0.0190)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [470/985]  eta: 0:08:11  lr: 0.000002  loss: 0.0167 (0.0190)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [480/985]  eta: 0:08:01  lr: 0.000002  loss: 0.0168 (0.0190)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [490/985]  eta: 0:07:52  lr: 0.000002  loss: 0.0179 (0.0190)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [500/985]  eta: 0:07:42  lr: 0.000002  loss: 0.0171 (0.0190)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [510/985]  eta: 0:07:33  lr: 0.000002  loss: 0.0172 (0.0190)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [520/985]  eta: 0:07:23  lr: 0.000002  loss: 0.0176 (0.0190)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [530/985]  eta: 0:07:14  lr: 0.000002  loss: 0.0177 (0.0189)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [540/985]  eta: 0:07:04  lr: 0.000002  loss: 0.0174 (0.0189)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [550/985]  eta: 0:06:55  lr: 0.000002  loss: 0.0172 (0.0189)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [560/985]  eta: 0:06:45  lr: 0.000002  loss: 0.0185 (0.0189)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [570/985]  eta: 0:06:35  lr: 0.000002  loss: 0.0185 (0.0189)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [580/985]  eta: 0:06:26  lr: 0.000002  loss: 0.0188 (0.0189)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [590/985]  eta: 0:06:16  lr: 0.000002  loss: 0.0190 (0.0189)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [600/985]  eta: 0:06:07  lr: 0.000002  loss: 0.0175 (0.0189)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [610/985]  eta: 0:05:57  lr: 0.000002  loss: 0.0164 (0.0189)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [620/985]  eta: 0:05:48  lr: 0.000002  loss: 0.0180 (0.0189)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [630/985]  eta: 0:05:38  lr: 0.000002  loss: 0.0189 (0.0189)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [640/985]  eta: 0:05:28  lr: 0.000002  loss: 0.0178 (0.0189)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [650/985]  eta: 0:05:19  lr: 0.000002  loss: 0.0183 (0.0189)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [660/985]  eta: 0:05:09  lr: 0.000002  loss: 0.0182 (0.0189)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [670/985]  eta: 0:05:00  lr: 0.000002  loss: 0.0170 (0.0188)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [680/985]  eta: 0:04:50  lr: 0.000002  loss: 0.0167 (0.0188)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [690/985]  eta: 0:04:41  lr: 0.000002  loss: 0.0182 (0.0188)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [700/985]  eta: 0:04:31  lr: 0.000002  loss: 0.0196 (0.0189)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [710/985]  eta: 0:04:22  lr: 0.000002  loss: 0.0186 (0.0189)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [720/985]  eta: 0:04:12  lr: 0.000002  loss: 0.0223 (0.0189)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [730/985]  eta: 0:04:03  lr: 0.000002  loss: 0.0206 (0.0189)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [740/985]  eta: 0:03:53  lr: 0.000002  loss: 0.0168 (0.0189)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [750/985]  eta: 0:03:43  lr: 0.000002  loss: 0.0184 (0.0189)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [760/985]  eta: 0:03:34  lr: 0.000002  loss: 0.0188 (0.0189)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [770/985]  eta: 0:03:24  lr: 0.000002  loss: 0.0183 (0.0189)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [780/985]  eta: 0:03:15  lr: 0.000002  loss: 0.0167 (0.0189)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [790/985]  eta: 0:03:05  lr: 0.000002  loss: 0.0166 (0.0189)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [800/985]  eta: 0:02:56  lr: 0.000002  loss: 0.0170 (0.0189)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [810/985]  eta: 0:02:46  lr: 0.000002  loss: 0.0173 (0.0189)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [820/985]  eta: 0:02:37  lr: 0.000002  loss: 0.0182 (0.0189)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [830/985]  eta: 0:02:27  lr: 0.000002  loss: 0.0184 (0.0190)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [840/985]  eta: 0:02:18  lr: 0.000002  loss: 0.0189 (0.0189)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [850/985]  eta: 0:02:08  lr: 0.000002  loss: 0.0171 (0.0189)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [860/985]  eta: 0:01:59  lr: 0.000002  loss: 0.0171 (0.0189)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [870/985]  eta: 0:01:49  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [880/985]  eta: 0:01:40  lr: 0.000002  loss: 0.0177 (0.0189)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [890/985]  eta: 0:01:30  lr: 0.000002  loss: 0.0174 (0.0189)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [900/985]  eta: 0:01:20  lr: 0.000002  loss: 0.0174 (0.0190)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [910/985]  eta: 0:01:11  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [920/985]  eta: 0:01:01  lr: 0.000002  loss: 0.0186 (0.0190)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [930/985]  eta: 0:00:52  lr: 0.000002  loss: 0.0195 (0.0190)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [940/985]  eta: 0:00:42  lr: 0.000002  loss: 0.0189 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [950/985]  eta: 0:00:33  lr: 0.000002  loss: 0.0203 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [960/985]  eta: 0:00:23  lr: 0.000002  loss: 0.0220 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [970/985]  eta: 0:00:14  lr: 0.000002  loss: 0.0204 (0.0191)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792]  [980/985]  eta: 0:00:04  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:792]  [984/985]  eta: 0:00:00  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:792] Total time: 0:15:38 (0.9526 s / it)\n",
      "Averaged stats: lr: 0.000002  loss: 0.0180 (0.0191)\n",
      "Valid: [epoch:792]  [ 0/14]  eta: 0:02:42  loss: 0.0143 (0.0143)  time: 11.6188  data: 0.5159  max mem: 41892\n",
      "Valid: [epoch:792]  [13/14]  eta: 0:00:11  loss: 0.0145 (0.0145)  time: 11.1166  data: 0.0369  max mem: 41892\n",
      "Valid: [epoch:792] Total time: 0:02:35 (11.1227 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_792_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:793]  [  0/985]  eta: 0:57:10  lr: 0.000002  loss: 0.0259 (0.0259)  time: 3.4825  data: 2.4163  max mem: 41892\n",
      "Train: [epoch:793]  [ 10/985]  eta: 0:18:57  lr: 0.000002  loss: 0.0187 (0.0200)  time: 1.1667  data: 0.2198  max mem: 41892\n",
      "Train: [epoch:793]  [ 20/985]  eta: 0:16:57  lr: 0.000002  loss: 0.0183 (0.0201)  time: 0.9333  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [ 30/985]  eta: 0:16:09  lr: 0.000002  loss: 0.0183 (0.0202)  time: 0.9320  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [ 40/985]  eta: 0:15:41  lr: 0.000002  loss: 0.0170 (0.0194)  time: 0.9346  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [ 50/985]  eta: 0:15:20  lr: 0.000002  loss: 0.0162 (0.0191)  time: 0.9372  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [ 60/985]  eta: 0:15:04  lr: 0.000002  loss: 0.0165 (0.0190)  time: 0.9391  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [ 70/985]  eta: 0:14:49  lr: 0.000002  loss: 0.0181 (0.0189)  time: 0.9413  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [ 80/985]  eta: 0:14:37  lr: 0.000002  loss: 0.0173 (0.0186)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [ 90/985]  eta: 0:14:26  lr: 0.000002  loss: 0.0175 (0.0186)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [100/985]  eta: 0:14:15  lr: 0.000002  loss: 0.0175 (0.0185)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [110/985]  eta: 0:14:05  lr: 0.000002  loss: 0.0170 (0.0185)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [120/985]  eta: 0:13:54  lr: 0.000002  loss: 0.0177 (0.0186)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [130/985]  eta: 0:13:44  lr: 0.000002  loss: 0.0172 (0.0185)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [140/985]  eta: 0:13:34  lr: 0.000002  loss: 0.0174 (0.0185)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [150/985]  eta: 0:13:23  lr: 0.000002  loss: 0.0178 (0.0185)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [160/985]  eta: 0:13:13  lr: 0.000002  loss: 0.0170 (0.0184)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [170/985]  eta: 0:13:03  lr: 0.000002  loss: 0.0170 (0.0184)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [180/985]  eta: 0:12:53  lr: 0.000002  loss: 0.0188 (0.0185)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [190/985]  eta: 0:12:43  lr: 0.000002  loss: 0.0190 (0.0186)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [200/985]  eta: 0:12:33  lr: 0.000002  loss: 0.0177 (0.0186)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [210/985]  eta: 0:12:23  lr: 0.000002  loss: 0.0178 (0.0186)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [220/985]  eta: 0:12:13  lr: 0.000002  loss: 0.0186 (0.0187)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [230/985]  eta: 0:12:03  lr: 0.000002  loss: 0.0188 (0.0187)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [240/985]  eta: 0:11:53  lr: 0.000002  loss: 0.0188 (0.0187)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [250/985]  eta: 0:11:44  lr: 0.000002  loss: 0.0161 (0.0186)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [260/985]  eta: 0:11:34  lr: 0.000002  loss: 0.0172 (0.0186)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [270/985]  eta: 0:11:24  lr: 0.000002  loss: 0.0172 (0.0186)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [280/985]  eta: 0:11:14  lr: 0.000002  loss: 0.0183 (0.0187)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [290/985]  eta: 0:11:05  lr: 0.000002  loss: 0.0190 (0.0187)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [300/985]  eta: 0:10:55  lr: 0.000002  loss: 0.0202 (0.0188)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [310/985]  eta: 0:10:45  lr: 0.000002  loss: 0.0177 (0.0187)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [320/985]  eta: 0:10:36  lr: 0.000002  loss: 0.0173 (0.0188)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [330/985]  eta: 0:10:26  lr: 0.000002  loss: 0.0184 (0.0188)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [340/985]  eta: 0:10:16  lr: 0.000002  loss: 0.0184 (0.0188)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [350/985]  eta: 0:10:07  lr: 0.000002  loss: 0.0176 (0.0188)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [360/985]  eta: 0:09:57  lr: 0.000002  loss: 0.0185 (0.0188)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [370/985]  eta: 0:09:47  lr: 0.000002  loss: 0.0186 (0.0188)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [380/985]  eta: 0:09:38  lr: 0.000002  loss: 0.0178 (0.0188)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [390/985]  eta: 0:09:28  lr: 0.000002  loss: 0.0184 (0.0188)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [400/985]  eta: 0:09:18  lr: 0.000002  loss: 0.0193 (0.0189)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [410/985]  eta: 0:09:09  lr: 0.000002  loss: 0.0195 (0.0190)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [420/985]  eta: 0:08:59  lr: 0.000002  loss: 0.0188 (0.0189)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [430/985]  eta: 0:08:50  lr: 0.000002  loss: 0.0191 (0.0190)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [440/985]  eta: 0:08:40  lr: 0.000002  loss: 0.0191 (0.0190)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [450/985]  eta: 0:08:30  lr: 0.000002  loss: 0.0181 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [460/985]  eta: 0:08:21  lr: 0.000002  loss: 0.0191 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [470/985]  eta: 0:08:11  lr: 0.000002  loss: 0.0207 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [480/985]  eta: 0:08:02  lr: 0.000002  loss: 0.0197 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [490/985]  eta: 0:07:52  lr: 0.000002  loss: 0.0192 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [500/985]  eta: 0:07:42  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [510/985]  eta: 0:07:33  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [520/985]  eta: 0:07:23  lr: 0.000002  loss: 0.0185 (0.0192)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [530/985]  eta: 0:07:14  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [540/985]  eta: 0:07:04  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [550/985]  eta: 0:06:54  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [560/985]  eta: 0:06:45  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [570/985]  eta: 0:06:35  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [580/985]  eta: 0:06:26  lr: 0.000002  loss: 0.0179 (0.0192)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [590/985]  eta: 0:06:16  lr: 0.000002  loss: 0.0179 (0.0192)  time: 0.9491  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:793]  [600/985]  eta: 0:06:07  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [610/985]  eta: 0:05:57  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [620/985]  eta: 0:05:48  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [630/985]  eta: 0:05:38  lr: 0.000002  loss: 0.0173 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [640/985]  eta: 0:05:28  lr: 0.000002  loss: 0.0170 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [650/985]  eta: 0:05:19  lr: 0.000002  loss: 0.0189 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [660/985]  eta: 0:05:09  lr: 0.000002  loss: 0.0194 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [670/985]  eta: 0:05:00  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [680/985]  eta: 0:04:50  lr: 0.000002  loss: 0.0168 (0.0191)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [690/985]  eta: 0:04:41  lr: 0.000002  loss: 0.0174 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [700/985]  eta: 0:04:31  lr: 0.000002  loss: 0.0186 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [710/985]  eta: 0:04:22  lr: 0.000002  loss: 0.0185 (0.0191)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [720/985]  eta: 0:04:12  lr: 0.000002  loss: 0.0206 (0.0192)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [730/985]  eta: 0:04:03  lr: 0.000002  loss: 0.0177 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [740/985]  eta: 0:03:53  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [750/985]  eta: 0:03:43  lr: 0.000002  loss: 0.0176 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [760/985]  eta: 0:03:34  lr: 0.000002  loss: 0.0189 (0.0192)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [770/985]  eta: 0:03:24  lr: 0.000002  loss: 0.0198 (0.0192)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [780/985]  eta: 0:03:15  lr: 0.000002  loss: 0.0179 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [790/985]  eta: 0:03:05  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [800/985]  eta: 0:02:56  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [810/985]  eta: 0:02:46  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [820/985]  eta: 0:02:37  lr: 0.000002  loss: 0.0193 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [830/985]  eta: 0:02:27  lr: 0.000002  loss: 0.0178 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [840/985]  eta: 0:02:18  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [850/985]  eta: 0:02:08  lr: 0.000002  loss: 0.0171 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [860/985]  eta: 0:01:59  lr: 0.000002  loss: 0.0176 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [870/985]  eta: 0:01:49  lr: 0.000002  loss: 0.0196 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [880/985]  eta: 0:01:40  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [890/985]  eta: 0:01:30  lr: 0.000002  loss: 0.0172 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [900/985]  eta: 0:01:20  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [910/985]  eta: 0:01:11  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [920/985]  eta: 0:01:01  lr: 0.000002  loss: 0.0183 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [930/985]  eta: 0:00:52  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [940/985]  eta: 0:00:42  lr: 0.000002  loss: 0.0190 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [950/985]  eta: 0:00:33  lr: 0.000002  loss: 0.0181 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [960/985]  eta: 0:00:23  lr: 0.000002  loss: 0.0180 (0.0191)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [970/985]  eta: 0:00:14  lr: 0.000002  loss: 0.0184 (0.0191)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [980/985]  eta: 0:00:04  lr: 0.000002  loss: 0.0182 (0.0191)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793]  [984/985]  eta: 0:00:00  lr: 0.000002  loss: 0.0187 (0.0191)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:793] Total time: 0:15:38 (0.9527 s / it)\n",
      "Averaged stats: lr: 0.000002  loss: 0.0187 (0.0191)\n",
      "Valid: [epoch:793]  [ 0/14]  eta: 0:02:42  loss: 0.0148 (0.0148)  time: 11.5771  data: 0.5177  max mem: 41892\n",
      "Valid: [epoch:793]  [13/14]  eta: 0:00:11  loss: 0.0148 (0.0147)  time: 11.1297  data: 0.0371  max mem: 41892\n",
      "Valid: [epoch:793] Total time: 0:02:35 (11.1361 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_793_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:794]  [  0/985]  eta: 0:51:58  lr: 0.000001  loss: 0.0143 (0.0143)  time: 3.1656  data: 2.2098  max mem: 41892\n",
      "Train: [epoch:794]  [ 10/985]  eta: 0:18:27  lr: 0.000001  loss: 0.0222 (0.0204)  time: 1.1356  data: 0.2010  max mem: 41892\n",
      "Train: [epoch:794]  [ 20/985]  eta: 0:16:42  lr: 0.000001  loss: 0.0186 (0.0197)  time: 0.9327  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [ 30/985]  eta: 0:16:00  lr: 0.000001  loss: 0.0185 (0.0200)  time: 0.9338  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [ 40/985]  eta: 0:15:34  lr: 0.000001  loss: 0.0164 (0.0191)  time: 0.9355  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [ 50/985]  eta: 0:15:15  lr: 0.000001  loss: 0.0155 (0.0188)  time: 0.9374  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [ 60/985]  eta: 0:14:59  lr: 0.000001  loss: 0.0185 (0.0190)  time: 0.9395  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [ 70/985]  eta: 0:14:45  lr: 0.000001  loss: 0.0185 (0.0189)  time: 0.9417  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [ 80/985]  eta: 0:14:34  lr: 0.000001  loss: 0.0174 (0.0188)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [ 90/985]  eta: 0:14:22  lr: 0.000001  loss: 0.0179 (0.0189)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [100/985]  eta: 0:14:11  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9453  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [110/985]  eta: 0:14:00  lr: 0.000001  loss: 0.0176 (0.0188)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [120/985]  eta: 0:13:49  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [130/985]  eta: 0:13:39  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [140/985]  eta: 0:13:29  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [150/985]  eta: 0:13:19  lr: 0.000001  loss: 0.0183 (0.0192)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [160/985]  eta: 0:13:09  lr: 0.000001  loss: 0.0192 (0.0193)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [170/985]  eta: 0:12:59  lr: 0.000001  loss: 0.0187 (0.0194)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [180/985]  eta: 0:12:50  lr: 0.000001  loss: 0.0187 (0.0194)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [190/985]  eta: 0:12:40  lr: 0.000001  loss: 0.0187 (0.0194)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [200/985]  eta: 0:12:30  lr: 0.000001  loss: 0.0194 (0.0194)  time: 0.9506  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:794]  [210/985]  eta: 0:12:20  lr: 0.000001  loss: 0.0194 (0.0194)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [220/985]  eta: 0:12:11  lr: 0.000001  loss: 0.0167 (0.0193)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [230/985]  eta: 0:12:01  lr: 0.000001  loss: 0.0166 (0.0192)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [240/985]  eta: 0:11:51  lr: 0.000001  loss: 0.0192 (0.0193)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [250/985]  eta: 0:11:41  lr: 0.000001  loss: 0.0192 (0.0193)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [260/985]  eta: 0:11:32  lr: 0.000001  loss: 0.0177 (0.0192)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [270/985]  eta: 0:11:22  lr: 0.000001  loss: 0.0172 (0.0192)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [280/985]  eta: 0:11:12  lr: 0.000001  loss: 0.0180 (0.0192)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [290/985]  eta: 0:11:03  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [300/985]  eta: 0:10:53  lr: 0.000001  loss: 0.0192 (0.0192)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [310/985]  eta: 0:10:44  lr: 0.000001  loss: 0.0195 (0.0192)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [320/985]  eta: 0:10:34  lr: 0.000001  loss: 0.0177 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [330/985]  eta: 0:10:24  lr: 0.000001  loss: 0.0193 (0.0192)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [340/985]  eta: 0:10:15  lr: 0.000001  loss: 0.0195 (0.0192)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [350/985]  eta: 0:10:05  lr: 0.000001  loss: 0.0181 (0.0192)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [360/985]  eta: 0:09:56  lr: 0.000001  loss: 0.0170 (0.0192)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [370/985]  eta: 0:09:46  lr: 0.000001  loss: 0.0188 (0.0192)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [380/985]  eta: 0:09:37  lr: 0.000001  loss: 0.0188 (0.0192)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [390/985]  eta: 0:09:27  lr: 0.000001  loss: 0.0178 (0.0192)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [400/985]  eta: 0:09:18  lr: 0.000001  loss: 0.0184 (0.0192)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [410/985]  eta: 0:09:08  lr: 0.000001  loss: 0.0190 (0.0192)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [420/985]  eta: 0:08:58  lr: 0.000001  loss: 0.0178 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [430/985]  eta: 0:08:49  lr: 0.000001  loss: 0.0176 (0.0192)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [440/985]  eta: 0:08:39  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [450/985]  eta: 0:08:30  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [460/985]  eta: 0:08:20  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [470/985]  eta: 0:08:11  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [480/985]  eta: 0:08:01  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [490/985]  eta: 0:07:51  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [500/985]  eta: 0:07:42  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [510/985]  eta: 0:07:32  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [520/985]  eta: 0:07:23  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [530/985]  eta: 0:07:13  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [540/985]  eta: 0:07:04  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [550/985]  eta: 0:06:54  lr: 0.000001  loss: 0.0169 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [560/985]  eta: 0:06:45  lr: 0.000001  loss: 0.0169 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [570/985]  eta: 0:06:35  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [580/985]  eta: 0:06:26  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [590/985]  eta: 0:06:16  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [600/985]  eta: 0:06:06  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [610/985]  eta: 0:05:57  lr: 0.000001  loss: 0.0191 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [620/985]  eta: 0:05:47  lr: 0.000001  loss: 0.0195 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [630/985]  eta: 0:05:38  lr: 0.000001  loss: 0.0177 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [640/985]  eta: 0:05:28  lr: 0.000001  loss: 0.0162 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [650/985]  eta: 0:05:19  lr: 0.000001  loss: 0.0170 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [660/985]  eta: 0:05:09  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [670/985]  eta: 0:05:00  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [680/985]  eta: 0:04:50  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [690/985]  eta: 0:04:41  lr: 0.000001  loss: 0.0162 (0.0190)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [700/985]  eta: 0:04:31  lr: 0.000001  loss: 0.0170 (0.0190)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [710/985]  eta: 0:04:21  lr: 0.000001  loss: 0.0186 (0.0190)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [720/985]  eta: 0:04:12  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [730/985]  eta: 0:04:02  lr: 0.000001  loss: 0.0187 (0.0190)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [740/985]  eta: 0:03:53  lr: 0.000001  loss: 0.0169 (0.0190)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [750/985]  eta: 0:03:43  lr: 0.000001  loss: 0.0173 (0.0190)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [760/985]  eta: 0:03:34  lr: 0.000001  loss: 0.0182 (0.0190)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [770/985]  eta: 0:03:24  lr: 0.000001  loss: 0.0177 (0.0190)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [780/985]  eta: 0:03:15  lr: 0.000001  loss: 0.0177 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [790/985]  eta: 0:03:05  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [800/985]  eta: 0:02:56  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [810/985]  eta: 0:02:46  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [820/985]  eta: 0:02:37  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [830/985]  eta: 0:02:27  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9484  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [840/985]  eta: 0:02:18  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [850/985]  eta: 0:02:08  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [860/985]  eta: 0:01:59  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:794]  [870/985]  eta: 0:01:49  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [880/985]  eta: 0:01:39  lr: 0.000001  loss: 0.0177 (0.0190)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [890/985]  eta: 0:01:30  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [900/985]  eta: 0:01:20  lr: 0.000001  loss: 0.0207 (0.0191)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [910/985]  eta: 0:01:11  lr: 0.000001  loss: 0.0235 (0.0192)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [920/985]  eta: 0:01:01  lr: 0.000001  loss: 0.0195 (0.0192)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [930/985]  eta: 0:00:52  lr: 0.000001  loss: 0.0187 (0.0192)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [940/985]  eta: 0:00:42  lr: 0.000001  loss: 0.0173 (0.0192)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [950/985]  eta: 0:00:33  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [960/985]  eta: 0:00:23  lr: 0.000001  loss: 0.0191 (0.0192)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [970/985]  eta: 0:00:14  lr: 0.000001  loss: 0.0210 (0.0192)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [980/985]  eta: 0:00:04  lr: 0.000001  loss: 0.0179 (0.0192)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794]  [984/985]  eta: 0:00:00  lr: 0.000001  loss: 0.0184 (0.0192)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:794] Total time: 0:15:37 (0.9518 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.0184 (0.0192)\n",
      "Valid: [epoch:794]  [ 0/14]  eta: 0:02:42  loss: 0.0137 (0.0137)  time: 11.6234  data: 0.5535  max mem: 41892\n",
      "Valid: [epoch:794]  [13/14]  eta: 0:00:11  loss: 0.0145 (0.0145)  time: 11.2283  data: 0.0396  max mem: 41892\n",
      "Valid: [epoch:794] Total time: 0:02:37 (11.2346 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_794_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:795]  [  0/985]  eta: 0:59:12  lr: 0.000001  loss: 0.0213 (0.0213)  time: 3.6066  data: 2.6456  max mem: 41892\n",
      "Train: [epoch:795]  [ 10/985]  eta: 0:19:03  lr: 0.000001  loss: 0.0188 (0.0187)  time: 1.1729  data: 0.2406  max mem: 41892\n",
      "Train: [epoch:795]  [ 20/985]  eta: 0:17:01  lr: 0.000001  loss: 0.0180 (0.0192)  time: 0.9307  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [ 30/985]  eta: 0:16:12  lr: 0.000001  loss: 0.0182 (0.0190)  time: 0.9333  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [ 40/985]  eta: 0:15:43  lr: 0.000001  loss: 0.0173 (0.0183)  time: 0.9354  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [ 50/985]  eta: 0:15:23  lr: 0.000001  loss: 0.0158 (0.0184)  time: 0.9396  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [ 60/985]  eta: 0:15:06  lr: 0.000001  loss: 0.0179 (0.0185)  time: 0.9420  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [ 70/985]  eta: 0:14:52  lr: 0.000001  loss: 0.0172 (0.0184)  time: 0.9440  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [ 80/985]  eta: 0:14:39  lr: 0.000001  loss: 0.0162 (0.0184)  time: 0.9453  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [ 90/985]  eta: 0:14:26  lr: 0.000001  loss: 0.0172 (0.0184)  time: 0.9447  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [100/985]  eta: 0:14:15  lr: 0.000001  loss: 0.0181 (0.0185)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [110/985]  eta: 0:14:04  lr: 0.000001  loss: 0.0186 (0.0187)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [120/985]  eta: 0:13:54  lr: 0.000001  loss: 0.0189 (0.0188)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [130/985]  eta: 0:13:44  lr: 0.000001  loss: 0.0203 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [140/985]  eta: 0:13:35  lr: 0.000001  loss: 0.0195 (0.0190)  time: 0.9661  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [150/985]  eta: 0:13:25  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9663  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [160/985]  eta: 0:13:15  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [170/985]  eta: 0:13:05  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [180/985]  eta: 0:12:55  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [190/985]  eta: 0:12:46  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [200/985]  eta: 0:12:36  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [210/985]  eta: 0:12:26  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [220/985]  eta: 0:12:16  lr: 0.000001  loss: 0.0178 (0.0190)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [230/985]  eta: 0:12:06  lr: 0.000001  loss: 0.0175 (0.0190)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [240/985]  eta: 0:11:56  lr: 0.000001  loss: 0.0186 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [250/985]  eta: 0:11:46  lr: 0.000001  loss: 0.0171 (0.0189)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [260/985]  eta: 0:11:36  lr: 0.000001  loss: 0.0168 (0.0189)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [270/985]  eta: 0:11:26  lr: 0.000001  loss: 0.0168 (0.0188)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [280/985]  eta: 0:11:17  lr: 0.000001  loss: 0.0174 (0.0189)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [290/985]  eta: 0:11:07  lr: 0.000001  loss: 0.0179 (0.0189)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [300/985]  eta: 0:10:57  lr: 0.000001  loss: 0.0181 (0.0189)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [310/985]  eta: 0:10:47  lr: 0.000001  loss: 0.0176 (0.0188)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [320/985]  eta: 0:10:37  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [330/985]  eta: 0:10:28  lr: 0.000001  loss: 0.0195 (0.0189)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [340/985]  eta: 0:10:18  lr: 0.000001  loss: 0.0179 (0.0189)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [350/985]  eta: 0:10:08  lr: 0.000001  loss: 0.0172 (0.0189)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [360/985]  eta: 0:09:59  lr: 0.000001  loss: 0.0176 (0.0189)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [370/985]  eta: 0:09:49  lr: 0.000001  loss: 0.0176 (0.0189)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [380/985]  eta: 0:09:39  lr: 0.000001  loss: 0.0172 (0.0189)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [390/985]  eta: 0:09:29  lr: 0.000001  loss: 0.0179 (0.0189)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [400/985]  eta: 0:09:20  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [410/985]  eta: 0:09:10  lr: 0.000001  loss: 0.0190 (0.0190)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [420/985]  eta: 0:09:01  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [430/985]  eta: 0:08:51  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [440/985]  eta: 0:08:41  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [450/985]  eta: 0:08:32  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [460/985]  eta: 0:08:22  lr: 0.000001  loss: 0.0186 (0.0190)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [470/985]  eta: 0:08:13  lr: 0.000001  loss: 0.0186 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:795]  [480/985]  eta: 0:08:03  lr: 0.000001  loss: 0.0187 (0.0190)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [490/985]  eta: 0:07:54  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [500/985]  eta: 0:07:44  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [510/985]  eta: 0:07:34  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9644  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [520/985]  eta: 0:07:25  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9641  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [530/985]  eta: 0:07:15  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [540/985]  eta: 0:07:06  lr: 0.000001  loss: 0.0175 (0.0190)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [550/985]  eta: 0:06:56  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9638  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [560/985]  eta: 0:06:47  lr: 0.000001  loss: 0.0189 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [570/985]  eta: 0:06:37  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [580/985]  eta: 0:06:27  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [590/985]  eta: 0:06:18  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [600/985]  eta: 0:06:08  lr: 0.000001  loss: 0.0170 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [610/985]  eta: 0:05:59  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [620/985]  eta: 0:05:49  lr: 0.000001  loss: 0.0189 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [630/985]  eta: 0:05:39  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [640/985]  eta: 0:05:30  lr: 0.000001  loss: 0.0170 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [650/985]  eta: 0:05:20  lr: 0.000001  loss: 0.0174 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [660/985]  eta: 0:05:11  lr: 0.000001  loss: 0.0193 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [670/985]  eta: 0:05:01  lr: 0.000001  loss: 0.0171 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [680/985]  eta: 0:04:51  lr: 0.000001  loss: 0.0168 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [690/985]  eta: 0:04:42  lr: 0.000001  loss: 0.0170 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [700/985]  eta: 0:04:32  lr: 0.000001  loss: 0.0173 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [710/985]  eta: 0:04:23  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [720/985]  eta: 0:04:13  lr: 0.000001  loss: 0.0193 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [730/985]  eta: 0:04:03  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [740/985]  eta: 0:03:54  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [750/985]  eta: 0:03:44  lr: 0.000001  loss: 0.0187 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [760/985]  eta: 0:03:35  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [770/985]  eta: 0:03:25  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [780/985]  eta: 0:03:16  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [790/985]  eta: 0:03:06  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [800/985]  eta: 0:02:56  lr: 0.000001  loss: 0.0162 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [810/985]  eta: 0:02:47  lr: 0.000001  loss: 0.0173 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [820/985]  eta: 0:02:37  lr: 0.000001  loss: 0.0189 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [830/985]  eta: 0:02:28  lr: 0.000001  loss: 0.0191 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [840/985]  eta: 0:02:18  lr: 0.000001  loss: 0.0173 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [850/985]  eta: 0:02:09  lr: 0.000001  loss: 0.0166 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [860/985]  eta: 0:01:59  lr: 0.000001  loss: 0.0167 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [870/985]  eta: 0:01:49  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [880/985]  eta: 0:01:40  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [890/985]  eta: 0:01:30  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [900/985]  eta: 0:01:21  lr: 0.000001  loss: 0.0189 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [910/985]  eta: 0:01:11  lr: 0.000001  loss: 0.0189 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [920/985]  eta: 0:01:02  lr: 0.000001  loss: 0.0193 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [930/985]  eta: 0:00:52  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [940/985]  eta: 0:00:42  lr: 0.000001  loss: 0.0189 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [950/985]  eta: 0:00:33  lr: 0.000001  loss: 0.0192 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [960/985]  eta: 0:00:23  lr: 0.000001  loss: 0.0197 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [970/985]  eta: 0:00:14  lr: 0.000001  loss: 0.0197 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [980/985]  eta: 0:00:04  lr: 0.000001  loss: 0.0177 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795]  [984/985]  eta: 0:00:00  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:795] Total time: 0:15:41 (0.9554 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.0186 (0.0191)\n",
      "Valid: [epoch:795]  [ 0/14]  eta: 0:02:43  loss: 0.0149 (0.0149)  time: 11.6579  data: 0.5296  max mem: 41892\n",
      "Valid: [epoch:795]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0146)  time: 11.1126  data: 0.0379  max mem: 41892\n",
      "Valid: [epoch:795] Total time: 0:02:35 (11.1193 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_795_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:796]  [  0/985]  eta: 1:02:03  lr: 0.000001  loss: 0.0188 (0.0188)  time: 3.7801  data: 2.8123  max mem: 41892\n",
      "Train: [epoch:796]  [ 10/985]  eta: 0:19:21  lr: 0.000001  loss: 0.0190 (0.0192)  time: 1.1913  data: 0.2558  max mem: 41892\n",
      "Train: [epoch:796]  [ 20/985]  eta: 0:17:13  lr: 0.000001  loss: 0.0192 (0.0196)  time: 0.9350  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [ 30/985]  eta: 0:16:20  lr: 0.000001  loss: 0.0184 (0.0196)  time: 0.9364  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [ 40/985]  eta: 0:15:50  lr: 0.000001  loss: 0.0168 (0.0185)  time: 0.9379  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [ 50/985]  eta: 0:15:28  lr: 0.000001  loss: 0.0153 (0.0182)  time: 0.9414  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [ 60/985]  eta: 0:15:11  lr: 0.000001  loss: 0.0178 (0.0185)  time: 0.9450  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [ 70/985]  eta: 0:14:56  lr: 0.000001  loss: 0.0175 (0.0182)  time: 0.9453  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [ 80/985]  eta: 0:14:42  lr: 0.000001  loss: 0.0165 (0.0181)  time: 0.9440  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:796]  [ 90/985]  eta: 0:14:30  lr: 0.000001  loss: 0.0178 (0.0183)  time: 0.9454  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [100/985]  eta: 0:14:18  lr: 0.000001  loss: 0.0191 (0.0184)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [110/985]  eta: 0:14:07  lr: 0.000001  loss: 0.0181 (0.0184)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [120/985]  eta: 0:13:56  lr: 0.000001  loss: 0.0181 (0.0184)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [130/985]  eta: 0:13:45  lr: 0.000001  loss: 0.0191 (0.0186)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [140/985]  eta: 0:13:34  lr: 0.000001  loss: 0.0194 (0.0187)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [150/985]  eta: 0:13:24  lr: 0.000001  loss: 0.0186 (0.0187)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [160/985]  eta: 0:13:13  lr: 0.000001  loss: 0.0197 (0.0189)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [170/985]  eta: 0:13:03  lr: 0.000001  loss: 0.0190 (0.0189)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [180/985]  eta: 0:12:54  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [190/985]  eta: 0:12:44  lr: 0.000001  loss: 0.0190 (0.0189)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [200/985]  eta: 0:12:33  lr: 0.000001  loss: 0.0190 (0.0190)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [210/985]  eta: 0:12:24  lr: 0.000001  loss: 0.0172 (0.0189)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [220/985]  eta: 0:12:14  lr: 0.000001  loss: 0.0167 (0.0188)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [230/985]  eta: 0:12:04  lr: 0.000001  loss: 0.0170 (0.0188)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [240/985]  eta: 0:11:54  lr: 0.000001  loss: 0.0187 (0.0189)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [250/985]  eta: 0:11:44  lr: 0.000001  loss: 0.0184 (0.0189)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [260/985]  eta: 0:11:35  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [270/985]  eta: 0:11:25  lr: 0.000001  loss: 0.0182 (0.0188)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [280/985]  eta: 0:11:15  lr: 0.000001  loss: 0.0182 (0.0188)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [290/985]  eta: 0:11:05  lr: 0.000001  loss: 0.0187 (0.0189)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [300/985]  eta: 0:10:56  lr: 0.000001  loss: 0.0202 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [310/985]  eta: 0:10:46  lr: 0.000001  loss: 0.0198 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [320/985]  eta: 0:10:37  lr: 0.000001  loss: 0.0172 (0.0190)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [330/985]  eta: 0:10:27  lr: 0.000001  loss: 0.0173 (0.0189)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [340/985]  eta: 0:10:17  lr: 0.000001  loss: 0.0171 (0.0189)  time: 0.9536  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [350/985]  eta: 0:10:08  lr: 0.000001  loss: 0.0173 (0.0189)  time: 0.9580  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [360/985]  eta: 0:09:58  lr: 0.000001  loss: 0.0177 (0.0189)  time: 0.9572  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [370/985]  eta: 0:09:48  lr: 0.000001  loss: 0.0184 (0.0189)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [380/985]  eta: 0:09:39  lr: 0.000001  loss: 0.0172 (0.0189)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [390/985]  eta: 0:09:29  lr: 0.000001  loss: 0.0185 (0.0189)  time: 0.9555  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [400/985]  eta: 0:09:20  lr: 0.000001  loss: 0.0191 (0.0190)  time: 0.9521  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [410/985]  eta: 0:09:10  lr: 0.000001  loss: 0.0190 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [420/985]  eta: 0:09:00  lr: 0.000001  loss: 0.0181 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [430/985]  eta: 0:08:51  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [440/985]  eta: 0:08:41  lr: 0.000001  loss: 0.0192 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [450/985]  eta: 0:08:31  lr: 0.000001  loss: 0.0192 (0.0190)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [460/985]  eta: 0:08:22  lr: 0.000001  loss: 0.0191 (0.0190)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [470/985]  eta: 0:08:12  lr: 0.000001  loss: 0.0176 (0.0190)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [480/985]  eta: 0:08:03  lr: 0.000001  loss: 0.0176 (0.0190)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [490/985]  eta: 0:07:53  lr: 0.000001  loss: 0.0187 (0.0190)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [500/985]  eta: 0:07:43  lr: 0.000001  loss: 0.0197 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [510/985]  eta: 0:07:34  lr: 0.000001  loss: 0.0197 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [520/985]  eta: 0:07:24  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [530/985]  eta: 0:07:14  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [540/985]  eta: 0:07:05  lr: 0.000001  loss: 0.0170 (0.0190)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [550/985]  eta: 0:06:55  lr: 0.000001  loss: 0.0168 (0.0190)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [560/985]  eta: 0:06:46  lr: 0.000001  loss: 0.0168 (0.0190)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [570/985]  eta: 0:06:36  lr: 0.000001  loss: 0.0172 (0.0190)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [580/985]  eta: 0:06:26  lr: 0.000001  loss: 0.0182 (0.0190)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [590/985]  eta: 0:06:17  lr: 0.000001  loss: 0.0197 (0.0191)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [600/985]  eta: 0:06:07  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [610/985]  eta: 0:05:58  lr: 0.000001  loss: 0.0171 (0.0190)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [620/985]  eta: 0:05:48  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [630/985]  eta: 0:05:38  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [640/985]  eta: 0:05:29  lr: 0.000001  loss: 0.0174 (0.0190)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [650/985]  eta: 0:05:19  lr: 0.000001  loss: 0.0170 (0.0190)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [660/985]  eta: 0:05:10  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [670/985]  eta: 0:05:00  lr: 0.000001  loss: 0.0177 (0.0190)  time: 0.9473  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [680/985]  eta: 0:04:51  lr: 0.000001  loss: 0.0173 (0.0190)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [690/985]  eta: 0:04:41  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [700/985]  eta: 0:04:31  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [710/985]  eta: 0:04:22  lr: 0.000001  loss: 0.0186 (0.0190)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [720/985]  eta: 0:04:12  lr: 0.000001  loss: 0.0186 (0.0190)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [730/985]  eta: 0:04:03  lr: 0.000001  loss: 0.0181 (0.0190)  time: 0.9461  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [740/985]  eta: 0:03:53  lr: 0.000001  loss: 0.0164 (0.0190)  time: 0.9465  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:796]  [750/985]  eta: 0:03:44  lr: 0.000001  loss: 0.0176 (0.0190)  time: 0.9465  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [760/985]  eta: 0:03:34  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [770/985]  eta: 0:03:25  lr: 0.000001  loss: 0.0192 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [780/985]  eta: 0:03:15  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9672  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [790/985]  eta: 0:03:06  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9624  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [800/985]  eta: 0:02:56  lr: 0.000001  loss: 0.0172 (0.0190)  time: 0.9547  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [810/985]  eta: 0:02:46  lr: 0.000001  loss: 0.0167 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [820/985]  eta: 0:02:37  lr: 0.000001  loss: 0.0165 (0.0190)  time: 0.9594  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [830/985]  eta: 0:02:27  lr: 0.000001  loss: 0.0177 (0.0190)  time: 0.9655  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [840/985]  eta: 0:02:18  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9652  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [850/985]  eta: 0:02:08  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [860/985]  eta: 0:01:59  lr: 0.000001  loss: 0.0177 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [870/985]  eta: 0:01:49  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [880/985]  eta: 0:01:40  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [890/985]  eta: 0:01:30  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [900/985]  eta: 0:01:21  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [910/985]  eta: 0:01:11  lr: 0.000001  loss: 0.0199 (0.0191)  time: 0.9557  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:796]  [920/985]  eta: 0:01:02  lr: 0.000001  loss: 0.0191 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [930/985]  eta: 0:00:52  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [940/985]  eta: 0:00:42  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [950/985]  eta: 0:00:33  lr: 0.000001  loss: 0.0199 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [960/985]  eta: 0:00:23  lr: 0.000001  loss: 0.0204 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [970/985]  eta: 0:00:14  lr: 0.000001  loss: 0.0202 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [980/985]  eta: 0:00:04  lr: 0.000001  loss: 0.0195 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796]  [984/985]  eta: 0:00:00  lr: 0.000001  loss: 0.0177 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:796] Total time: 0:15:40 (0.9548 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.0177 (0.0191)\n",
      "Valid: [epoch:796]  [ 0/14]  eta: 0:02:57  loss: 0.0153 (0.0153)  time: 12.7074  data: 0.6013  max mem: 41892\n",
      "Valid: [epoch:796]  [13/14]  eta: 0:00:12  loss: 0.0147 (0.0147)  time: 12.2270  data: 0.0431  max mem: 41892\n",
      "Valid: [epoch:796] Total time: 0:02:51 (12.2339 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_796_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:797]  [  0/985]  eta: 1:15:22  lr: 0.000001  loss: 0.0179 (0.0179)  time: 4.5912  data: 3.5835  max mem: 41892\n",
      "Train: [epoch:797]  [ 10/985]  eta: 0:20:42  lr: 0.000001  loss: 0.0182 (0.0191)  time: 1.2739  data: 0.3259  max mem: 41892\n",
      "Train: [epoch:797]  [ 20/985]  eta: 0:17:55  lr: 0.000001  loss: 0.0186 (0.0197)  time: 0.9408  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [ 30/985]  eta: 0:16:50  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9401  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [ 40/985]  eta: 0:16:13  lr: 0.000001  loss: 0.0184 (0.0193)  time: 0.9420  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [ 50/985]  eta: 0:15:47  lr: 0.000001  loss: 0.0178 (0.0189)  time: 0.9434  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [ 60/985]  eta: 0:15:26  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9436  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [ 70/985]  eta: 0:15:09  lr: 0.000001  loss: 0.0176 (0.0187)  time: 0.9451  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [ 80/985]  eta: 0:14:54  lr: 0.000001  loss: 0.0148 (0.0184)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [ 90/985]  eta: 0:14:41  lr: 0.000001  loss: 0.0163 (0.0185)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [100/985]  eta: 0:14:29  lr: 0.000001  loss: 0.0177 (0.0184)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [110/985]  eta: 0:14:16  lr: 0.000001  loss: 0.0174 (0.0183)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [120/985]  eta: 0:14:05  lr: 0.000001  loss: 0.0183 (0.0185)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [130/985]  eta: 0:13:54  lr: 0.000001  loss: 0.0198 (0.0188)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [140/985]  eta: 0:13:43  lr: 0.000001  loss: 0.0193 (0.0188)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [150/985]  eta: 0:13:32  lr: 0.000001  loss: 0.0181 (0.0187)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [160/985]  eta: 0:13:21  lr: 0.000001  loss: 0.0178 (0.0187)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [170/985]  eta: 0:13:11  lr: 0.000001  loss: 0.0178 (0.0187)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [180/985]  eta: 0:13:00  lr: 0.000001  loss: 0.0180 (0.0187)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [190/985]  eta: 0:12:50  lr: 0.000001  loss: 0.0195 (0.0188)  time: 0.9593  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [200/985]  eta: 0:12:40  lr: 0.000001  loss: 0.0183 (0.0188)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [210/985]  eta: 0:12:30  lr: 0.000001  loss: 0.0166 (0.0188)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [220/985]  eta: 0:12:20  lr: 0.000001  loss: 0.0172 (0.0187)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [230/985]  eta: 0:12:09  lr: 0.000001  loss: 0.0185 (0.0188)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [240/985]  eta: 0:11:59  lr: 0.000001  loss: 0.0185 (0.0187)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [250/985]  eta: 0:11:49  lr: 0.000001  loss: 0.0188 (0.0188)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [260/985]  eta: 0:11:40  lr: 0.000001  loss: 0.0184 (0.0188)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [270/985]  eta: 0:11:30  lr: 0.000001  loss: 0.0171 (0.0187)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [280/985]  eta: 0:11:20  lr: 0.000001  loss: 0.0171 (0.0187)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [290/985]  eta: 0:11:10  lr: 0.000001  loss: 0.0183 (0.0188)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [300/985]  eta: 0:11:00  lr: 0.000001  loss: 0.0185 (0.0188)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [310/985]  eta: 0:10:50  lr: 0.000001  loss: 0.0175 (0.0188)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [320/985]  eta: 0:10:41  lr: 0.000001  loss: 0.0194 (0.0189)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [330/985]  eta: 0:10:31  lr: 0.000001  loss: 0.0206 (0.0189)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [340/985]  eta: 0:10:21  lr: 0.000001  loss: 0.0199 (0.0189)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [350/985]  eta: 0:10:11  lr: 0.000001  loss: 0.0181 (0.0189)  time: 0.9518  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:797]  [360/985]  eta: 0:10:01  lr: 0.000001  loss: 0.0182 (0.0189)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [370/985]  eta: 0:09:52  lr: 0.000001  loss: 0.0181 (0.0189)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [380/985]  eta: 0:09:42  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [390/985]  eta: 0:09:32  lr: 0.000001  loss: 0.0194 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [400/985]  eta: 0:09:22  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [410/985]  eta: 0:09:13  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [420/985]  eta: 0:09:03  lr: 0.000001  loss: 0.0176 (0.0190)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [430/985]  eta: 0:08:53  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [440/985]  eta: 0:08:43  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [450/985]  eta: 0:08:34  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [460/985]  eta: 0:08:24  lr: 0.000001  loss: 0.0187 (0.0191)  time: 0.9546  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [470/985]  eta: 0:08:14  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [480/985]  eta: 0:08:05  lr: 0.000001  loss: 0.0177 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [490/985]  eta: 0:07:55  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [500/985]  eta: 0:07:45  lr: 0.000001  loss: 0.0205 (0.0192)  time: 0.9559  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [510/985]  eta: 0:07:36  lr: 0.000001  loss: 0.0199 (0.0192)  time: 0.9553  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [520/985]  eta: 0:07:26  lr: 0.000001  loss: 0.0184 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [530/985]  eta: 0:07:16  lr: 0.000001  loss: 0.0176 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [540/985]  eta: 0:07:07  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [550/985]  eta: 0:06:57  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [560/985]  eta: 0:06:48  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [570/985]  eta: 0:06:38  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [580/985]  eta: 0:06:28  lr: 0.000001  loss: 0.0170 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [590/985]  eta: 0:06:19  lr: 0.000001  loss: 0.0174 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [600/985]  eta: 0:06:09  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [610/985]  eta: 0:06:00  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9652  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [620/985]  eta: 0:05:50  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9642  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [630/985]  eta: 0:05:40  lr: 0.000001  loss: 0.0198 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [640/985]  eta: 0:05:31  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [650/985]  eta: 0:05:21  lr: 0.000001  loss: 0.0171 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [660/985]  eta: 0:05:12  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9605  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [670/985]  eta: 0:05:02  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [680/985]  eta: 0:04:52  lr: 0.000001  loss: 0.0173 (0.0190)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [690/985]  eta: 0:04:43  lr: 0.000001  loss: 0.0182 (0.0190)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [700/985]  eta: 0:04:33  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [710/985]  eta: 0:04:23  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [720/985]  eta: 0:04:14  lr: 0.000001  loss: 0.0193 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [730/985]  eta: 0:04:04  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [740/985]  eta: 0:03:55  lr: 0.000001  loss: 0.0167 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [750/985]  eta: 0:03:45  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [760/985]  eta: 0:03:35  lr: 0.000001  loss: 0.0198 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [770/985]  eta: 0:03:26  lr: 0.000001  loss: 0.0198 (0.0191)  time: 0.9594  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [780/985]  eta: 0:03:16  lr: 0.000001  loss: 0.0203 (0.0191)  time: 0.9599  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [790/985]  eta: 0:03:07  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [800/985]  eta: 0:02:57  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [810/985]  eta: 0:02:47  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [820/985]  eta: 0:02:38  lr: 0.000001  loss: 0.0201 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [830/985]  eta: 0:02:28  lr: 0.000001  loss: 0.0194 (0.0192)  time: 0.9602  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [840/985]  eta: 0:02:19  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [850/985]  eta: 0:02:09  lr: 0.000001  loss: 0.0182 (0.0192)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [860/985]  eta: 0:01:59  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [870/985]  eta: 0:01:50  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [880/985]  eta: 0:01:40  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [890/985]  eta: 0:01:31  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9630  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [900/985]  eta: 0:01:21  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9616  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:797]  [910/985]  eta: 0:01:11  lr: 0.000001  loss: 0.0194 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [920/985]  eta: 0:01:02  lr: 0.000001  loss: 0.0203 (0.0191)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [930/985]  eta: 0:00:52  lr: 0.000001  loss: 0.0191 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [940/985]  eta: 0:00:43  lr: 0.000001  loss: 0.0166 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [950/985]  eta: 0:00:33  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [960/985]  eta: 0:00:23  lr: 0.000001  loss: 0.0194 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [970/985]  eta: 0:00:14  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [980/985]  eta: 0:00:04  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797]  [984/985]  eta: 0:00:00  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:797] Total time: 0:15:44 (0.9589 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.0180 (0.0191)\n",
      "Valid: [epoch:797]  [ 0/14]  eta: 0:02:51  loss: 0.0146 (0.0146)  time: 12.2842  data: 0.5668  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:797]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0146)  time: 11.9423  data: 0.0406  max mem: 41892\n",
      "Valid: [epoch:797] Total time: 0:02:47 (11.9489 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_797_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:798]  [  0/985]  eta: 1:20:40  lr: 0.000001  loss: 0.0165 (0.0165)  time: 4.9138  data: 3.8677  max mem: 41892\n",
      "Train: [epoch:798]  [ 10/985]  eta: 0:21:13  lr: 0.000001  loss: 0.0213 (0.0197)  time: 1.3058  data: 0.3518  max mem: 41892\n",
      "Train: [epoch:798]  [ 20/985]  eta: 0:18:10  lr: 0.000001  loss: 0.0186 (0.0189)  time: 0.9412  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [ 30/985]  eta: 0:17:03  lr: 0.000001  loss: 0.0179 (0.0187)  time: 0.9433  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [ 40/985]  eta: 0:16:23  lr: 0.000001  loss: 0.0176 (0.0183)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [ 50/985]  eta: 0:15:54  lr: 0.000001  loss: 0.0180 (0.0185)  time: 0.9426  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [ 60/985]  eta: 0:15:32  lr: 0.000001  loss: 0.0182 (0.0185)  time: 0.9420  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [ 70/985]  eta: 0:15:14  lr: 0.000001  loss: 0.0176 (0.0183)  time: 0.9444  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [ 80/985]  eta: 0:14:59  lr: 0.000001  loss: 0.0170 (0.0181)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [ 90/985]  eta: 0:14:44  lr: 0.000001  loss: 0.0170 (0.0180)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [100/985]  eta: 0:14:31  lr: 0.000001  loss: 0.0175 (0.0182)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [110/985]  eta: 0:14:18  lr: 0.000001  loss: 0.0187 (0.0183)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [120/985]  eta: 0:14:06  lr: 0.000001  loss: 0.0180 (0.0182)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [130/985]  eta: 0:13:55  lr: 0.000001  loss: 0.0178 (0.0184)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [140/985]  eta: 0:13:44  lr: 0.000001  loss: 0.0196 (0.0185)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [150/985]  eta: 0:13:33  lr: 0.000001  loss: 0.0198 (0.0185)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [160/985]  eta: 0:13:22  lr: 0.000001  loss: 0.0184 (0.0187)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [170/985]  eta: 0:13:12  lr: 0.000001  loss: 0.0181 (0.0186)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [180/985]  eta: 0:13:01  lr: 0.000001  loss: 0.0169 (0.0186)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [190/985]  eta: 0:12:51  lr: 0.000001  loss: 0.0176 (0.0187)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [200/985]  eta: 0:12:41  lr: 0.000001  loss: 0.0185 (0.0187)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [210/985]  eta: 0:12:30  lr: 0.000001  loss: 0.0186 (0.0188)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [220/985]  eta: 0:12:20  lr: 0.000001  loss: 0.0169 (0.0187)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [230/985]  eta: 0:12:10  lr: 0.000001  loss: 0.0172 (0.0188)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [240/985]  eta: 0:12:00  lr: 0.000001  loss: 0.0183 (0.0188)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [250/985]  eta: 0:11:50  lr: 0.000001  loss: 0.0179 (0.0187)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [260/985]  eta: 0:11:40  lr: 0.000001  loss: 0.0173 (0.0187)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [270/985]  eta: 0:11:30  lr: 0.000001  loss: 0.0177 (0.0187)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [280/985]  eta: 0:11:20  lr: 0.000001  loss: 0.0182 (0.0188)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [290/985]  eta: 0:11:10  lr: 0.000001  loss: 0.0175 (0.0188)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [300/985]  eta: 0:11:00  lr: 0.000001  loss: 0.0172 (0.0188)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [310/985]  eta: 0:10:50  lr: 0.000001  loss: 0.0175 (0.0188)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [320/985]  eta: 0:10:40  lr: 0.000001  loss: 0.0180 (0.0188)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [330/985]  eta: 0:10:31  lr: 0.000001  loss: 0.0182 (0.0188)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [340/985]  eta: 0:10:21  lr: 0.000001  loss: 0.0187 (0.0188)  time: 0.9548  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:798]  [350/985]  eta: 0:10:11  lr: 0.000001  loss: 0.0189 (0.0188)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [360/985]  eta: 0:10:01  lr: 0.000001  loss: 0.0187 (0.0188)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [370/985]  eta: 0:09:52  lr: 0.000001  loss: 0.0187 (0.0188)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [380/985]  eta: 0:09:42  lr: 0.000001  loss: 0.0181 (0.0188)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [390/985]  eta: 0:09:32  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [400/985]  eta: 0:09:22  lr: 0.000001  loss: 0.0191 (0.0190)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [410/985]  eta: 0:09:12  lr: 0.000001  loss: 0.0182 (0.0189)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [420/985]  eta: 0:09:03  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [430/985]  eta: 0:08:53  lr: 0.000001  loss: 0.0182 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [440/985]  eta: 0:08:43  lr: 0.000001  loss: 0.0186 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [450/985]  eta: 0:08:33  lr: 0.000001  loss: 0.0193 (0.0190)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [460/985]  eta: 0:08:24  lr: 0.000001  loss: 0.0182 (0.0190)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [470/985]  eta: 0:08:14  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [480/985]  eta: 0:08:04  lr: 0.000001  loss: 0.0180 (0.0190)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [490/985]  eta: 0:07:55  lr: 0.000001  loss: 0.0193 (0.0190)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [500/985]  eta: 0:07:45  lr: 0.000001  loss: 0.0200 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [510/985]  eta: 0:07:35  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9546  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:798]  [520/985]  eta: 0:07:26  lr: 0.000001  loss: 0.0187 (0.0191)  time: 0.9587  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:798]  [530/985]  eta: 0:07:16  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [540/985]  eta: 0:07:06  lr: 0.000001  loss: 0.0168 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [550/985]  eta: 0:06:57  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [560/985]  eta: 0:06:47  lr: 0.000001  loss: 0.0174 (0.0190)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [570/985]  eta: 0:06:38  lr: 0.000001  loss: 0.0169 (0.0190)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [580/985]  eta: 0:06:28  lr: 0.000001  loss: 0.0176 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [590/985]  eta: 0:06:18  lr: 0.000001  loss: 0.0181 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [600/985]  eta: 0:06:09  lr: 0.000001  loss: 0.0174 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [610/985]  eta: 0:05:59  lr: 0.000001  loss: 0.0175 (0.0190)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [620/985]  eta: 0:05:49  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:798]  [630/985]  eta: 0:05:40  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [640/985]  eta: 0:05:30  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [650/985]  eta: 0:05:21  lr: 0.000001  loss: 0.0187 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [660/985]  eta: 0:05:11  lr: 0.000001  loss: 0.0174 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [670/985]  eta: 0:05:01  lr: 0.000001  loss: 0.0166 (0.0190)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [680/985]  eta: 0:04:52  lr: 0.000001  loss: 0.0170 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [690/985]  eta: 0:04:42  lr: 0.000001  loss: 0.0181 (0.0190)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [700/985]  eta: 0:04:33  lr: 0.000001  loss: 0.0183 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [710/985]  eta: 0:04:23  lr: 0.000001  loss: 0.0182 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [720/985]  eta: 0:04:13  lr: 0.000001  loss: 0.0181 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [730/985]  eta: 0:04:04  lr: 0.000001  loss: 0.0173 (0.0190)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [740/985]  eta: 0:03:54  lr: 0.000001  loss: 0.0169 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [750/985]  eta: 0:03:45  lr: 0.000001  loss: 0.0187 (0.0190)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [760/985]  eta: 0:03:35  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [770/985]  eta: 0:03:25  lr: 0.000001  loss: 0.0195 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [780/985]  eta: 0:03:16  lr: 0.000001  loss: 0.0189 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [790/985]  eta: 0:03:06  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [800/985]  eta: 0:02:57  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [810/985]  eta: 0:02:47  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [820/985]  eta: 0:02:38  lr: 0.000001  loss: 0.0194 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [830/985]  eta: 0:02:28  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [840/985]  eta: 0:02:18  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [850/985]  eta: 0:02:09  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [860/985]  eta: 0:01:59  lr: 0.000001  loss: 0.0166 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [870/985]  eta: 0:01:50  lr: 0.000001  loss: 0.0166 (0.0190)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [880/985]  eta: 0:01:40  lr: 0.000001  loss: 0.0181 (0.0190)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [890/985]  eta: 0:01:30  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [900/985]  eta: 0:01:21  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [910/985]  eta: 0:01:11  lr: 0.000001  loss: 0.0199 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [920/985]  eta: 0:01:02  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [930/985]  eta: 0:00:52  lr: 0.000001  loss: 0.0174 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [940/985]  eta: 0:00:43  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [950/985]  eta: 0:00:33  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [960/985]  eta: 0:00:23  lr: 0.000001  loss: 0.0193 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [970/985]  eta: 0:00:14  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [980/985]  eta: 0:00:04  lr: 0.000001  loss: 0.0194 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798]  [984/985]  eta: 0:00:00  lr: 0.000001  loss: 0.0201 (0.0191)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:798] Total time: 0:15:42 (0.9570 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.0201 (0.0191)\n",
      "Valid: [epoch:798]  [ 0/14]  eta: 0:02:57  loss: 0.0146 (0.0146)  time: 12.6668  data: 0.5651  max mem: 41892\n",
      "Valid: [epoch:798]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0146)  time: 12.0499  data: 0.0405  max mem: 41892\n",
      "Valid: [epoch:798] Total time: 0:02:48 (12.0562 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_798_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:799]  [  0/985]  eta: 1:08:27  lr: 0.000001  loss: 0.0246 (0.0246)  time: 4.1705  data: 3.2092  max mem: 41892\n",
      "Train: [epoch:799]  [ 10/985]  eta: 0:20:08  lr: 0.000001  loss: 0.0186 (0.0200)  time: 1.2391  data: 0.2919  max mem: 41892\n",
      "Train: [epoch:799]  [ 20/985]  eta: 0:17:38  lr: 0.000001  loss: 0.0183 (0.0198)  time: 0.9435  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [ 30/985]  eta: 0:16:41  lr: 0.000001  loss: 0.0178 (0.0194)  time: 0.9436  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [ 40/985]  eta: 0:16:06  lr: 0.000001  loss: 0.0168 (0.0186)  time: 0.9443  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [ 50/985]  eta: 0:15:42  lr: 0.000001  loss: 0.0168 (0.0185)  time: 0.9442  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [ 60/985]  eta: 0:15:21  lr: 0.000001  loss: 0.0175 (0.0185)  time: 0.9432  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [ 70/985]  eta: 0:15:05  lr: 0.000001  loss: 0.0180 (0.0185)  time: 0.9436  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [ 80/985]  eta: 0:14:50  lr: 0.000001  loss: 0.0173 (0.0182)  time: 0.9452  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [ 90/985]  eta: 0:14:37  lr: 0.000001  loss: 0.0177 (0.0184)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [100/985]  eta: 0:14:24  lr: 0.000001  loss: 0.0181 (0.0183)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [110/985]  eta: 0:14:13  lr: 0.000001  loss: 0.0167 (0.0183)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [120/985]  eta: 0:14:02  lr: 0.000001  loss: 0.0179 (0.0183)  time: 0.9559  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:799]  [130/985]  eta: 0:13:50  lr: 0.000001  loss: 0.0186 (0.0184)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [140/985]  eta: 0:13:39  lr: 0.000001  loss: 0.0182 (0.0184)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [150/985]  eta: 0:13:29  lr: 0.000001  loss: 0.0178 (0.0184)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [160/985]  eta: 0:13:18  lr: 0.000001  loss: 0.0179 (0.0184)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [170/985]  eta: 0:13:08  lr: 0.000001  loss: 0.0180 (0.0184)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [180/985]  eta: 0:12:58  lr: 0.000001  loss: 0.0180 (0.0184)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [190/985]  eta: 0:12:47  lr: 0.000001  loss: 0.0186 (0.0185)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [200/985]  eta: 0:12:37  lr: 0.000001  loss: 0.0190 (0.0185)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [210/985]  eta: 0:12:27  lr: 0.000001  loss: 0.0191 (0.0186)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [220/985]  eta: 0:12:17  lr: 0.000001  loss: 0.0174 (0.0186)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [230/985]  eta: 0:12:08  lr: 0.000001  loss: 0.0194 (0.0187)  time: 0.9609  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:799]  [240/985]  eta: 0:11:58  lr: 0.000001  loss: 0.0189 (0.0187)  time: 0.9593  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:799]  [250/985]  eta: 0:11:48  lr: 0.000001  loss: 0.0180 (0.0187)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [260/985]  eta: 0:11:38  lr: 0.000001  loss: 0.0178 (0.0187)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [270/985]  eta: 0:11:28  lr: 0.000001  loss: 0.0173 (0.0187)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [280/985]  eta: 0:11:18  lr: 0.000001  loss: 0.0173 (0.0187)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [290/985]  eta: 0:11:08  lr: 0.000001  loss: 0.0171 (0.0186)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [300/985]  eta: 0:10:59  lr: 0.000001  loss: 0.0173 (0.0186)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [310/985]  eta: 0:10:49  lr: 0.000001  loss: 0.0177 (0.0187)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [320/985]  eta: 0:10:39  lr: 0.000001  loss: 0.0177 (0.0187)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [330/985]  eta: 0:10:29  lr: 0.000001  loss: 0.0181 (0.0187)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [340/985]  eta: 0:10:19  lr: 0.000001  loss: 0.0182 (0.0187)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [350/985]  eta: 0:10:10  lr: 0.000001  loss: 0.0180 (0.0187)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [360/985]  eta: 0:10:00  lr: 0.000001  loss: 0.0182 (0.0187)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [370/985]  eta: 0:09:50  lr: 0.000001  loss: 0.0185 (0.0188)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [380/985]  eta: 0:09:40  lr: 0.000001  loss: 0.0191 (0.0188)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [390/985]  eta: 0:09:31  lr: 0.000001  loss: 0.0194 (0.0189)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [400/985]  eta: 0:09:21  lr: 0.000001  loss: 0.0189 (0.0189)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [410/985]  eta: 0:09:11  lr: 0.000001  loss: 0.0182 (0.0189)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [420/985]  eta: 0:09:01  lr: 0.000001  loss: 0.0184 (0.0189)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [430/985]  eta: 0:08:52  lr: 0.000001  loss: 0.0179 (0.0189)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [440/985]  eta: 0:08:42  lr: 0.000001  loss: 0.0178 (0.0189)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [450/985]  eta: 0:08:32  lr: 0.000001  loss: 0.0171 (0.0189)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [460/985]  eta: 0:08:23  lr: 0.000001  loss: 0.0195 (0.0190)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [470/985]  eta: 0:08:13  lr: 0.000001  loss: 0.0208 (0.0190)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [480/985]  eta: 0:08:03  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [490/985]  eta: 0:07:54  lr: 0.000001  loss: 0.0187 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [500/985]  eta: 0:07:44  lr: 0.000001  loss: 0.0214 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [510/985]  eta: 0:07:35  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [520/985]  eta: 0:07:25  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [530/985]  eta: 0:07:15  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [540/985]  eta: 0:07:06  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [550/985]  eta: 0:06:56  lr: 0.000001  loss: 0.0174 (0.0191)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [560/985]  eta: 0:06:46  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [570/985]  eta: 0:06:37  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [580/985]  eta: 0:06:27  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [590/985]  eta: 0:06:18  lr: 0.000001  loss: 0.0192 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [600/985]  eta: 0:06:08  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [610/985]  eta: 0:05:58  lr: 0.000001  loss: 0.0158 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [620/985]  eta: 0:05:49  lr: 0.000001  loss: 0.0180 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [630/985]  eta: 0:05:39  lr: 0.000001  loss: 0.0180 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [640/985]  eta: 0:05:30  lr: 0.000001  loss: 0.0173 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [650/985]  eta: 0:05:20  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [660/985]  eta: 0:05:10  lr: 0.000001  loss: 0.0195 (0.0192)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [670/985]  eta: 0:05:01  lr: 0.000001  loss: 0.0198 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [680/985]  eta: 0:04:51  lr: 0.000001  loss: 0.0171 (0.0192)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [690/985]  eta: 0:04:42  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [700/985]  eta: 0:04:32  lr: 0.000001  loss: 0.0177 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [710/985]  eta: 0:04:23  lr: 0.000001  loss: 0.0188 (0.0192)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [720/985]  eta: 0:04:13  lr: 0.000001  loss: 0.0188 (0.0192)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [730/985]  eta: 0:04:03  lr: 0.000001  loss: 0.0176 (0.0192)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [740/985]  eta: 0:03:54  lr: 0.000001  loss: 0.0169 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [750/985]  eta: 0:03:44  lr: 0.000001  loss: 0.0186 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [760/985]  eta: 0:03:35  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [770/985]  eta: 0:03:25  lr: 0.000001  loss: 0.0174 (0.0191)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [780/985]  eta: 0:03:16  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [790/985]  eta: 0:03:06  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [800/985]  eta: 0:02:56  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [810/985]  eta: 0:02:47  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [820/985]  eta: 0:02:37  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [830/985]  eta: 0:02:28  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [840/985]  eta: 0:02:18  lr: 0.000001  loss: 0.0174 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [850/985]  eta: 0:02:09  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [860/985]  eta: 0:01:59  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [870/985]  eta: 0:01:50  lr: 0.000001  loss: 0.0177 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [880/985]  eta: 0:01:40  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [890/985]  eta: 0:01:30  lr: 0.000001  loss: 0.0171 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:799]  [900/985]  eta: 0:01:21  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [910/985]  eta: 0:01:11  lr: 0.000001  loss: 0.0193 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [920/985]  eta: 0:01:02  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [930/985]  eta: 0:00:52  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [940/985]  eta: 0:00:43  lr: 0.000001  loss: 0.0191 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [950/985]  eta: 0:00:33  lr: 0.000001  loss: 0.0194 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [960/985]  eta: 0:00:23  lr: 0.000001  loss: 0.0193 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [970/985]  eta: 0:00:14  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [980/985]  eta: 0:00:04  lr: 0.000001  loss: 0.0177 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799]  [984/985]  eta: 0:00:00  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:799] Total time: 0:15:42 (0.9564 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.0178 (0.0191)\n",
      "Valid: [epoch:799]  [ 0/14]  eta: 0:02:59  loss: 0.0135 (0.0135)  time: 12.7942  data: 0.5239  max mem: 41892\n",
      "Valid: [epoch:799]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0146)  time: 12.1632  data: 0.0375  max mem: 41892\n",
      "Valid: [epoch:799] Total time: 0:02:50 (12.1698 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_799_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:800]  [  0/985]  eta: 1:11:06  lr: 0.000001  loss: 0.0157 (0.0157)  time: 4.3312  data: 3.3326  max mem: 41892\n",
      "Train: [epoch:800]  [ 10/985]  eta: 0:20:23  lr: 0.000001  loss: 0.0201 (0.0212)  time: 1.2545  data: 0.3031  max mem: 41892\n",
      "Train: [epoch:800]  [ 20/985]  eta: 0:17:43  lr: 0.000001  loss: 0.0190 (0.0198)  time: 0.9404  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [ 30/985]  eta: 0:16:41  lr: 0.000001  loss: 0.0180 (0.0198)  time: 0.9350  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [ 40/985]  eta: 0:16:05  lr: 0.000001  loss: 0.0180 (0.0194)  time: 0.9371  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [ 50/985]  eta: 0:15:42  lr: 0.000001  loss: 0.0173 (0.0189)  time: 0.9444  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [ 60/985]  eta: 0:15:22  lr: 0.000001  loss: 0.0178 (0.0190)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [ 70/985]  eta: 0:15:06  lr: 0.000001  loss: 0.0178 (0.0188)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [ 80/985]  eta: 0:14:51  lr: 0.000001  loss: 0.0165 (0.0184)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [ 90/985]  eta: 0:14:38  lr: 0.000001  loss: 0.0171 (0.0186)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [100/985]  eta: 0:14:26  lr: 0.000001  loss: 0.0180 (0.0185)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [110/985]  eta: 0:14:14  lr: 0.000001  loss: 0.0180 (0.0185)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [120/985]  eta: 0:14:02  lr: 0.000001  loss: 0.0180 (0.0185)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [130/985]  eta: 0:13:51  lr: 0.000001  loss: 0.0185 (0.0186)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [140/985]  eta: 0:13:40  lr: 0.000001  loss: 0.0184 (0.0186)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [150/985]  eta: 0:13:29  lr: 0.000001  loss: 0.0184 (0.0187)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [160/985]  eta: 0:13:19  lr: 0.000001  loss: 0.0181 (0.0187)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [170/985]  eta: 0:13:08  lr: 0.000001  loss: 0.0179 (0.0186)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [180/985]  eta: 0:12:58  lr: 0.000001  loss: 0.0176 (0.0186)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [190/985]  eta: 0:12:47  lr: 0.000001  loss: 0.0181 (0.0187)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [200/985]  eta: 0:12:37  lr: 0.000001  loss: 0.0181 (0.0187)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [210/985]  eta: 0:12:28  lr: 0.000001  loss: 0.0180 (0.0187)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [220/985]  eta: 0:12:18  lr: 0.000001  loss: 0.0185 (0.0187)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [230/985]  eta: 0:12:08  lr: 0.000001  loss: 0.0167 (0.0187)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [240/985]  eta: 0:11:58  lr: 0.000001  loss: 0.0185 (0.0187)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [250/985]  eta: 0:11:48  lr: 0.000001  loss: 0.0185 (0.0187)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [260/985]  eta: 0:11:38  lr: 0.000001  loss: 0.0171 (0.0187)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [270/985]  eta: 0:11:28  lr: 0.000001  loss: 0.0173 (0.0187)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [280/985]  eta: 0:11:18  lr: 0.000001  loss: 0.0182 (0.0187)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [290/985]  eta: 0:11:08  lr: 0.000001  loss: 0.0171 (0.0187)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [300/985]  eta: 0:10:59  lr: 0.000001  loss: 0.0179 (0.0188)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [310/985]  eta: 0:10:49  lr: 0.000001  loss: 0.0177 (0.0187)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [320/985]  eta: 0:10:39  lr: 0.000001  loss: 0.0164 (0.0187)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [330/985]  eta: 0:10:29  lr: 0.000001  loss: 0.0176 (0.0187)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [340/985]  eta: 0:10:20  lr: 0.000001  loss: 0.0182 (0.0187)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [350/985]  eta: 0:10:10  lr: 0.000001  loss: 0.0176 (0.0187)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [360/985]  eta: 0:10:00  lr: 0.000001  loss: 0.0180 (0.0188)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [370/985]  eta: 0:09:51  lr: 0.000001  loss: 0.0177 (0.0187)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [380/985]  eta: 0:09:41  lr: 0.000001  loss: 0.0188 (0.0188)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [390/985]  eta: 0:09:31  lr: 0.000001  loss: 0.0190 (0.0188)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [400/985]  eta: 0:09:22  lr: 0.000001  loss: 0.0199 (0.0189)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [410/985]  eta: 0:09:12  lr: 0.000001  loss: 0.0179 (0.0189)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [420/985]  eta: 0:09:02  lr: 0.000001  loss: 0.0174 (0.0189)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [430/985]  eta: 0:08:52  lr: 0.000001  loss: 0.0179 (0.0189)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [440/985]  eta: 0:08:43  lr: 0.000001  loss: 0.0173 (0.0189)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [450/985]  eta: 0:08:33  lr: 0.000001  loss: 0.0178 (0.0189)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [460/985]  eta: 0:08:23  lr: 0.000001  loss: 0.0188 (0.0189)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [470/985]  eta: 0:08:14  lr: 0.000001  loss: 0.0191 (0.0190)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [480/985]  eta: 0:08:04  lr: 0.000001  loss: 0.0190 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [490/985]  eta: 0:07:54  lr: 0.000001  loss: 0.0190 (0.0190)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [500/985]  eta: 0:07:45  lr: 0.000001  loss: 0.0191 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:800]  [510/985]  eta: 0:07:35  lr: 0.000001  loss: 0.0196 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [520/985]  eta: 0:07:26  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [530/985]  eta: 0:07:16  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [540/985]  eta: 0:07:06  lr: 0.000001  loss: 0.0169 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [550/985]  eta: 0:06:57  lr: 0.000001  loss: 0.0169 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [560/985]  eta: 0:06:47  lr: 0.000001  loss: 0.0174 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [570/985]  eta: 0:06:37  lr: 0.000001  loss: 0.0178 (0.0190)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [580/985]  eta: 0:06:28  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [590/985]  eta: 0:06:18  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [600/985]  eta: 0:06:09  lr: 0.000001  loss: 0.0192 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [610/985]  eta: 0:05:59  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [620/985]  eta: 0:05:49  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [630/985]  eta: 0:05:40  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [640/985]  eta: 0:05:30  lr: 0.000001  loss: 0.0172 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [650/985]  eta: 0:05:21  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [660/985]  eta: 0:05:11  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [670/985]  eta: 0:05:01  lr: 0.000001  loss: 0.0174 (0.0190)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [680/985]  eta: 0:04:52  lr: 0.000001  loss: 0.0166 (0.0190)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [690/985]  eta: 0:04:42  lr: 0.000001  loss: 0.0177 (0.0190)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [700/985]  eta: 0:04:32  lr: 0.000001  loss: 0.0193 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [710/985]  eta: 0:04:23  lr: 0.000001  loss: 0.0203 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [720/985]  eta: 0:04:13  lr: 0.000001  loss: 0.0203 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [730/985]  eta: 0:04:04  lr: 0.000001  loss: 0.0196 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [740/985]  eta: 0:03:54  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [750/985]  eta: 0:03:45  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [760/985]  eta: 0:03:35  lr: 0.000001  loss: 0.0196 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [770/985]  eta: 0:03:25  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [780/985]  eta: 0:03:16  lr: 0.000001  loss: 0.0177 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [790/985]  eta: 0:03:06  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [800/985]  eta: 0:02:57  lr: 0.000001  loss: 0.0202 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [810/985]  eta: 0:02:47  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [820/985]  eta: 0:02:37  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [830/985]  eta: 0:02:28  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [840/985]  eta: 0:02:18  lr: 0.000001  loss: 0.0173 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [850/985]  eta: 0:02:09  lr: 0.000001  loss: 0.0167 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [860/985]  eta: 0:01:59  lr: 0.000001  loss: 0.0172 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [870/985]  eta: 0:01:50  lr: 0.000001  loss: 0.0161 (0.0190)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [880/985]  eta: 0:01:40  lr: 0.000001  loss: 0.0167 (0.0190)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [890/985]  eta: 0:01:30  lr: 0.000001  loss: 0.0177 (0.0190)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [900/985]  eta: 0:01:21  lr: 0.000001  loss: 0.0192 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [910/985]  eta: 0:01:11  lr: 0.000001  loss: 0.0225 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [920/985]  eta: 0:01:02  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [930/985]  eta: 0:00:52  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [940/985]  eta: 0:00:43  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [950/985]  eta: 0:00:33  lr: 0.000001  loss: 0.0188 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [960/985]  eta: 0:00:23  lr: 0.000001  loss: 0.0185 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [970/985]  eta: 0:00:14  lr: 0.000001  loss: 0.0176 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [980/985]  eta: 0:00:04  lr: 0.000001  loss: 0.0183 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800]  [984/985]  eta: 0:00:00  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:800] Total time: 0:15:42 (0.9568 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.0179 (0.0191)\n",
      "Valid: [epoch:800]  [ 0/14]  eta: 0:02:57  loss: 0.0172 (0.0172)  time: 12.6800  data: 0.5373  max mem: 41892\n",
      "Valid: [epoch:800]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0146)  time: 11.7784  data: 0.0385  max mem: 41892\n",
      "Valid: [epoch:800] Total time: 0:02:44 (11.7843 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_800_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:801]  [  0/985]  eta: 1:09:03  lr: 0.000001  loss: 0.0184 (0.0184)  time: 4.2071  data: 3.2298  max mem: 41892\n",
      "Train: [epoch:801]  [ 10/985]  eta: 0:20:03  lr: 0.000001  loss: 0.0184 (0.0194)  time: 1.2341  data: 0.2937  max mem: 41892\n",
      "Train: [epoch:801]  [ 20/985]  eta: 0:17:32  lr: 0.000001  loss: 0.0186 (0.0195)  time: 0.9348  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [ 30/985]  eta: 0:16:34  lr: 0.000001  loss: 0.0188 (0.0195)  time: 0.9352  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [ 40/985]  eta: 0:16:00  lr: 0.000001  loss: 0.0179 (0.0189)  time: 0.9376  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [ 50/985]  eta: 0:15:37  lr: 0.000001  loss: 0.0154 (0.0185)  time: 0.9442  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [ 60/985]  eta: 0:15:18  lr: 0.000001  loss: 0.0189 (0.0190)  time: 0.9456  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [ 70/985]  eta: 0:15:02  lr: 0.000001  loss: 0.0177 (0.0187)  time: 0.9426  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [ 80/985]  eta: 0:14:49  lr: 0.000001  loss: 0.0154 (0.0185)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [ 90/985]  eta: 0:14:36  lr: 0.000001  loss: 0.0166 (0.0187)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [100/985]  eta: 0:14:23  lr: 0.000001  loss: 0.0173 (0.0185)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [110/985]  eta: 0:14:11  lr: 0.000001  loss: 0.0172 (0.0184)  time: 0.9487  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:801]  [120/985]  eta: 0:14:01  lr: 0.000001  loss: 0.0198 (0.0187)  time: 0.9540  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [130/985]  eta: 0:13:50  lr: 0.000001  loss: 0.0210 (0.0188)  time: 0.9561  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [140/985]  eta: 0:13:39  lr: 0.000001  loss: 0.0195 (0.0189)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [150/985]  eta: 0:13:28  lr: 0.000001  loss: 0.0175 (0.0188)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [160/985]  eta: 0:13:18  lr: 0.000001  loss: 0.0173 (0.0188)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [170/985]  eta: 0:13:07  lr: 0.000001  loss: 0.0181 (0.0188)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [180/985]  eta: 0:12:57  lr: 0.000001  loss: 0.0186 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [190/985]  eta: 0:12:47  lr: 0.000001  loss: 0.0198 (0.0190)  time: 0.9530  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [200/985]  eta: 0:12:37  lr: 0.000001  loss: 0.0198 (0.0190)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [210/985]  eta: 0:12:27  lr: 0.000001  loss: 0.0171 (0.0189)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [220/985]  eta: 0:12:17  lr: 0.000001  loss: 0.0165 (0.0188)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [230/985]  eta: 0:12:07  lr: 0.000001  loss: 0.0168 (0.0189)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [240/985]  eta: 0:11:57  lr: 0.000001  loss: 0.0176 (0.0188)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [250/985]  eta: 0:11:47  lr: 0.000001  loss: 0.0173 (0.0187)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [260/985]  eta: 0:11:37  lr: 0.000001  loss: 0.0167 (0.0187)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [270/985]  eta: 0:11:27  lr: 0.000001  loss: 0.0170 (0.0187)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [280/985]  eta: 0:11:18  lr: 0.000001  loss: 0.0178 (0.0187)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [290/985]  eta: 0:11:08  lr: 0.000001  loss: 0.0185 (0.0187)  time: 0.9547  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [300/985]  eta: 0:10:58  lr: 0.000001  loss: 0.0187 (0.0188)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [310/985]  eta: 0:10:48  lr: 0.000001  loss: 0.0184 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [320/985]  eta: 0:10:38  lr: 0.000001  loss: 0.0174 (0.0187)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [330/985]  eta: 0:10:29  lr: 0.000001  loss: 0.0188 (0.0187)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [340/985]  eta: 0:10:19  lr: 0.000001  loss: 0.0188 (0.0187)  time: 0.9600  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [350/985]  eta: 0:10:10  lr: 0.000001  loss: 0.0178 (0.0187)  time: 0.9603  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [360/985]  eta: 0:10:00  lr: 0.000001  loss: 0.0182 (0.0187)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [370/985]  eta: 0:09:50  lr: 0.000001  loss: 0.0180 (0.0187)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [380/985]  eta: 0:09:40  lr: 0.000001  loss: 0.0177 (0.0187)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [390/985]  eta: 0:09:31  lr: 0.000001  loss: 0.0178 (0.0187)  time: 0.9604  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [400/985]  eta: 0:09:21  lr: 0.000001  loss: 0.0186 (0.0187)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [410/985]  eta: 0:09:11  lr: 0.000001  loss: 0.0192 (0.0188)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [420/985]  eta: 0:09:02  lr: 0.000001  loss: 0.0183 (0.0187)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [430/985]  eta: 0:08:52  lr: 0.000001  loss: 0.0178 (0.0188)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [440/985]  eta: 0:08:43  lr: 0.000001  loss: 0.0207 (0.0188)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [450/985]  eta: 0:08:33  lr: 0.000001  loss: 0.0192 (0.0188)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [460/985]  eta: 0:08:23  lr: 0.000001  loss: 0.0190 (0.0189)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [470/985]  eta: 0:08:14  lr: 0.000001  loss: 0.0178 (0.0189)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [480/985]  eta: 0:08:04  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [490/985]  eta: 0:07:54  lr: 0.000001  loss: 0.0180 (0.0189)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [500/985]  eta: 0:07:45  lr: 0.000001  loss: 0.0183 (0.0189)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [510/985]  eta: 0:07:35  lr: 0.000001  loss: 0.0189 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [520/985]  eta: 0:07:25  lr: 0.000001  loss: 0.0203 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [530/985]  eta: 0:07:16  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [540/985]  eta: 0:07:06  lr: 0.000001  loss: 0.0172 (0.0189)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [550/985]  eta: 0:06:57  lr: 0.000001  loss: 0.0176 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [560/985]  eta: 0:06:47  lr: 0.000001  loss: 0.0185 (0.0190)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [570/985]  eta: 0:06:37  lr: 0.000001  loss: 0.0172 (0.0189)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [580/985]  eta: 0:06:28  lr: 0.000001  loss: 0.0185 (0.0189)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [590/985]  eta: 0:06:18  lr: 0.000001  loss: 0.0183 (0.0189)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [600/985]  eta: 0:06:08  lr: 0.000001  loss: 0.0182 (0.0189)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [610/985]  eta: 0:05:59  lr: 0.000001  loss: 0.0182 (0.0190)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [620/985]  eta: 0:05:49  lr: 0.000001  loss: 0.0193 (0.0190)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [630/985]  eta: 0:05:40  lr: 0.000001  loss: 0.0189 (0.0190)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [640/985]  eta: 0:05:30  lr: 0.000001  loss: 0.0179 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [650/985]  eta: 0:05:20  lr: 0.000001  loss: 0.0181 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [660/985]  eta: 0:05:11  lr: 0.000001  loss: 0.0181 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [670/985]  eta: 0:05:01  lr: 0.000001  loss: 0.0176 (0.0190)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [680/985]  eta: 0:04:52  lr: 0.000001  loss: 0.0184 (0.0190)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [690/985]  eta: 0:04:42  lr: 0.000001  loss: 0.0186 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [700/985]  eta: 0:04:32  lr: 0.000001  loss: 0.0187 (0.0190)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [710/985]  eta: 0:04:23  lr: 0.000001  loss: 0.0191 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [720/985]  eta: 0:04:13  lr: 0.000001  loss: 0.0189 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [730/985]  eta: 0:04:04  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [740/985]  eta: 0:03:54  lr: 0.000001  loss: 0.0181 (0.0191)  time: 0.9643  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [750/985]  eta: 0:03:45  lr: 0.000001  loss: 0.0195 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [760/985]  eta: 0:03:35  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [770/985]  eta: 0:03:25  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:801]  [780/985]  eta: 0:03:16  lr: 0.000001  loss: 0.0182 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [790/985]  eta: 0:03:06  lr: 0.000001  loss: 0.0165 (0.0191)  time: 0.9627  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [800/985]  eta: 0:02:57  lr: 0.000001  loss: 0.0170 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [810/985]  eta: 0:02:47  lr: 0.000001  loss: 0.0173 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [820/985]  eta: 0:02:38  lr: 0.000001  loss: 0.0195 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [830/985]  eta: 0:02:28  lr: 0.000001  loss: 0.0190 (0.0191)  time: 0.9550  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [840/985]  eta: 0:02:18  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [850/985]  eta: 0:02:09  lr: 0.000001  loss: 0.0179 (0.0191)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [860/985]  eta: 0:01:59  lr: 0.000001  loss: 0.0178 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [870/985]  eta: 0:01:50  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [880/985]  eta: 0:01:40  lr: 0.000001  loss: 0.0175 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [890/985]  eta: 0:01:30  lr: 0.000001  loss: 0.0184 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [900/985]  eta: 0:01:21  lr: 0.000001  loss: 0.0186 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [910/985]  eta: 0:01:11  lr: 0.000001  loss: 0.0192 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [920/985]  eta: 0:01:02  lr: 0.000001  loss: 0.0192 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [930/985]  eta: 0:00:52  lr: 0.000001  loss: 0.0184 (0.0192)  time: 0.9522  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:801]  [940/985]  eta: 0:00:43  lr: 0.000001  loss: 0.0182 (0.0192)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [950/985]  eta: 0:00:33  lr: 0.000001  loss: 0.0182 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [960/985]  eta: 0:00:23  lr: 0.000001  loss: 0.0181 (0.0192)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [970/985]  eta: 0:00:14  lr: 0.000001  loss: 0.0180 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [980/985]  eta: 0:00:04  lr: 0.000001  loss: 0.0182 (0.0192)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801]  [984/985]  eta: 0:00:00  lr: 0.000001  loss: 0.0197 (0.0192)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:801] Total time: 0:15:42 (0.9572 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.0197 (0.0192)\n",
      "Valid: [epoch:801]  [ 0/14]  eta: 0:02:50  loss: 0.0135 (0.0135)  time: 12.1865  data: 0.4866  max mem: 41892\n",
      "Valid: [epoch:801]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0146)  time: 11.7116  data: 0.0348  max mem: 41892\n",
      "Valid: [epoch:801] Total time: 0:02:44 (11.7177 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_801_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n",
      "Train: [epoch:802]  [  0/985]  eta: 0:54:30  lr: 0.000003  loss: 0.0148 (0.0148)  time: 3.3203  data: 2.3245  max mem: 41892\n",
      "Train: [epoch:802]  [ 10/985]  eta: 0:18:47  lr: 0.000003  loss: 0.0188 (0.0196)  time: 1.1566  data: 0.2114  max mem: 41892\n",
      "Train: [epoch:802]  [ 20/985]  eta: 0:16:59  lr: 0.000003  loss: 0.0188 (0.0202)  time: 0.9432  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [ 30/985]  eta: 0:16:12  lr: 0.000003  loss: 0.0191 (0.0201)  time: 0.9419  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [ 40/985]  eta: 0:15:43  lr: 0.000003  loss: 0.0178 (0.0193)  time: 0.9377  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [ 50/985]  eta: 0:15:23  lr: 0.000003  loss: 0.0162 (0.0188)  time: 0.9397  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [ 60/985]  eta: 0:15:06  lr: 0.000003  loss: 0.0173 (0.0192)  time: 0.9409  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [ 70/985]  eta: 0:14:51  lr: 0.000003  loss: 0.0183 (0.0189)  time: 0.9417  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [ 80/985]  eta: 0:14:38  lr: 0.000003  loss: 0.0171 (0.0188)  time: 0.9440  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [ 90/985]  eta: 0:14:26  lr: 0.000003  loss: 0.0174 (0.0190)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [100/985]  eta: 0:14:15  lr: 0.000003  loss: 0.0194 (0.0192)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [110/985]  eta: 0:14:04  lr: 0.000003  loss: 0.0177 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [120/985]  eta: 0:13:53  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [130/985]  eta: 0:13:42  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [140/985]  eta: 0:13:32  lr: 0.000003  loss: 0.0190 (0.0195)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [150/985]  eta: 0:13:22  lr: 0.000003  loss: 0.0208 (0.0194)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [160/985]  eta: 0:13:12  lr: 0.000003  loss: 0.0184 (0.0195)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [170/985]  eta: 0:13:02  lr: 0.000003  loss: 0.0165 (0.0194)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [180/985]  eta: 0:12:52  lr: 0.000003  loss: 0.0174 (0.0195)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [190/985]  eta: 0:12:42  lr: 0.000003  loss: 0.0179 (0.0195)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [200/985]  eta: 0:12:32  lr: 0.000003  loss: 0.0168 (0.0193)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [210/985]  eta: 0:12:22  lr: 0.000003  loss: 0.0168 (0.0192)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [220/985]  eta: 0:12:12  lr: 0.000003  loss: 0.0168 (0.0191)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [230/985]  eta: 0:12:03  lr: 0.000003  loss: 0.0168 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [240/985]  eta: 0:11:53  lr: 0.000003  loss: 0.0188 (0.0192)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [250/985]  eta: 0:11:43  lr: 0.000003  loss: 0.0186 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [260/985]  eta: 0:11:34  lr: 0.000003  loss: 0.0179 (0.0192)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [270/985]  eta: 0:11:24  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [280/985]  eta: 0:11:14  lr: 0.000003  loss: 0.0173 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [290/985]  eta: 0:11:05  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [300/985]  eta: 0:10:55  lr: 0.000003  loss: 0.0199 (0.0192)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [310/985]  eta: 0:10:46  lr: 0.000003  loss: 0.0179 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [320/985]  eta: 0:10:36  lr: 0.000003  loss: 0.0179 (0.0192)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [330/985]  eta: 0:10:26  lr: 0.000003  loss: 0.0190 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [340/985]  eta: 0:10:17  lr: 0.000003  loss: 0.0193 (0.0192)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [350/985]  eta: 0:10:07  lr: 0.000003  loss: 0.0181 (0.0192)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [360/985]  eta: 0:09:57  lr: 0.000003  loss: 0.0176 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [370/985]  eta: 0:09:48  lr: 0.000003  loss: 0.0176 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [380/985]  eta: 0:09:38  lr: 0.000003  loss: 0.0174 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:802]  [390/985]  eta: 0:09:29  lr: 0.000003  loss: 0.0176 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [400/985]  eta: 0:09:19  lr: 0.000003  loss: 0.0194 (0.0192)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [410/985]  eta: 0:09:10  lr: 0.000003  loss: 0.0190 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [420/985]  eta: 0:09:00  lr: 0.000003  loss: 0.0181 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [430/985]  eta: 0:08:50  lr: 0.000003  loss: 0.0183 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [440/985]  eta: 0:08:41  lr: 0.000003  loss: 0.0186 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [450/985]  eta: 0:08:31  lr: 0.000003  loss: 0.0188 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [460/985]  eta: 0:08:22  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [470/985]  eta: 0:08:12  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [480/985]  eta: 0:08:03  lr: 0.000003  loss: 0.0174 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [490/985]  eta: 0:07:53  lr: 0.000003  loss: 0.0180 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [500/985]  eta: 0:07:43  lr: 0.000003  loss: 0.0186 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [510/985]  eta: 0:07:34  lr: 0.000003  loss: 0.0195 (0.0192)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [520/985]  eta: 0:07:24  lr: 0.000003  loss: 0.0196 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [530/985]  eta: 0:07:15  lr: 0.000003  loss: 0.0187 (0.0192)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [540/985]  eta: 0:07:05  lr: 0.000003  loss: 0.0181 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [550/985]  eta: 0:06:56  lr: 0.000003  loss: 0.0177 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [560/985]  eta: 0:06:46  lr: 0.000003  loss: 0.0177 (0.0191)  time: 0.9626  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [570/985]  eta: 0:06:36  lr: 0.000003  loss: 0.0178 (0.0191)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [580/985]  eta: 0:06:27  lr: 0.000003  loss: 0.0184 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [590/985]  eta: 0:06:17  lr: 0.000003  loss: 0.0184 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [600/985]  eta: 0:06:08  lr: 0.000003  loss: 0.0179 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [610/985]  eta: 0:05:58  lr: 0.000003  loss: 0.0159 (0.0190)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [620/985]  eta: 0:05:49  lr: 0.000003  loss: 0.0171 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [630/985]  eta: 0:05:39  lr: 0.000003  loss: 0.0176 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [640/985]  eta: 0:05:30  lr: 0.000003  loss: 0.0175 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [650/985]  eta: 0:05:20  lr: 0.000003  loss: 0.0188 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [660/985]  eta: 0:05:10  lr: 0.000003  loss: 0.0186 (0.0191)  time: 0.9572  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:802]  [670/985]  eta: 0:05:01  lr: 0.000003  loss: 0.0174 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [680/985]  eta: 0:04:51  lr: 0.000003  loss: 0.0168 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [690/985]  eta: 0:04:42  lr: 0.000003  loss: 0.0174 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [700/985]  eta: 0:04:32  lr: 0.000003  loss: 0.0171 (0.0190)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [710/985]  eta: 0:04:23  lr: 0.000003  loss: 0.0178 (0.0190)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [720/985]  eta: 0:04:13  lr: 0.000003  loss: 0.0183 (0.0190)  time: 0.9578  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:802]  [730/985]  eta: 0:04:04  lr: 0.000003  loss: 0.0177 (0.0190)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [740/985]  eta: 0:03:54  lr: 0.000003  loss: 0.0177 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [750/985]  eta: 0:03:44  lr: 0.000003  loss: 0.0194 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [760/985]  eta: 0:03:35  lr: 0.000003  loss: 0.0190 (0.0190)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [770/985]  eta: 0:03:25  lr: 0.000003  loss: 0.0185 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [780/985]  eta: 0:03:16  lr: 0.000003  loss: 0.0179 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [790/985]  eta: 0:03:06  lr: 0.000003  loss: 0.0167 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [800/985]  eta: 0:02:57  lr: 0.000003  loss: 0.0175 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [810/985]  eta: 0:02:47  lr: 0.000003  loss: 0.0180 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [820/985]  eta: 0:02:37  lr: 0.000003  loss: 0.0175 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [830/985]  eta: 0:02:28  lr: 0.000003  loss: 0.0167 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [840/985]  eta: 0:02:18  lr: 0.000003  loss: 0.0169 (0.0190)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [850/985]  eta: 0:02:09  lr: 0.000003  loss: 0.0171 (0.0190)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [860/985]  eta: 0:01:59  lr: 0.000003  loss: 0.0169 (0.0189)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [870/985]  eta: 0:01:50  lr: 0.000003  loss: 0.0173 (0.0190)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [880/985]  eta: 0:01:40  lr: 0.000003  loss: 0.0171 (0.0189)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [890/985]  eta: 0:01:30  lr: 0.000003  loss: 0.0178 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [900/985]  eta: 0:01:21  lr: 0.000003  loss: 0.0190 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [910/985]  eta: 0:01:11  lr: 0.000003  loss: 0.0194 (0.0190)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [920/985]  eta: 0:01:02  lr: 0.000003  loss: 0.0183 (0.0190)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [930/985]  eta: 0:00:52  lr: 0.000003  loss: 0.0169 (0.0190)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [940/985]  eta: 0:00:43  lr: 0.000003  loss: 0.0174 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [950/985]  eta: 0:00:33  lr: 0.000003  loss: 0.0193 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [960/985]  eta: 0:00:23  lr: 0.000003  loss: 0.0197 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [970/985]  eta: 0:00:14  lr: 0.000003  loss: 0.0192 (0.0190)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [980/985]  eta: 0:00:04  lr: 0.000003  loss: 0.0187 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802]  [984/985]  eta: 0:00:00  lr: 0.000003  loss: 0.0196 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:802] Total time: 0:15:42 (0.9568 s / it)\n",
      "Averaged stats: lr: 0.000003  loss: 0.0196 (0.0191)\n",
      "Valid: [epoch:802]  [ 0/14]  eta: 0:02:58  loss: 0.0135 (0.0135)  time: 12.7740  data: 0.6055  max mem: 41892\n",
      "Valid: [epoch:802]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0146)  time: 12.2578  data: 0.0434  max mem: 41892\n",
      "Valid: [epoch:802] Total time: 0:02:51 (12.2642 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_802_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 791.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:803]  [  0/985]  eta: 1:22:27  lr: 0.000004  loss: 0.0175 (0.0175)  time: 5.0224  data: 3.9668  max mem: 41892\n",
      "Train: [epoch:803]  [ 10/985]  eta: 0:21:39  lr: 0.000004  loss: 0.0204 (0.0229)  time: 1.3333  data: 0.3608  max mem: 41892\n",
      "Train: [epoch:803]  [ 20/985]  eta: 0:18:25  lr: 0.000004  loss: 0.0204 (0.0216)  time: 0.9523  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:803]  [ 30/985]  eta: 0:17:09  lr: 0.000004  loss: 0.0188 (0.0205)  time: 0.9380  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [ 40/985]  eta: 0:16:29  lr: 0.000004  loss: 0.0173 (0.0197)  time: 0.9422  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [ 50/985]  eta: 0:15:58  lr: 0.000004  loss: 0.0166 (0.0193)  time: 0.9439  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [ 60/985]  eta: 0:15:35  lr: 0.000004  loss: 0.0171 (0.0191)  time: 0.9406  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [ 70/985]  eta: 0:15:17  lr: 0.000004  loss: 0.0175 (0.0188)  time: 0.9427  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [ 80/985]  eta: 0:15:00  lr: 0.000004  loss: 0.0156 (0.0185)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [ 90/985]  eta: 0:14:46  lr: 0.000004  loss: 0.0172 (0.0186)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [100/985]  eta: 0:14:32  lr: 0.000004  loss: 0.0187 (0.0187)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [110/985]  eta: 0:14:20  lr: 0.000004  loss: 0.0172 (0.0185)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [120/985]  eta: 0:14:08  lr: 0.000004  loss: 0.0178 (0.0186)  time: 0.9516  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:803]  [130/985]  eta: 0:13:57  lr: 0.000004  loss: 0.0181 (0.0187)  time: 0.9581  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:803]  [140/985]  eta: 0:13:45  lr: 0.000004  loss: 0.0178 (0.0189)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [150/985]  eta: 0:13:35  lr: 0.000004  loss: 0.0180 (0.0188)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [160/985]  eta: 0:13:24  lr: 0.000004  loss: 0.0188 (0.0189)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [170/985]  eta: 0:13:13  lr: 0.000004  loss: 0.0176 (0.0188)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [180/985]  eta: 0:13:03  lr: 0.000004  loss: 0.0176 (0.0188)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [190/985]  eta: 0:12:52  lr: 0.000004  loss: 0.0182 (0.0189)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [200/985]  eta: 0:12:42  lr: 0.000004  loss: 0.0188 (0.0189)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [210/985]  eta: 0:12:31  lr: 0.000004  loss: 0.0177 (0.0189)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [220/985]  eta: 0:12:21  lr: 0.000004  loss: 0.0175 (0.0188)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [230/985]  eta: 0:12:11  lr: 0.000004  loss: 0.0177 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [240/985]  eta: 0:12:01  lr: 0.000004  loss: 0.0192 (0.0190)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [250/985]  eta: 0:11:50  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [260/985]  eta: 0:11:40  lr: 0.000004  loss: 0.0179 (0.0190)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [270/985]  eta: 0:11:30  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [280/985]  eta: 0:11:20  lr: 0.000004  loss: 0.0189 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [290/985]  eta: 0:11:10  lr: 0.000004  loss: 0.0190 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [300/985]  eta: 0:11:00  lr: 0.000004  loss: 0.0181 (0.0192)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [310/985]  eta: 0:10:51  lr: 0.000004  loss: 0.0186 (0.0192)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [320/985]  eta: 0:10:41  lr: 0.000004  loss: 0.0174 (0.0192)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [330/985]  eta: 0:10:31  lr: 0.000004  loss: 0.0171 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [340/985]  eta: 0:10:21  lr: 0.000004  loss: 0.0186 (0.0192)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [350/985]  eta: 0:10:11  lr: 0.000004  loss: 0.0184 (0.0192)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [360/985]  eta: 0:10:02  lr: 0.000004  loss: 0.0179 (0.0192)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [370/985]  eta: 0:09:52  lr: 0.000004  loss: 0.0187 (0.0192)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [380/985]  eta: 0:09:42  lr: 0.000004  loss: 0.0192 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [390/985]  eta: 0:09:32  lr: 0.000004  loss: 0.0184 (0.0192)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [400/985]  eta: 0:09:23  lr: 0.000004  loss: 0.0208 (0.0193)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [410/985]  eta: 0:09:13  lr: 0.000004  loss: 0.0192 (0.0193)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [420/985]  eta: 0:09:03  lr: 0.000004  loss: 0.0172 (0.0193)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [430/985]  eta: 0:08:54  lr: 0.000004  loss: 0.0183 (0.0193)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [440/985]  eta: 0:08:44  lr: 0.000004  loss: 0.0179 (0.0193)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [450/985]  eta: 0:08:34  lr: 0.000004  loss: 0.0184 (0.0193)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [460/985]  eta: 0:08:24  lr: 0.000004  loss: 0.0184 (0.0193)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [470/985]  eta: 0:08:15  lr: 0.000004  loss: 0.0169 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [480/985]  eta: 0:08:05  lr: 0.000004  loss: 0.0169 (0.0192)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [490/985]  eta: 0:07:55  lr: 0.000004  loss: 0.0183 (0.0192)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [500/985]  eta: 0:07:46  lr: 0.000004  loss: 0.0206 (0.0193)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [510/985]  eta: 0:07:36  lr: 0.000004  loss: 0.0195 (0.0193)  time: 0.9574  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:803]  [520/985]  eta: 0:07:26  lr: 0.000004  loss: 0.0182 (0.0193)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [530/985]  eta: 0:07:17  lr: 0.000004  loss: 0.0175 (0.0192)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [540/985]  eta: 0:07:07  lr: 0.000004  loss: 0.0170 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [550/985]  eta: 0:06:57  lr: 0.000004  loss: 0.0177 (0.0192)  time: 0.9559  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:803]  [560/985]  eta: 0:06:48  lr: 0.000004  loss: 0.0168 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [570/985]  eta: 0:06:38  lr: 0.000004  loss: 0.0168 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [580/985]  eta: 0:06:28  lr: 0.000004  loss: 0.0178 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [590/985]  eta: 0:06:19  lr: 0.000004  loss: 0.0193 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [600/985]  eta: 0:06:09  lr: 0.000004  loss: 0.0170 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [610/985]  eta: 0:06:00  lr: 0.000004  loss: 0.0169 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [620/985]  eta: 0:05:50  lr: 0.000004  loss: 0.0182 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [630/985]  eta: 0:05:40  lr: 0.000004  loss: 0.0175 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [640/985]  eta: 0:05:31  lr: 0.000004  loss: 0.0161 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [650/985]  eta: 0:05:21  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:803]  [660/985]  eta: 0:05:11  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [670/985]  eta: 0:05:02  lr: 0.000004  loss: 0.0187 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [680/985]  eta: 0:04:52  lr: 0.000004  loss: 0.0189 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [690/985]  eta: 0:04:43  lr: 0.000004  loss: 0.0178 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [700/985]  eta: 0:04:33  lr: 0.000004  loss: 0.0177 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [710/985]  eta: 0:04:23  lr: 0.000004  loss: 0.0193 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [720/985]  eta: 0:04:14  lr: 0.000004  loss: 0.0193 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [730/985]  eta: 0:04:04  lr: 0.000004  loss: 0.0185 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [740/985]  eta: 0:03:54  lr: 0.000004  loss: 0.0176 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [750/985]  eta: 0:03:45  lr: 0.000004  loss: 0.0195 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [760/985]  eta: 0:03:35  lr: 0.000004  loss: 0.0183 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [770/985]  eta: 0:03:26  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [780/985]  eta: 0:03:16  lr: 0.000004  loss: 0.0187 (0.0192)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [790/985]  eta: 0:03:06  lr: 0.000004  loss: 0.0188 (0.0192)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [800/985]  eta: 0:02:57  lr: 0.000004  loss: 0.0164 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [810/985]  eta: 0:02:47  lr: 0.000004  loss: 0.0180 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [820/985]  eta: 0:02:38  lr: 0.000004  loss: 0.0190 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [830/985]  eta: 0:02:28  lr: 0.000004  loss: 0.0186 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [840/985]  eta: 0:02:18  lr: 0.000004  loss: 0.0180 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [850/985]  eta: 0:02:09  lr: 0.000004  loss: 0.0178 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [860/985]  eta: 0:01:59  lr: 0.000004  loss: 0.0179 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [870/985]  eta: 0:01:50  lr: 0.000004  loss: 0.0176 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [880/985]  eta: 0:01:40  lr: 0.000004  loss: 0.0176 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [890/985]  eta: 0:01:30  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [900/985]  eta: 0:01:21  lr: 0.000004  loss: 0.0192 (0.0191)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [910/985]  eta: 0:01:11  lr: 0.000004  loss: 0.0192 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [920/985]  eta: 0:01:02  lr: 0.000004  loss: 0.0173 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [930/985]  eta: 0:00:52  lr: 0.000004  loss: 0.0166 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [940/985]  eta: 0:00:43  lr: 0.000004  loss: 0.0174 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [950/985]  eta: 0:00:33  lr: 0.000004  loss: 0.0184 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [960/985]  eta: 0:00:23  lr: 0.000004  loss: 0.0185 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [970/985]  eta: 0:00:14  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [980/985]  eta: 0:00:04  lr: 0.000004  loss: 0.0182 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803]  [984/985]  eta: 0:00:00  lr: 0.000004  loss: 0.0181 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:803] Total time: 0:15:43 (0.9575 s / it)\n",
      "Averaged stats: lr: 0.000004  loss: 0.0181 (0.0191)\n",
      "Valid: [epoch:803]  [ 0/14]  eta: 0:02:59  loss: 0.0127 (0.0127)  time: 12.8306  data: 0.6410  max mem: 41892\n",
      "Valid: [epoch:803]  [13/14]  eta: 0:00:12  loss: 0.0143 (0.0143)  time: 12.1272  data: 0.0459  max mem: 41892\n",
      "Valid: [epoch:803] Total time: 0:02:49 (12.1344 s / it)\n",
      "Averaged stats: loss: 0.0143 (0.0143)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_803_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:804]  [  0/985]  eta: 1:26:57  lr: 0.000006  loss: 0.0156 (0.0156)  time: 5.2971  data: 4.2191  max mem: 41892\n",
      "Train: [epoch:804]  [ 10/985]  eta: 0:21:46  lr: 0.000006  loss: 0.0197 (0.0198)  time: 1.3396  data: 0.3837  max mem: 41892\n",
      "Train: [epoch:804]  [ 20/985]  eta: 0:18:27  lr: 0.000006  loss: 0.0184 (0.0189)  time: 0.9400  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [ 30/985]  eta: 0:17:11  lr: 0.000006  loss: 0.0176 (0.0187)  time: 0.9371  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [ 40/985]  eta: 0:16:31  lr: 0.000006  loss: 0.0169 (0.0180)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [ 50/985]  eta: 0:16:02  lr: 0.000006  loss: 0.0165 (0.0181)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [ 60/985]  eta: 0:15:41  lr: 0.000006  loss: 0.0185 (0.0181)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [ 70/985]  eta: 0:15:21  lr: 0.000006  loss: 0.0188 (0.0183)  time: 0.9516  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:804]  [ 80/985]  eta: 0:15:05  lr: 0.000006  loss: 0.0201 (0.0187)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [ 90/985]  eta: 0:14:51  lr: 0.000006  loss: 0.0172 (0.0186)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [100/985]  eta: 0:14:37  lr: 0.000006  loss: 0.0172 (0.0186)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [110/985]  eta: 0:14:24  lr: 0.000006  loss: 0.0184 (0.0186)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [120/985]  eta: 0:14:12  lr: 0.000006  loss: 0.0182 (0.0185)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [130/985]  eta: 0:14:00  lr: 0.000006  loss: 0.0183 (0.0187)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [140/985]  eta: 0:13:49  lr: 0.000006  loss: 0.0185 (0.0187)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [150/985]  eta: 0:13:37  lr: 0.000006  loss: 0.0177 (0.0187)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [160/985]  eta: 0:13:26  lr: 0.000006  loss: 0.0178 (0.0188)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [170/985]  eta: 0:13:15  lr: 0.000006  loss: 0.0197 (0.0189)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [180/985]  eta: 0:13:05  lr: 0.000006  loss: 0.0192 (0.0189)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [190/985]  eta: 0:12:54  lr: 0.000006  loss: 0.0192 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [200/985]  eta: 0:12:43  lr: 0.000006  loss: 0.0192 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [210/985]  eta: 0:12:33  lr: 0.000006  loss: 0.0180 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [220/985]  eta: 0:12:23  lr: 0.000006  loss: 0.0169 (0.0189)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [230/985]  eta: 0:12:13  lr: 0.000006  loss: 0.0182 (0.0189)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [240/985]  eta: 0:12:03  lr: 0.000006  loss: 0.0179 (0.0189)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [250/985]  eta: 0:11:53  lr: 0.000006  loss: 0.0191 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [260/985]  eta: 0:11:42  lr: 0.000006  loss: 0.0200 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:804]  [270/985]  eta: 0:11:32  lr: 0.000006  loss: 0.0173 (0.0189)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [280/985]  eta: 0:11:22  lr: 0.000006  loss: 0.0173 (0.0189)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [290/985]  eta: 0:11:12  lr: 0.000006  loss: 0.0178 (0.0189)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [300/985]  eta: 0:11:02  lr: 0.000006  loss: 0.0175 (0.0188)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [310/985]  eta: 0:10:52  lr: 0.000006  loss: 0.0181 (0.0189)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [320/985]  eta: 0:10:42  lr: 0.000006  loss: 0.0202 (0.0189)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [330/985]  eta: 0:10:32  lr: 0.000006  loss: 0.0194 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [340/985]  eta: 0:10:23  lr: 0.000006  loss: 0.0177 (0.0189)  time: 0.9585  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:804]  [350/985]  eta: 0:10:13  lr: 0.000006  loss: 0.0175 (0.0190)  time: 0.9601  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:804]  [360/985]  eta: 0:10:03  lr: 0.000006  loss: 0.0198 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [370/985]  eta: 0:09:53  lr: 0.000006  loss: 0.0195 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [380/985]  eta: 0:09:43  lr: 0.000006  loss: 0.0179 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [390/985]  eta: 0:09:34  lr: 0.000006  loss: 0.0182 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [400/985]  eta: 0:09:24  lr: 0.000006  loss: 0.0186 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [410/985]  eta: 0:09:14  lr: 0.000006  loss: 0.0216 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [420/985]  eta: 0:09:04  lr: 0.000006  loss: 0.0199 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [430/985]  eta: 0:08:54  lr: 0.000006  loss: 0.0196 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [440/985]  eta: 0:08:45  lr: 0.000006  loss: 0.0183 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [450/985]  eta: 0:08:35  lr: 0.000006  loss: 0.0183 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [460/985]  eta: 0:08:25  lr: 0.000006  loss: 0.0183 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [470/985]  eta: 0:08:15  lr: 0.000006  loss: 0.0175 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [480/985]  eta: 0:08:06  lr: 0.000006  loss: 0.0182 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [490/985]  eta: 0:07:56  lr: 0.000006  loss: 0.0183 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [500/985]  eta: 0:07:46  lr: 0.000006  loss: 0.0192 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [510/985]  eta: 0:07:37  lr: 0.000006  loss: 0.0193 (0.0192)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [520/985]  eta: 0:07:27  lr: 0.000006  loss: 0.0187 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [530/985]  eta: 0:07:17  lr: 0.000006  loss: 0.0177 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [540/985]  eta: 0:07:08  lr: 0.000006  loss: 0.0175 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [550/985]  eta: 0:06:58  lr: 0.000006  loss: 0.0175 (0.0191)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [560/985]  eta: 0:06:48  lr: 0.000006  loss: 0.0184 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [570/985]  eta: 0:06:38  lr: 0.000006  loss: 0.0186 (0.0192)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [580/985]  eta: 0:06:29  lr: 0.000006  loss: 0.0196 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [590/985]  eta: 0:06:19  lr: 0.000006  loss: 0.0178 (0.0192)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [600/985]  eta: 0:06:10  lr: 0.000006  loss: 0.0178 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [610/985]  eta: 0:06:00  lr: 0.000006  loss: 0.0174 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [620/985]  eta: 0:05:50  lr: 0.000006  loss: 0.0179 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [630/985]  eta: 0:05:41  lr: 0.000006  loss: 0.0175 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [640/985]  eta: 0:05:31  lr: 0.000006  loss: 0.0170 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [650/985]  eta: 0:05:21  lr: 0.000006  loss: 0.0179 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [660/985]  eta: 0:05:12  lr: 0.000006  loss: 0.0178 (0.0191)  time: 0.9545  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:804]  [670/985]  eta: 0:05:02  lr: 0.000006  loss: 0.0170 (0.0191)  time: 0.9546  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:804]  [680/985]  eta: 0:04:52  lr: 0.000006  loss: 0.0169 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [690/985]  eta: 0:04:43  lr: 0.000006  loss: 0.0169 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [700/985]  eta: 0:04:33  lr: 0.000006  loss: 0.0200 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [710/985]  eta: 0:04:24  lr: 0.000006  loss: 0.0200 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [720/985]  eta: 0:04:14  lr: 0.000006  loss: 0.0182 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [730/985]  eta: 0:04:04  lr: 0.000006  loss: 0.0181 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [740/985]  eta: 0:03:55  lr: 0.000006  loss: 0.0168 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [750/985]  eta: 0:03:45  lr: 0.000006  loss: 0.0185 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [760/985]  eta: 0:03:36  lr: 0.000006  loss: 0.0184 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [770/985]  eta: 0:03:26  lr: 0.000006  loss: 0.0182 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [780/985]  eta: 0:03:16  lr: 0.000006  loss: 0.0183 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [790/985]  eta: 0:03:07  lr: 0.000006  loss: 0.0185 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [800/985]  eta: 0:02:57  lr: 0.000006  loss: 0.0187 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [810/985]  eta: 0:02:47  lr: 0.000006  loss: 0.0181 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [820/985]  eta: 0:02:38  lr: 0.000006  loss: 0.0179 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [830/985]  eta: 0:02:28  lr: 0.000006  loss: 0.0189 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [840/985]  eta: 0:02:19  lr: 0.000006  loss: 0.0183 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [850/985]  eta: 0:02:09  lr: 0.000006  loss: 0.0171 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [860/985]  eta: 0:01:59  lr: 0.000006  loss: 0.0172 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [870/985]  eta: 0:01:50  lr: 0.000006  loss: 0.0185 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [880/985]  eta: 0:01:40  lr: 0.000006  loss: 0.0167 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [890/985]  eta: 0:01:31  lr: 0.000006  loss: 0.0173 (0.0191)  time: 0.9541  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:804]  [900/985]  eta: 0:01:21  lr: 0.000006  loss: 0.0188 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [910/985]  eta: 0:01:11  lr: 0.000006  loss: 0.0192 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [920/985]  eta: 0:01:02  lr: 0.000006  loss: 0.0184 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:804]  [930/985]  eta: 0:00:52  lr: 0.000006  loss: 0.0184 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [940/985]  eta: 0:00:43  lr: 0.000006  loss: 0.0182 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [950/985]  eta: 0:00:33  lr: 0.000006  loss: 0.0187 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [960/985]  eta: 0:00:23  lr: 0.000006  loss: 0.0190 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [970/985]  eta: 0:00:14  lr: 0.000006  loss: 0.0186 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [980/985]  eta: 0:00:04  lr: 0.000006  loss: 0.0183 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804]  [984/985]  eta: 0:00:00  lr: 0.000006  loss: 0.0183 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:804] Total time: 0:15:44 (0.9591 s / it)\n",
      "Averaged stats: lr: 0.000006  loss: 0.0183 (0.0191)\n",
      "Valid: [epoch:804]  [ 0/14]  eta: 0:03:02  loss: 0.0141 (0.0141)  time: 13.0260  data: 0.5551  max mem: 41892\n",
      "Valid: [epoch:804]  [13/14]  eta: 0:00:12  loss: 0.0147 (0.0146)  time: 12.4858  data: 0.0398  max mem: 41892\n",
      "Valid: [epoch:804] Total time: 0:02:54 (12.4915 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_804_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:805]  [  0/985]  eta: 1:13:37  lr: 0.000007  loss: 0.0260 (0.0260)  time: 4.4850  data: 3.4871  max mem: 41892\n",
      "Train: [epoch:805]  [ 10/985]  eta: 0:20:40  lr: 0.000007  loss: 0.0185 (0.0187)  time: 1.2725  data: 0.3172  max mem: 41892\n",
      "Train: [epoch:805]  [ 20/985]  eta: 0:17:54  lr: 0.000007  loss: 0.0182 (0.0188)  time: 0.9447  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [ 30/985]  eta: 0:16:52  lr: 0.000007  loss: 0.0190 (0.0191)  time: 0.9434  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [ 40/985]  eta: 0:16:14  lr: 0.000007  loss: 0.0186 (0.0190)  time: 0.9442  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [ 50/985]  eta: 0:15:49  lr: 0.000007  loss: 0.0180 (0.0190)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [ 60/985]  eta: 0:15:29  lr: 0.000007  loss: 0.0182 (0.0190)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [ 70/985]  eta: 0:15:11  lr: 0.000007  loss: 0.0177 (0.0189)  time: 0.9480  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:805]  [ 80/985]  eta: 0:14:57  lr: 0.000007  loss: 0.0175 (0.0189)  time: 0.9544  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:805]  [ 90/985]  eta: 0:14:43  lr: 0.000007  loss: 0.0180 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [100/985]  eta: 0:14:31  lr: 0.000007  loss: 0.0183 (0.0190)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [110/985]  eta: 0:14:18  lr: 0.000007  loss: 0.0185 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [120/985]  eta: 0:14:06  lr: 0.000007  loss: 0.0185 (0.0190)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [130/985]  eta: 0:13:55  lr: 0.000007  loss: 0.0180 (0.0190)  time: 0.9511  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:805]  [140/985]  eta: 0:13:44  lr: 0.000007  loss: 0.0180 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [150/985]  eta: 0:13:33  lr: 0.000007  loss: 0.0188 (0.0192)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [160/985]  eta: 0:13:22  lr: 0.000007  loss: 0.0179 (0.0192)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [170/985]  eta: 0:13:12  lr: 0.000007  loss: 0.0181 (0.0192)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [180/985]  eta: 0:13:01  lr: 0.000007  loss: 0.0181 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [190/985]  eta: 0:12:51  lr: 0.000007  loss: 0.0193 (0.0193)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [200/985]  eta: 0:12:40  lr: 0.000007  loss: 0.0184 (0.0192)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [210/985]  eta: 0:12:30  lr: 0.000007  loss: 0.0167 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [220/985]  eta: 0:12:20  lr: 0.000007  loss: 0.0167 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [230/985]  eta: 0:12:10  lr: 0.000007  loss: 0.0171 (0.0190)  time: 0.9588  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:805]  [240/985]  eta: 0:12:00  lr: 0.000007  loss: 0.0182 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [250/985]  eta: 0:11:50  lr: 0.000007  loss: 0.0182 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [260/985]  eta: 0:11:40  lr: 0.000007  loss: 0.0169 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [270/985]  eta: 0:11:30  lr: 0.000007  loss: 0.0176 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [280/985]  eta: 0:11:20  lr: 0.000007  loss: 0.0187 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [290/985]  eta: 0:11:10  lr: 0.000007  loss: 0.0183 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [300/985]  eta: 0:11:00  lr: 0.000007  loss: 0.0187 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [310/985]  eta: 0:10:51  lr: 0.000007  loss: 0.0190 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [320/985]  eta: 0:10:41  lr: 0.000007  loss: 0.0176 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [330/985]  eta: 0:10:31  lr: 0.000007  loss: 0.0176 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [340/985]  eta: 0:10:21  lr: 0.000007  loss: 0.0183 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [350/985]  eta: 0:10:11  lr: 0.000007  loss: 0.0179 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [360/985]  eta: 0:10:02  lr: 0.000007  loss: 0.0173 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [370/985]  eta: 0:09:52  lr: 0.000007  loss: 0.0179 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [380/985]  eta: 0:09:42  lr: 0.000007  loss: 0.0188 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [390/985]  eta: 0:09:33  lr: 0.000007  loss: 0.0188 (0.0191)  time: 0.9641  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [400/985]  eta: 0:09:23  lr: 0.000007  loss: 0.0187 (0.0190)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [410/985]  eta: 0:09:13  lr: 0.000007  loss: 0.0187 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [420/985]  eta: 0:09:04  lr: 0.000007  loss: 0.0183 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [430/985]  eta: 0:08:54  lr: 0.000007  loss: 0.0175 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [440/985]  eta: 0:08:44  lr: 0.000007  loss: 0.0179 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [450/985]  eta: 0:08:34  lr: 0.000007  loss: 0.0175 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [460/985]  eta: 0:08:25  lr: 0.000007  loss: 0.0176 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [470/985]  eta: 0:08:15  lr: 0.000007  loss: 0.0187 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [480/985]  eta: 0:08:05  lr: 0.000007  loss: 0.0187 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [490/985]  eta: 0:07:56  lr: 0.000007  loss: 0.0199 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [500/985]  eta: 0:07:46  lr: 0.000007  loss: 0.0185 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [510/985]  eta: 0:07:36  lr: 0.000007  loss: 0.0183 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [520/985]  eta: 0:07:27  lr: 0.000007  loss: 0.0185 (0.0192)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [530/985]  eta: 0:07:17  lr: 0.000007  loss: 0.0191 (0.0192)  time: 0.9540  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:805]  [540/985]  eta: 0:07:07  lr: 0.000007  loss: 0.0191 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [550/985]  eta: 0:06:58  lr: 0.000007  loss: 0.0172 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [560/985]  eta: 0:06:48  lr: 0.000007  loss: 0.0166 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [570/985]  eta: 0:06:38  lr: 0.000007  loss: 0.0173 (0.0192)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [580/985]  eta: 0:06:29  lr: 0.000007  loss: 0.0182 (0.0192)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [590/985]  eta: 0:06:19  lr: 0.000007  loss: 0.0181 (0.0192)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [600/985]  eta: 0:06:10  lr: 0.000007  loss: 0.0176 (0.0192)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [610/985]  eta: 0:06:00  lr: 0.000007  loss: 0.0179 (0.0192)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [620/985]  eta: 0:05:50  lr: 0.000007  loss: 0.0191 (0.0192)  time: 0.9533  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:805]  [630/985]  eta: 0:05:41  lr: 0.000007  loss: 0.0191 (0.0192)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [640/985]  eta: 0:05:31  lr: 0.000007  loss: 0.0164 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [650/985]  eta: 0:05:21  lr: 0.000007  loss: 0.0165 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [660/985]  eta: 0:05:12  lr: 0.000007  loss: 0.0179 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [670/985]  eta: 0:05:02  lr: 0.000007  loss: 0.0177 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [680/985]  eta: 0:04:53  lr: 0.000007  loss: 0.0186 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [690/985]  eta: 0:04:43  lr: 0.000007  loss: 0.0191 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [700/985]  eta: 0:04:33  lr: 0.000007  loss: 0.0184 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [710/985]  eta: 0:04:24  lr: 0.000007  loss: 0.0174 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [720/985]  eta: 0:04:14  lr: 0.000007  loss: 0.0182 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [730/985]  eta: 0:04:04  lr: 0.000007  loss: 0.0182 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [740/985]  eta: 0:03:55  lr: 0.000007  loss: 0.0172 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [750/985]  eta: 0:03:45  lr: 0.000007  loss: 0.0174 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [760/985]  eta: 0:03:36  lr: 0.000007  loss: 0.0175 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [770/985]  eta: 0:03:26  lr: 0.000007  loss: 0.0175 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [780/985]  eta: 0:03:16  lr: 0.000007  loss: 0.0182 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [790/985]  eta: 0:03:07  lr: 0.000007  loss: 0.0181 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [800/985]  eta: 0:02:57  lr: 0.000007  loss: 0.0175 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [810/985]  eta: 0:02:47  lr: 0.000007  loss: 0.0181 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [820/985]  eta: 0:02:38  lr: 0.000007  loss: 0.0181 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [830/985]  eta: 0:02:28  lr: 0.000007  loss: 0.0177 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [840/985]  eta: 0:02:19  lr: 0.000007  loss: 0.0176 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [850/985]  eta: 0:02:09  lr: 0.000007  loss: 0.0172 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [860/985]  eta: 0:01:59  lr: 0.000007  loss: 0.0166 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [870/985]  eta: 0:01:50  lr: 0.000007  loss: 0.0177 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [880/985]  eta: 0:01:40  lr: 0.000007  loss: 0.0169 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [890/985]  eta: 0:01:31  lr: 0.000007  loss: 0.0183 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [900/985]  eta: 0:01:21  lr: 0.000007  loss: 0.0204 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [910/985]  eta: 0:01:11  lr: 0.000007  loss: 0.0186 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [920/985]  eta: 0:01:02  lr: 0.000007  loss: 0.0181 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [930/985]  eta: 0:00:52  lr: 0.000007  loss: 0.0178 (0.0191)  time: 0.9579  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:805]  [940/985]  eta: 0:00:43  lr: 0.000007  loss: 0.0191 (0.0191)  time: 0.9557  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:805]  [950/985]  eta: 0:00:33  lr: 0.000007  loss: 0.0203 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [960/985]  eta: 0:00:23  lr: 0.000007  loss: 0.0186 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [970/985]  eta: 0:00:14  lr: 0.000007  loss: 0.0166 (0.0192)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [980/985]  eta: 0:00:04  lr: 0.000007  loss: 0.0185 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805]  [984/985]  eta: 0:00:00  lr: 0.000007  loss: 0.0194 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:805] Total time: 0:15:45 (0.9594 s / it)\n",
      "Averaged stats: lr: 0.000007  loss: 0.0194 (0.0191)\n",
      "Valid: [epoch:805]  [ 0/14]  eta: 0:03:03  loss: 0.0138 (0.0138)  time: 13.0828  data: 0.6573  max mem: 41892\n",
      "Valid: [epoch:805]  [13/14]  eta: 0:00:12  loss: 0.0147 (0.0147)  time: 12.3082  data: 0.0471  max mem: 41892\n",
      "Valid: [epoch:805] Total time: 0:02:52 (12.3145 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_805_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:806]  [  0/985]  eta: 1:14:45  lr: 0.000009  loss: 0.0210 (0.0210)  time: 4.5538  data: 3.5630  max mem: 41892\n",
      "Train: [epoch:806]  [ 10/985]  eta: 0:20:50  lr: 0.000009  loss: 0.0187 (0.0191)  time: 1.2824  data: 0.3240  max mem: 41892\n",
      "Train: [epoch:806]  [ 20/985]  eta: 0:18:02  lr: 0.000009  loss: 0.0188 (0.0202)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [ 30/985]  eta: 0:17:01  lr: 0.000009  loss: 0.0191 (0.0197)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [ 40/985]  eta: 0:16:23  lr: 0.000009  loss: 0.0175 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [ 50/985]  eta: 0:15:56  lr: 0.000009  loss: 0.0161 (0.0187)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [ 60/985]  eta: 0:15:35  lr: 0.000009  loss: 0.0180 (0.0189)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [ 70/985]  eta: 0:15:17  lr: 0.000009  loss: 0.0185 (0.0188)  time: 0.9512  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:806]  [ 80/985]  eta: 0:15:02  lr: 0.000009  loss: 0.0160 (0.0185)  time: 0.9544  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:806]  [ 90/985]  eta: 0:14:47  lr: 0.000009  loss: 0.0164 (0.0185)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [100/985]  eta: 0:14:34  lr: 0.000009  loss: 0.0170 (0.0186)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [110/985]  eta: 0:14:22  lr: 0.000009  loss: 0.0170 (0.0184)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [120/985]  eta: 0:14:09  lr: 0.000009  loss: 0.0174 (0.0185)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [130/985]  eta: 0:13:59  lr: 0.000009  loss: 0.0176 (0.0185)  time: 0.9603  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:806]  [140/985]  eta: 0:13:47  lr: 0.000009  loss: 0.0174 (0.0186)  time: 0.9603  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:806]  [150/985]  eta: 0:13:36  lr: 0.000009  loss: 0.0179 (0.0187)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [160/985]  eta: 0:13:25  lr: 0.000009  loss: 0.0182 (0.0187)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [170/985]  eta: 0:13:15  lr: 0.000009  loss: 0.0179 (0.0186)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [180/985]  eta: 0:13:04  lr: 0.000009  loss: 0.0184 (0.0187)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [190/985]  eta: 0:12:54  lr: 0.000009  loss: 0.0193 (0.0188)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [200/985]  eta: 0:12:43  lr: 0.000009  loss: 0.0187 (0.0188)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [210/985]  eta: 0:12:33  lr: 0.000009  loss: 0.0176 (0.0187)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [220/985]  eta: 0:12:22  lr: 0.000009  loss: 0.0170 (0.0187)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [230/985]  eta: 0:12:12  lr: 0.000009  loss: 0.0170 (0.0187)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [240/985]  eta: 0:12:02  lr: 0.000009  loss: 0.0175 (0.0186)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [250/985]  eta: 0:11:52  lr: 0.000009  loss: 0.0175 (0.0187)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [260/985]  eta: 0:11:42  lr: 0.000009  loss: 0.0178 (0.0187)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [270/985]  eta: 0:11:32  lr: 0.000009  loss: 0.0172 (0.0187)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [280/985]  eta: 0:11:22  lr: 0.000009  loss: 0.0174 (0.0187)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [290/985]  eta: 0:11:12  lr: 0.000009  loss: 0.0174 (0.0187)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [300/985]  eta: 0:11:02  lr: 0.000009  loss: 0.0190 (0.0188)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [310/985]  eta: 0:10:52  lr: 0.000009  loss: 0.0212 (0.0189)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [320/985]  eta: 0:10:42  lr: 0.000009  loss: 0.0187 (0.0189)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [330/985]  eta: 0:10:32  lr: 0.000009  loss: 0.0179 (0.0189)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [340/985]  eta: 0:10:23  lr: 0.000009  loss: 0.0180 (0.0189)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [350/985]  eta: 0:10:13  lr: 0.000009  loss: 0.0192 (0.0189)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [360/985]  eta: 0:10:03  lr: 0.000009  loss: 0.0181 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [370/985]  eta: 0:09:53  lr: 0.000009  loss: 0.0172 (0.0189)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [380/985]  eta: 0:09:43  lr: 0.000009  loss: 0.0171 (0.0189)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [390/985]  eta: 0:09:34  lr: 0.000009  loss: 0.0175 (0.0189)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [400/985]  eta: 0:09:24  lr: 0.000009  loss: 0.0185 (0.0189)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [410/985]  eta: 0:09:14  lr: 0.000009  loss: 0.0176 (0.0189)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [420/985]  eta: 0:09:04  lr: 0.000009  loss: 0.0179 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [430/985]  eta: 0:08:55  lr: 0.000009  loss: 0.0190 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [440/985]  eta: 0:08:45  lr: 0.000009  loss: 0.0182 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [450/985]  eta: 0:08:35  lr: 0.000009  loss: 0.0182 (0.0190)  time: 0.9599  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:806]  [460/985]  eta: 0:08:25  lr: 0.000009  loss: 0.0182 (0.0190)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [470/985]  eta: 0:08:16  lr: 0.000009  loss: 0.0174 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [480/985]  eta: 0:08:06  lr: 0.000009  loss: 0.0167 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [490/985]  eta: 0:07:56  lr: 0.000009  loss: 0.0187 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [500/985]  eta: 0:07:47  lr: 0.000009  loss: 0.0199 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [510/985]  eta: 0:07:37  lr: 0.000009  loss: 0.0187 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [520/985]  eta: 0:07:27  lr: 0.000009  loss: 0.0180 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [530/985]  eta: 0:07:18  lr: 0.000009  loss: 0.0186 (0.0190)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [540/985]  eta: 0:07:08  lr: 0.000009  loss: 0.0183 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [550/985]  eta: 0:06:58  lr: 0.000009  loss: 0.0177 (0.0190)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [560/985]  eta: 0:06:49  lr: 0.000009  loss: 0.0178 (0.0190)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [570/985]  eta: 0:06:39  lr: 0.000009  loss: 0.0172 (0.0190)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [580/985]  eta: 0:06:29  lr: 0.000009  loss: 0.0175 (0.0190)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [590/985]  eta: 0:06:20  lr: 0.000009  loss: 0.0199 (0.0190)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [600/985]  eta: 0:06:10  lr: 0.000009  loss: 0.0199 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [610/985]  eta: 0:06:00  lr: 0.000009  loss: 0.0184 (0.0190)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [620/985]  eta: 0:05:51  lr: 0.000009  loss: 0.0181 (0.0190)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [630/985]  eta: 0:05:41  lr: 0.000009  loss: 0.0184 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [640/985]  eta: 0:05:31  lr: 0.000009  loss: 0.0173 (0.0190)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [650/985]  eta: 0:05:22  lr: 0.000009  loss: 0.0178 (0.0190)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [660/985]  eta: 0:05:12  lr: 0.000009  loss: 0.0186 (0.0190)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [670/985]  eta: 0:05:02  lr: 0.000009  loss: 0.0175 (0.0189)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [680/985]  eta: 0:04:53  lr: 0.000009  loss: 0.0167 (0.0189)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [690/985]  eta: 0:04:43  lr: 0.000009  loss: 0.0170 (0.0189)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [700/985]  eta: 0:04:33  lr: 0.000009  loss: 0.0193 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [710/985]  eta: 0:04:24  lr: 0.000009  loss: 0.0205 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [720/985]  eta: 0:04:14  lr: 0.000009  loss: 0.0199 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [730/985]  eta: 0:04:05  lr: 0.000009  loss: 0.0180 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [740/985]  eta: 0:03:55  lr: 0.000009  loss: 0.0180 (0.0190)  time: 0.9584  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:806]  [750/985]  eta: 0:03:45  lr: 0.000009  loss: 0.0184 (0.0190)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [760/985]  eta: 0:03:36  lr: 0.000009  loss: 0.0195 (0.0190)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [770/985]  eta: 0:03:26  lr: 0.000009  loss: 0.0195 (0.0190)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [780/985]  eta: 0:03:16  lr: 0.000009  loss: 0.0186 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [790/985]  eta: 0:03:07  lr: 0.000009  loss: 0.0187 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [800/985]  eta: 0:02:57  lr: 0.000009  loss: 0.0188 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:806]  [810/985]  eta: 0:02:48  lr: 0.000009  loss: 0.0196 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [820/985]  eta: 0:02:38  lr: 0.000009  loss: 0.0182 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [830/985]  eta: 0:02:28  lr: 0.000009  loss: 0.0178 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [840/985]  eta: 0:02:19  lr: 0.000009  loss: 0.0168 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [850/985]  eta: 0:02:09  lr: 0.000009  loss: 0.0171 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [860/985]  eta: 0:02:00  lr: 0.000009  loss: 0.0191 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [870/985]  eta: 0:01:50  lr: 0.000009  loss: 0.0181 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [880/985]  eta: 0:01:40  lr: 0.000009  loss: 0.0175 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [890/985]  eta: 0:01:31  lr: 0.000009  loss: 0.0185 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [900/985]  eta: 0:01:21  lr: 0.000009  loss: 0.0180 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [910/985]  eta: 0:01:12  lr: 0.000009  loss: 0.0190 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [920/985]  eta: 0:01:02  lr: 0.000009  loss: 0.0190 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [930/985]  eta: 0:00:52  lr: 0.000009  loss: 0.0179 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [940/985]  eta: 0:00:43  lr: 0.000009  loss: 0.0179 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [950/985]  eta: 0:00:33  lr: 0.000009  loss: 0.0196 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [960/985]  eta: 0:00:24  lr: 0.000009  loss: 0.0196 (0.0192)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [970/985]  eta: 0:00:14  lr: 0.000009  loss: 0.0191 (0.0192)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [980/985]  eta: 0:00:04  lr: 0.000009  loss: 0.0186 (0.0192)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806]  [984/985]  eta: 0:00:00  lr: 0.000009  loss: 0.0188 (0.0192)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:806] Total time: 0:15:45 (0.9601 s / it)\n",
      "Averaged stats: lr: 0.000009  loss: 0.0188 (0.0192)\n",
      "Valid: [epoch:806]  [ 0/14]  eta: 0:03:00  loss: 0.0145 (0.0145)  time: 12.8658  data: 0.5412  max mem: 41892\n",
      "Valid: [epoch:806]  [13/14]  eta: 0:00:12  loss: 0.0148 (0.0147)  time: 12.3524  data: 0.0388  max mem: 41892\n",
      "Valid: [epoch:806] Total time: 0:02:53 (12.3582 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_806_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:807]  [  0/985]  eta: 1:20:02  lr: 0.000010  loss: 0.0190 (0.0190)  time: 4.8761  data: 3.8789  max mem: 41892\n",
      "Train: [epoch:807]  [ 10/985]  eta: 0:21:13  lr: 0.000010  loss: 0.0184 (0.0188)  time: 1.3058  data: 0.3528  max mem: 41892\n",
      "Train: [epoch:807]  [ 20/985]  eta: 0:18:11  lr: 0.000010  loss: 0.0182 (0.0187)  time: 0.9440  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:807]  [ 30/985]  eta: 0:17:00  lr: 0.000010  loss: 0.0174 (0.0182)  time: 0.9384  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [ 40/985]  eta: 0:16:20  lr: 0.000010  loss: 0.0163 (0.0179)  time: 0.9389  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [ 50/985]  eta: 0:15:53  lr: 0.000010  loss: 0.0156 (0.0179)  time: 0.9432  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [ 60/985]  eta: 0:15:32  lr: 0.000010  loss: 0.0175 (0.0181)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [ 70/985]  eta: 0:15:15  lr: 0.000010  loss: 0.0171 (0.0179)  time: 0.9505  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:807]  [ 80/985]  eta: 0:14:59  lr: 0.000010  loss: 0.0164 (0.0180)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [ 90/985]  eta: 0:14:46  lr: 0.000010  loss: 0.0164 (0.0178)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [100/985]  eta: 0:14:33  lr: 0.000010  loss: 0.0165 (0.0179)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [110/985]  eta: 0:14:20  lr: 0.000010  loss: 0.0171 (0.0180)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [120/985]  eta: 0:14:08  lr: 0.000010  loss: 0.0184 (0.0181)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [130/985]  eta: 0:13:57  lr: 0.000010  loss: 0.0186 (0.0184)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [140/985]  eta: 0:13:46  lr: 0.000010  loss: 0.0188 (0.0184)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [150/985]  eta: 0:13:35  lr: 0.000010  loss: 0.0188 (0.0185)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [160/985]  eta: 0:13:24  lr: 0.000010  loss: 0.0180 (0.0184)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [170/985]  eta: 0:13:13  lr: 0.000010  loss: 0.0173 (0.0184)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [180/985]  eta: 0:13:03  lr: 0.000010  loss: 0.0175 (0.0185)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [190/985]  eta: 0:12:53  lr: 0.000010  loss: 0.0179 (0.0186)  time: 0.9575  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:807]  [200/985]  eta: 0:12:42  lr: 0.000010  loss: 0.0184 (0.0186)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [210/985]  eta: 0:12:32  lr: 0.000010  loss: 0.0184 (0.0186)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [220/985]  eta: 0:12:22  lr: 0.000010  loss: 0.0182 (0.0186)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [230/985]  eta: 0:12:11  lr: 0.000010  loss: 0.0186 (0.0186)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [240/985]  eta: 0:12:01  lr: 0.000010  loss: 0.0186 (0.0187)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [250/985]  eta: 0:11:51  lr: 0.000010  loss: 0.0182 (0.0187)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [260/985]  eta: 0:11:41  lr: 0.000010  loss: 0.0172 (0.0187)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [270/985]  eta: 0:11:31  lr: 0.000010  loss: 0.0172 (0.0187)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [280/985]  eta: 0:11:21  lr: 0.000010  loss: 0.0178 (0.0187)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [290/985]  eta: 0:11:12  lr: 0.000010  loss: 0.0181 (0.0187)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [300/985]  eta: 0:11:02  lr: 0.000010  loss: 0.0181 (0.0187)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [310/985]  eta: 0:10:52  lr: 0.000010  loss: 0.0183 (0.0187)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [320/985]  eta: 0:10:42  lr: 0.000010  loss: 0.0171 (0.0186)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [330/985]  eta: 0:10:32  lr: 0.000010  loss: 0.0176 (0.0186)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [340/985]  eta: 0:10:23  lr: 0.000010  loss: 0.0183 (0.0186)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [350/985]  eta: 0:10:13  lr: 0.000010  loss: 0.0177 (0.0186)  time: 0.9578  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:807]  [360/985]  eta: 0:10:03  lr: 0.000010  loss: 0.0177 (0.0186)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [370/985]  eta: 0:09:53  lr: 0.000010  loss: 0.0180 (0.0186)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [380/985]  eta: 0:09:44  lr: 0.000010  loss: 0.0180 (0.0187)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [390/985]  eta: 0:09:34  lr: 0.000010  loss: 0.0191 (0.0187)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [400/985]  eta: 0:09:24  lr: 0.000010  loss: 0.0209 (0.0188)  time: 0.9596  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:807]  [410/985]  eta: 0:09:14  lr: 0.000010  loss: 0.0214 (0.0188)  time: 0.9590  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:807]  [420/985]  eta: 0:09:05  lr: 0.000010  loss: 0.0186 (0.0189)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [430/985]  eta: 0:08:55  lr: 0.000010  loss: 0.0184 (0.0189)  time: 0.9626  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [440/985]  eta: 0:08:45  lr: 0.000010  loss: 0.0201 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [450/985]  eta: 0:08:35  lr: 0.000010  loss: 0.0190 (0.0189)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [460/985]  eta: 0:08:26  lr: 0.000010  loss: 0.0181 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [470/985]  eta: 0:08:16  lr: 0.000010  loss: 0.0177 (0.0189)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [480/985]  eta: 0:08:06  lr: 0.000010  loss: 0.0177 (0.0189)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [490/985]  eta: 0:07:57  lr: 0.000010  loss: 0.0204 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [500/985]  eta: 0:07:47  lr: 0.000010  loss: 0.0205 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [510/985]  eta: 0:07:37  lr: 0.000010  loss: 0.0184 (0.0191)  time: 0.9539  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:807]  [520/985]  eta: 0:07:27  lr: 0.000010  loss: 0.0182 (0.0190)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [530/985]  eta: 0:07:18  lr: 0.000010  loss: 0.0180 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [540/985]  eta: 0:07:08  lr: 0.000010  loss: 0.0185 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [550/985]  eta: 0:06:58  lr: 0.000010  loss: 0.0180 (0.0190)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [560/985]  eta: 0:06:49  lr: 0.000010  loss: 0.0173 (0.0190)  time: 0.9629  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:807]  [570/985]  eta: 0:06:39  lr: 0.000010  loss: 0.0175 (0.0190)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [580/985]  eta: 0:06:29  lr: 0.000010  loss: 0.0184 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [590/985]  eta: 0:06:20  lr: 0.000010  loss: 0.0182 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [600/985]  eta: 0:06:10  lr: 0.000010  loss: 0.0180 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [610/985]  eta: 0:06:00  lr: 0.000010  loss: 0.0178 (0.0190)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [620/985]  eta: 0:05:51  lr: 0.000010  loss: 0.0201 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [630/985]  eta: 0:05:41  lr: 0.000010  loss: 0.0190 (0.0190)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [640/985]  eta: 0:05:31  lr: 0.000010  loss: 0.0176 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [650/985]  eta: 0:05:22  lr: 0.000010  loss: 0.0193 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [660/985]  eta: 0:05:12  lr: 0.000010  loss: 0.0195 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [670/985]  eta: 0:05:02  lr: 0.000010  loss: 0.0167 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [680/985]  eta: 0:04:53  lr: 0.000010  loss: 0.0165 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [690/985]  eta: 0:04:43  lr: 0.000010  loss: 0.0173 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [700/985]  eta: 0:04:34  lr: 0.000010  loss: 0.0192 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [710/985]  eta: 0:04:24  lr: 0.000010  loss: 0.0186 (0.0190)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [720/985]  eta: 0:04:14  lr: 0.000010  loss: 0.0173 (0.0190)  time: 0.9567  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:807]  [730/985]  eta: 0:04:05  lr: 0.000010  loss: 0.0177 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [740/985]  eta: 0:03:55  lr: 0.000010  loss: 0.0181 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [750/985]  eta: 0:03:45  lr: 0.000010  loss: 0.0182 (0.0190)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [760/985]  eta: 0:03:36  lr: 0.000010  loss: 0.0182 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [770/985]  eta: 0:03:26  lr: 0.000010  loss: 0.0181 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [780/985]  eta: 0:03:16  lr: 0.000010  loss: 0.0173 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [790/985]  eta: 0:03:07  lr: 0.000010  loss: 0.0173 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [800/985]  eta: 0:02:57  lr: 0.000010  loss: 0.0199 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [810/985]  eta: 0:02:48  lr: 0.000010  loss: 0.0199 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [820/985]  eta: 0:02:38  lr: 0.000010  loss: 0.0195 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [830/985]  eta: 0:02:28  lr: 0.000010  loss: 0.0194 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [840/985]  eta: 0:02:19  lr: 0.000010  loss: 0.0187 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [850/985]  eta: 0:02:09  lr: 0.000010  loss: 0.0182 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [860/985]  eta: 0:02:00  lr: 0.000010  loss: 0.0185 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [870/985]  eta: 0:01:50  lr: 0.000010  loss: 0.0183 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [880/985]  eta: 0:01:40  lr: 0.000010  loss: 0.0173 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [890/985]  eta: 0:01:31  lr: 0.000010  loss: 0.0173 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [900/985]  eta: 0:01:21  lr: 0.000010  loss: 0.0187 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [910/985]  eta: 0:01:12  lr: 0.000010  loss: 0.0197 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [920/985]  eta: 0:01:02  lr: 0.000010  loss: 0.0191 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [930/985]  eta: 0:00:52  lr: 0.000010  loss: 0.0192 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [940/985]  eta: 0:00:43  lr: 0.000010  loss: 0.0193 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [950/985]  eta: 0:00:33  lr: 0.000010  loss: 0.0210 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [960/985]  eta: 0:00:23  lr: 0.000010  loss: 0.0184 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [970/985]  eta: 0:00:14  lr: 0.000010  loss: 0.0173 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [980/985]  eta: 0:00:04  lr: 0.000010  loss: 0.0173 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807]  [984/985]  eta: 0:00:00  lr: 0.000010  loss: 0.0190 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:807] Total time: 0:15:45 (0.9599 s / it)\n",
      "Averaged stats: lr: 0.000010  loss: 0.0190 (0.0192)\n",
      "Valid: [epoch:807]  [ 0/14]  eta: 0:03:02  loss: 0.0137 (0.0137)  time: 13.0695  data: 0.5343  max mem: 41892\n",
      "Valid: [epoch:807]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0145)  time: 12.3306  data: 0.0383  max mem: 41892\n",
      "Valid: [epoch:807] Total time: 0:02:52 (12.3378 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_807_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:808]  [  0/985]  eta: 1:10:15  lr: 0.000012  loss: 0.0213 (0.0213)  time: 4.2799  data: 3.2942  max mem: 41892\n",
      "Train: [epoch:808]  [ 10/985]  eta: 0:20:11  lr: 0.000012  loss: 0.0195 (0.0203)  time: 1.2423  data: 0.2996  max mem: 41892\n",
      "Train: [epoch:808]  [ 20/985]  eta: 0:17:39  lr: 0.000012  loss: 0.0182 (0.0192)  time: 0.9385  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:808]  [ 30/985]  eta: 0:16:40  lr: 0.000012  loss: 0.0180 (0.0192)  time: 0.9404  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [ 40/985]  eta: 0:16:05  lr: 0.000012  loss: 0.0170 (0.0185)  time: 0.9417  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [ 50/985]  eta: 0:15:41  lr: 0.000012  loss: 0.0161 (0.0182)  time: 0.9427  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [ 60/985]  eta: 0:15:21  lr: 0.000012  loss: 0.0179 (0.0184)  time: 0.9453  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [ 70/985]  eta: 0:15:05  lr: 0.000012  loss: 0.0184 (0.0186)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [ 80/985]  eta: 0:14:51  lr: 0.000012  loss: 0.0174 (0.0184)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [ 90/985]  eta: 0:14:37  lr: 0.000012  loss: 0.0171 (0.0184)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [100/985]  eta: 0:14:25  lr: 0.000012  loss: 0.0177 (0.0184)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [110/985]  eta: 0:14:13  lr: 0.000012  loss: 0.0176 (0.0184)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [120/985]  eta: 0:14:02  lr: 0.000012  loss: 0.0184 (0.0186)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [130/985]  eta: 0:13:51  lr: 0.000012  loss: 0.0207 (0.0188)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [140/985]  eta: 0:13:41  lr: 0.000012  loss: 0.0195 (0.0188)  time: 0.9578  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [150/985]  eta: 0:13:30  lr: 0.000012  loss: 0.0193 (0.0189)  time: 0.9600  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [160/985]  eta: 0:13:20  lr: 0.000012  loss: 0.0193 (0.0189)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [170/985]  eta: 0:13:09  lr: 0.000012  loss: 0.0170 (0.0188)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [180/985]  eta: 0:12:59  lr: 0.000012  loss: 0.0171 (0.0188)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [190/985]  eta: 0:12:49  lr: 0.000012  loss: 0.0180 (0.0188)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [200/985]  eta: 0:12:39  lr: 0.000012  loss: 0.0183 (0.0188)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [210/985]  eta: 0:12:29  lr: 0.000012  loss: 0.0186 (0.0188)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [220/985]  eta: 0:12:19  lr: 0.000012  loss: 0.0168 (0.0187)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [230/985]  eta: 0:12:09  lr: 0.000012  loss: 0.0169 (0.0187)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [240/985]  eta: 0:11:59  lr: 0.000012  loss: 0.0169 (0.0187)  time: 0.9636  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [250/985]  eta: 0:11:49  lr: 0.000012  loss: 0.0169 (0.0186)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [260/985]  eta: 0:11:39  lr: 0.000012  loss: 0.0176 (0.0186)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [270/985]  eta: 0:11:30  lr: 0.000012  loss: 0.0179 (0.0186)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [280/985]  eta: 0:11:20  lr: 0.000012  loss: 0.0204 (0.0187)  time: 0.9605  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [290/985]  eta: 0:11:10  lr: 0.000012  loss: 0.0193 (0.0187)  time: 0.9587  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [300/985]  eta: 0:11:00  lr: 0.000012  loss: 0.0197 (0.0188)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [310/985]  eta: 0:10:51  lr: 0.000012  loss: 0.0194 (0.0188)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [320/985]  eta: 0:10:41  lr: 0.000012  loss: 0.0173 (0.0187)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [330/985]  eta: 0:10:31  lr: 0.000012  loss: 0.0173 (0.0187)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [340/985]  eta: 0:10:21  lr: 0.000012  loss: 0.0189 (0.0188)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [350/985]  eta: 0:10:11  lr: 0.000012  loss: 0.0185 (0.0188)  time: 0.9549  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [360/985]  eta: 0:10:02  lr: 0.000012  loss: 0.0190 (0.0188)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [370/985]  eta: 0:09:52  lr: 0.000012  loss: 0.0197 (0.0188)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [380/985]  eta: 0:09:42  lr: 0.000012  loss: 0.0173 (0.0188)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [390/985]  eta: 0:09:33  lr: 0.000012  loss: 0.0173 (0.0188)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [400/985]  eta: 0:09:23  lr: 0.000012  loss: 0.0183 (0.0188)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [410/985]  eta: 0:09:13  lr: 0.000012  loss: 0.0187 (0.0188)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [420/985]  eta: 0:09:04  lr: 0.000012  loss: 0.0181 (0.0188)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [430/985]  eta: 0:08:54  lr: 0.000012  loss: 0.0176 (0.0189)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [440/985]  eta: 0:08:44  lr: 0.000012  loss: 0.0175 (0.0189)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [450/985]  eta: 0:08:34  lr: 0.000012  loss: 0.0180 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [460/985]  eta: 0:08:25  lr: 0.000012  loss: 0.0180 (0.0189)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [470/985]  eta: 0:08:15  lr: 0.000012  loss: 0.0182 (0.0189)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [480/985]  eta: 0:08:05  lr: 0.000012  loss: 0.0172 (0.0189)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [490/985]  eta: 0:07:56  lr: 0.000012  loss: 0.0186 (0.0189)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [500/985]  eta: 0:07:46  lr: 0.000012  loss: 0.0205 (0.0189)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [510/985]  eta: 0:07:36  lr: 0.000012  loss: 0.0175 (0.0189)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [520/985]  eta: 0:07:27  lr: 0.000012  loss: 0.0173 (0.0189)  time: 0.9631  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [530/985]  eta: 0:07:17  lr: 0.000012  loss: 0.0171 (0.0188)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [540/985]  eta: 0:07:07  lr: 0.000012  loss: 0.0175 (0.0188)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [550/985]  eta: 0:06:58  lr: 0.000012  loss: 0.0174 (0.0188)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [560/985]  eta: 0:06:48  lr: 0.000012  loss: 0.0172 (0.0188)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [570/985]  eta: 0:06:38  lr: 0.000012  loss: 0.0174 (0.0188)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [580/985]  eta: 0:06:29  lr: 0.000012  loss: 0.0179 (0.0188)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [590/985]  eta: 0:06:19  lr: 0.000012  loss: 0.0185 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [600/985]  eta: 0:06:10  lr: 0.000012  loss: 0.0176 (0.0189)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [610/985]  eta: 0:06:00  lr: 0.000012  loss: 0.0169 (0.0188)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [620/985]  eta: 0:05:50  lr: 0.000012  loss: 0.0189 (0.0189)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [630/985]  eta: 0:05:41  lr: 0.000012  loss: 0.0189 (0.0189)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [640/985]  eta: 0:05:31  lr: 0.000012  loss: 0.0161 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [650/985]  eta: 0:05:21  lr: 0.000012  loss: 0.0167 (0.0188)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [660/985]  eta: 0:05:12  lr: 0.000012  loss: 0.0182 (0.0188)  time: 0.9611  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [670/985]  eta: 0:05:02  lr: 0.000012  loss: 0.0179 (0.0188)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [680/985]  eta: 0:04:53  lr: 0.000012  loss: 0.0161 (0.0188)  time: 0.9608  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:808]  [690/985]  eta: 0:04:43  lr: 0.000012  loss: 0.0165 (0.0188)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [700/985]  eta: 0:04:33  lr: 0.000012  loss: 0.0179 (0.0188)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [710/985]  eta: 0:04:24  lr: 0.000012  loss: 0.0191 (0.0188)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [720/985]  eta: 0:04:14  lr: 0.000012  loss: 0.0201 (0.0189)  time: 0.9567  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [730/985]  eta: 0:04:04  lr: 0.000012  loss: 0.0193 (0.0189)  time: 0.9618  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [740/985]  eta: 0:03:55  lr: 0.000012  loss: 0.0181 (0.0189)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [750/985]  eta: 0:03:45  lr: 0.000012  loss: 0.0174 (0.0189)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [760/985]  eta: 0:03:36  lr: 0.000012  loss: 0.0174 (0.0189)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [770/985]  eta: 0:03:26  lr: 0.000012  loss: 0.0174 (0.0189)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [780/985]  eta: 0:03:16  lr: 0.000012  loss: 0.0186 (0.0189)  time: 0.9633  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [790/985]  eta: 0:03:07  lr: 0.000012  loss: 0.0182 (0.0189)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [800/985]  eta: 0:02:57  lr: 0.000012  loss: 0.0182 (0.0189)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [810/985]  eta: 0:02:48  lr: 0.000012  loss: 0.0190 (0.0189)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [820/985]  eta: 0:02:38  lr: 0.000012  loss: 0.0190 (0.0189)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [830/985]  eta: 0:02:28  lr: 0.000012  loss: 0.0186 (0.0189)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [840/985]  eta: 0:02:19  lr: 0.000012  loss: 0.0180 (0.0189)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [850/985]  eta: 0:02:09  lr: 0.000012  loss: 0.0183 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [860/985]  eta: 0:01:59  lr: 0.000012  loss: 0.0189 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [870/985]  eta: 0:01:50  lr: 0.000012  loss: 0.0177 (0.0190)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [880/985]  eta: 0:01:40  lr: 0.000012  loss: 0.0172 (0.0190)  time: 0.9601  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [890/985]  eta: 0:01:31  lr: 0.000012  loss: 0.0192 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [900/985]  eta: 0:01:21  lr: 0.000012  loss: 0.0194 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [910/985]  eta: 0:01:11  lr: 0.000012  loss: 0.0197 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [920/985]  eta: 0:01:02  lr: 0.000012  loss: 0.0232 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [930/985]  eta: 0:00:52  lr: 0.000012  loss: 0.0189 (0.0191)  time: 0.9565  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [940/985]  eta: 0:00:43  lr: 0.000012  loss: 0.0181 (0.0191)  time: 0.9584  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:808]  [950/985]  eta: 0:00:33  lr: 0.000012  loss: 0.0187 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [960/985]  eta: 0:00:23  lr: 0.000012  loss: 0.0196 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [970/985]  eta: 0:00:14  lr: 0.000012  loss: 0.0206 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [980/985]  eta: 0:00:04  lr: 0.000012  loss: 0.0198 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808]  [984/985]  eta: 0:00:00  lr: 0.000012  loss: 0.0186 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:808] Total time: 0:15:45 (0.9596 s / it)\n",
      "Averaged stats: lr: 0.000012  loss: 0.0186 (0.0191)\n",
      "Valid: [epoch:808]  [ 0/14]  eta: 0:03:00  loss: 0.0151 (0.0151)  time: 12.8790  data: 0.5624  max mem: 41892\n",
      "Valid: [epoch:808]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0145)  time: 12.2927  data: 0.0403  max mem: 41892\n",
      "Valid: [epoch:808] Total time: 0:02:52 (12.2994 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_808_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:809]  [  0/985]  eta: 1:08:32  lr: 0.000014  loss: 0.0153 (0.0153)  time: 4.1751  data: 3.1912  max mem: 41892\n",
      "Train: [epoch:809]  [ 10/985]  eta: 0:20:13  lr: 0.000014  loss: 0.0188 (0.0196)  time: 1.2451  data: 0.2903  max mem: 41892\n",
      "Train: [epoch:809]  [ 20/985]  eta: 0:17:46  lr: 0.000014  loss: 0.0177 (0.0190)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [ 30/985]  eta: 0:16:44  lr: 0.000014  loss: 0.0185 (0.0194)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [ 40/985]  eta: 0:16:08  lr: 0.000014  loss: 0.0176 (0.0189)  time: 0.9414  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [ 50/985]  eta: 0:15:44  lr: 0.000014  loss: 0.0162 (0.0185)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [ 60/985]  eta: 0:15:24  lr: 0.000014  loss: 0.0173 (0.0187)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [ 70/985]  eta: 0:15:09  lr: 0.000014  loss: 0.0192 (0.0188)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [ 80/985]  eta: 0:14:54  lr: 0.000014  loss: 0.0168 (0.0185)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [ 90/985]  eta: 0:14:41  lr: 0.000014  loss: 0.0170 (0.0186)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [100/985]  eta: 0:14:28  lr: 0.000014  loss: 0.0181 (0.0187)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [110/985]  eta: 0:14:16  lr: 0.000014  loss: 0.0181 (0.0187)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [120/985]  eta: 0:14:04  lr: 0.000014  loss: 0.0185 (0.0188)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [130/985]  eta: 0:13:53  lr: 0.000014  loss: 0.0192 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [140/985]  eta: 0:13:42  lr: 0.000014  loss: 0.0192 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [150/985]  eta: 0:13:32  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [160/985]  eta: 0:13:21  lr: 0.000014  loss: 0.0187 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [170/985]  eta: 0:13:11  lr: 0.000014  loss: 0.0175 (0.0190)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [180/985]  eta: 0:13:01  lr: 0.000014  loss: 0.0177 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [190/985]  eta: 0:12:51  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [200/985]  eta: 0:12:41  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [210/985]  eta: 0:12:30  lr: 0.000014  loss: 0.0189 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [220/985]  eta: 0:12:20  lr: 0.000014  loss: 0.0186 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [230/985]  eta: 0:12:10  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [240/985]  eta: 0:12:00  lr: 0.000014  loss: 0.0184 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [250/985]  eta: 0:11:50  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [260/985]  eta: 0:11:40  lr: 0.000014  loss: 0.0169 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [270/985]  eta: 0:11:30  lr: 0.000014  loss: 0.0169 (0.0189)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [280/985]  eta: 0:11:20  lr: 0.000014  loss: 0.0178 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [290/985]  eta: 0:11:10  lr: 0.000014  loss: 0.0180 (0.0189)  time: 0.9556  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:809]  [300/985]  eta: 0:11:01  lr: 0.000014  loss: 0.0186 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [310/985]  eta: 0:10:51  lr: 0.000014  loss: 0.0187 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [320/985]  eta: 0:10:41  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [330/985]  eta: 0:10:31  lr: 0.000014  loss: 0.0205 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [340/985]  eta: 0:10:21  lr: 0.000014  loss: 0.0185 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [350/985]  eta: 0:10:12  lr: 0.000014  loss: 0.0175 (0.0190)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [360/985]  eta: 0:10:02  lr: 0.000014  loss: 0.0172 (0.0189)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [370/985]  eta: 0:09:52  lr: 0.000014  loss: 0.0172 (0.0189)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [380/985]  eta: 0:09:42  lr: 0.000014  loss: 0.0176 (0.0189)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [390/985]  eta: 0:09:32  lr: 0.000014  loss: 0.0178 (0.0189)  time: 0.9552  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:809]  [400/985]  eta: 0:09:23  lr: 0.000014  loss: 0.0187 (0.0189)  time: 0.9568  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:809]  [410/985]  eta: 0:09:13  lr: 0.000014  loss: 0.0182 (0.0189)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [420/985]  eta: 0:09:03  lr: 0.000014  loss: 0.0175 (0.0189)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [430/985]  eta: 0:08:54  lr: 0.000014  loss: 0.0179 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [440/985]  eta: 0:08:44  lr: 0.000014  loss: 0.0182 (0.0190)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [450/985]  eta: 0:08:34  lr: 0.000014  loss: 0.0177 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [460/985]  eta: 0:08:25  lr: 0.000014  loss: 0.0181 (0.0190)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [470/985]  eta: 0:08:15  lr: 0.000014  loss: 0.0184 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [480/985]  eta: 0:08:05  lr: 0.000014  loss: 0.0189 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [490/985]  eta: 0:07:55  lr: 0.000014  loss: 0.0184 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [500/985]  eta: 0:07:46  lr: 0.000014  loss: 0.0165 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [510/985]  eta: 0:07:36  lr: 0.000014  loss: 0.0187 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [520/985]  eta: 0:07:27  lr: 0.000014  loss: 0.0184 (0.0190)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [530/985]  eta: 0:07:17  lr: 0.000014  loss: 0.0184 (0.0190)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [540/985]  eta: 0:07:07  lr: 0.000014  loss: 0.0184 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [550/985]  eta: 0:06:58  lr: 0.000014  loss: 0.0182 (0.0190)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [560/985]  eta: 0:06:48  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9544  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:809]  [570/985]  eta: 0:06:38  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [580/985]  eta: 0:06:29  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [590/985]  eta: 0:06:19  lr: 0.000014  loss: 0.0182 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [600/985]  eta: 0:06:09  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [610/985]  eta: 0:06:00  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [620/985]  eta: 0:05:50  lr: 0.000014  loss: 0.0185 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [630/985]  eta: 0:05:40  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [640/985]  eta: 0:05:31  lr: 0.000014  loss: 0.0178 (0.0190)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [650/985]  eta: 0:05:21  lr: 0.000014  loss: 0.0176 (0.0190)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [660/985]  eta: 0:05:12  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [670/985]  eta: 0:05:02  lr: 0.000014  loss: 0.0167 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [680/985]  eta: 0:04:52  lr: 0.000014  loss: 0.0169 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [690/985]  eta: 0:04:43  lr: 0.000014  loss: 0.0182 (0.0190)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [700/985]  eta: 0:04:33  lr: 0.000014  loss: 0.0198 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [710/985]  eta: 0:04:24  lr: 0.000014  loss: 0.0188 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [720/985]  eta: 0:04:14  lr: 0.000014  loss: 0.0187 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [730/985]  eta: 0:04:04  lr: 0.000014  loss: 0.0187 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [740/985]  eta: 0:03:55  lr: 0.000014  loss: 0.0187 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [750/985]  eta: 0:03:45  lr: 0.000014  loss: 0.0184 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [760/985]  eta: 0:03:35  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [770/985]  eta: 0:03:26  lr: 0.000014  loss: 0.0174 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [780/985]  eta: 0:03:16  lr: 0.000014  loss: 0.0164 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [790/985]  eta: 0:03:07  lr: 0.000014  loss: 0.0165 (0.0189)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [800/985]  eta: 0:02:57  lr: 0.000014  loss: 0.0179 (0.0189)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [810/985]  eta: 0:02:47  lr: 0.000014  loss: 0.0186 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [820/985]  eta: 0:02:38  lr: 0.000014  loss: 0.0191 (0.0190)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [830/985]  eta: 0:02:28  lr: 0.000014  loss: 0.0176 (0.0190)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [840/985]  eta: 0:02:19  lr: 0.000014  loss: 0.0172 (0.0190)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [850/985]  eta: 0:02:09  lr: 0.000014  loss: 0.0171 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [860/985]  eta: 0:01:59  lr: 0.000014  loss: 0.0174 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [870/985]  eta: 0:01:50  lr: 0.000014  loss: 0.0172 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [880/985]  eta: 0:01:40  lr: 0.000014  loss: 0.0186 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [890/985]  eta: 0:01:31  lr: 0.000014  loss: 0.0192 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [900/985]  eta: 0:01:21  lr: 0.000014  loss: 0.0199 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [910/985]  eta: 0:01:11  lr: 0.000014  loss: 0.0223 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [920/985]  eta: 0:01:02  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [930/985]  eta: 0:00:52  lr: 0.000014  loss: 0.0170 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [940/985]  eta: 0:00:43  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [950/985]  eta: 0:00:33  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:809]  [960/985]  eta: 0:00:23  lr: 0.000014  loss: 0.0191 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [970/985]  eta: 0:00:14  lr: 0.000014  loss: 0.0188 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [980/985]  eta: 0:00:04  lr: 0.000014  loss: 0.0168 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809]  [984/985]  eta: 0:00:00  lr: 0.000014  loss: 0.0168 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:809] Total time: 0:15:45 (0.9594 s / it)\n",
      "Averaged stats: lr: 0.000014  loss: 0.0168 (0.0191)\n",
      "Valid: [epoch:809]  [ 0/14]  eta: 0:03:05  loss: 0.0148 (0.0148)  time: 13.2190  data: 0.5900  max mem: 41892\n",
      "Valid: [epoch:809]  [13/14]  eta: 0:00:12  loss: 0.0145 (0.0145)  time: 12.3563  data: 0.0422  max mem: 41892\n",
      "Valid: [epoch:809] Total time: 0:02:53 (12.3627 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_809_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:810]  [  0/985]  eta: 1:18:10  lr: 0.000015  loss: 0.0240 (0.0240)  time: 4.7618  data: 3.7637  max mem: 41892\n",
      "Train: [epoch:810]  [ 10/985]  eta: 0:21:02  lr: 0.000015  loss: 0.0199 (0.0207)  time: 1.2948  data: 0.3423  max mem: 41892\n",
      "Train: [epoch:810]  [ 20/985]  eta: 0:18:08  lr: 0.000015  loss: 0.0181 (0.0193)  time: 0.9462  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [ 30/985]  eta: 0:17:00  lr: 0.000015  loss: 0.0179 (0.0192)  time: 0.9437  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [ 40/985]  eta: 0:16:24  lr: 0.000015  loss: 0.0162 (0.0182)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [ 50/985]  eta: 0:15:56  lr: 0.000015  loss: 0.0156 (0.0180)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [ 60/985]  eta: 0:15:37  lr: 0.000015  loss: 0.0170 (0.0180)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [ 70/985]  eta: 0:15:19  lr: 0.000015  loss: 0.0176 (0.0179)  time: 0.9572  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [ 80/985]  eta: 0:15:03  lr: 0.000015  loss: 0.0161 (0.0178)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [ 90/985]  eta: 0:14:49  lr: 0.000015  loss: 0.0172 (0.0180)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [100/985]  eta: 0:14:35  lr: 0.000015  loss: 0.0190 (0.0182)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [110/985]  eta: 0:14:24  lr: 0.000015  loss: 0.0183 (0.0181)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [120/985]  eta: 0:14:11  lr: 0.000015  loss: 0.0176 (0.0182)  time: 0.9600  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [130/985]  eta: 0:14:00  lr: 0.000015  loss: 0.0194 (0.0186)  time: 0.9529  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [140/985]  eta: 0:13:48  lr: 0.000015  loss: 0.0195 (0.0186)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [150/985]  eta: 0:13:38  lr: 0.000015  loss: 0.0183 (0.0187)  time: 0.9634  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [160/985]  eta: 0:13:27  lr: 0.000015  loss: 0.0196 (0.0188)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [170/985]  eta: 0:13:17  lr: 0.000015  loss: 0.0188 (0.0188)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [180/985]  eta: 0:13:06  lr: 0.000015  loss: 0.0183 (0.0190)  time: 0.9649  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [190/985]  eta: 0:12:55  lr: 0.000015  loss: 0.0205 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [200/985]  eta: 0:12:45  lr: 0.000015  loss: 0.0205 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [210/985]  eta: 0:12:34  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [220/985]  eta: 0:12:24  lr: 0.000015  loss: 0.0163 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [230/985]  eta: 0:12:14  lr: 0.000015  loss: 0.0187 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [240/985]  eta: 0:12:03  lr: 0.000015  loss: 0.0181 (0.0190)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [250/985]  eta: 0:11:53  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [260/985]  eta: 0:11:43  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [270/985]  eta: 0:11:33  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [280/985]  eta: 0:11:23  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9651  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [290/985]  eta: 0:11:13  lr: 0.000015  loss: 0.0193 (0.0191)  time: 0.9576  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [300/985]  eta: 0:11:03  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [310/985]  eta: 0:10:53  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [320/985]  eta: 0:10:43  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [330/985]  eta: 0:10:33  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [340/985]  eta: 0:10:23  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [350/985]  eta: 0:10:14  lr: 0.000015  loss: 0.0174 (0.0190)  time: 0.9568  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [360/985]  eta: 0:10:04  lr: 0.000015  loss: 0.0173 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [370/985]  eta: 0:09:54  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [380/985]  eta: 0:09:44  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [390/985]  eta: 0:09:34  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [400/985]  eta: 0:09:25  lr: 0.000015  loss: 0.0194 (0.0191)  time: 0.9633  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [410/985]  eta: 0:09:15  lr: 0.000015  loss: 0.0204 (0.0192)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [420/985]  eta: 0:09:05  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [430/985]  eta: 0:08:55  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [440/985]  eta: 0:08:46  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [450/985]  eta: 0:08:36  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [460/985]  eta: 0:08:26  lr: 0.000015  loss: 0.0192 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [470/985]  eta: 0:08:16  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [480/985]  eta: 0:08:07  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9662  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [490/985]  eta: 0:07:57  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [500/985]  eta: 0:07:47  lr: 0.000015  loss: 0.0170 (0.0191)  time: 0.9569  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [510/985]  eta: 0:07:38  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [520/985]  eta: 0:07:28  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [530/985]  eta: 0:07:18  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [540/985]  eta: 0:07:09  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [550/985]  eta: 0:06:59  lr: 0.000015  loss: 0.0193 (0.0191)  time: 0.9608  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [560/985]  eta: 0:06:49  lr: 0.000015  loss: 0.0167 (0.0191)  time: 0.9648  data: 0.0002  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:810]  [570/985]  eta: 0:06:40  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9647  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [580/985]  eta: 0:06:30  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [590/985]  eta: 0:06:20  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [600/985]  eta: 0:06:10  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [610/985]  eta: 0:06:01  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [620/985]  eta: 0:05:51  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [630/985]  eta: 0:05:41  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [640/985]  eta: 0:05:32  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [650/985]  eta: 0:05:22  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [660/985]  eta: 0:05:12  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9582  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [670/985]  eta: 0:05:03  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9585  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [680/985]  eta: 0:04:53  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [690/985]  eta: 0:04:43  lr: 0.000015  loss: 0.0166 (0.0190)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [700/985]  eta: 0:04:34  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [710/985]  eta: 0:04:24  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [720/985]  eta: 0:04:14  lr: 0.000015  loss: 0.0194 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [730/985]  eta: 0:04:05  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [740/985]  eta: 0:03:55  lr: 0.000015  loss: 0.0166 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [750/985]  eta: 0:03:46  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [760/985]  eta: 0:03:36  lr: 0.000015  loss: 0.0177 (0.0190)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [770/985]  eta: 0:03:26  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [780/985]  eta: 0:03:17  lr: 0.000015  loss: 0.0189 (0.0191)  time: 0.9597  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [790/985]  eta: 0:03:07  lr: 0.000015  loss: 0.0172 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [800/985]  eta: 0:02:57  lr: 0.000015  loss: 0.0167 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [810/985]  eta: 0:02:48  lr: 0.000015  loss: 0.0166 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [820/985]  eta: 0:02:38  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [830/985]  eta: 0:02:29  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9590  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:810]  [840/985]  eta: 0:02:19  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [850/985]  eta: 0:02:09  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [860/985]  eta: 0:02:00  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [870/985]  eta: 0:01:50  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [880/985]  eta: 0:01:40  lr: 0.000015  loss: 0.0169 (0.0190)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [890/985]  eta: 0:01:31  lr: 0.000015  loss: 0.0176 (0.0190)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [900/985]  eta: 0:01:21  lr: 0.000015  loss: 0.0202 (0.0191)  time: 0.9661  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [910/985]  eta: 0:01:12  lr: 0.000015  loss: 0.0202 (0.0191)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [920/985]  eta: 0:01:02  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [930/985]  eta: 0:00:52  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [940/985]  eta: 0:00:43  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [950/985]  eta: 0:00:33  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [960/985]  eta: 0:00:24  lr: 0.000015  loss: 0.0198 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [970/985]  eta: 0:00:14  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [980/985]  eta: 0:00:04  lr: 0.000015  loss: 0.0197 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810]  [984/985]  eta: 0:00:00  lr: 0.000015  loss: 0.0199 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:810] Total time: 0:15:46 (0.9612 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.0199 (0.0191)\n",
      "Valid: [epoch:810]  [ 0/14]  eta: 0:03:04  loss: 0.0136 (0.0136)  time: 13.1730  data: 0.5393  max mem: 41892\n",
      "Valid: [epoch:810]  [13/14]  eta: 0:00:12  loss: 0.0144 (0.0143)  time: 12.2685  data: 0.0386  max mem: 41892\n",
      "Valid: [epoch:810] Total time: 0:02:51 (12.2745 s / it)\n",
      "Averaged stats: loss: 0.0144 (0.0143)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_810_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:811]  [  0/985]  eta: 1:18:12  lr: 0.000017  loss: 0.0217 (0.0217)  time: 4.7640  data: 3.7552  max mem: 41892\n",
      "Train: [epoch:811]  [ 10/985]  eta: 0:21:01  lr: 0.000017  loss: 0.0185 (0.0196)  time: 1.2935  data: 0.3415  max mem: 41892\n",
      "Train: [epoch:811]  [ 20/985]  eta: 0:18:05  lr: 0.000017  loss: 0.0185 (0.0194)  time: 0.9426  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [ 30/985]  eta: 0:16:57  lr: 0.000017  loss: 0.0192 (0.0199)  time: 0.9400  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [ 40/985]  eta: 0:16:19  lr: 0.000017  loss: 0.0181 (0.0193)  time: 0.9430  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [ 50/985]  eta: 0:15:52  lr: 0.000017  loss: 0.0162 (0.0190)  time: 0.9450  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [ 60/985]  eta: 0:15:30  lr: 0.000017  loss: 0.0172 (0.0192)  time: 0.9451  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [ 70/985]  eta: 0:15:12  lr: 0.000017  loss: 0.0172 (0.0189)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [ 80/985]  eta: 0:14:57  lr: 0.000017  loss: 0.0163 (0.0187)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [ 90/985]  eta: 0:14:43  lr: 0.000017  loss: 0.0167 (0.0188)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [100/985]  eta: 0:14:30  lr: 0.000017  loss: 0.0177 (0.0188)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [110/985]  eta: 0:14:17  lr: 0.000017  loss: 0.0177 (0.0189)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [120/985]  eta: 0:14:06  lr: 0.000017  loss: 0.0190 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [130/985]  eta: 0:13:55  lr: 0.000017  loss: 0.0189 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [140/985]  eta: 0:13:43  lr: 0.000017  loss: 0.0198 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [150/985]  eta: 0:13:33  lr: 0.000017  loss: 0.0198 (0.0192)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [160/985]  eta: 0:13:22  lr: 0.000017  loss: 0.0185 (0.0192)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [170/985]  eta: 0:13:12  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9620  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:811]  [180/985]  eta: 0:13:02  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9601  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [190/985]  eta: 0:12:51  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [200/985]  eta: 0:12:41  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [210/985]  eta: 0:12:31  lr: 0.000017  loss: 0.0194 (0.0192)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [220/985]  eta: 0:12:21  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [230/985]  eta: 0:12:11  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9557  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [240/985]  eta: 0:12:01  lr: 0.000017  loss: 0.0189 (0.0192)  time: 0.9573  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [250/985]  eta: 0:11:51  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [260/985]  eta: 0:11:41  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [270/985]  eta: 0:11:31  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [280/985]  eta: 0:11:21  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [290/985]  eta: 0:11:11  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [300/985]  eta: 0:11:01  lr: 0.000017  loss: 0.0180 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [310/985]  eta: 0:10:51  lr: 0.000017  loss: 0.0177 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [320/985]  eta: 0:10:41  lr: 0.000017  loss: 0.0178 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [330/985]  eta: 0:10:31  lr: 0.000017  loss: 0.0184 (0.0190)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [340/985]  eta: 0:10:22  lr: 0.000017  loss: 0.0202 (0.0191)  time: 0.9571  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [350/985]  eta: 0:10:12  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [360/985]  eta: 0:10:02  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [370/985]  eta: 0:09:52  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [380/985]  eta: 0:09:42  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [390/985]  eta: 0:09:33  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [400/985]  eta: 0:09:23  lr: 0.000017  loss: 0.0183 (0.0192)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [410/985]  eta: 0:09:13  lr: 0.000017  loss: 0.0181 (0.0192)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [420/985]  eta: 0:09:04  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [430/985]  eta: 0:08:54  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [440/985]  eta: 0:08:44  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [450/985]  eta: 0:08:35  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [460/985]  eta: 0:08:25  lr: 0.000017  loss: 0.0189 (0.0192)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [470/985]  eta: 0:08:15  lr: 0.000017  loss: 0.0175 (0.0192)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [480/985]  eta: 0:08:05  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [490/985]  eta: 0:07:56  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [500/985]  eta: 0:07:46  lr: 0.000017  loss: 0.0206 (0.0192)  time: 0.9594  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [510/985]  eta: 0:07:37  lr: 0.000017  loss: 0.0198 (0.0192)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [520/985]  eta: 0:07:27  lr: 0.000017  loss: 0.0177 (0.0192)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [530/985]  eta: 0:07:17  lr: 0.000017  loss: 0.0160 (0.0191)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [540/985]  eta: 0:07:08  lr: 0.000017  loss: 0.0163 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [550/985]  eta: 0:06:58  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [560/985]  eta: 0:06:48  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [570/985]  eta: 0:06:39  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [580/985]  eta: 0:06:29  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [590/985]  eta: 0:06:19  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [600/985]  eta: 0:06:10  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [610/985]  eta: 0:06:00  lr: 0.000017  loss: 0.0177 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [620/985]  eta: 0:05:50  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [630/985]  eta: 0:05:41  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [640/985]  eta: 0:05:31  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [650/985]  eta: 0:05:21  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [660/985]  eta: 0:05:12  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9564  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [670/985]  eta: 0:05:02  lr: 0.000017  loss: 0.0167 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [680/985]  eta: 0:04:53  lr: 0.000017  loss: 0.0167 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [690/985]  eta: 0:04:43  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [700/985]  eta: 0:04:33  lr: 0.000017  loss: 0.0198 (0.0191)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [710/985]  eta: 0:04:24  lr: 0.000017  loss: 0.0209 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [720/985]  eta: 0:04:14  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [730/985]  eta: 0:04:05  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [740/985]  eta: 0:03:55  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [750/985]  eta: 0:03:45  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [760/985]  eta: 0:03:36  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [770/985]  eta: 0:03:26  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [780/985]  eta: 0:03:16  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [790/985]  eta: 0:03:07  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [800/985]  eta: 0:02:57  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [810/985]  eta: 0:02:48  lr: 0.000017  loss: 0.0199 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [820/985]  eta: 0:02:38  lr: 0.000017  loss: 0.0204 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [830/985]  eta: 0:02:28  lr: 0.000017  loss: 0.0204 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:811]  [840/985]  eta: 0:02:19  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [850/985]  eta: 0:02:09  lr: 0.000017  loss: 0.0170 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [860/985]  eta: 0:02:00  lr: 0.000017  loss: 0.0169 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [870/985]  eta: 0:01:50  lr: 0.000017  loss: 0.0167 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [880/985]  eta: 0:01:40  lr: 0.000017  loss: 0.0162 (0.0191)  time: 0.9569  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [890/985]  eta: 0:01:31  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9556  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [900/985]  eta: 0:01:21  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [910/985]  eta: 0:01:11  lr: 0.000017  loss: 0.0202 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [920/985]  eta: 0:01:02  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [930/985]  eta: 0:00:52  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9567  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [940/985]  eta: 0:00:43  lr: 0.000017  loss: 0.0184 (0.0192)  time: 0.9623  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:811]  [950/985]  eta: 0:00:33  lr: 0.000017  loss: 0.0195 (0.0192)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [960/985]  eta: 0:00:23  lr: 0.000017  loss: 0.0187 (0.0192)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [970/985]  eta: 0:00:14  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [980/985]  eta: 0:00:04  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811]  [984/985]  eta: 0:00:00  lr: 0.000017  loss: 0.0181 (0.0192)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:811] Total time: 0:15:45 (0.9597 s / it)\n",
      "Averaged stats: lr: 0.000017  loss: 0.0181 (0.0192)\n",
      "Valid: [epoch:811]  [ 0/14]  eta: 0:03:00  loss: 0.0134 (0.0134)  time: 12.8882  data: 0.7266  max mem: 41892\n",
      "Valid: [epoch:811]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0145)  time: 12.1176  data: 0.0520  max mem: 41892\n",
      "Valid: [epoch:811] Total time: 0:02:49 (12.1241 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_811_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:812]  [  0/985]  eta: 1:06:59  lr: 0.000017  loss: 0.0194 (0.0194)  time: 4.0811  data: 3.0793  max mem: 41892\n",
      "Train: [epoch:812]  [ 10/985]  eta: 0:20:10  lr: 0.000017  loss: 0.0178 (0.0190)  time: 1.2417  data: 0.2801  max mem: 41892\n",
      "Train: [epoch:812]  [ 20/985]  eta: 0:17:38  lr: 0.000017  loss: 0.0177 (0.0189)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [ 30/985]  eta: 0:16:38  lr: 0.000017  loss: 0.0183 (0.0189)  time: 0.9383  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [ 40/985]  eta: 0:16:04  lr: 0.000017  loss: 0.0181 (0.0186)  time: 0.9411  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [ 50/985]  eta: 0:15:40  lr: 0.000017  loss: 0.0174 (0.0184)  time: 0.9450  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [ 60/985]  eta: 0:15:22  lr: 0.000017  loss: 0.0203 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [ 70/985]  eta: 0:15:06  lr: 0.000017  loss: 0.0210 (0.0192)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [ 80/985]  eta: 0:14:51  lr: 0.000017  loss: 0.0195 (0.0192)  time: 0.9485  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [ 90/985]  eta: 0:14:39  lr: 0.000017  loss: 0.0173 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [100/985]  eta: 0:14:26  lr: 0.000017  loss: 0.0177 (0.0193)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [110/985]  eta: 0:14:14  lr: 0.000017  loss: 0.0177 (0.0192)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [120/985]  eta: 0:14:03  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [130/985]  eta: 0:13:52  lr: 0.000017  loss: 0.0189 (0.0192)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [140/985]  eta: 0:13:41  lr: 0.000017  loss: 0.0196 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [150/985]  eta: 0:13:30  lr: 0.000017  loss: 0.0184 (0.0192)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [160/985]  eta: 0:13:19  lr: 0.000017  loss: 0.0192 (0.0194)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [170/985]  eta: 0:13:09  lr: 0.000017  loss: 0.0197 (0.0194)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [180/985]  eta: 0:12:58  lr: 0.000017  loss: 0.0196 (0.0195)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [190/985]  eta: 0:12:48  lr: 0.000017  loss: 0.0173 (0.0193)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [200/985]  eta: 0:12:38  lr: 0.000017  loss: 0.0173 (0.0193)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [210/985]  eta: 0:12:28  lr: 0.000017  loss: 0.0176 (0.0193)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [220/985]  eta: 0:12:18  lr: 0.000017  loss: 0.0163 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [230/985]  eta: 0:12:08  lr: 0.000017  loss: 0.0163 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [240/985]  eta: 0:11:58  lr: 0.000017  loss: 0.0163 (0.0190)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [250/985]  eta: 0:11:48  lr: 0.000017  loss: 0.0164 (0.0189)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [260/985]  eta: 0:11:39  lr: 0.000017  loss: 0.0174 (0.0190)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [270/985]  eta: 0:11:29  lr: 0.000017  loss: 0.0188 (0.0189)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [280/985]  eta: 0:11:19  lr: 0.000017  loss: 0.0187 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [290/985]  eta: 0:11:09  lr: 0.000017  loss: 0.0183 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [300/985]  eta: 0:11:00  lr: 0.000017  loss: 0.0182 (0.0189)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [310/985]  eta: 0:10:50  lr: 0.000017  loss: 0.0182 (0.0189)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [320/985]  eta: 0:10:40  lr: 0.000017  loss: 0.0185 (0.0189)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [330/985]  eta: 0:10:30  lr: 0.000017  loss: 0.0189 (0.0189)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [340/985]  eta: 0:10:20  lr: 0.000017  loss: 0.0187 (0.0189)  time: 0.9529  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [350/985]  eta: 0:10:11  lr: 0.000017  loss: 0.0175 (0.0189)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [360/985]  eta: 0:10:01  lr: 0.000017  loss: 0.0177 (0.0189)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [370/985]  eta: 0:09:51  lr: 0.000017  loss: 0.0189 (0.0189)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [380/985]  eta: 0:09:41  lr: 0.000017  loss: 0.0183 (0.0190)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [390/985]  eta: 0:09:32  lr: 0.000017  loss: 0.0188 (0.0190)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [400/985]  eta: 0:09:22  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [410/985]  eta: 0:09:12  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [420/985]  eta: 0:09:03  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [430/985]  eta: 0:08:53  lr: 0.000017  loss: 0.0183 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [440/985]  eta: 0:08:43  lr: 0.000017  loss: 0.0195 (0.0192)  time: 0.9601  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:812]  [450/985]  eta: 0:08:34  lr: 0.000017  loss: 0.0184 (0.0192)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [460/985]  eta: 0:08:24  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9585  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [470/985]  eta: 0:08:14  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9571  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [480/985]  eta: 0:08:05  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [490/985]  eta: 0:07:55  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [500/985]  eta: 0:07:46  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [510/985]  eta: 0:07:36  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9617  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [520/985]  eta: 0:07:26  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [530/985]  eta: 0:07:17  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [540/985]  eta: 0:07:07  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [550/985]  eta: 0:06:57  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [560/985]  eta: 0:06:48  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [570/985]  eta: 0:06:38  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [580/985]  eta: 0:06:28  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [590/985]  eta: 0:06:19  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [600/985]  eta: 0:06:09  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [610/985]  eta: 0:06:00  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [620/985]  eta: 0:05:50  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [630/985]  eta: 0:05:40  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [640/985]  eta: 0:05:31  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [650/985]  eta: 0:05:21  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [660/985]  eta: 0:05:11  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [670/985]  eta: 0:05:02  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [680/985]  eta: 0:04:52  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [690/985]  eta: 0:04:43  lr: 0.000017  loss: 0.0178 (0.0190)  time: 0.9598  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [700/985]  eta: 0:04:33  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [710/985]  eta: 0:04:23  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [720/985]  eta: 0:04:14  lr: 0.000017  loss: 0.0201 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [730/985]  eta: 0:04:04  lr: 0.000017  loss: 0.0192 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [740/985]  eta: 0:03:55  lr: 0.000017  loss: 0.0171 (0.0191)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [750/985]  eta: 0:03:45  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [760/985]  eta: 0:03:35  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [770/985]  eta: 0:03:26  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [780/985]  eta: 0:03:16  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [790/985]  eta: 0:03:07  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [800/985]  eta: 0:02:57  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [810/985]  eta: 0:02:47  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [820/985]  eta: 0:02:38  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9598  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [830/985]  eta: 0:02:28  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [840/985]  eta: 0:02:19  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [850/985]  eta: 0:02:09  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9633  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [860/985]  eta: 0:01:59  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [870/985]  eta: 0:01:50  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [880/985]  eta: 0:01:40  lr: 0.000017  loss: 0.0166 (0.0191)  time: 0.9645  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [890/985]  eta: 0:01:31  lr: 0.000017  loss: 0.0167 (0.0191)  time: 0.9612  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [900/985]  eta: 0:01:21  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9625  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [910/985]  eta: 0:01:11  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [920/985]  eta: 0:01:02  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [930/985]  eta: 0:00:52  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9578  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:812]  [940/985]  eta: 0:00:43  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [950/985]  eta: 0:00:33  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [960/985]  eta: 0:00:23  lr: 0.000017  loss: 0.0191 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [970/985]  eta: 0:00:14  lr: 0.000017  loss: 0.0195 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [980/985]  eta: 0:00:04  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812]  [984/985]  eta: 0:00:00  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:812] Total time: 0:15:45 (0.9596 s / it)\n",
      "Averaged stats: lr: 0.000017  loss: 0.0179 (0.0191)\n",
      "Valid: [epoch:812]  [ 0/14]  eta: 0:02:54  loss: 0.0154 (0.0154)  time: 12.4791  data: 0.5608  max mem: 41892\n",
      "Valid: [epoch:812]  [13/14]  eta: 0:00:12  loss: 0.0150 (0.0149)  time: 12.0396  data: 0.0402  max mem: 41892\n",
      "Valid: [epoch:812] Total time: 0:02:48 (12.0461 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_812_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:813]  [  0/985]  eta: 0:43:24  lr: 0.000017  loss: 0.0192 (0.0192)  time: 2.6442  data: 1.6582  max mem: 41892\n",
      "Train: [epoch:813]  [ 10/985]  eta: 0:18:27  lr: 0.000017  loss: 0.0192 (0.0207)  time: 1.1354  data: 0.1813  max mem: 41892\n",
      "Train: [epoch:813]  [ 20/985]  eta: 0:16:43  lr: 0.000017  loss: 0.0192 (0.0208)  time: 0.9593  data: 0.0168  max mem: 41892\n",
      "Train: [epoch:813]  [ 30/985]  eta: 0:16:02  lr: 0.000017  loss: 0.0178 (0.0197)  time: 0.9378  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [ 40/985]  eta: 0:15:37  lr: 0.000017  loss: 0.0165 (0.0190)  time: 0.9415  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [ 50/985]  eta: 0:15:18  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9432  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:813]  [ 60/985]  eta: 0:15:03  lr: 0.000017  loss: 0.0190 (0.0190)  time: 0.9451  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [ 70/985]  eta: 0:14:50  lr: 0.000017  loss: 0.0186 (0.0190)  time: 0.9492  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [ 80/985]  eta: 0:14:38  lr: 0.000017  loss: 0.0174 (0.0188)  time: 0.9522  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [ 90/985]  eta: 0:14:26  lr: 0.000017  loss: 0.0179 (0.0188)  time: 0.9502  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [100/985]  eta: 0:14:15  lr: 0.000017  loss: 0.0180 (0.0188)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [110/985]  eta: 0:14:04  lr: 0.000017  loss: 0.0196 (0.0190)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [120/985]  eta: 0:13:54  lr: 0.000017  loss: 0.0185 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [130/985]  eta: 0:13:44  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9579  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [140/985]  eta: 0:13:33  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [150/985]  eta: 0:13:23  lr: 0.000017  loss: 0.0189 (0.0192)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [160/985]  eta: 0:13:13  lr: 0.000017  loss: 0.0187 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [170/985]  eta: 0:13:03  lr: 0.000017  loss: 0.0183 (0.0193)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [180/985]  eta: 0:12:53  lr: 0.000017  loss: 0.0187 (0.0194)  time: 0.9554  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [190/985]  eta: 0:12:43  lr: 0.000017  loss: 0.0190 (0.0195)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [200/985]  eta: 0:12:34  lr: 0.000017  loss: 0.0188 (0.0194)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [210/985]  eta: 0:12:24  lr: 0.000017  loss: 0.0186 (0.0194)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [220/985]  eta: 0:12:14  lr: 0.000017  loss: 0.0169 (0.0193)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [230/985]  eta: 0:12:04  lr: 0.000017  loss: 0.0176 (0.0193)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [240/985]  eta: 0:11:54  lr: 0.000017  loss: 0.0181 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [250/985]  eta: 0:11:44  lr: 0.000017  loss: 0.0168 (0.0192)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [260/985]  eta: 0:11:35  lr: 0.000017  loss: 0.0180 (0.0192)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [270/985]  eta: 0:11:25  lr: 0.000017  loss: 0.0181 (0.0192)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [280/985]  eta: 0:11:15  lr: 0.000017  loss: 0.0183 (0.0193)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [290/985]  eta: 0:11:06  lr: 0.000017  loss: 0.0189 (0.0193)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [300/985]  eta: 0:10:56  lr: 0.000017  loss: 0.0183 (0.0193)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [310/985]  eta: 0:10:47  lr: 0.000017  loss: 0.0181 (0.0192)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [320/985]  eta: 0:10:37  lr: 0.000017  loss: 0.0181 (0.0193)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [330/985]  eta: 0:10:28  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [340/985]  eta: 0:10:18  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9624  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [350/985]  eta: 0:10:09  lr: 0.000017  loss: 0.0191 (0.0192)  time: 0.9640  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [360/985]  eta: 0:09:59  lr: 0.000017  loss: 0.0188 (0.0192)  time: 0.9640  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [370/985]  eta: 0:09:49  lr: 0.000017  loss: 0.0182 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [380/985]  eta: 0:09:40  lr: 0.000017  loss: 0.0171 (0.0191)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [390/985]  eta: 0:09:30  lr: 0.000017  loss: 0.0185 (0.0192)  time: 0.9609  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [400/985]  eta: 0:09:20  lr: 0.000017  loss: 0.0191 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [410/985]  eta: 0:09:11  lr: 0.000017  loss: 0.0185 (0.0192)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [420/985]  eta: 0:09:01  lr: 0.000017  loss: 0.0189 (0.0192)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [430/985]  eta: 0:08:51  lr: 0.000017  loss: 0.0188 (0.0192)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [440/985]  eta: 0:08:42  lr: 0.000017  loss: 0.0188 (0.0192)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [450/985]  eta: 0:08:32  lr: 0.000017  loss: 0.0198 (0.0193)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [460/985]  eta: 0:08:23  lr: 0.000017  loss: 0.0190 (0.0193)  time: 0.9606  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [470/985]  eta: 0:08:13  lr: 0.000017  loss: 0.0185 (0.0193)  time: 0.9621  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [480/985]  eta: 0:08:03  lr: 0.000017  loss: 0.0179 (0.0193)  time: 0.9544  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:813]  [490/985]  eta: 0:07:54  lr: 0.000017  loss: 0.0177 (0.0193)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [500/985]  eta: 0:07:44  lr: 0.000017  loss: 0.0185 (0.0193)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [510/985]  eta: 0:07:35  lr: 0.000017  loss: 0.0186 (0.0193)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [520/985]  eta: 0:07:25  lr: 0.000017  loss: 0.0188 (0.0193)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [530/985]  eta: 0:07:15  lr: 0.000017  loss: 0.0181 (0.0193)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [540/985]  eta: 0:07:06  lr: 0.000017  loss: 0.0178 (0.0193)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [550/985]  eta: 0:06:56  lr: 0.000017  loss: 0.0177 (0.0192)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [560/985]  eta: 0:06:47  lr: 0.000017  loss: 0.0165 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [570/985]  eta: 0:06:37  lr: 0.000017  loss: 0.0165 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [580/985]  eta: 0:06:27  lr: 0.000017  loss: 0.0174 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [590/985]  eta: 0:06:18  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [600/985]  eta: 0:06:08  lr: 0.000017  loss: 0.0167 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [610/985]  eta: 0:05:59  lr: 0.000017  loss: 0.0168 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [620/985]  eta: 0:05:49  lr: 0.000017  loss: 0.0186 (0.0192)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [630/985]  eta: 0:05:39  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [640/985]  eta: 0:05:30  lr: 0.000017  loss: 0.0170 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [650/985]  eta: 0:05:20  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [660/985]  eta: 0:05:11  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [670/985]  eta: 0:05:01  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [680/985]  eta: 0:04:52  lr: 0.000017  loss: 0.0170 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [690/985]  eta: 0:04:42  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [700/985]  eta: 0:04:32  lr: 0.000017  loss: 0.0197 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [710/985]  eta: 0:04:23  lr: 0.000017  loss: 0.0202 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:813]  [720/985]  eta: 0:04:13  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [730/985]  eta: 0:04:04  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [740/985]  eta: 0:03:54  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [750/985]  eta: 0:03:45  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [760/985]  eta: 0:03:35  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [770/985]  eta: 0:03:25  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [780/985]  eta: 0:03:16  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [790/985]  eta: 0:03:06  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [800/985]  eta: 0:02:57  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [810/985]  eta: 0:02:47  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [820/985]  eta: 0:02:37  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [830/985]  eta: 0:02:28  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [840/985]  eta: 0:02:18  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [850/985]  eta: 0:02:09  lr: 0.000017  loss: 0.0169 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [860/985]  eta: 0:01:59  lr: 0.000017  loss: 0.0169 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [870/985]  eta: 0:01:50  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [880/985]  eta: 0:01:40  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [890/985]  eta: 0:01:30  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [900/985]  eta: 0:01:21  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [910/985]  eta: 0:01:11  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [920/985]  eta: 0:01:02  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [930/985]  eta: 0:00:52  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [940/985]  eta: 0:00:43  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [950/985]  eta: 0:00:33  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [960/985]  eta: 0:00:23  lr: 0.000017  loss: 0.0192 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [970/985]  eta: 0:00:14  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [980/985]  eta: 0:00:04  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813]  [984/985]  eta: 0:00:00  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:813] Total time: 0:15:42 (0.9567 s / it)\n",
      "Averaged stats: lr: 0.000017  loss: 0.0190 (0.0191)\n",
      "Valid: [epoch:813]  [ 0/14]  eta: 0:02:57  loss: 0.0131 (0.0131)  time: 12.6567  data: 0.5936  max mem: 41892\n",
      "Valid: [epoch:813]  [13/14]  eta: 0:00:12  loss: 0.0147 (0.0146)  time: 12.0090  data: 0.0425  max mem: 41892\n",
      "Valid: [epoch:813] Total time: 0:02:48 (12.0156 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_813_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:814]  [  0/985]  eta: 1:09:26  lr: 0.000017  loss: 0.0235 (0.0235)  time: 4.2297  data: 3.1390  max mem: 41892\n",
      "Train: [epoch:814]  [ 10/985]  eta: 0:20:18  lr: 0.000017  loss: 0.0184 (0.0191)  time: 1.2495  data: 0.2855  max mem: 41892\n",
      "Train: [epoch:814]  [ 20/985]  eta: 0:17:44  lr: 0.000017  loss: 0.0177 (0.0183)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [ 30/985]  eta: 0:16:41  lr: 0.000017  loss: 0.0186 (0.0188)  time: 0.9386  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [ 40/985]  eta: 0:16:08  lr: 0.000017  loss: 0.0164 (0.0182)  time: 0.9430  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [ 50/985]  eta: 0:15:44  lr: 0.000017  loss: 0.0156 (0.0180)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [ 60/985]  eta: 0:15:27  lr: 0.000017  loss: 0.0168 (0.0180)  time: 0.9579  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:814]  [ 70/985]  eta: 0:15:11  lr: 0.000017  loss: 0.0170 (0.0179)  time: 0.9618  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:814]  [ 80/985]  eta: 0:14:57  lr: 0.000017  loss: 0.0170 (0.0181)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [ 90/985]  eta: 0:14:44  lr: 0.000017  loss: 0.0181 (0.0182)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [100/985]  eta: 0:14:31  lr: 0.000017  loss: 0.0176 (0.0182)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [110/985]  eta: 0:14:18  lr: 0.000017  loss: 0.0176 (0.0183)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [120/985]  eta: 0:14:06  lr: 0.000017  loss: 0.0188 (0.0184)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [130/985]  eta: 0:13:55  lr: 0.000017  loss: 0.0188 (0.0185)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [140/985]  eta: 0:13:44  lr: 0.000017  loss: 0.0181 (0.0185)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [150/985]  eta: 0:13:32  lr: 0.000017  loss: 0.0184 (0.0187)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [160/985]  eta: 0:13:22  lr: 0.000017  loss: 0.0186 (0.0187)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [170/985]  eta: 0:13:11  lr: 0.000017  loss: 0.0181 (0.0188)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [180/985]  eta: 0:13:00  lr: 0.000017  loss: 0.0198 (0.0189)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [190/985]  eta: 0:12:50  lr: 0.000017  loss: 0.0204 (0.0189)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [200/985]  eta: 0:12:39  lr: 0.000017  loss: 0.0179 (0.0189)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [210/985]  eta: 0:12:29  lr: 0.000017  loss: 0.0168 (0.0188)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [220/985]  eta: 0:12:19  lr: 0.000017  loss: 0.0168 (0.0187)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [230/985]  eta: 0:12:09  lr: 0.000017  loss: 0.0179 (0.0188)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [240/985]  eta: 0:11:59  lr: 0.000017  loss: 0.0185 (0.0187)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [250/985]  eta: 0:11:48  lr: 0.000017  loss: 0.0179 (0.0188)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [260/985]  eta: 0:11:38  lr: 0.000017  loss: 0.0171 (0.0187)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [270/985]  eta: 0:11:28  lr: 0.000017  loss: 0.0164 (0.0187)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [280/985]  eta: 0:11:18  lr: 0.000017  loss: 0.0171 (0.0186)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [290/985]  eta: 0:11:09  lr: 0.000017  loss: 0.0184 (0.0187)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [300/985]  eta: 0:10:59  lr: 0.000017  loss: 0.0190 (0.0187)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [310/985]  eta: 0:10:49  lr: 0.000017  loss: 0.0181 (0.0188)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [320/985]  eta: 0:10:39  lr: 0.000017  loss: 0.0176 (0.0188)  time: 0.9553  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:814]  [330/985]  eta: 0:10:29  lr: 0.000017  loss: 0.0177 (0.0188)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [340/985]  eta: 0:10:20  lr: 0.000017  loss: 0.0189 (0.0188)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [350/985]  eta: 0:10:10  lr: 0.000017  loss: 0.0204 (0.0189)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [360/985]  eta: 0:10:00  lr: 0.000017  loss: 0.0186 (0.0189)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [370/985]  eta: 0:09:50  lr: 0.000017  loss: 0.0181 (0.0189)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [380/985]  eta: 0:09:41  lr: 0.000017  loss: 0.0183 (0.0189)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [390/985]  eta: 0:09:31  lr: 0.000017  loss: 0.0183 (0.0189)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [400/985]  eta: 0:09:21  lr: 0.000017  loss: 0.0189 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [410/985]  eta: 0:09:12  lr: 0.000017  loss: 0.0180 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [420/985]  eta: 0:09:02  lr: 0.000017  loss: 0.0179 (0.0189)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [430/985]  eta: 0:08:52  lr: 0.000017  loss: 0.0204 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [440/985]  eta: 0:08:43  lr: 0.000017  loss: 0.0185 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [450/985]  eta: 0:08:33  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [460/985]  eta: 0:08:23  lr: 0.000017  loss: 0.0176 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [470/985]  eta: 0:08:14  lr: 0.000017  loss: 0.0169 (0.0189)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [480/985]  eta: 0:08:04  lr: 0.000017  loss: 0.0181 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [490/985]  eta: 0:07:54  lr: 0.000017  loss: 0.0188 (0.0189)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [500/985]  eta: 0:07:45  lr: 0.000017  loss: 0.0206 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [510/985]  eta: 0:07:35  lr: 0.000017  loss: 0.0186 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [520/985]  eta: 0:07:26  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [530/985]  eta: 0:07:16  lr: 0.000017  loss: 0.0182 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [540/985]  eta: 0:07:06  lr: 0.000017  loss: 0.0177 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [550/985]  eta: 0:06:57  lr: 0.000017  loss: 0.0184 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [560/985]  eta: 0:06:47  lr: 0.000017  loss: 0.0188 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [570/985]  eta: 0:06:38  lr: 0.000017  loss: 0.0186 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [580/985]  eta: 0:06:28  lr: 0.000017  loss: 0.0190 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [590/985]  eta: 0:06:18  lr: 0.000017  loss: 0.0196 (0.0190)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [600/985]  eta: 0:06:09  lr: 0.000017  loss: 0.0185 (0.0190)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [610/985]  eta: 0:05:59  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [620/985]  eta: 0:05:50  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [630/985]  eta: 0:05:40  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [640/985]  eta: 0:05:30  lr: 0.000017  loss: 0.0174 (0.0190)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [650/985]  eta: 0:05:21  lr: 0.000017  loss: 0.0166 (0.0189)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [660/985]  eta: 0:05:11  lr: 0.000017  loss: 0.0169 (0.0190)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [670/985]  eta: 0:05:02  lr: 0.000017  loss: 0.0185 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [680/985]  eta: 0:04:52  lr: 0.000017  loss: 0.0171 (0.0189)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [690/985]  eta: 0:04:42  lr: 0.000017  loss: 0.0167 (0.0189)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [700/985]  eta: 0:04:33  lr: 0.000017  loss: 0.0189 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [710/985]  eta: 0:04:23  lr: 0.000017  loss: 0.0189 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [720/985]  eta: 0:04:14  lr: 0.000017  loss: 0.0180 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [730/985]  eta: 0:04:04  lr: 0.000017  loss: 0.0180 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [740/985]  eta: 0:03:54  lr: 0.000017  loss: 0.0176 (0.0190)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [750/985]  eta: 0:03:45  lr: 0.000017  loss: 0.0193 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [760/985]  eta: 0:03:35  lr: 0.000017  loss: 0.0196 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [770/985]  eta: 0:03:26  lr: 0.000017  loss: 0.0182 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [780/985]  eta: 0:03:16  lr: 0.000017  loss: 0.0182 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [790/985]  eta: 0:03:06  lr: 0.000017  loss: 0.0185 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [800/985]  eta: 0:02:57  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [810/985]  eta: 0:02:47  lr: 0.000017  loss: 0.0186 (0.0190)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [820/985]  eta: 0:02:38  lr: 0.000017  loss: 0.0196 (0.0190)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [830/985]  eta: 0:02:28  lr: 0.000017  loss: 0.0191 (0.0190)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [840/985]  eta: 0:02:19  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [850/985]  eta: 0:02:09  lr: 0.000017  loss: 0.0170 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [860/985]  eta: 0:01:59  lr: 0.000017  loss: 0.0172 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [870/985]  eta: 0:01:50  lr: 0.000017  loss: 0.0182 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [880/985]  eta: 0:01:40  lr: 0.000017  loss: 0.0170 (0.0190)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [890/985]  eta: 0:01:31  lr: 0.000017  loss: 0.0170 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [900/985]  eta: 0:01:21  lr: 0.000017  loss: 0.0193 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [910/985]  eta: 0:01:11  lr: 0.000017  loss: 0.0193 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [920/985]  eta: 0:01:02  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [930/985]  eta: 0:00:52  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [940/985]  eta: 0:00:43  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [950/985]  eta: 0:00:33  lr: 0.000017  loss: 0.0193 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [960/985]  eta: 0:00:23  lr: 0.000017  loss: 0.0196 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [970/985]  eta: 0:00:14  lr: 0.000017  loss: 0.0193 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814]  [980/985]  eta: 0:00:04  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9521  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:814]  [984/985]  eta: 0:00:00  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:814] Total time: 0:15:44 (0.9584 s / it)\n",
      "Averaged stats: lr: 0.000017  loss: 0.0182 (0.0191)\n",
      "Valid: [epoch:814]  [ 0/14]  eta: 0:02:51  loss: 0.0136 (0.0136)  time: 12.2564  data: 0.5769  max mem: 41892\n",
      "Valid: [epoch:814]  [13/14]  eta: 0:00:11  loss: 0.0147 (0.0146)  time: 11.7435  data: 0.0413  max mem: 41892\n",
      "Valid: [epoch:814] Total time: 0:02:44 (11.7506 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_814_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:815]  [  0/985]  eta: 1:07:53  lr: 0.000017  loss: 0.0160 (0.0160)  time: 4.1359  data: 3.0404  max mem: 41892\n",
      "Train: [epoch:815]  [ 10/985]  eta: 0:20:04  lr: 0.000017  loss: 0.0201 (0.0201)  time: 1.2353  data: 0.2765  max mem: 41892\n",
      "Train: [epoch:815]  [ 20/985]  eta: 0:17:34  lr: 0.000017  loss: 0.0190 (0.0193)  time: 0.9411  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [ 30/985]  eta: 0:16:36  lr: 0.000017  loss: 0.0175 (0.0192)  time: 0.9372  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [ 40/985]  eta: 0:16:02  lr: 0.000017  loss: 0.0171 (0.0188)  time: 0.9391  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [ 50/985]  eta: 0:15:37  lr: 0.000017  loss: 0.0169 (0.0190)  time: 0.9411  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [ 60/985]  eta: 0:15:18  lr: 0.000017  loss: 0.0197 (0.0191)  time: 0.9423  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [ 70/985]  eta: 0:15:03  lr: 0.000017  loss: 0.0165 (0.0187)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [ 80/985]  eta: 0:14:48  lr: 0.000017  loss: 0.0165 (0.0188)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [ 90/985]  eta: 0:14:35  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9453  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [100/985]  eta: 0:14:24  lr: 0.000017  loss: 0.0186 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [110/985]  eta: 0:14:12  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [120/985]  eta: 0:14:01  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [130/985]  eta: 0:13:49  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [140/985]  eta: 0:13:38  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [150/985]  eta: 0:13:28  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [160/985]  eta: 0:13:18  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [170/985]  eta: 0:13:07  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [180/985]  eta: 0:12:57  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [190/985]  eta: 0:12:47  lr: 0.000017  loss: 0.0198 (0.0192)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [200/985]  eta: 0:12:37  lr: 0.000017  loss: 0.0194 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [210/985]  eta: 0:12:27  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [220/985]  eta: 0:12:17  lr: 0.000017  loss: 0.0172 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [230/985]  eta: 0:12:07  lr: 0.000017  loss: 0.0170 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [240/985]  eta: 0:11:57  lr: 0.000017  loss: 0.0188 (0.0190)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [250/985]  eta: 0:11:47  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [260/985]  eta: 0:11:37  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [270/985]  eta: 0:11:27  lr: 0.000017  loss: 0.0191 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [280/985]  eta: 0:11:18  lr: 0.000017  loss: 0.0178 (0.0190)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [290/985]  eta: 0:11:08  lr: 0.000017  loss: 0.0173 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [300/985]  eta: 0:10:58  lr: 0.000017  loss: 0.0195 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [310/985]  eta: 0:10:48  lr: 0.000017  loss: 0.0196 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [320/985]  eta: 0:10:38  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [330/985]  eta: 0:10:28  lr: 0.000017  loss: 0.0194 (0.0192)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [340/985]  eta: 0:10:19  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [350/985]  eta: 0:10:09  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [360/985]  eta: 0:09:59  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [370/985]  eta: 0:09:49  lr: 0.000017  loss: 0.0166 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [380/985]  eta: 0:09:40  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [390/985]  eta: 0:09:30  lr: 0.000017  loss: 0.0192 (0.0191)  time: 0.9491  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [400/985]  eta: 0:09:20  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [410/985]  eta: 0:09:11  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [420/985]  eta: 0:09:01  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [430/985]  eta: 0:08:51  lr: 0.000017  loss: 0.0194 (0.0192)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [440/985]  eta: 0:08:42  lr: 0.000017  loss: 0.0197 (0.0192)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [450/985]  eta: 0:08:32  lr: 0.000017  loss: 0.0164 (0.0192)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [460/985]  eta: 0:08:22  lr: 0.000017  loss: 0.0164 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [470/985]  eta: 0:08:13  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [480/985]  eta: 0:08:03  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [490/985]  eta: 0:07:53  lr: 0.000017  loss: 0.0189 (0.0192)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [500/985]  eta: 0:07:44  lr: 0.000017  loss: 0.0207 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [510/985]  eta: 0:07:34  lr: 0.000017  loss: 0.0180 (0.0192)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [520/985]  eta: 0:07:25  lr: 0.000017  loss: 0.0181 (0.0192)  time: 0.9639  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [530/985]  eta: 0:07:15  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9675  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [540/985]  eta: 0:07:06  lr: 0.000017  loss: 0.0184 (0.0192)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [550/985]  eta: 0:06:56  lr: 0.000017  loss: 0.0185 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [560/985]  eta: 0:06:46  lr: 0.000017  loss: 0.0184 (0.0192)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [570/985]  eta: 0:06:37  lr: 0.000017  loss: 0.0184 (0.0192)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [580/985]  eta: 0:06:27  lr: 0.000017  loss: 0.0172 (0.0192)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [590/985]  eta: 0:06:18  lr: 0.000017  loss: 0.0183 (0.0192)  time: 0.9517  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:815]  [600/985]  eta: 0:06:08  lr: 0.000017  loss: 0.0183 (0.0192)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [610/985]  eta: 0:05:58  lr: 0.000017  loss: 0.0173 (0.0192)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [620/985]  eta: 0:05:49  lr: 0.000017  loss: 0.0180 (0.0192)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [630/985]  eta: 0:05:39  lr: 0.000017  loss: 0.0181 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [640/985]  eta: 0:05:30  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [650/985]  eta: 0:05:20  lr: 0.000017  loss: 0.0182 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [660/985]  eta: 0:05:10  lr: 0.000017  loss: 0.0193 (0.0192)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [670/985]  eta: 0:05:01  lr: 0.000017  loss: 0.0194 (0.0192)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [680/985]  eta: 0:04:51  lr: 0.000017  loss: 0.0185 (0.0192)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [690/985]  eta: 0:04:42  lr: 0.000017  loss: 0.0180 (0.0192)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [700/985]  eta: 0:04:32  lr: 0.000017  loss: 0.0183 (0.0192)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [710/985]  eta: 0:04:23  lr: 0.000017  loss: 0.0190 (0.0192)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [720/985]  eta: 0:04:13  lr: 0.000017  loss: 0.0193 (0.0192)  time: 0.9664  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [730/985]  eta: 0:04:03  lr: 0.000017  loss: 0.0171 (0.0192)  time: 0.9626  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [740/985]  eta: 0:03:54  lr: 0.000017  loss: 0.0169 (0.0192)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [750/985]  eta: 0:03:44  lr: 0.000017  loss: 0.0183 (0.0192)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [760/985]  eta: 0:03:35  lr: 0.000017  loss: 0.0183 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [770/985]  eta: 0:03:25  lr: 0.000017  loss: 0.0166 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [780/985]  eta: 0:03:16  lr: 0.000017  loss: 0.0176 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [790/985]  eta: 0:03:06  lr: 0.000017  loss: 0.0178 (0.0192)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [800/985]  eta: 0:02:56  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [810/985]  eta: 0:02:47  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [820/985]  eta: 0:02:37  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [830/985]  eta: 0:02:28  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [840/985]  eta: 0:02:18  lr: 0.000017  loss: 0.0167 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [850/985]  eta: 0:02:09  lr: 0.000017  loss: 0.0164 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [860/985]  eta: 0:01:59  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [870/985]  eta: 0:01:49  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [880/985]  eta: 0:01:40  lr: 0.000017  loss: 0.0171 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [890/985]  eta: 0:01:30  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [900/985]  eta: 0:01:21  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [910/985]  eta: 0:01:11  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [920/985]  eta: 0:01:02  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [930/985]  eta: 0:00:52  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [940/985]  eta: 0:00:43  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [950/985]  eta: 0:00:33  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [960/985]  eta: 0:00:23  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [970/985]  eta: 0:00:14  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [980/985]  eta: 0:00:04  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815]  [984/985]  eta: 0:00:00  lr: 0.000017  loss: 0.0196 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:815] Total time: 0:15:42 (0.9564 s / it)\n",
      "Averaged stats: lr: 0.000017  loss: 0.0196 (0.0191)\n",
      "Valid: [epoch:815]  [ 0/14]  eta: 0:03:24  loss: 0.0155 (0.0155)  time: 14.6291  data: 0.5462  max mem: 41892\n",
      "Valid: [epoch:815]  [13/14]  eta: 0:00:12  loss: 0.0149 (0.0148)  time: 12.7893  data: 0.0391  max mem: 41892\n",
      "Valid: [epoch:815] Total time: 0:02:59 (12.7952 s / it)\n",
      "Averaged stats: loss: 0.0149 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_815_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:816]  [  0/985]  eta: 1:13:49  lr: 0.000017  loss: 0.0195 (0.0195)  time: 4.4969  data: 3.5165  max mem: 41892\n",
      "Train: [epoch:816]  [ 10/985]  eta: 0:20:35  lr: 0.000017  loss: 0.0184 (0.0186)  time: 1.2670  data: 0.3198  max mem: 41892\n",
      "Train: [epoch:816]  [ 20/985]  eta: 0:17:49  lr: 0.000017  loss: 0.0184 (0.0196)  time: 0.9391  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [ 30/985]  eta: 0:16:46  lr: 0.000017  loss: 0.0180 (0.0192)  time: 0.9365  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [ 40/985]  eta: 0:16:09  lr: 0.000017  loss: 0.0173 (0.0185)  time: 0.9386  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [ 50/985]  eta: 0:15:45  lr: 0.000017  loss: 0.0155 (0.0181)  time: 0.9459  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [ 60/985]  eta: 0:15:25  lr: 0.000017  loss: 0.0176 (0.0186)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [ 70/985]  eta: 0:15:09  lr: 0.000017  loss: 0.0178 (0.0185)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [ 80/985]  eta: 0:14:54  lr: 0.000017  loss: 0.0167 (0.0184)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [ 90/985]  eta: 0:14:39  lr: 0.000017  loss: 0.0167 (0.0184)  time: 0.9453  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [100/985]  eta: 0:14:27  lr: 0.000017  loss: 0.0173 (0.0183)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [110/985]  eta: 0:14:15  lr: 0.000017  loss: 0.0178 (0.0184)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [120/985]  eta: 0:14:04  lr: 0.000017  loss: 0.0176 (0.0185)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [130/985]  eta: 0:13:53  lr: 0.000017  loss: 0.0173 (0.0186)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [140/985]  eta: 0:13:42  lr: 0.000017  loss: 0.0184 (0.0186)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [150/985]  eta: 0:13:31  lr: 0.000017  loss: 0.0188 (0.0186)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [160/985]  eta: 0:13:20  lr: 0.000017  loss: 0.0168 (0.0185)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [170/985]  eta: 0:13:10  lr: 0.000017  loss: 0.0180 (0.0186)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [180/985]  eta: 0:12:59  lr: 0.000017  loss: 0.0189 (0.0186)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [190/985]  eta: 0:12:49  lr: 0.000017  loss: 0.0189 (0.0187)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [200/985]  eta: 0:12:39  lr: 0.000017  loss: 0.0188 (0.0188)  time: 0.9552  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:816]  [210/985]  eta: 0:12:29  lr: 0.000017  loss: 0.0188 (0.0189)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [220/985]  eta: 0:12:19  lr: 0.000017  loss: 0.0185 (0.0189)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [230/985]  eta: 0:12:09  lr: 0.000017  loss: 0.0185 (0.0189)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [240/985]  eta: 0:11:59  lr: 0.000017  loss: 0.0184 (0.0188)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [250/985]  eta: 0:11:49  lr: 0.000017  loss: 0.0182 (0.0189)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [260/985]  eta: 0:11:39  lr: 0.000017  loss: 0.0175 (0.0189)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [270/985]  eta: 0:11:29  lr: 0.000017  loss: 0.0172 (0.0189)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [280/985]  eta: 0:11:19  lr: 0.000017  loss: 0.0179 (0.0189)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [290/985]  eta: 0:11:09  lr: 0.000017  loss: 0.0179 (0.0189)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [300/985]  eta: 0:10:59  lr: 0.000017  loss: 0.0179 (0.0189)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [310/985]  eta: 0:10:50  lr: 0.000017  loss: 0.0193 (0.0189)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [320/985]  eta: 0:10:40  lr: 0.000017  loss: 0.0177 (0.0189)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [330/985]  eta: 0:10:30  lr: 0.000017  loss: 0.0180 (0.0189)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [340/985]  eta: 0:10:20  lr: 0.000017  loss: 0.0178 (0.0189)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [350/985]  eta: 0:10:11  lr: 0.000017  loss: 0.0178 (0.0189)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [360/985]  eta: 0:10:01  lr: 0.000017  loss: 0.0186 (0.0189)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [370/985]  eta: 0:09:51  lr: 0.000017  loss: 0.0196 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [380/985]  eta: 0:09:41  lr: 0.000017  loss: 0.0204 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [390/985]  eta: 0:09:32  lr: 0.000017  loss: 0.0191 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [400/985]  eta: 0:09:22  lr: 0.000017  loss: 0.0191 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [410/985]  eta: 0:09:12  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [420/985]  eta: 0:09:02  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [430/985]  eta: 0:08:53  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [440/985]  eta: 0:08:43  lr: 0.000017  loss: 0.0193 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [450/985]  eta: 0:08:33  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [460/985]  eta: 0:08:24  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [470/985]  eta: 0:08:14  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [480/985]  eta: 0:08:04  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [490/985]  eta: 0:07:54  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [500/985]  eta: 0:07:45  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [510/985]  eta: 0:07:35  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [520/985]  eta: 0:07:25  lr: 0.000017  loss: 0.0191 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [530/985]  eta: 0:07:16  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [540/985]  eta: 0:07:06  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [550/985]  eta: 0:06:57  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [560/985]  eta: 0:06:47  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [570/985]  eta: 0:06:37  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [580/985]  eta: 0:06:28  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [590/985]  eta: 0:06:18  lr: 0.000017  loss: 0.0197 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [600/985]  eta: 0:06:08  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [610/985]  eta: 0:05:59  lr: 0.000017  loss: 0.0167 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [620/985]  eta: 0:05:49  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [630/985]  eta: 0:05:40  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [640/985]  eta: 0:05:30  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [650/985]  eta: 0:05:20  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [660/985]  eta: 0:05:11  lr: 0.000017  loss: 0.0194 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [670/985]  eta: 0:05:01  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [680/985]  eta: 0:04:52  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [690/985]  eta: 0:04:42  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [700/985]  eta: 0:04:32  lr: 0.000017  loss: 0.0210 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [710/985]  eta: 0:04:23  lr: 0.000017  loss: 0.0214 (0.0191)  time: 0.9657  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [720/985]  eta: 0:04:13  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9646  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [730/985]  eta: 0:04:04  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [740/985]  eta: 0:03:54  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [750/985]  eta: 0:03:45  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [760/985]  eta: 0:03:35  lr: 0.000017  loss: 0.0188 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [770/985]  eta: 0:03:25  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [780/985]  eta: 0:03:16  lr: 0.000017  loss: 0.0167 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [790/985]  eta: 0:03:06  lr: 0.000017  loss: 0.0171 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [800/985]  eta: 0:02:57  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [810/985]  eta: 0:02:47  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [820/985]  eta: 0:02:37  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [830/985]  eta: 0:02:28  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [840/985]  eta: 0:02:18  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [850/985]  eta: 0:02:09  lr: 0.000017  loss: 0.0169 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [860/985]  eta: 0:01:59  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:816]  [870/985]  eta: 0:01:50  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [880/985]  eta: 0:01:40  lr: 0.000017  loss: 0.0168 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [890/985]  eta: 0:01:30  lr: 0.000017  loss: 0.0171 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [900/985]  eta: 0:01:21  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [910/985]  eta: 0:01:11  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [920/985]  eta: 0:01:02  lr: 0.000017  loss: 0.0193 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [930/985]  eta: 0:00:52  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [940/985]  eta: 0:00:43  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [950/985]  eta: 0:00:33  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [960/985]  eta: 0:00:23  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [970/985]  eta: 0:00:14  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [980/985]  eta: 0:00:04  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816]  [984/985]  eta: 0:00:00  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:816] Total time: 0:15:43 (0.9575 s / it)\n",
      "Averaged stats: lr: 0.000017  loss: 0.0181 (0.0191)\n",
      "Valid: [epoch:816]  [ 0/14]  eta: 0:03:25  loss: 0.0151 (0.0151)  time: 14.6570  data: 0.5828  max mem: 41892\n",
      "Valid: [epoch:816]  [13/14]  eta: 0:00:13  loss: 0.0148 (0.0147)  time: 13.9791  data: 0.0417  max mem: 41892\n",
      "Valid: [epoch:816] Total time: 0:03:15 (13.9852 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_816_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 803.000\n",
      "Train: [epoch:817]  [  0/985]  eta: 1:07:38  lr: 0.000017  loss: 0.0183 (0.0183)  time: 4.1207  data: 3.0684  max mem: 41892\n",
      "Train: [epoch:817]  [ 10/985]  eta: 0:20:01  lr: 0.000017  loss: 0.0187 (0.0191)  time: 1.2320  data: 0.2791  max mem: 41892\n",
      "Train: [epoch:817]  [ 20/985]  eta: 0:17:36  lr: 0.000017  loss: 0.0195 (0.0198)  time: 0.9433  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [ 30/985]  eta: 0:16:36  lr: 0.000017  loss: 0.0192 (0.0193)  time: 0.9394  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [ 40/985]  eta: 0:16:01  lr: 0.000017  loss: 0.0163 (0.0185)  time: 0.9365  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [ 50/985]  eta: 0:15:37  lr: 0.000017  loss: 0.0174 (0.0188)  time: 0.9397  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [ 60/985]  eta: 0:15:17  lr: 0.000017  loss: 0.0184 (0.0188)  time: 0.9411  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [ 70/985]  eta: 0:15:03  lr: 0.000017  loss: 0.0179 (0.0189)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [ 80/985]  eta: 0:14:48  lr: 0.000017  loss: 0.0179 (0.0188)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [ 90/985]  eta: 0:14:35  lr: 0.000017  loss: 0.0172 (0.0188)  time: 0.9446  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [100/985]  eta: 0:14:22  lr: 0.000017  loss: 0.0187 (0.0187)  time: 0.9466  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [110/985]  eta: 0:14:10  lr: 0.000017  loss: 0.0188 (0.0189)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [120/985]  eta: 0:14:00  lr: 0.000017  loss: 0.0190 (0.0189)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [130/985]  eta: 0:13:49  lr: 0.000017  loss: 0.0183 (0.0189)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [140/985]  eta: 0:13:38  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [150/985]  eta: 0:13:27  lr: 0.000017  loss: 0.0193 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [160/985]  eta: 0:13:17  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [170/985]  eta: 0:13:06  lr: 0.000017  loss: 0.0183 (0.0191)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [180/985]  eta: 0:12:57  lr: 0.000017  loss: 0.0196 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [190/985]  eta: 0:12:47  lr: 0.000017  loss: 0.0200 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [200/985]  eta: 0:12:36  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [210/985]  eta: 0:12:26  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [220/985]  eta: 0:12:16  lr: 0.000017  loss: 0.0182 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [230/985]  eta: 0:12:06  lr: 0.000017  loss: 0.0180 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [240/985]  eta: 0:11:56  lr: 0.000017  loss: 0.0193 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [250/985]  eta: 0:11:47  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [260/985]  eta: 0:11:37  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [270/985]  eta: 0:11:27  lr: 0.000017  loss: 0.0171 (0.0190)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [280/985]  eta: 0:11:17  lr: 0.000017  loss: 0.0171 (0.0190)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [290/985]  eta: 0:11:07  lr: 0.000017  loss: 0.0174 (0.0189)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [300/985]  eta: 0:10:57  lr: 0.000017  loss: 0.0184 (0.0190)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [310/985]  eta: 0:10:47  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [320/985]  eta: 0:10:38  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [330/985]  eta: 0:10:28  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [340/985]  eta: 0:10:18  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [350/985]  eta: 0:10:09  lr: 0.000017  loss: 0.0172 (0.0190)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [360/985]  eta: 0:09:59  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [370/985]  eta: 0:09:49  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [380/985]  eta: 0:09:40  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [390/985]  eta: 0:09:30  lr: 0.000017  loss: 0.0200 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [400/985]  eta: 0:09:20  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [410/985]  eta: 0:09:10  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [420/985]  eta: 0:09:01  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [430/985]  eta: 0:08:51  lr: 0.000017  loss: 0.0179 (0.0191)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [440/985]  eta: 0:08:41  lr: 0.000017  loss: 0.0169 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [450/985]  eta: 0:08:32  lr: 0.000017  loss: 0.0175 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [460/985]  eta: 0:08:22  lr: 0.000017  loss: 0.0189 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [470/985]  eta: 0:08:13  lr: 0.000017  loss: 0.0194 (0.0192)  time: 0.9523  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:817]  [480/985]  eta: 0:08:03  lr: 0.000017  loss: 0.0181 (0.0192)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [490/985]  eta: 0:07:53  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [500/985]  eta: 0:07:44  lr: 0.000017  loss: 0.0190 (0.0192)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [510/985]  eta: 0:07:34  lr: 0.000017  loss: 0.0192 (0.0192)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [520/985]  eta: 0:07:25  lr: 0.000017  loss: 0.0185 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [530/985]  eta: 0:07:15  lr: 0.000017  loss: 0.0172 (0.0192)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [540/985]  eta: 0:07:05  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [550/985]  eta: 0:06:56  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [560/985]  eta: 0:06:46  lr: 0.000017  loss: 0.0178 (0.0192)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [570/985]  eta: 0:06:37  lr: 0.000017  loss: 0.0171 (0.0192)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [580/985]  eta: 0:06:27  lr: 0.000017  loss: 0.0168 (0.0192)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [590/985]  eta: 0:06:17  lr: 0.000017  loss: 0.0196 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [600/985]  eta: 0:06:08  lr: 0.000017  loss: 0.0194 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [610/985]  eta: 0:05:58  lr: 0.000017  loss: 0.0175 (0.0192)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [620/985]  eta: 0:05:49  lr: 0.000017  loss: 0.0193 (0.0192)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [630/985]  eta: 0:05:39  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [640/985]  eta: 0:05:29  lr: 0.000017  loss: 0.0171 (0.0192)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [650/985]  eta: 0:05:20  lr: 0.000017  loss: 0.0179 (0.0192)  time: 0.9638  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [660/985]  eta: 0:05:10  lr: 0.000017  loss: 0.0188 (0.0192)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [670/985]  eta: 0:05:01  lr: 0.000017  loss: 0.0164 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [680/985]  eta: 0:04:51  lr: 0.000017  loss: 0.0170 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [690/985]  eta: 0:04:42  lr: 0.000017  loss: 0.0174 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [700/985]  eta: 0:04:32  lr: 0.000017  loss: 0.0190 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [710/985]  eta: 0:04:22  lr: 0.000017  loss: 0.0217 (0.0192)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [720/985]  eta: 0:04:13  lr: 0.000017  loss: 0.0188 (0.0192)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [730/985]  eta: 0:04:03  lr: 0.000017  loss: 0.0172 (0.0192)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [740/985]  eta: 0:03:54  lr: 0.000017  loss: 0.0177 (0.0192)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [750/985]  eta: 0:03:44  lr: 0.000017  loss: 0.0193 (0.0192)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [760/985]  eta: 0:03:35  lr: 0.000017  loss: 0.0192 (0.0192)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [770/985]  eta: 0:03:25  lr: 0.000017  loss: 0.0188 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [780/985]  eta: 0:03:15  lr: 0.000017  loss: 0.0183 (0.0192)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [790/985]  eta: 0:03:06  lr: 0.000017  loss: 0.0178 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [800/985]  eta: 0:02:56  lr: 0.000017  loss: 0.0172 (0.0192)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [810/985]  eta: 0:02:47  lr: 0.000017  loss: 0.0182 (0.0192)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [820/985]  eta: 0:02:37  lr: 0.000017  loss: 0.0186 (0.0192)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [830/985]  eta: 0:02:28  lr: 0.000017  loss: 0.0185 (0.0192)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [840/985]  eta: 0:02:18  lr: 0.000017  loss: 0.0175 (0.0192)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [850/985]  eta: 0:02:09  lr: 0.000017  loss: 0.0170 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [860/985]  eta: 0:01:59  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [870/985]  eta: 0:01:49  lr: 0.000017  loss: 0.0169 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [880/985]  eta: 0:01:40  lr: 0.000017  loss: 0.0166 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [890/985]  eta: 0:01:30  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [900/985]  eta: 0:01:21  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [910/985]  eta: 0:01:11  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [920/985]  eta: 0:01:02  lr: 0.000017  loss: 0.0177 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [930/985]  eta: 0:00:52  lr: 0.000017  loss: 0.0172 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [940/985]  eta: 0:00:42  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [950/985]  eta: 0:00:33  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [960/985]  eta: 0:00:23  lr: 0.000017  loss: 0.0187 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [970/985]  eta: 0:00:14  lr: 0.000017  loss: 0.0176 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [980/985]  eta: 0:00:04  lr: 0.000017  loss: 0.0173 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817]  [984/985]  eta: 0:00:00  lr: 0.000017  loss: 0.0182 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:817] Total time: 0:15:41 (0.9555 s / it)\n",
      "Averaged stats: lr: 0.000017  loss: 0.0182 (0.0191)\n",
      "Valid: [epoch:817]  [ 0/14]  eta: 0:02:54  loss: 0.0146 (0.0146)  time: 12.4644  data: 0.8233  max mem: 41892\n",
      "Valid: [epoch:817]  [13/14]  eta: 0:00:11  loss: 0.0144 (0.0143)  time: 11.8127  data: 0.0589  max mem: 41892\n",
      "Valid: [epoch:817] Total time: 0:02:45 (11.8188 s / it)\n",
      "Averaged stats: loss: 0.0144 (0.0143)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_817_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:818]  [  0/985]  eta: 1:11:54  lr: 0.000017  loss: 0.0188 (0.0188)  time: 4.3797  data: 3.3867  max mem: 41892\n",
      "Train: [epoch:818]  [ 10/985]  eta: 0:20:27  lr: 0.000017  loss: 0.0188 (0.0212)  time: 1.2587  data: 0.3080  max mem: 41892\n",
      "Train: [epoch:818]  [ 20/985]  eta: 0:17:47  lr: 0.000017  loss: 0.0192 (0.0204)  time: 0.9424  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [ 30/985]  eta: 0:16:48  lr: 0.000017  loss: 0.0178 (0.0193)  time: 0.9450  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [ 40/985]  eta: 0:16:10  lr: 0.000017  loss: 0.0167 (0.0187)  time: 0.9449  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [ 50/985]  eta: 0:15:45  lr: 0.000017  loss: 0.0162 (0.0187)  time: 0.9402  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [ 60/985]  eta: 0:15:24  lr: 0.000017  loss: 0.0189 (0.0188)  time: 0.9425  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [ 70/985]  eta: 0:15:07  lr: 0.000017  loss: 0.0172 (0.0188)  time: 0.9450  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [ 80/985]  eta: 0:14:54  lr: 0.000017  loss: 0.0165 (0.0187)  time: 0.9532  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:818]  [ 90/985]  eta: 0:14:40  lr: 0.000017  loss: 0.0168 (0.0187)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [100/985]  eta: 0:14:27  lr: 0.000017  loss: 0.0180 (0.0188)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [110/985]  eta: 0:14:15  lr: 0.000017  loss: 0.0183 (0.0188)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [120/985]  eta: 0:14:03  lr: 0.000017  loss: 0.0185 (0.0189)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [130/985]  eta: 0:13:53  lr: 0.000017  loss: 0.0182 (0.0188)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [140/985]  eta: 0:13:42  lr: 0.000017  loss: 0.0175 (0.0189)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [150/985]  eta: 0:13:31  lr: 0.000017  loss: 0.0175 (0.0189)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [160/985]  eta: 0:13:20  lr: 0.000017  loss: 0.0182 (0.0189)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [170/985]  eta: 0:13:10  lr: 0.000017  loss: 0.0180 (0.0188)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [180/985]  eta: 0:12:59  lr: 0.000017  loss: 0.0173 (0.0188)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [190/985]  eta: 0:12:49  lr: 0.000017  loss: 0.0186 (0.0189)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [200/985]  eta: 0:12:39  lr: 0.000017  loss: 0.0187 (0.0189)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [210/985]  eta: 0:12:28  lr: 0.000017  loss: 0.0171 (0.0189)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [220/985]  eta: 0:12:18  lr: 0.000017  loss: 0.0158 (0.0188)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [230/985]  eta: 0:12:08  lr: 0.000017  loss: 0.0164 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [240/985]  eta: 0:11:58  lr: 0.000017  loss: 0.0183 (0.0188)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [250/985]  eta: 0:11:48  lr: 0.000017  loss: 0.0183 (0.0188)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [260/985]  eta: 0:11:38  lr: 0.000017  loss: 0.0166 (0.0188)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [270/985]  eta: 0:11:28  lr: 0.000017  loss: 0.0174 (0.0187)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [280/985]  eta: 0:11:18  lr: 0.000017  loss: 0.0173 (0.0187)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [290/985]  eta: 0:11:09  lr: 0.000017  loss: 0.0173 (0.0187)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [300/985]  eta: 0:10:59  lr: 0.000017  loss: 0.0176 (0.0187)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [310/985]  eta: 0:10:49  lr: 0.000017  loss: 0.0180 (0.0187)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [320/985]  eta: 0:10:39  lr: 0.000017  loss: 0.0177 (0.0187)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [330/985]  eta: 0:10:29  lr: 0.000017  loss: 0.0178 (0.0187)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [340/985]  eta: 0:10:20  lr: 0.000017  loss: 0.0190 (0.0188)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [350/985]  eta: 0:10:10  lr: 0.000017  loss: 0.0187 (0.0188)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [360/985]  eta: 0:10:00  lr: 0.000017  loss: 0.0188 (0.0188)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [370/985]  eta: 0:09:51  lr: 0.000017  loss: 0.0189 (0.0188)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [380/985]  eta: 0:09:41  lr: 0.000017  loss: 0.0189 (0.0188)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [390/985]  eta: 0:09:31  lr: 0.000017  loss: 0.0190 (0.0188)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [400/985]  eta: 0:09:22  lr: 0.000017  loss: 0.0201 (0.0189)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [410/985]  eta: 0:09:12  lr: 0.000017  loss: 0.0202 (0.0189)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [420/985]  eta: 0:09:02  lr: 0.000017  loss: 0.0183 (0.0190)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [430/985]  eta: 0:08:52  lr: 0.000017  loss: 0.0179 (0.0190)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [440/985]  eta: 0:08:43  lr: 0.000017  loss: 0.0185 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [450/985]  eta: 0:08:33  lr: 0.000017  loss: 0.0183 (0.0190)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [460/985]  eta: 0:08:23  lr: 0.000017  loss: 0.0184 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [470/985]  eta: 0:08:14  lr: 0.000017  loss: 0.0174 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [480/985]  eta: 0:08:04  lr: 0.000017  loss: 0.0174 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [490/985]  eta: 0:07:55  lr: 0.000017  loss: 0.0176 (0.0190)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [500/985]  eta: 0:07:45  lr: 0.000017  loss: 0.0183 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [510/985]  eta: 0:07:35  lr: 0.000017  loss: 0.0183 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [520/985]  eta: 0:07:26  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [530/985]  eta: 0:07:16  lr: 0.000017  loss: 0.0177 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [540/985]  eta: 0:07:06  lr: 0.000017  loss: 0.0177 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [550/985]  eta: 0:06:57  lr: 0.000017  loss: 0.0199 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [560/985]  eta: 0:06:47  lr: 0.000017  loss: 0.0184 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [570/985]  eta: 0:06:38  lr: 0.000017  loss: 0.0179 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [580/985]  eta: 0:06:28  lr: 0.000017  loss: 0.0179 (0.0190)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [590/985]  eta: 0:06:18  lr: 0.000017  loss: 0.0191 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [600/985]  eta: 0:06:09  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [610/985]  eta: 0:05:59  lr: 0.000017  loss: 0.0172 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [620/985]  eta: 0:05:50  lr: 0.000017  loss: 0.0172 (0.0190)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [630/985]  eta: 0:05:40  lr: 0.000017  loss: 0.0172 (0.0190)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [640/985]  eta: 0:05:30  lr: 0.000017  loss: 0.0160 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [650/985]  eta: 0:05:21  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [660/985]  eta: 0:05:11  lr: 0.000017  loss: 0.0187 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [670/985]  eta: 0:05:02  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [680/985]  eta: 0:04:52  lr: 0.000017  loss: 0.0165 (0.0189)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [690/985]  eta: 0:04:42  lr: 0.000017  loss: 0.0172 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [700/985]  eta: 0:04:33  lr: 0.000017  loss: 0.0188 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [710/985]  eta: 0:04:23  lr: 0.000017  loss: 0.0186 (0.0189)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [720/985]  eta: 0:04:14  lr: 0.000017  loss: 0.0175 (0.0190)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [730/985]  eta: 0:04:04  lr: 0.000017  loss: 0.0173 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [740/985]  eta: 0:03:54  lr: 0.000017  loss: 0.0167 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:818]  [750/985]  eta: 0:03:45  lr: 0.000017  loss: 0.0204 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [760/985]  eta: 0:03:35  lr: 0.000017  loss: 0.0187 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [770/985]  eta: 0:03:26  lr: 0.000017  loss: 0.0180 (0.0190)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [780/985]  eta: 0:03:16  lr: 0.000017  loss: 0.0183 (0.0190)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [790/985]  eta: 0:03:06  lr: 0.000017  loss: 0.0202 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [800/985]  eta: 0:02:57  lr: 0.000017  loss: 0.0189 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [810/985]  eta: 0:02:47  lr: 0.000017  loss: 0.0181 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [820/985]  eta: 0:02:38  lr: 0.000017  loss: 0.0188 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [830/985]  eta: 0:02:28  lr: 0.000017  loss: 0.0191 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [840/985]  eta: 0:02:18  lr: 0.000017  loss: 0.0178 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [850/985]  eta: 0:02:09  lr: 0.000017  loss: 0.0179 (0.0190)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [860/985]  eta: 0:01:59  lr: 0.000017  loss: 0.0180 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [870/985]  eta: 0:01:50  lr: 0.000017  loss: 0.0174 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [880/985]  eta: 0:01:40  lr: 0.000017  loss: 0.0174 (0.0190)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [890/985]  eta: 0:01:31  lr: 0.000017  loss: 0.0187 (0.0190)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [900/985]  eta: 0:01:21  lr: 0.000017  loss: 0.0196 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [910/985]  eta: 0:01:11  lr: 0.000017  loss: 0.0192 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [920/985]  eta: 0:01:02  lr: 0.000017  loss: 0.0189 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [930/985]  eta: 0:00:52  lr: 0.000017  loss: 0.0184 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [940/985]  eta: 0:00:43  lr: 0.000017  loss: 0.0178 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [950/985]  eta: 0:00:33  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [960/985]  eta: 0:00:23  lr: 0.000017  loss: 0.0181 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [970/985]  eta: 0:00:14  lr: 0.000017  loss: 0.0185 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [980/985]  eta: 0:00:04  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818]  [984/985]  eta: 0:00:00  lr: 0.000017  loss: 0.0186 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:818] Total time: 0:15:43 (0.9583 s / it)\n",
      "Averaged stats: lr: 0.000017  loss: 0.0186 (0.0191)\n",
      "Valid: [epoch:818]  [ 0/14]  eta: 0:02:58  loss: 0.0150 (0.0150)  time: 12.7396  data: 0.5716  max mem: 41892\n",
      "Valid: [epoch:818]  [13/14]  eta: 0:00:12  loss: 0.0147 (0.0147)  time: 12.0816  data: 0.0409  max mem: 41892\n",
      "Valid: [epoch:818] Total time: 0:02:49 (12.0882 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_818_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:819]  [  0/985]  eta: 0:54:09  lr: 0.000016  loss: 0.0205 (0.0205)  time: 3.2993  data: 2.2935  max mem: 41892\n",
      "Train: [epoch:819]  [ 10/985]  eta: 0:19:01  lr: 0.000016  loss: 0.0195 (0.0198)  time: 1.1703  data: 0.2086  max mem: 41892\n",
      "Train: [epoch:819]  [ 20/985]  eta: 0:17:02  lr: 0.000016  loss: 0.0184 (0.0198)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [ 30/985]  eta: 0:16:14  lr: 0.000016  loss: 0.0183 (0.0198)  time: 0.9378  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [ 40/985]  eta: 0:15:46  lr: 0.000016  loss: 0.0180 (0.0192)  time: 0.9414  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [ 50/985]  eta: 0:15:25  lr: 0.000016  loss: 0.0168 (0.0187)  time: 0.9437  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [ 60/985]  eta: 0:15:09  lr: 0.000016  loss: 0.0188 (0.0193)  time: 0.9457  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [ 70/985]  eta: 0:14:54  lr: 0.000016  loss: 0.0188 (0.0190)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [ 80/985]  eta: 0:14:42  lr: 0.000016  loss: 0.0157 (0.0186)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [ 90/985]  eta: 0:14:30  lr: 0.000016  loss: 0.0162 (0.0187)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [100/985]  eta: 0:14:18  lr: 0.000016  loss: 0.0188 (0.0186)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [110/985]  eta: 0:14:08  lr: 0.000016  loss: 0.0191 (0.0189)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [120/985]  eta: 0:13:57  lr: 0.000016  loss: 0.0187 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [130/985]  eta: 0:13:46  lr: 0.000016  loss: 0.0185 (0.0190)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [140/985]  eta: 0:13:36  lr: 0.000016  loss: 0.0184 (0.0190)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [150/985]  eta: 0:13:26  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [160/985]  eta: 0:13:16  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [170/985]  eta: 0:13:06  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [180/985]  eta: 0:12:56  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [190/985]  eta: 0:12:46  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [200/985]  eta: 0:12:36  lr: 0.000016  loss: 0.0191 (0.0192)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [210/985]  eta: 0:12:26  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [220/985]  eta: 0:12:16  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [230/985]  eta: 0:12:06  lr: 0.000016  loss: 0.0167 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [240/985]  eta: 0:11:56  lr: 0.000016  loss: 0.0172 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [250/985]  eta: 0:11:47  lr: 0.000016  loss: 0.0170 (0.0190)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [260/985]  eta: 0:11:37  lr: 0.000016  loss: 0.0173 (0.0190)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [270/985]  eta: 0:11:27  lr: 0.000016  loss: 0.0165 (0.0189)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [280/985]  eta: 0:11:17  lr: 0.000016  loss: 0.0174 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [290/985]  eta: 0:11:08  lr: 0.000016  loss: 0.0191 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [300/985]  eta: 0:10:58  lr: 0.000016  loss: 0.0191 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [310/985]  eta: 0:10:48  lr: 0.000016  loss: 0.0181 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [320/985]  eta: 0:10:39  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [330/985]  eta: 0:10:29  lr: 0.000016  loss: 0.0194 (0.0192)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [340/985]  eta: 0:10:19  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [350/985]  eta: 0:10:10  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9615  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:819]  [360/985]  eta: 0:10:00  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [370/985]  eta: 0:09:50  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [380/985]  eta: 0:09:41  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [390/985]  eta: 0:09:31  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [400/985]  eta: 0:09:21  lr: 0.000016  loss: 0.0200 (0.0192)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [410/985]  eta: 0:09:12  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [420/985]  eta: 0:09:02  lr: 0.000016  loss: 0.0167 (0.0191)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [430/985]  eta: 0:08:53  lr: 0.000016  loss: 0.0188 (0.0192)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [440/985]  eta: 0:08:43  lr: 0.000016  loss: 0.0188 (0.0192)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [450/985]  eta: 0:08:33  lr: 0.000016  loss: 0.0182 (0.0192)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [460/985]  eta: 0:08:24  lr: 0.000016  loss: 0.0186 (0.0192)  time: 0.9679  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [470/985]  eta: 0:08:14  lr: 0.000016  loss: 0.0181 (0.0192)  time: 0.9646  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [480/985]  eta: 0:08:05  lr: 0.000016  loss: 0.0177 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [490/985]  eta: 0:07:55  lr: 0.000016  loss: 0.0190 (0.0192)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [500/985]  eta: 0:07:45  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [510/985]  eta: 0:07:36  lr: 0.000016  loss: 0.0179 (0.0192)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [520/985]  eta: 0:07:26  lr: 0.000016  loss: 0.0187 (0.0192)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [530/985]  eta: 0:07:16  lr: 0.000016  loss: 0.0182 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [540/985]  eta: 0:07:07  lr: 0.000016  loss: 0.0173 (0.0192)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [550/985]  eta: 0:06:57  lr: 0.000016  loss: 0.0173 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [560/985]  eta: 0:06:48  lr: 0.000016  loss: 0.0178 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [570/985]  eta: 0:06:38  lr: 0.000016  loss: 0.0173 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [580/985]  eta: 0:06:28  lr: 0.000016  loss: 0.0175 (0.0192)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [590/985]  eta: 0:06:19  lr: 0.000016  loss: 0.0186 (0.0192)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [600/985]  eta: 0:06:09  lr: 0.000016  loss: 0.0189 (0.0192)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [610/985]  eta: 0:05:59  lr: 0.000016  loss: 0.0160 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [620/985]  eta: 0:05:50  lr: 0.000016  loss: 0.0185 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [630/985]  eta: 0:05:40  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [640/985]  eta: 0:05:31  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [650/985]  eta: 0:05:21  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [660/985]  eta: 0:05:11  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [670/985]  eta: 0:05:02  lr: 0.000016  loss: 0.0170 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [680/985]  eta: 0:04:52  lr: 0.000016  loss: 0.0168 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [690/985]  eta: 0:04:42  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [700/985]  eta: 0:04:33  lr: 0.000016  loss: 0.0178 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [710/985]  eta: 0:04:23  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [720/985]  eta: 0:04:14  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [730/985]  eta: 0:04:04  lr: 0.000016  loss: 0.0173 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [740/985]  eta: 0:03:54  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9636  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [750/985]  eta: 0:03:45  lr: 0.000016  loss: 0.0193 (0.0191)  time: 0.9633  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [760/985]  eta: 0:03:35  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0199 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [780/985]  eta: 0:03:16  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [800/985]  eta: 0:02:57  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [810/985]  eta: 0:02:47  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [830/985]  eta: 0:02:28  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [860/985]  eta: 0:01:59  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [880/985]  eta: 0:01:40  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [910/985]  eta: 0:01:11  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0169 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0199 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [960/985]  eta: 0:00:23  lr: 0.000016  loss: 0.0196 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0175 (0.0191)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0175 (0.0191)  time: 0.9483  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:819] Total time: 0:15:43 (0.9583 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0175 (0.0191)\n",
      "Valid: [epoch:819]  [ 0/14]  eta: 0:02:56  loss: 0.0141 (0.0141)  time: 12.6027  data: 0.5819  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:819]  [13/14]  eta: 0:00:11  loss: 0.0150 (0.0149)  time: 11.9805  data: 0.0416  max mem: 41892\n",
      "Valid: [epoch:819] Total time: 0:02:47 (11.9868 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_819_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:820]  [  0/985]  eta: 0:58:37  lr: 0.000016  loss: 0.0272 (0.0272)  time: 3.5711  data: 2.5869  max mem: 41892\n",
      "Train: [epoch:820]  [ 10/985]  eta: 0:19:19  lr: 0.000016  loss: 0.0183 (0.0209)  time: 1.1891  data: 0.2353  max mem: 41892\n",
      "Train: [epoch:820]  [ 20/985]  eta: 0:17:10  lr: 0.000016  loss: 0.0182 (0.0197)  time: 0.9431  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [ 30/985]  eta: 0:16:20  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9366  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [ 40/985]  eta: 0:15:49  lr: 0.000016  loss: 0.0165 (0.0187)  time: 0.9391  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [ 50/985]  eta: 0:15:28  lr: 0.000016  loss: 0.0167 (0.0184)  time: 0.9412  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [ 60/985]  eta: 0:15:10  lr: 0.000016  loss: 0.0176 (0.0184)  time: 0.9425  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [ 70/985]  eta: 0:14:57  lr: 0.000016  loss: 0.0180 (0.0184)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [ 80/985]  eta: 0:14:43  lr: 0.000016  loss: 0.0172 (0.0183)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [ 90/985]  eta: 0:14:30  lr: 0.000016  loss: 0.0175 (0.0184)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [100/985]  eta: 0:14:19  lr: 0.000016  loss: 0.0183 (0.0185)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [110/985]  eta: 0:14:08  lr: 0.000016  loss: 0.0179 (0.0186)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [120/985]  eta: 0:13:57  lr: 0.000016  loss: 0.0176 (0.0186)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [130/985]  eta: 0:13:46  lr: 0.000016  loss: 0.0187 (0.0187)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [140/985]  eta: 0:13:36  lr: 0.000016  loss: 0.0193 (0.0189)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [150/985]  eta: 0:13:25  lr: 0.000016  loss: 0.0193 (0.0189)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [160/985]  eta: 0:13:15  lr: 0.000016  loss: 0.0192 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [170/985]  eta: 0:13:05  lr: 0.000016  loss: 0.0178 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [180/985]  eta: 0:12:55  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [190/985]  eta: 0:12:45  lr: 0.000016  loss: 0.0188 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [200/985]  eta: 0:12:35  lr: 0.000016  loss: 0.0180 (0.0190)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [210/985]  eta: 0:12:25  lr: 0.000016  loss: 0.0166 (0.0189)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [220/985]  eta: 0:12:15  lr: 0.000016  loss: 0.0166 (0.0189)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [230/985]  eta: 0:12:06  lr: 0.000016  loss: 0.0190 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [240/985]  eta: 0:11:56  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [250/985]  eta: 0:11:47  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9657  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [260/985]  eta: 0:11:37  lr: 0.000016  loss: 0.0178 (0.0192)  time: 0.9644  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [270/985]  eta: 0:11:27  lr: 0.000016  loss: 0.0167 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [280/985]  eta: 0:11:18  lr: 0.000016  loss: 0.0161 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [290/985]  eta: 0:11:08  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [300/985]  eta: 0:10:58  lr: 0.000016  loss: 0.0183 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [310/985]  eta: 0:10:48  lr: 0.000016  loss: 0.0174 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [320/985]  eta: 0:10:39  lr: 0.000016  loss: 0.0183 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [330/985]  eta: 0:10:29  lr: 0.000016  loss: 0.0192 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [340/985]  eta: 0:10:19  lr: 0.000016  loss: 0.0187 (0.0190)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [350/985]  eta: 0:10:10  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [360/985]  eta: 0:10:00  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [370/985]  eta: 0:09:50  lr: 0.000016  loss: 0.0186 (0.0192)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [380/985]  eta: 0:09:41  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [390/985]  eta: 0:09:31  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [400/985]  eta: 0:09:21  lr: 0.000016  loss: 0.0185 (0.0192)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [410/985]  eta: 0:09:12  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [420/985]  eta: 0:09:02  lr: 0.000016  loss: 0.0173 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [430/985]  eta: 0:08:52  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [440/985]  eta: 0:08:43  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [450/985]  eta: 0:08:33  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [460/985]  eta: 0:08:24  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [470/985]  eta: 0:08:14  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [480/985]  eta: 0:08:04  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [490/985]  eta: 0:07:55  lr: 0.000016  loss: 0.0191 (0.0192)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [500/985]  eta: 0:07:45  lr: 0.000016  loss: 0.0191 (0.0192)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [510/985]  eta: 0:07:35  lr: 0.000016  loss: 0.0190 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [520/985]  eta: 0:07:26  lr: 0.000016  loss: 0.0189 (0.0192)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [530/985]  eta: 0:07:16  lr: 0.000016  loss: 0.0184 (0.0192)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [540/985]  eta: 0:07:07  lr: 0.000016  loss: 0.0182 (0.0192)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [550/985]  eta: 0:06:57  lr: 0.000016  loss: 0.0173 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [560/985]  eta: 0:06:47  lr: 0.000016  loss: 0.0178 (0.0192)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [570/985]  eta: 0:06:38  lr: 0.000016  loss: 0.0178 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [580/985]  eta: 0:06:28  lr: 0.000016  loss: 0.0179 (0.0192)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [590/985]  eta: 0:06:19  lr: 0.000016  loss: 0.0191 (0.0192)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [600/985]  eta: 0:06:09  lr: 0.000016  loss: 0.0184 (0.0192)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [610/985]  eta: 0:05:59  lr: 0.000016  loss: 0.0178 (0.0192)  time: 0.9661  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [620/985]  eta: 0:05:50  lr: 0.000016  loss: 0.0175 (0.0192)  time: 0.9655  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:820]  [630/985]  eta: 0:05:40  lr: 0.000016  loss: 0.0170 (0.0191)  time: 0.9631  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [640/985]  eta: 0:05:31  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9667  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [650/985]  eta: 0:05:21  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [660/985]  eta: 0:05:11  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [670/985]  eta: 0:05:02  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9642  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [680/985]  eta: 0:04:52  lr: 0.000016  loss: 0.0168 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [690/985]  eta: 0:04:43  lr: 0.000016  loss: 0.0168 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [700/985]  eta: 0:04:33  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [710/985]  eta: 0:04:23  lr: 0.000016  loss: 0.0196 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [720/985]  eta: 0:04:14  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [730/985]  eta: 0:04:04  lr: 0.000016  loss: 0.0172 (0.0191)  time: 0.9547  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:820]  [740/985]  eta: 0:03:55  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [750/985]  eta: 0:03:45  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [760/985]  eta: 0:03:35  lr: 0.000016  loss: 0.0208 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0197 (0.0191)  time: 0.9595  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:820]  [780/985]  eta: 0:03:16  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9599  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:820]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [800/985]  eta: 0:02:57  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [810/985]  eta: 0:02:47  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [830/985]  eta: 0:02:28  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0175 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [860/985]  eta: 0:01:59  lr: 0.000016  loss: 0.0170 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [880/985]  eta: 0:01:40  lr: 0.000016  loss: 0.0163 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0175 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [910/985]  eta: 0:01:11  lr: 0.000016  loss: 0.0196 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0205 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0205 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [960/985]  eta: 0:00:23  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:820] Total time: 0:15:44 (0.9589 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0189 (0.0191)\n",
      "Valid: [epoch:820]  [ 0/14]  eta: 0:02:58  loss: 0.0171 (0.0171)  time: 12.7627  data: 0.5569  max mem: 41892\n",
      "Valid: [epoch:820]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0145)  time: 12.0366  data: 0.0399  max mem: 41892\n",
      "Valid: [epoch:820] Total time: 0:02:48 (12.0429 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_820_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:821]  [  0/985]  eta: 1:24:18  lr: 0.000016  loss: 0.0186 (0.0186)  time: 5.1360  data: 4.1337  max mem: 41892\n",
      "Train: [epoch:821]  [ 10/985]  eta: 0:21:29  lr: 0.000016  loss: 0.0186 (0.0185)  time: 1.3224  data: 0.3759  max mem: 41892\n",
      "Train: [epoch:821]  [ 20/985]  eta: 0:18:20  lr: 0.000016  loss: 0.0176 (0.0187)  time: 0.9407  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [ 30/985]  eta: 0:17:10  lr: 0.000016  loss: 0.0173 (0.0184)  time: 0.9447  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [ 40/985]  eta: 0:16:27  lr: 0.000016  loss: 0.0165 (0.0182)  time: 0.9441  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [ 50/985]  eta: 0:15:58  lr: 0.000016  loss: 0.0163 (0.0180)  time: 0.9415  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [ 60/985]  eta: 0:15:36  lr: 0.000016  loss: 0.0173 (0.0183)  time: 0.9453  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [ 70/985]  eta: 0:15:17  lr: 0.000016  loss: 0.0173 (0.0181)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [ 80/985]  eta: 0:15:03  lr: 0.000016  loss: 0.0161 (0.0180)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [ 90/985]  eta: 0:14:48  lr: 0.000016  loss: 0.0172 (0.0181)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [100/985]  eta: 0:14:35  lr: 0.000016  loss: 0.0195 (0.0184)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [110/985]  eta: 0:14:22  lr: 0.000016  loss: 0.0193 (0.0184)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [120/985]  eta: 0:14:10  lr: 0.000016  loss: 0.0184 (0.0186)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [130/985]  eta: 0:13:59  lr: 0.000016  loss: 0.0191 (0.0188)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [140/985]  eta: 0:13:48  lr: 0.000016  loss: 0.0190 (0.0189)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [150/985]  eta: 0:13:36  lr: 0.000016  loss: 0.0177 (0.0189)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [160/985]  eta: 0:13:25  lr: 0.000016  loss: 0.0174 (0.0189)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [170/985]  eta: 0:13:14  lr: 0.000016  loss: 0.0174 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [180/985]  eta: 0:13:04  lr: 0.000016  loss: 0.0182 (0.0189)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [190/985]  eta: 0:12:54  lr: 0.000016  loss: 0.0182 (0.0189)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [200/985]  eta: 0:12:43  lr: 0.000016  loss: 0.0182 (0.0189)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [210/985]  eta: 0:12:33  lr: 0.000016  loss: 0.0179 (0.0188)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [220/985]  eta: 0:12:22  lr: 0.000016  loss: 0.0168 (0.0188)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [230/985]  eta: 0:12:12  lr: 0.000016  loss: 0.0178 (0.0188)  time: 0.9616  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:821]  [240/985]  eta: 0:12:03  lr: 0.000016  loss: 0.0184 (0.0189)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [250/985]  eta: 0:11:52  lr: 0.000016  loss: 0.0180 (0.0189)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [260/985]  eta: 0:11:42  lr: 0.000016  loss: 0.0177 (0.0189)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [270/985]  eta: 0:11:32  lr: 0.000016  loss: 0.0169 (0.0188)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [280/985]  eta: 0:11:22  lr: 0.000016  loss: 0.0179 (0.0188)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [290/985]  eta: 0:11:12  lr: 0.000016  loss: 0.0187 (0.0189)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [300/985]  eta: 0:11:02  lr: 0.000016  loss: 0.0187 (0.0189)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [310/985]  eta: 0:10:52  lr: 0.000016  loss: 0.0175 (0.0189)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [320/985]  eta: 0:10:42  lr: 0.000016  loss: 0.0162 (0.0189)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [330/985]  eta: 0:10:32  lr: 0.000016  loss: 0.0168 (0.0188)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [340/985]  eta: 0:10:23  lr: 0.000016  loss: 0.0170 (0.0188)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [350/985]  eta: 0:10:13  lr: 0.000016  loss: 0.0170 (0.0188)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [360/985]  eta: 0:10:03  lr: 0.000016  loss: 0.0173 (0.0188)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [370/985]  eta: 0:09:53  lr: 0.000016  loss: 0.0173 (0.0188)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [380/985]  eta: 0:09:43  lr: 0.000016  loss: 0.0166 (0.0187)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [390/985]  eta: 0:09:34  lr: 0.000016  loss: 0.0177 (0.0188)  time: 0.9643  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [400/985]  eta: 0:09:24  lr: 0.000016  loss: 0.0200 (0.0188)  time: 0.9667  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [410/985]  eta: 0:09:14  lr: 0.000016  loss: 0.0201 (0.0189)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [420/985]  eta: 0:09:04  lr: 0.000016  loss: 0.0186 (0.0188)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [430/985]  eta: 0:08:55  lr: 0.000016  loss: 0.0175 (0.0188)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [440/985]  eta: 0:08:45  lr: 0.000016  loss: 0.0176 (0.0188)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [450/985]  eta: 0:08:35  lr: 0.000016  loss: 0.0176 (0.0188)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [460/985]  eta: 0:08:25  lr: 0.000016  loss: 0.0171 (0.0188)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [470/985]  eta: 0:08:16  lr: 0.000016  loss: 0.0177 (0.0188)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [480/985]  eta: 0:08:06  lr: 0.000016  loss: 0.0186 (0.0188)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [490/985]  eta: 0:07:56  lr: 0.000016  loss: 0.0177 (0.0188)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [500/985]  eta: 0:07:47  lr: 0.000016  loss: 0.0191 (0.0189)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [510/985]  eta: 0:07:37  lr: 0.000016  loss: 0.0197 (0.0189)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [520/985]  eta: 0:07:27  lr: 0.000016  loss: 0.0190 (0.0189)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [530/985]  eta: 0:07:18  lr: 0.000016  loss: 0.0190 (0.0189)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [540/985]  eta: 0:07:08  lr: 0.000016  loss: 0.0175 (0.0189)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [550/985]  eta: 0:06:58  lr: 0.000016  loss: 0.0174 (0.0189)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [560/985]  eta: 0:06:49  lr: 0.000016  loss: 0.0194 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [570/985]  eta: 0:06:39  lr: 0.000016  loss: 0.0182 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [580/985]  eta: 0:06:29  lr: 0.000016  loss: 0.0185 (0.0190)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [590/985]  eta: 0:06:20  lr: 0.000016  loss: 0.0198 (0.0190)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [600/985]  eta: 0:06:10  lr: 0.000016  loss: 0.0179 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [610/985]  eta: 0:06:00  lr: 0.000016  loss: 0.0180 (0.0190)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [620/985]  eta: 0:05:51  lr: 0.000016  loss: 0.0186 (0.0190)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [630/985]  eta: 0:05:41  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [640/985]  eta: 0:05:31  lr: 0.000016  loss: 0.0172 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [650/985]  eta: 0:05:22  lr: 0.000016  loss: 0.0180 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [660/985]  eta: 0:05:12  lr: 0.000016  loss: 0.0186 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [670/985]  eta: 0:05:02  lr: 0.000016  loss: 0.0181 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [680/985]  eta: 0:04:53  lr: 0.000016  loss: 0.0186 (0.0190)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [690/985]  eta: 0:04:43  lr: 0.000016  loss: 0.0179 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [700/985]  eta: 0:04:34  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [710/985]  eta: 0:04:24  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [720/985]  eta: 0:04:14  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [730/985]  eta: 0:04:05  lr: 0.000016  loss: 0.0170 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [740/985]  eta: 0:03:55  lr: 0.000016  loss: 0.0171 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [750/985]  eta: 0:03:45  lr: 0.000016  loss: 0.0171 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [760/985]  eta: 0:03:36  lr: 0.000016  loss: 0.0183 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0189 (0.0189)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [780/985]  eta: 0:03:16  lr: 0.000016  loss: 0.0178 (0.0189)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0173 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [800/985]  eta: 0:02:57  lr: 0.000016  loss: 0.0181 (0.0189)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [810/985]  eta: 0:02:48  lr: 0.000016  loss: 0.0177 (0.0189)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0182 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [830/985]  eta: 0:02:28  lr: 0.000016  loss: 0.0183 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0193 (0.0190)  time: 0.9630  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [860/985]  eta: 0:02:00  lr: 0.000016  loss: 0.0208 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [880/985]  eta: 0:01:40  lr: 0.000016  loss: 0.0168 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0172 (0.0190)  time: 0.9612  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:821]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0183 (0.0190)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [910/985]  eta: 0:01:12  lr: 0.000016  loss: 0.0184 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0189 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0212 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0205 (0.0190)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [960/985]  eta: 0:00:24  lr: 0.000016  loss: 0.0198 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0202 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0200 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0202 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:821] Total time: 0:15:45 (0.9601 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0202 (0.0191)\n",
      "Valid: [epoch:821]  [ 0/14]  eta: 0:02:58  loss: 0.0147 (0.0147)  time: 12.7697  data: 0.5466  max mem: 41892\n",
      "Valid: [epoch:821]  [13/14]  eta: 0:00:11  loss: 0.0144 (0.0143)  time: 11.9990  data: 0.0391  max mem: 41892\n",
      "Valid: [epoch:821] Total time: 0:02:48 (12.0057 s / it)\n",
      "Averaged stats: loss: 0.0144 (0.0143)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_821_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:822]  [  0/985]  eta: 1:17:26  lr: 0.000016  loss: 0.0155 (0.0155)  time: 4.7175  data: 3.5844  max mem: 41892\n",
      "Train: [epoch:822]  [ 10/985]  eta: 0:21:01  lr: 0.000016  loss: 0.0185 (0.0188)  time: 1.2934  data: 0.3260  max mem: 41892\n",
      "Train: [epoch:822]  [ 20/985]  eta: 0:18:04  lr: 0.000016  loss: 0.0185 (0.0193)  time: 0.9439  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [ 30/985]  eta: 0:16:56  lr: 0.000016  loss: 0.0182 (0.0190)  time: 0.9386  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [ 40/985]  eta: 0:16:18  lr: 0.000016  loss: 0.0154 (0.0182)  time: 0.9432  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [ 50/985]  eta: 0:15:51  lr: 0.000016  loss: 0.0154 (0.0179)  time: 0.9461  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [ 60/985]  eta: 0:15:33  lr: 0.000016  loss: 0.0169 (0.0182)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [ 70/985]  eta: 0:15:15  lr: 0.000016  loss: 0.0175 (0.0183)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [ 80/985]  eta: 0:14:59  lr: 0.000016  loss: 0.0166 (0.0181)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [ 90/985]  eta: 0:14:46  lr: 0.000016  loss: 0.0166 (0.0180)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [100/985]  eta: 0:14:33  lr: 0.000016  loss: 0.0170 (0.0180)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [110/985]  eta: 0:14:21  lr: 0.000016  loss: 0.0179 (0.0180)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [120/985]  eta: 0:14:09  lr: 0.000016  loss: 0.0185 (0.0182)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [130/985]  eta: 0:13:57  lr: 0.000016  loss: 0.0197 (0.0184)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [140/985]  eta: 0:13:46  lr: 0.000016  loss: 0.0185 (0.0185)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [150/985]  eta: 0:13:35  lr: 0.000016  loss: 0.0182 (0.0185)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [160/985]  eta: 0:13:24  lr: 0.000016  loss: 0.0182 (0.0185)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [170/985]  eta: 0:13:13  lr: 0.000016  loss: 0.0191 (0.0186)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [180/985]  eta: 0:13:03  lr: 0.000016  loss: 0.0191 (0.0187)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [190/985]  eta: 0:12:53  lr: 0.000016  loss: 0.0189 (0.0188)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [200/985]  eta: 0:12:42  lr: 0.000016  loss: 0.0183 (0.0188)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [210/985]  eta: 0:12:32  lr: 0.000016  loss: 0.0185 (0.0188)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [220/985]  eta: 0:12:22  lr: 0.000016  loss: 0.0178 (0.0188)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [230/985]  eta: 0:12:12  lr: 0.000016  loss: 0.0172 (0.0188)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [240/985]  eta: 0:12:02  lr: 0.000016  loss: 0.0181 (0.0188)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [250/985]  eta: 0:11:52  lr: 0.000016  loss: 0.0188 (0.0188)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [260/985]  eta: 0:11:42  lr: 0.000016  loss: 0.0182 (0.0188)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [270/985]  eta: 0:11:32  lr: 0.000016  loss: 0.0172 (0.0188)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [280/985]  eta: 0:11:22  lr: 0.000016  loss: 0.0169 (0.0188)  time: 0.9575  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:822]  [290/985]  eta: 0:11:12  lr: 0.000016  loss: 0.0169 (0.0188)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [300/985]  eta: 0:11:02  lr: 0.000016  loss: 0.0174 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [310/985]  eta: 0:10:52  lr: 0.000016  loss: 0.0181 (0.0188)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [320/985]  eta: 0:10:42  lr: 0.000016  loss: 0.0175 (0.0188)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [330/985]  eta: 0:10:33  lr: 0.000016  loss: 0.0176 (0.0188)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [340/985]  eta: 0:10:23  lr: 0.000016  loss: 0.0176 (0.0187)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [350/985]  eta: 0:10:13  lr: 0.000016  loss: 0.0171 (0.0187)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [360/985]  eta: 0:10:03  lr: 0.000016  loss: 0.0174 (0.0187)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [370/985]  eta: 0:09:53  lr: 0.000016  loss: 0.0179 (0.0187)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [380/985]  eta: 0:09:44  lr: 0.000016  loss: 0.0184 (0.0187)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [390/985]  eta: 0:09:34  lr: 0.000016  loss: 0.0190 (0.0188)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [400/985]  eta: 0:09:24  lr: 0.000016  loss: 0.0188 (0.0188)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [410/985]  eta: 0:09:14  lr: 0.000016  loss: 0.0180 (0.0188)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [420/985]  eta: 0:09:05  lr: 0.000016  loss: 0.0176 (0.0188)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [430/985]  eta: 0:08:55  lr: 0.000016  loss: 0.0174 (0.0188)  time: 0.9677  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [440/985]  eta: 0:08:45  lr: 0.000016  loss: 0.0179 (0.0188)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [450/985]  eta: 0:08:36  lr: 0.000016  loss: 0.0173 (0.0188)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [460/985]  eta: 0:08:26  lr: 0.000016  loss: 0.0181 (0.0188)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [470/985]  eta: 0:08:16  lr: 0.000016  loss: 0.0193 (0.0188)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [480/985]  eta: 0:08:07  lr: 0.000016  loss: 0.0176 (0.0189)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [490/985]  eta: 0:07:57  lr: 0.000016  loss: 0.0203 (0.0189)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [500/985]  eta: 0:07:47  lr: 0.000016  loss: 0.0197 (0.0189)  time: 0.9551  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:822]  [510/985]  eta: 0:07:37  lr: 0.000016  loss: 0.0196 (0.0189)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [520/985]  eta: 0:07:28  lr: 0.000016  loss: 0.0182 (0.0189)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [530/985]  eta: 0:07:18  lr: 0.000016  loss: 0.0175 (0.0189)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [540/985]  eta: 0:07:08  lr: 0.000016  loss: 0.0172 (0.0189)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [550/985]  eta: 0:06:59  lr: 0.000016  loss: 0.0172 (0.0189)  time: 0.9637  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [560/985]  eta: 0:06:49  lr: 0.000016  loss: 0.0178 (0.0189)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [570/985]  eta: 0:06:39  lr: 0.000016  loss: 0.0199 (0.0189)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [580/985]  eta: 0:06:30  lr: 0.000016  loss: 0.0176 (0.0189)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [590/985]  eta: 0:06:20  lr: 0.000016  loss: 0.0175 (0.0189)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [600/985]  eta: 0:06:10  lr: 0.000016  loss: 0.0161 (0.0189)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [610/985]  eta: 0:06:01  lr: 0.000016  loss: 0.0178 (0.0189)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [620/985]  eta: 0:05:51  lr: 0.000016  loss: 0.0191 (0.0190)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [630/985]  eta: 0:05:41  lr: 0.000016  loss: 0.0204 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [640/985]  eta: 0:05:32  lr: 0.000016  loss: 0.0191 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [650/985]  eta: 0:05:22  lr: 0.000016  loss: 0.0195 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [660/985]  eta: 0:05:12  lr: 0.000016  loss: 0.0197 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [670/985]  eta: 0:05:03  lr: 0.000016  loss: 0.0165 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [680/985]  eta: 0:04:53  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [690/985]  eta: 0:04:43  lr: 0.000016  loss: 0.0183 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [700/985]  eta: 0:04:34  lr: 0.000016  loss: 0.0183 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [710/985]  eta: 0:04:24  lr: 0.000016  loss: 0.0184 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [720/985]  eta: 0:04:15  lr: 0.000016  loss: 0.0192 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [730/985]  eta: 0:04:05  lr: 0.000016  loss: 0.0192 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [740/985]  eta: 0:03:55  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [750/985]  eta: 0:03:46  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [760/985]  eta: 0:03:36  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [780/985]  eta: 0:03:17  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0173 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [800/985]  eta: 0:02:57  lr: 0.000016  loss: 0.0173 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [810/985]  eta: 0:02:48  lr: 0.000016  loss: 0.0193 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [830/985]  eta: 0:02:29  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0169 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0165 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [860/985]  eta: 0:02:00  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [880/985]  eta: 0:01:40  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [910/985]  eta: 0:01:12  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9665  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9637  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0200 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0200 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [960/985]  eta: 0:00:24  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0190 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0190 (0.0192)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0188 (0.0192)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:822] Total time: 0:15:46 (0.9608 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0188 (0.0192)\n",
      "Valid: [epoch:822]  [ 0/14]  eta: 0:02:57  loss: 0.0136 (0.0136)  time: 12.7005  data: 0.5622  max mem: 41892\n",
      "Valid: [epoch:822]  [13/14]  eta: 0:00:12  loss: 0.0148 (0.0147)  time: 12.1342  data: 0.0403  max mem: 41892\n",
      "Valid: [epoch:822] Total time: 0:02:49 (12.1414 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_822_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:823]  [  0/985]  eta: 1:24:04  lr: 0.000016  loss: 0.0177 (0.0177)  time: 5.1214  data: 4.1315  max mem: 41892\n",
      "Train: [epoch:823]  [ 10/985]  eta: 0:21:37  lr: 0.000016  loss: 0.0171 (0.0177)  time: 1.3306  data: 0.3757  max mem: 41892\n",
      "Train: [epoch:823]  [ 20/985]  eta: 0:18:24  lr: 0.000016  loss: 0.0171 (0.0178)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [ 30/985]  eta: 0:17:09  lr: 0.000016  loss: 0.0179 (0.0183)  time: 0.9391  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [ 40/985]  eta: 0:16:27  lr: 0.000016  loss: 0.0170 (0.0176)  time: 0.9405  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [ 50/985]  eta: 0:15:58  lr: 0.000016  loss: 0.0160 (0.0174)  time: 0.9425  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [ 60/985]  eta: 0:15:35  lr: 0.000016  loss: 0.0175 (0.0181)  time: 0.9436  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [ 70/985]  eta: 0:15:17  lr: 0.000016  loss: 0.0174 (0.0178)  time: 0.9459  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [ 80/985]  eta: 0:15:02  lr: 0.000016  loss: 0.0161 (0.0178)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [ 90/985]  eta: 0:14:47  lr: 0.000016  loss: 0.0180 (0.0180)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [100/985]  eta: 0:14:34  lr: 0.000016  loss: 0.0195 (0.0182)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [110/985]  eta: 0:14:21  lr: 0.000016  loss: 0.0183 (0.0182)  time: 0.9545  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:823]  [120/985]  eta: 0:14:09  lr: 0.000016  loss: 0.0180 (0.0182)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [130/985]  eta: 0:13:58  lr: 0.000016  loss: 0.0187 (0.0185)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [140/985]  eta: 0:13:46  lr: 0.000016  loss: 0.0193 (0.0185)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [150/985]  eta: 0:13:35  lr: 0.000016  loss: 0.0181 (0.0186)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [160/985]  eta: 0:13:25  lr: 0.000016  loss: 0.0178 (0.0186)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [170/985]  eta: 0:13:14  lr: 0.000016  loss: 0.0177 (0.0186)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [180/985]  eta: 0:13:03  lr: 0.000016  loss: 0.0174 (0.0186)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [190/985]  eta: 0:12:53  lr: 0.000016  loss: 0.0178 (0.0187)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [200/985]  eta: 0:12:43  lr: 0.000016  loss: 0.0181 (0.0187)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [210/985]  eta: 0:12:32  lr: 0.000016  loss: 0.0185 (0.0188)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [220/985]  eta: 0:12:22  lr: 0.000016  loss: 0.0182 (0.0188)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [230/985]  eta: 0:12:12  lr: 0.000016  loss: 0.0179 (0.0187)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [240/985]  eta: 0:12:02  lr: 0.000016  loss: 0.0187 (0.0188)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [250/985]  eta: 0:11:52  lr: 0.000016  loss: 0.0174 (0.0188)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [260/985]  eta: 0:11:42  lr: 0.000016  loss: 0.0168 (0.0187)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [270/985]  eta: 0:11:32  lr: 0.000016  loss: 0.0178 (0.0187)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [280/985]  eta: 0:11:22  lr: 0.000016  loss: 0.0175 (0.0187)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [290/985]  eta: 0:11:12  lr: 0.000016  loss: 0.0177 (0.0187)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [300/985]  eta: 0:11:02  lr: 0.000016  loss: 0.0180 (0.0188)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [310/985]  eta: 0:10:52  lr: 0.000016  loss: 0.0195 (0.0188)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [320/985]  eta: 0:10:42  lr: 0.000016  loss: 0.0199 (0.0188)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [330/985]  eta: 0:10:32  lr: 0.000016  loss: 0.0185 (0.0188)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [340/985]  eta: 0:10:23  lr: 0.000016  loss: 0.0176 (0.0188)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [350/985]  eta: 0:10:13  lr: 0.000016  loss: 0.0172 (0.0188)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [360/985]  eta: 0:10:03  lr: 0.000016  loss: 0.0176 (0.0188)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [370/985]  eta: 0:09:53  lr: 0.000016  loss: 0.0176 (0.0188)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [380/985]  eta: 0:09:43  lr: 0.000016  loss: 0.0181 (0.0188)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [390/985]  eta: 0:09:34  lr: 0.000016  loss: 0.0190 (0.0188)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [400/985]  eta: 0:09:24  lr: 0.000016  loss: 0.0202 (0.0189)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [410/985]  eta: 0:09:14  lr: 0.000016  loss: 0.0188 (0.0189)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [420/985]  eta: 0:09:04  lr: 0.000016  loss: 0.0184 (0.0189)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [430/985]  eta: 0:08:55  lr: 0.000016  loss: 0.0180 (0.0189)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [440/985]  eta: 0:08:45  lr: 0.000016  loss: 0.0177 (0.0188)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [450/985]  eta: 0:08:35  lr: 0.000016  loss: 0.0183 (0.0189)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [460/985]  eta: 0:08:26  lr: 0.000016  loss: 0.0203 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [470/985]  eta: 0:08:16  lr: 0.000016  loss: 0.0190 (0.0189)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [480/985]  eta: 0:08:06  lr: 0.000016  loss: 0.0180 (0.0189)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [490/985]  eta: 0:07:56  lr: 0.000016  loss: 0.0180 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [500/985]  eta: 0:07:47  lr: 0.000016  loss: 0.0181 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [510/985]  eta: 0:07:37  lr: 0.000016  loss: 0.0204 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [520/985]  eta: 0:07:27  lr: 0.000016  loss: 0.0182 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [530/985]  eta: 0:07:18  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [540/985]  eta: 0:07:08  lr: 0.000016  loss: 0.0173 (0.0190)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [550/985]  eta: 0:06:58  lr: 0.000016  loss: 0.0171 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [560/985]  eta: 0:06:48  lr: 0.000016  loss: 0.0181 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [570/985]  eta: 0:06:39  lr: 0.000016  loss: 0.0188 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [580/985]  eta: 0:06:29  lr: 0.000016  loss: 0.0186 (0.0190)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [590/985]  eta: 0:06:20  lr: 0.000016  loss: 0.0188 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [600/985]  eta: 0:06:10  lr: 0.000016  loss: 0.0188 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [610/985]  eta: 0:06:00  lr: 0.000016  loss: 0.0197 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [620/985]  eta: 0:05:51  lr: 0.000016  loss: 0.0194 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [630/985]  eta: 0:05:41  lr: 0.000016  loss: 0.0173 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [640/985]  eta: 0:05:31  lr: 0.000016  loss: 0.0165 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [650/985]  eta: 0:05:22  lr: 0.000016  loss: 0.0173 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [660/985]  eta: 0:05:12  lr: 0.000016  loss: 0.0174 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [670/985]  eta: 0:05:02  lr: 0.000016  loss: 0.0172 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [680/985]  eta: 0:04:53  lr: 0.000016  loss: 0.0170 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [690/985]  eta: 0:04:43  lr: 0.000016  loss: 0.0176 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [700/985]  eta: 0:04:33  lr: 0.000016  loss: 0.0189 (0.0190)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [710/985]  eta: 0:04:24  lr: 0.000016  loss: 0.0189 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [720/985]  eta: 0:04:14  lr: 0.000016  loss: 0.0198 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [730/985]  eta: 0:04:05  lr: 0.000016  loss: 0.0189 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [740/985]  eta: 0:03:55  lr: 0.000016  loss: 0.0176 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [750/985]  eta: 0:03:45  lr: 0.000016  loss: 0.0201 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [760/985]  eta: 0:03:36  lr: 0.000016  loss: 0.0195 (0.0190)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0173 (0.0190)  time: 0.9524  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:823]  [780/985]  eta: 0:03:16  lr: 0.000016  loss: 0.0180 (0.0190)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0189 (0.0190)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [800/985]  eta: 0:02:57  lr: 0.000016  loss: 0.0181 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [810/985]  eta: 0:02:48  lr: 0.000016  loss: 0.0187 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [830/985]  eta: 0:02:28  lr: 0.000016  loss: 0.0189 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0178 (0.0190)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [860/985]  eta: 0:02:00  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [880/985]  eta: 0:01:40  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0189 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [910/985]  eta: 0:01:11  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0198 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [960/985]  eta: 0:00:23  lr: 0.000016  loss: 0.0198 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:823] Total time: 0:15:45 (0.9596 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0180 (0.0191)\n",
      "Valid: [epoch:823]  [ 0/14]  eta: 0:03:00  loss: 0.0145 (0.0145)  time: 12.9024  data: 0.5862  max mem: 41892\n",
      "Valid: [epoch:823]  [13/14]  eta: 0:00:12  loss: 0.0148 (0.0148)  time: 12.1074  data: 0.0420  max mem: 41892\n",
      "Valid: [epoch:823] Total time: 0:02:49 (12.1132 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_823_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:824]  [  0/985]  eta: 1:14:07  lr: 0.000016  loss: 0.0176 (0.0176)  time: 4.5152  data: 3.5355  max mem: 41892\n",
      "Train: [epoch:824]  [ 10/985]  eta: 0:20:50  lr: 0.000016  loss: 0.0180 (0.0201)  time: 1.2825  data: 0.3215  max mem: 41892\n",
      "Train: [epoch:824]  [ 20/985]  eta: 0:18:03  lr: 0.000016  loss: 0.0200 (0.0207)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [ 30/985]  eta: 0:16:56  lr: 0.000016  loss: 0.0200 (0.0206)  time: 0.9439  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [ 40/985]  eta: 0:16:18  lr: 0.000016  loss: 0.0184 (0.0198)  time: 0.9445  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [ 50/985]  eta: 0:15:51  lr: 0.000016  loss: 0.0165 (0.0191)  time: 0.9461  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [ 60/985]  eta: 0:15:30  lr: 0.000016  loss: 0.0169 (0.0193)  time: 0.9450  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [ 70/985]  eta: 0:15:12  lr: 0.000016  loss: 0.0173 (0.0190)  time: 0.9465  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [ 80/985]  eta: 0:14:57  lr: 0.000016  loss: 0.0157 (0.0187)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [ 90/985]  eta: 0:14:43  lr: 0.000016  loss: 0.0172 (0.0187)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [100/985]  eta: 0:14:30  lr: 0.000016  loss: 0.0179 (0.0186)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [110/985]  eta: 0:14:18  lr: 0.000016  loss: 0.0180 (0.0187)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [120/985]  eta: 0:14:06  lr: 0.000016  loss: 0.0195 (0.0189)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [130/985]  eta: 0:13:55  lr: 0.000016  loss: 0.0196 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [140/985]  eta: 0:13:44  lr: 0.000016  loss: 0.0196 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [150/985]  eta: 0:13:33  lr: 0.000016  loss: 0.0199 (0.0192)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [160/985]  eta: 0:13:22  lr: 0.000016  loss: 0.0184 (0.0192)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [170/985]  eta: 0:13:11  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [180/985]  eta: 0:13:01  lr: 0.000016  loss: 0.0165 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [190/985]  eta: 0:12:50  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [200/985]  eta: 0:12:41  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [210/985]  eta: 0:12:30  lr: 0.000016  loss: 0.0179 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [220/985]  eta: 0:12:20  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [230/985]  eta: 0:12:10  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [240/985]  eta: 0:12:00  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [250/985]  eta: 0:11:50  lr: 0.000016  loss: 0.0182 (0.0190)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [260/985]  eta: 0:11:40  lr: 0.000016  loss: 0.0165 (0.0190)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [270/985]  eta: 0:11:30  lr: 0.000016  loss: 0.0166 (0.0189)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [280/985]  eta: 0:11:21  lr: 0.000016  loss: 0.0176 (0.0189)  time: 0.9648  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [290/985]  eta: 0:11:11  lr: 0.000016  loss: 0.0188 (0.0190)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [300/985]  eta: 0:11:01  lr: 0.000016  loss: 0.0192 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [310/985]  eta: 0:10:51  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [320/985]  eta: 0:10:41  lr: 0.000016  loss: 0.0174 (0.0190)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [330/985]  eta: 0:10:31  lr: 0.000016  loss: 0.0185 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [340/985]  eta: 0:10:22  lr: 0.000016  loss: 0.0185 (0.0190)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [350/985]  eta: 0:10:12  lr: 0.000016  loss: 0.0194 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [360/985]  eta: 0:10:02  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [370/985]  eta: 0:09:52  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [380/985]  eta: 0:09:42  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:824]  [390/985]  eta: 0:09:33  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [400/985]  eta: 0:09:23  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [410/985]  eta: 0:09:13  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [420/985]  eta: 0:09:04  lr: 0.000016  loss: 0.0180 (0.0192)  time: 0.9650  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [430/985]  eta: 0:08:54  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9645  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [440/985]  eta: 0:08:44  lr: 0.000016  loss: 0.0180 (0.0192)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [450/985]  eta: 0:08:35  lr: 0.000016  loss: 0.0180 (0.0192)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [460/985]  eta: 0:08:25  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [470/985]  eta: 0:08:15  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [480/985]  eta: 0:08:06  lr: 0.000016  loss: 0.0188 (0.0192)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [490/985]  eta: 0:07:56  lr: 0.000016  loss: 0.0198 (0.0192)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [500/985]  eta: 0:07:46  lr: 0.000016  loss: 0.0189 (0.0192)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [510/985]  eta: 0:07:36  lr: 0.000016  loss: 0.0185 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [520/985]  eta: 0:07:27  lr: 0.000016  loss: 0.0178 (0.0192)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [530/985]  eta: 0:07:17  lr: 0.000016  loss: 0.0178 (0.0192)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [540/985]  eta: 0:07:07  lr: 0.000016  loss: 0.0178 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [550/985]  eta: 0:06:58  lr: 0.000016  loss: 0.0171 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [560/985]  eta: 0:06:48  lr: 0.000016  loss: 0.0175 (0.0192)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [570/985]  eta: 0:06:38  lr: 0.000016  loss: 0.0179 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [580/985]  eta: 0:06:29  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [590/985]  eta: 0:06:19  lr: 0.000016  loss: 0.0189 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [600/985]  eta: 0:06:09  lr: 0.000016  loss: 0.0192 (0.0192)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [610/985]  eta: 0:06:00  lr: 0.000016  loss: 0.0182 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [620/985]  eta: 0:05:50  lr: 0.000016  loss: 0.0182 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [630/985]  eta: 0:05:41  lr: 0.000016  loss: 0.0182 (0.0192)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [640/985]  eta: 0:05:31  lr: 0.000016  loss: 0.0156 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [650/985]  eta: 0:05:21  lr: 0.000016  loss: 0.0171 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [660/985]  eta: 0:05:12  lr: 0.000016  loss: 0.0196 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [670/985]  eta: 0:05:02  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [680/985]  eta: 0:04:52  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [690/985]  eta: 0:04:43  lr: 0.000016  loss: 0.0170 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [700/985]  eta: 0:04:33  lr: 0.000016  loss: 0.0172 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [710/985]  eta: 0:04:24  lr: 0.000016  loss: 0.0194 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [720/985]  eta: 0:04:14  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [730/985]  eta: 0:04:04  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [740/985]  eta: 0:03:55  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [750/985]  eta: 0:03:45  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [760/985]  eta: 0:03:35  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [780/985]  eta: 0:03:16  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [800/985]  eta: 0:02:57  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [810/985]  eta: 0:02:47  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0193 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [830/985]  eta: 0:02:28  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0173 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [860/985]  eta: 0:01:59  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [880/985]  eta: 0:01:40  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [910/985]  eta: 0:01:11  lr: 0.000016  loss: 0.0182 (0.0192)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0189 (0.0192)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [960/985]  eta: 0:00:23  lr: 0.000016  loss: 0.0186 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0191 (0.0192)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0191 (0.0192)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:824] Total time: 0:15:44 (0.9591 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0191 (0.0192)\n",
      "Valid: [epoch:824]  [ 0/14]  eta: 0:03:03  loss: 0.0131 (0.0131)  time: 13.1346  data: 0.5610  max mem: 41892\n",
      "Valid: [epoch:824]  [13/14]  eta: 0:00:12  loss: 0.0143 (0.0143)  time: 12.1600  data: 0.0402  max mem: 41892\n",
      "Valid: [epoch:824] Total time: 0:02:50 (12.1665 s / it)\n",
      "Averaged stats: loss: 0.0143 (0.0143)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_824_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:825]  [  0/985]  eta: 1:15:43  lr: 0.000016  loss: 0.0142 (0.0142)  time: 4.6129  data: 3.6313  max mem: 41892\n",
      "Train: [epoch:825]  [ 10/985]  eta: 0:20:49  lr: 0.000016  loss: 0.0187 (0.0186)  time: 1.2819  data: 0.3303  max mem: 41892\n",
      "Train: [epoch:825]  [ 20/985]  eta: 0:18:00  lr: 0.000016  loss: 0.0173 (0.0178)  time: 0.9447  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [ 30/985]  eta: 0:16:56  lr: 0.000016  loss: 0.0169 (0.0177)  time: 0.9440  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [ 40/985]  eta: 0:16:18  lr: 0.000016  loss: 0.0170 (0.0176)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [ 50/985]  eta: 0:15:53  lr: 0.000016  loss: 0.0181 (0.0177)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [ 60/985]  eta: 0:15:33  lr: 0.000016  loss: 0.0187 (0.0180)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [ 70/985]  eta: 0:15:17  lr: 0.000016  loss: 0.0179 (0.0179)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [ 80/985]  eta: 0:15:02  lr: 0.000016  loss: 0.0169 (0.0178)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [ 90/985]  eta: 0:14:47  lr: 0.000016  loss: 0.0166 (0.0179)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [100/985]  eta: 0:14:34  lr: 0.000016  loss: 0.0177 (0.0179)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [110/985]  eta: 0:14:21  lr: 0.000016  loss: 0.0182 (0.0180)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [120/985]  eta: 0:14:09  lr: 0.000016  loss: 0.0179 (0.0181)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [130/985]  eta: 0:13:58  lr: 0.000016  loss: 0.0187 (0.0183)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [140/985]  eta: 0:13:46  lr: 0.000016  loss: 0.0180 (0.0183)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [150/985]  eta: 0:13:36  lr: 0.000016  loss: 0.0181 (0.0184)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [160/985]  eta: 0:13:25  lr: 0.000016  loss: 0.0183 (0.0184)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [170/985]  eta: 0:13:14  lr: 0.000016  loss: 0.0175 (0.0183)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [180/985]  eta: 0:13:03  lr: 0.000016  loss: 0.0177 (0.0185)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [190/985]  eta: 0:12:52  lr: 0.000016  loss: 0.0181 (0.0185)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [200/985]  eta: 0:12:42  lr: 0.000016  loss: 0.0171 (0.0185)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [210/985]  eta: 0:12:32  lr: 0.000016  loss: 0.0177 (0.0186)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [220/985]  eta: 0:12:22  lr: 0.000016  loss: 0.0165 (0.0185)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [230/985]  eta: 0:12:12  lr: 0.000016  loss: 0.0171 (0.0185)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [240/985]  eta: 0:12:02  lr: 0.000016  loss: 0.0183 (0.0186)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [250/985]  eta: 0:11:52  lr: 0.000016  loss: 0.0184 (0.0186)  time: 0.9639  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [260/985]  eta: 0:11:42  lr: 0.000016  loss: 0.0177 (0.0186)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [270/985]  eta: 0:11:32  lr: 0.000016  loss: 0.0177 (0.0186)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [280/985]  eta: 0:11:22  lr: 0.000016  loss: 0.0180 (0.0186)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [290/985]  eta: 0:11:12  lr: 0.000016  loss: 0.0186 (0.0187)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [300/985]  eta: 0:11:02  lr: 0.000016  loss: 0.0189 (0.0187)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [310/985]  eta: 0:10:52  lr: 0.000016  loss: 0.0192 (0.0188)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [320/985]  eta: 0:10:42  lr: 0.000016  loss: 0.0186 (0.0188)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [330/985]  eta: 0:10:32  lr: 0.000016  loss: 0.0168 (0.0187)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [340/985]  eta: 0:10:23  lr: 0.000016  loss: 0.0179 (0.0188)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [350/985]  eta: 0:10:13  lr: 0.000016  loss: 0.0181 (0.0188)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [360/985]  eta: 0:10:03  lr: 0.000016  loss: 0.0172 (0.0188)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [370/985]  eta: 0:09:53  lr: 0.000016  loss: 0.0181 (0.0188)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [380/985]  eta: 0:09:43  lr: 0.000016  loss: 0.0199 (0.0189)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [390/985]  eta: 0:09:33  lr: 0.000016  loss: 0.0182 (0.0189)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [400/985]  eta: 0:09:24  lr: 0.000016  loss: 0.0185 (0.0189)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [410/985]  eta: 0:09:14  lr: 0.000016  loss: 0.0182 (0.0189)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [420/985]  eta: 0:09:04  lr: 0.000016  loss: 0.0182 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [430/985]  eta: 0:08:54  lr: 0.000016  loss: 0.0179 (0.0189)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [440/985]  eta: 0:08:45  lr: 0.000016  loss: 0.0169 (0.0189)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [450/985]  eta: 0:08:35  lr: 0.000016  loss: 0.0173 (0.0189)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [460/985]  eta: 0:08:25  lr: 0.000016  loss: 0.0179 (0.0189)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [470/985]  eta: 0:08:15  lr: 0.000016  loss: 0.0179 (0.0189)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [480/985]  eta: 0:08:06  lr: 0.000016  loss: 0.0172 (0.0189)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [490/985]  eta: 0:07:56  lr: 0.000016  loss: 0.0190 (0.0189)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [500/985]  eta: 0:07:46  lr: 0.000016  loss: 0.0190 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [510/985]  eta: 0:07:37  lr: 0.000016  loss: 0.0178 (0.0189)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [520/985]  eta: 0:07:27  lr: 0.000016  loss: 0.0180 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [530/985]  eta: 0:07:17  lr: 0.000016  loss: 0.0195 (0.0190)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [540/985]  eta: 0:07:08  lr: 0.000016  loss: 0.0172 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [550/985]  eta: 0:06:58  lr: 0.000016  loss: 0.0174 (0.0190)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [560/985]  eta: 0:06:48  lr: 0.000016  loss: 0.0175 (0.0189)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [570/985]  eta: 0:06:39  lr: 0.000016  loss: 0.0173 (0.0189)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [580/985]  eta: 0:06:29  lr: 0.000016  loss: 0.0181 (0.0189)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [590/985]  eta: 0:06:19  lr: 0.000016  loss: 0.0190 (0.0189)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [600/985]  eta: 0:06:10  lr: 0.000016  loss: 0.0180 (0.0189)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [610/985]  eta: 0:06:00  lr: 0.000016  loss: 0.0180 (0.0189)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [620/985]  eta: 0:05:50  lr: 0.000016  loss: 0.0203 (0.0190)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [630/985]  eta: 0:05:41  lr: 0.000016  loss: 0.0178 (0.0190)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [640/985]  eta: 0:05:31  lr: 0.000016  loss: 0.0169 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [650/985]  eta: 0:05:22  lr: 0.000016  loss: 0.0175 (0.0189)  time: 0.9590  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:825]  [660/985]  eta: 0:05:12  lr: 0.000016  loss: 0.0175 (0.0189)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [670/985]  eta: 0:05:02  lr: 0.000016  loss: 0.0172 (0.0189)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [680/985]  eta: 0:04:53  lr: 0.000016  loss: 0.0182 (0.0189)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [690/985]  eta: 0:04:43  lr: 0.000016  loss: 0.0194 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [700/985]  eta: 0:04:33  lr: 0.000016  loss: 0.0198 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [710/985]  eta: 0:04:24  lr: 0.000016  loss: 0.0193 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [720/985]  eta: 0:04:14  lr: 0.000016  loss: 0.0185 (0.0190)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [730/985]  eta: 0:04:05  lr: 0.000016  loss: 0.0184 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [740/985]  eta: 0:03:55  lr: 0.000016  loss: 0.0181 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [750/985]  eta: 0:03:45  lr: 0.000016  loss: 0.0180 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [760/985]  eta: 0:03:36  lr: 0.000016  loss: 0.0173 (0.0189)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0187 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [780/985]  eta: 0:03:16  lr: 0.000016  loss: 0.0207 (0.0190)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0198 (0.0190)  time: 0.9646  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [800/985]  eta: 0:02:57  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [810/985]  eta: 0:02:48  lr: 0.000016  loss: 0.0215 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0213 (0.0191)  time: 0.9626  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [830/985]  eta: 0:02:28  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0176 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0170 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [860/985]  eta: 0:02:00  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0176 (0.0190)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [880/985]  eta: 0:01:40  lr: 0.000016  loss: 0.0176 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0185 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0187 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [910/985]  eta: 0:01:12  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9636  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [960/985]  eta: 0:00:24  lr: 0.000016  loss: 0.0198 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:825] Total time: 0:15:45 (0.9601 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0195 (0.0191)\n",
      "Valid: [epoch:825]  [ 0/14]  eta: 0:03:00  loss: 0.0151 (0.0151)  time: 12.9267  data: 0.6233  max mem: 41892\n",
      "Valid: [epoch:825]  [13/14]  eta: 0:00:12  loss: 0.0148 (0.0148)  time: 12.0556  data: 0.0446  max mem: 41892\n",
      "Valid: [epoch:825] Total time: 0:02:48 (12.0623 s / it)\n",
      "Averaged stats: loss: 0.0148 (0.0148)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_825_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:826]  [  0/985]  eta: 1:30:59  lr: 0.000016  loss: 0.0221 (0.0221)  time: 5.5429  data: 4.5448  max mem: 41892\n",
      "Train: [epoch:826]  [ 10/985]  eta: 0:22:08  lr: 0.000016  loss: 0.0192 (0.0197)  time: 1.3626  data: 0.4133  max mem: 41892\n",
      "Train: [epoch:826]  [ 20/985]  eta: 0:18:40  lr: 0.000016  loss: 0.0185 (0.0197)  time: 0.9418  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [ 30/985]  eta: 0:17:20  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9389  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [ 40/985]  eta: 0:16:36  lr: 0.000016  loss: 0.0163 (0.0187)  time: 0.9431  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [ 50/985]  eta: 0:16:06  lr: 0.000016  loss: 0.0157 (0.0183)  time: 0.9465  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [ 60/985]  eta: 0:15:42  lr: 0.000016  loss: 0.0173 (0.0184)  time: 0.9467  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [ 70/985]  eta: 0:15:23  lr: 0.000016  loss: 0.0179 (0.0185)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [ 80/985]  eta: 0:15:07  lr: 0.000016  loss: 0.0162 (0.0181)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [ 90/985]  eta: 0:14:52  lr: 0.000016  loss: 0.0166 (0.0181)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [100/985]  eta: 0:14:38  lr: 0.000016  loss: 0.0175 (0.0181)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [110/985]  eta: 0:14:26  lr: 0.000016  loss: 0.0181 (0.0183)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [120/985]  eta: 0:14:14  lr: 0.000016  loss: 0.0187 (0.0184)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [130/985]  eta: 0:14:02  lr: 0.000016  loss: 0.0183 (0.0184)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [140/985]  eta: 0:13:51  lr: 0.000016  loss: 0.0174 (0.0183)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [150/985]  eta: 0:13:39  lr: 0.000016  loss: 0.0174 (0.0184)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [160/985]  eta: 0:13:28  lr: 0.000016  loss: 0.0189 (0.0186)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [170/985]  eta: 0:13:17  lr: 0.000016  loss: 0.0226 (0.0188)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [180/985]  eta: 0:13:06  lr: 0.000016  loss: 0.0195 (0.0188)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [190/985]  eta: 0:12:56  lr: 0.000016  loss: 0.0190 (0.0188)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [200/985]  eta: 0:12:45  lr: 0.000016  loss: 0.0185 (0.0188)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [210/985]  eta: 0:12:35  lr: 0.000016  loss: 0.0177 (0.0188)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [220/985]  eta: 0:12:24  lr: 0.000016  loss: 0.0177 (0.0188)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [230/985]  eta: 0:12:14  lr: 0.000016  loss: 0.0177 (0.0188)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [240/985]  eta: 0:12:04  lr: 0.000016  loss: 0.0182 (0.0188)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [250/985]  eta: 0:11:54  lr: 0.000016  loss: 0.0179 (0.0188)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [260/985]  eta: 0:11:43  lr: 0.000016  loss: 0.0172 (0.0188)  time: 0.9587  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:826]  [270/985]  eta: 0:11:33  lr: 0.000016  loss: 0.0172 (0.0188)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [280/985]  eta: 0:11:23  lr: 0.000016  loss: 0.0173 (0.0188)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [290/985]  eta: 0:11:13  lr: 0.000016  loss: 0.0184 (0.0188)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [300/985]  eta: 0:11:03  lr: 0.000016  loss: 0.0184 (0.0188)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [310/985]  eta: 0:10:53  lr: 0.000016  loss: 0.0183 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [320/985]  eta: 0:10:43  lr: 0.000016  loss: 0.0185 (0.0188)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [330/985]  eta: 0:10:33  lr: 0.000016  loss: 0.0181 (0.0188)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [340/985]  eta: 0:10:23  lr: 0.000016  loss: 0.0181 (0.0188)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [350/985]  eta: 0:10:14  lr: 0.000016  loss: 0.0190 (0.0189)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [360/985]  eta: 0:10:04  lr: 0.000016  loss: 0.0180 (0.0188)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [370/985]  eta: 0:09:54  lr: 0.000016  loss: 0.0178 (0.0188)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [380/985]  eta: 0:09:44  lr: 0.000016  loss: 0.0178 (0.0188)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [390/985]  eta: 0:09:34  lr: 0.000016  loss: 0.0189 (0.0189)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [400/985]  eta: 0:09:24  lr: 0.000016  loss: 0.0193 (0.0189)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [410/985]  eta: 0:09:15  lr: 0.000016  loss: 0.0190 (0.0190)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [420/985]  eta: 0:09:05  lr: 0.000016  loss: 0.0183 (0.0189)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [430/985]  eta: 0:08:55  lr: 0.000016  loss: 0.0171 (0.0189)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [440/985]  eta: 0:08:45  lr: 0.000016  loss: 0.0183 (0.0189)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [450/985]  eta: 0:08:36  lr: 0.000016  loss: 0.0194 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [460/985]  eta: 0:08:26  lr: 0.000016  loss: 0.0180 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [470/985]  eta: 0:08:16  lr: 0.000016  loss: 0.0179 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [480/985]  eta: 0:08:07  lr: 0.000016  loss: 0.0181 (0.0189)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [490/985]  eta: 0:07:57  lr: 0.000016  loss: 0.0183 (0.0189)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [500/985]  eta: 0:07:47  lr: 0.000016  loss: 0.0193 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [510/985]  eta: 0:07:37  lr: 0.000016  loss: 0.0196 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [520/985]  eta: 0:07:28  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [530/985]  eta: 0:07:18  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [540/985]  eta: 0:07:08  lr: 0.000016  loss: 0.0188 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [550/985]  eta: 0:06:59  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [560/985]  eta: 0:06:49  lr: 0.000016  loss: 0.0177 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [570/985]  eta: 0:06:39  lr: 0.000016  loss: 0.0169 (0.0190)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [580/985]  eta: 0:06:29  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [590/985]  eta: 0:06:20  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [600/985]  eta: 0:06:10  lr: 0.000016  loss: 0.0197 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [610/985]  eta: 0:06:00  lr: 0.000016  loss: 0.0200 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [620/985]  eta: 0:05:51  lr: 0.000016  loss: 0.0196 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [630/985]  eta: 0:05:41  lr: 0.000016  loss: 0.0196 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [640/985]  eta: 0:05:32  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [650/985]  eta: 0:05:22  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [660/985]  eta: 0:05:12  lr: 0.000016  loss: 0.0191 (0.0192)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [670/985]  eta: 0:05:03  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [680/985]  eta: 0:04:53  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [690/985]  eta: 0:04:43  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [700/985]  eta: 0:04:34  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [710/985]  eta: 0:04:24  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9652  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [720/985]  eta: 0:04:14  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [730/985]  eta: 0:04:05  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [740/985]  eta: 0:03:55  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [750/985]  eta: 0:03:46  lr: 0.000016  loss: 0.0185 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [760/985]  eta: 0:03:36  lr: 0.000016  loss: 0.0183 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0175 (0.0192)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [780/985]  eta: 0:03:17  lr: 0.000016  loss: 0.0175 (0.0192)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0181 (0.0192)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [800/985]  eta: 0:02:57  lr: 0.000016  loss: 0.0179 (0.0192)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [810/985]  eta: 0:02:48  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [830/985]  eta: 0:02:29  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9637  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [860/985]  eta: 0:02:00  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [880/985]  eta: 0:01:40  lr: 0.000016  loss: 0.0174 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0169 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [910/985]  eta: 0:01:12  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0183 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:826]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0175 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [960/985]  eta: 0:00:24  lr: 0.000016  loss: 0.0189 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:826] Total time: 0:15:46 (0.9609 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0186 (0.0191)\n",
      "Valid: [epoch:826]  [ 0/14]  eta: 0:02:57  loss: 0.0146 (0.0146)  time: 12.6620  data: 0.6056  max mem: 41892\n",
      "Valid: [epoch:826]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0145)  time: 12.2845  data: 0.0433  max mem: 41892\n",
      "Valid: [epoch:826] Total time: 0:02:52 (12.2916 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_826_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:827]  [  0/985]  eta: 1:14:35  lr: 0.000016  loss: 0.0165 (0.0165)  time: 4.5435  data: 3.5296  max mem: 41892\n",
      "Train: [epoch:827]  [ 10/985]  eta: 0:20:44  lr: 0.000016  loss: 0.0177 (0.0193)  time: 1.2763  data: 0.3210  max mem: 41892\n",
      "Train: [epoch:827]  [ 20/985]  eta: 0:17:58  lr: 0.000016  loss: 0.0178 (0.0194)  time: 0.9459  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [ 30/985]  eta: 0:16:51  lr: 0.000016  loss: 0.0178 (0.0189)  time: 0.9402  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [ 40/985]  eta: 0:16:14  lr: 0.000016  loss: 0.0176 (0.0188)  time: 0.9416  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [ 50/985]  eta: 0:15:51  lr: 0.000016  loss: 0.0160 (0.0183)  time: 0.9518  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [ 60/985]  eta: 0:15:29  lr: 0.000016  loss: 0.0169 (0.0186)  time: 0.9516  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [ 70/985]  eta: 0:15:13  lr: 0.000016  loss: 0.0172 (0.0184)  time: 0.9511  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [ 80/985]  eta: 0:14:57  lr: 0.000016  loss: 0.0172 (0.0185)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [ 90/985]  eta: 0:14:44  lr: 0.000016  loss: 0.0194 (0.0187)  time: 0.9492  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [100/985]  eta: 0:14:31  lr: 0.000016  loss: 0.0194 (0.0188)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [110/985]  eta: 0:14:20  lr: 0.000016  loss: 0.0193 (0.0188)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [120/985]  eta: 0:14:08  lr: 0.000016  loss: 0.0184 (0.0188)  time: 0.9653  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [130/985]  eta: 0:13:57  lr: 0.000016  loss: 0.0179 (0.0190)  time: 0.9583  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [140/985]  eta: 0:13:47  lr: 0.000016  loss: 0.0183 (0.0190)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [150/985]  eta: 0:13:36  lr: 0.000016  loss: 0.0186 (0.0190)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [160/985]  eta: 0:13:26  lr: 0.000016  loss: 0.0183 (0.0190)  time: 0.9659  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [170/985]  eta: 0:13:15  lr: 0.000016  loss: 0.0178 (0.0190)  time: 0.9650  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [180/985]  eta: 0:13:05  lr: 0.000016  loss: 0.0199 (0.0191)  time: 0.9597  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [190/985]  eta: 0:12:54  lr: 0.000016  loss: 0.0196 (0.0192)  time: 0.9614  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [200/985]  eta: 0:12:44  lr: 0.000016  loss: 0.0185 (0.0192)  time: 0.9588  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [210/985]  eta: 0:12:33  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9565  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:827]  [220/985]  eta: 0:12:23  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [230/985]  eta: 0:12:13  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [240/985]  eta: 0:12:03  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [250/985]  eta: 0:11:53  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9627  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [260/985]  eta: 0:11:43  lr: 0.000016  loss: 0.0182 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [270/985]  eta: 0:11:33  lr: 0.000016  loss: 0.0171 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [280/985]  eta: 0:11:23  lr: 0.000016  loss: 0.0179 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [290/985]  eta: 0:11:13  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [300/985]  eta: 0:11:03  lr: 0.000016  loss: 0.0203 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [310/985]  eta: 0:10:53  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [320/985]  eta: 0:10:43  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [330/985]  eta: 0:10:33  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [340/985]  eta: 0:10:24  lr: 0.000016  loss: 0.0184 (0.0191)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [350/985]  eta: 0:10:14  lr: 0.000016  loss: 0.0165 (0.0190)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [360/985]  eta: 0:10:04  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [370/985]  eta: 0:09:54  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [380/985]  eta: 0:09:44  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [390/985]  eta: 0:09:34  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [400/985]  eta: 0:09:25  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [410/985]  eta: 0:09:15  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [420/985]  eta: 0:09:05  lr: 0.000016  loss: 0.0177 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [430/985]  eta: 0:08:55  lr: 0.000016  loss: 0.0171 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [440/985]  eta: 0:08:46  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [450/985]  eta: 0:08:36  lr: 0.000016  loss: 0.0181 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [460/985]  eta: 0:08:26  lr: 0.000016  loss: 0.0162 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [470/985]  eta: 0:08:16  lr: 0.000016  loss: 0.0175 (0.0190)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [480/985]  eta: 0:08:07  lr: 0.000016  loss: 0.0191 (0.0190)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [490/985]  eta: 0:07:57  lr: 0.000016  loss: 0.0191 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [500/985]  eta: 0:07:47  lr: 0.000016  loss: 0.0199 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [510/985]  eta: 0:07:38  lr: 0.000016  loss: 0.0199 (0.0192)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [520/985]  eta: 0:07:28  lr: 0.000016  loss: 0.0188 (0.0192)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [530/985]  eta: 0:07:18  lr: 0.000016  loss: 0.0188 (0.0192)  time: 0.9637  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:827]  [540/985]  eta: 0:07:09  lr: 0.000016  loss: 0.0202 (0.0192)  time: 0.9653  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [550/985]  eta: 0:06:59  lr: 0.000016  loss: 0.0194 (0.0192)  time: 0.9659  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [560/985]  eta: 0:06:49  lr: 0.000016  loss: 0.0175 (0.0192)  time: 0.9679  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [570/985]  eta: 0:06:40  lr: 0.000016  loss: 0.0175 (0.0192)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [580/985]  eta: 0:06:30  lr: 0.000016  loss: 0.0172 (0.0192)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [590/985]  eta: 0:06:20  lr: 0.000016  loss: 0.0181 (0.0192)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [600/985]  eta: 0:06:11  lr: 0.000016  loss: 0.0176 (0.0192)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [610/985]  eta: 0:06:01  lr: 0.000016  loss: 0.0172 (0.0192)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [620/985]  eta: 0:05:51  lr: 0.000016  loss: 0.0179 (0.0192)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [630/985]  eta: 0:05:42  lr: 0.000016  loss: 0.0190 (0.0192)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [640/985]  eta: 0:05:32  lr: 0.000016  loss: 0.0165 (0.0192)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [650/985]  eta: 0:05:22  lr: 0.000016  loss: 0.0162 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [660/985]  eta: 0:05:13  lr: 0.000016  loss: 0.0176 (0.0192)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [670/985]  eta: 0:05:03  lr: 0.000016  loss: 0.0175 (0.0192)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [680/985]  eta: 0:04:53  lr: 0.000016  loss: 0.0165 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [690/985]  eta: 0:04:44  lr: 0.000016  loss: 0.0168 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [700/985]  eta: 0:04:34  lr: 0.000016  loss: 0.0181 (0.0191)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [710/985]  eta: 0:04:24  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9627  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [720/985]  eta: 0:04:15  lr: 0.000016  loss: 0.0195 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [730/985]  eta: 0:04:05  lr: 0.000016  loss: 0.0173 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [740/985]  eta: 0:03:55  lr: 0.000016  loss: 0.0164 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [750/985]  eta: 0:03:46  lr: 0.000016  loss: 0.0170 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [760/985]  eta: 0:03:36  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [770/985]  eta: 0:03:26  lr: 0.000016  loss: 0.0179 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [780/985]  eta: 0:03:17  lr: 0.000016  loss: 0.0170 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [790/985]  eta: 0:03:07  lr: 0.000016  loss: 0.0163 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [800/985]  eta: 0:02:58  lr: 0.000016  loss: 0.0180 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [810/985]  eta: 0:02:48  lr: 0.000016  loss: 0.0192 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [820/985]  eta: 0:02:38  lr: 0.000016  loss: 0.0197 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [830/985]  eta: 0:02:29  lr: 0.000016  loss: 0.0190 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [840/985]  eta: 0:02:19  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [850/985]  eta: 0:02:09  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [860/985]  eta: 0:02:00  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9670  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [870/985]  eta: 0:01:50  lr: 0.000016  loss: 0.0173 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [880/985]  eta: 0:01:41  lr: 0.000016  loss: 0.0173 (0.0191)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [890/985]  eta: 0:01:31  lr: 0.000016  loss: 0.0178 (0.0191)  time: 0.9641  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [900/985]  eta: 0:01:21  lr: 0.000016  loss: 0.0176 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [910/985]  eta: 0:01:12  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [920/985]  eta: 0:01:02  lr: 0.000016  loss: 0.0203 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [930/985]  eta: 0:00:52  lr: 0.000016  loss: 0.0192 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [940/985]  eta: 0:00:43  lr: 0.000016  loss: 0.0188 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [950/985]  eta: 0:00:33  lr: 0.000016  loss: 0.0186 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [960/985]  eta: 0:00:24  lr: 0.000016  loss: 0.0187 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [970/985]  eta: 0:00:14  lr: 0.000016  loss: 0.0185 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [980/985]  eta: 0:00:04  lr: 0.000016  loss: 0.0200 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827]  [984/985]  eta: 0:00:00  lr: 0.000016  loss: 0.0208 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:827] Total time: 0:15:47 (0.9618 s / it)\n",
      "Averaged stats: lr: 0.000016  loss: 0.0208 (0.0191)\n",
      "Valid: [epoch:827]  [ 0/14]  eta: 0:02:56  loss: 0.0142 (0.0142)  time: 12.6334  data: 0.5515  max mem: 41892\n",
      "Valid: [epoch:827]  [13/14]  eta: 0:00:11  loss: 0.0147 (0.0147)  time: 11.9989  data: 0.0395  max mem: 41892\n",
      "Valid: [epoch:827] Total time: 0:02:48 (12.0053 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_827_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:828]  [  0/985]  eta: 1:12:47  lr: 0.000015  loss: 0.0222 (0.0222)  time: 4.4344  data: 3.4061  max mem: 41892\n",
      "Train: [epoch:828]  [ 10/985]  eta: 0:20:47  lr: 0.000015  loss: 0.0195 (0.0208)  time: 1.2799  data: 0.3098  max mem: 41892\n",
      "Train: [epoch:828]  [ 20/985]  eta: 0:17:57  lr: 0.000015  loss: 0.0198 (0.0215)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [ 30/985]  eta: 0:16:57  lr: 0.000015  loss: 0.0198 (0.0207)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [ 40/985]  eta: 0:16:18  lr: 0.000015  loss: 0.0176 (0.0200)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [ 50/985]  eta: 0:15:52  lr: 0.000015  loss: 0.0160 (0.0193)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [ 60/985]  eta: 0:15:31  lr: 0.000015  loss: 0.0165 (0.0191)  time: 0.9487  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [ 70/985]  eta: 0:15:13  lr: 0.000015  loss: 0.0171 (0.0190)  time: 0.9462  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [ 80/985]  eta: 0:14:58  lr: 0.000015  loss: 0.0169 (0.0187)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [ 90/985]  eta: 0:14:44  lr: 0.000015  loss: 0.0169 (0.0186)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [100/985]  eta: 0:14:32  lr: 0.000015  loss: 0.0182 (0.0187)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [110/985]  eta: 0:14:20  lr: 0.000015  loss: 0.0187 (0.0187)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [120/985]  eta: 0:14:08  lr: 0.000015  loss: 0.0188 (0.0188)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [130/985]  eta: 0:13:57  lr: 0.000015  loss: 0.0203 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [140/985]  eta: 0:13:45  lr: 0.000015  loss: 0.0202 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:828]  [150/985]  eta: 0:13:35  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [160/985]  eta: 0:13:24  lr: 0.000015  loss: 0.0185 (0.0192)  time: 0.9626  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [170/985]  eta: 0:13:14  lr: 0.000015  loss: 0.0194 (0.0192)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [180/985]  eta: 0:13:03  lr: 0.000015  loss: 0.0181 (0.0192)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [190/985]  eta: 0:12:53  lr: 0.000015  loss: 0.0181 (0.0192)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [200/985]  eta: 0:12:43  lr: 0.000015  loss: 0.0184 (0.0193)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [210/985]  eta: 0:12:32  lr: 0.000015  loss: 0.0179 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [220/985]  eta: 0:12:22  lr: 0.000015  loss: 0.0177 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [230/985]  eta: 0:12:12  lr: 0.000015  loss: 0.0189 (0.0193)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [240/985]  eta: 0:12:02  lr: 0.000015  loss: 0.0194 (0.0193)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [250/985]  eta: 0:11:52  lr: 0.000015  loss: 0.0175 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [260/985]  eta: 0:11:42  lr: 0.000015  loss: 0.0167 (0.0192)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [270/985]  eta: 0:11:32  lr: 0.000015  loss: 0.0167 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [280/985]  eta: 0:11:22  lr: 0.000015  loss: 0.0163 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [290/985]  eta: 0:11:12  lr: 0.000015  loss: 0.0172 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [300/985]  eta: 0:11:02  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [310/985]  eta: 0:10:52  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [320/985]  eta: 0:10:42  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [330/985]  eta: 0:10:32  lr: 0.000015  loss: 0.0169 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [340/985]  eta: 0:10:22  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [350/985]  eta: 0:10:13  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [360/985]  eta: 0:10:03  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [370/985]  eta: 0:09:53  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [380/985]  eta: 0:09:43  lr: 0.000015  loss: 0.0195 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [390/985]  eta: 0:09:34  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [400/985]  eta: 0:09:24  lr: 0.000015  loss: 0.0188 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [410/985]  eta: 0:09:14  lr: 0.000015  loss: 0.0189 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [420/985]  eta: 0:09:04  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [430/985]  eta: 0:08:54  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [440/985]  eta: 0:08:45  lr: 0.000015  loss: 0.0173 (0.0190)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [450/985]  eta: 0:08:35  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [460/985]  eta: 0:08:25  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [470/985]  eta: 0:08:16  lr: 0.000015  loss: 0.0186 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [480/985]  eta: 0:08:06  lr: 0.000015  loss: 0.0179 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [490/985]  eta: 0:07:56  lr: 0.000015  loss: 0.0179 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [500/985]  eta: 0:07:47  lr: 0.000015  loss: 0.0180 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [510/985]  eta: 0:07:37  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [520/985]  eta: 0:07:27  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [530/985]  eta: 0:07:17  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [540/985]  eta: 0:07:08  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [550/985]  eta: 0:06:58  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [560/985]  eta: 0:06:48  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [570/985]  eta: 0:06:39  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [580/985]  eta: 0:06:29  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [590/985]  eta: 0:06:20  lr: 0.000015  loss: 0.0199 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [600/985]  eta: 0:06:10  lr: 0.000015  loss: 0.0198 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [610/985]  eta: 0:06:00  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [620/985]  eta: 0:05:51  lr: 0.000015  loss: 0.0194 (0.0191)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [630/985]  eta: 0:05:41  lr: 0.000015  loss: 0.0195 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [640/985]  eta: 0:05:31  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [650/985]  eta: 0:05:22  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [660/985]  eta: 0:05:12  lr: 0.000015  loss: 0.0189 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [670/985]  eta: 0:05:02  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [680/985]  eta: 0:04:53  lr: 0.000015  loss: 0.0168 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [690/985]  eta: 0:04:43  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [700/985]  eta: 0:04:33  lr: 0.000015  loss: 0.0193 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [710/985]  eta: 0:04:24  lr: 0.000015  loss: 0.0207 (0.0192)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [720/985]  eta: 0:04:14  lr: 0.000015  loss: 0.0198 (0.0192)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [730/985]  eta: 0:04:05  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [740/985]  eta: 0:03:55  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [750/985]  eta: 0:03:45  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [760/985]  eta: 0:03:36  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [770/985]  eta: 0:03:26  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [780/985]  eta: 0:03:16  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [790/985]  eta: 0:03:07  lr: 0.000015  loss: 0.0161 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [800/985]  eta: 0:02:57  lr: 0.000015  loss: 0.0161 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:828]  [810/985]  eta: 0:02:48  lr: 0.000015  loss: 0.0171 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [820/985]  eta: 0:02:38  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [830/985]  eta: 0:02:28  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [840/985]  eta: 0:02:19  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [850/985]  eta: 0:02:09  lr: 0.000015  loss: 0.0171 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [860/985]  eta: 0:02:00  lr: 0.000015  loss: 0.0162 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [870/985]  eta: 0:01:50  lr: 0.000015  loss: 0.0162 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [880/985]  eta: 0:01:40  lr: 0.000015  loss: 0.0175 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [890/985]  eta: 0:01:31  lr: 0.000015  loss: 0.0198 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [900/985]  eta: 0:01:21  lr: 0.000015  loss: 0.0195 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [910/985]  eta: 0:01:12  lr: 0.000015  loss: 0.0192 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [920/985]  eta: 0:01:02  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [930/985]  eta: 0:00:52  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [940/985]  eta: 0:00:43  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [950/985]  eta: 0:00:33  lr: 0.000015  loss: 0.0189 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [960/985]  eta: 0:00:23  lr: 0.000015  loss: 0.0201 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [970/985]  eta: 0:00:14  lr: 0.000015  loss: 0.0189 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [980/985]  eta: 0:00:04  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828]  [984/985]  eta: 0:00:00  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:828] Total time: 0:15:45 (0.9599 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.0180 (0.0191)\n",
      "Valid: [epoch:828]  [ 0/14]  eta: 0:02:57  loss: 0.0145 (0.0145)  time: 12.6660  data: 0.5787  max mem: 41892\n",
      "Valid: [epoch:828]  [13/14]  eta: 0:00:12  loss: 0.0147 (0.0147)  time: 12.0015  data: 0.0414  max mem: 41892\n",
      "Valid: [epoch:828] Total time: 0:02:48 (12.0077 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_828_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:829]  [  0/985]  eta: 1:24:41  lr: 0.000015  loss: 0.0202 (0.0202)  time: 5.1592  data: 4.1697  max mem: 41892\n",
      "Train: [epoch:829]  [ 10/985]  eta: 0:21:40  lr: 0.000015  loss: 0.0202 (0.0197)  time: 1.3335  data: 0.3792  max mem: 41892\n",
      "Train: [epoch:829]  [ 20/985]  eta: 0:18:26  lr: 0.000015  loss: 0.0201 (0.0199)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [ 30/985]  eta: 0:17:12  lr: 0.000015  loss: 0.0182 (0.0195)  time: 0.9427  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [ 40/985]  eta: 0:16:32  lr: 0.000015  loss: 0.0172 (0.0186)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [ 50/985]  eta: 0:16:03  lr: 0.000015  loss: 0.0156 (0.0183)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [ 60/985]  eta: 0:15:41  lr: 0.000015  loss: 0.0185 (0.0188)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [ 70/985]  eta: 0:15:23  lr: 0.000015  loss: 0.0188 (0.0186)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [ 80/985]  eta: 0:15:07  lr: 0.000015  loss: 0.0167 (0.0183)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [ 90/985]  eta: 0:14:52  lr: 0.000015  loss: 0.0172 (0.0184)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [100/985]  eta: 0:14:38  lr: 0.000015  loss: 0.0182 (0.0184)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [110/985]  eta: 0:14:26  lr: 0.000015  loss: 0.0182 (0.0186)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [120/985]  eta: 0:14:14  lr: 0.000015  loss: 0.0179 (0.0186)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [130/985]  eta: 0:14:02  lr: 0.000015  loss: 0.0186 (0.0187)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [140/985]  eta: 0:13:50  lr: 0.000015  loss: 0.0189 (0.0187)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [150/985]  eta: 0:13:39  lr: 0.000015  loss: 0.0187 (0.0187)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [160/985]  eta: 0:13:28  lr: 0.000015  loss: 0.0187 (0.0189)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [170/985]  eta: 0:13:17  lr: 0.000015  loss: 0.0185 (0.0188)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [180/985]  eta: 0:13:06  lr: 0.000015  loss: 0.0177 (0.0187)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [190/985]  eta: 0:12:55  lr: 0.000015  loss: 0.0168 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [200/985]  eta: 0:12:45  lr: 0.000015  loss: 0.0176 (0.0187)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [210/985]  eta: 0:12:35  lr: 0.000015  loss: 0.0182 (0.0187)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [220/985]  eta: 0:12:24  lr: 0.000015  loss: 0.0182 (0.0188)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [230/985]  eta: 0:12:14  lr: 0.000015  loss: 0.0182 (0.0188)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [240/985]  eta: 0:12:04  lr: 0.000015  loss: 0.0192 (0.0188)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [250/985]  eta: 0:11:54  lr: 0.000015  loss: 0.0185 (0.0187)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [260/985]  eta: 0:11:43  lr: 0.000015  loss: 0.0178 (0.0188)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [270/985]  eta: 0:11:33  lr: 0.000015  loss: 0.0187 (0.0188)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [280/985]  eta: 0:11:23  lr: 0.000015  loss: 0.0189 (0.0188)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [290/985]  eta: 0:11:13  lr: 0.000015  loss: 0.0189 (0.0189)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [300/985]  eta: 0:11:03  lr: 0.000015  loss: 0.0188 (0.0189)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [310/985]  eta: 0:10:53  lr: 0.000015  loss: 0.0184 (0.0189)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [320/985]  eta: 0:10:43  lr: 0.000015  loss: 0.0175 (0.0189)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [330/985]  eta: 0:10:33  lr: 0.000015  loss: 0.0189 (0.0189)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [340/985]  eta: 0:10:24  lr: 0.000015  loss: 0.0183 (0.0189)  time: 0.9620  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:829]  [350/985]  eta: 0:10:14  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [360/985]  eta: 0:10:04  lr: 0.000015  loss: 0.0190 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [370/985]  eta: 0:09:54  lr: 0.000015  loss: 0.0199 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [380/985]  eta: 0:09:44  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [390/985]  eta: 0:09:34  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [400/985]  eta: 0:09:25  lr: 0.000015  loss: 0.0197 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [410/985]  eta: 0:09:15  lr: 0.000015  loss: 0.0188 (0.0192)  time: 0.9610  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:829]  [420/985]  eta: 0:09:05  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [430/985]  eta: 0:08:55  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [440/985]  eta: 0:08:46  lr: 0.000015  loss: 0.0185 (0.0192)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [450/985]  eta: 0:08:36  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [460/985]  eta: 0:08:26  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [470/985]  eta: 0:08:16  lr: 0.000015  loss: 0.0170 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [480/985]  eta: 0:08:07  lr: 0.000015  loss: 0.0170 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [490/985]  eta: 0:07:57  lr: 0.000015  loss: 0.0196 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [500/985]  eta: 0:07:47  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [510/985]  eta: 0:07:37  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [520/985]  eta: 0:07:28  lr: 0.000015  loss: 0.0196 (0.0192)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [530/985]  eta: 0:07:18  lr: 0.000015  loss: 0.0196 (0.0192)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [540/985]  eta: 0:07:08  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [550/985]  eta: 0:06:59  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [560/985]  eta: 0:06:49  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [570/985]  eta: 0:06:39  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [580/985]  eta: 0:06:30  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [590/985]  eta: 0:06:20  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [600/985]  eta: 0:06:10  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [610/985]  eta: 0:06:01  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [620/985]  eta: 0:05:51  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [630/985]  eta: 0:05:41  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [640/985]  eta: 0:05:32  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [650/985]  eta: 0:05:22  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [660/985]  eta: 0:05:12  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [670/985]  eta: 0:05:03  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [680/985]  eta: 0:04:53  lr: 0.000015  loss: 0.0171 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [690/985]  eta: 0:04:43  lr: 0.000015  loss: 0.0160 (0.0190)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [700/985]  eta: 0:04:34  lr: 0.000015  loss: 0.0184 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [710/985]  eta: 0:04:24  lr: 0.000015  loss: 0.0203 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [720/985]  eta: 0:04:14  lr: 0.000015  loss: 0.0192 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [730/985]  eta: 0:04:05  lr: 0.000015  loss: 0.0192 (0.0191)  time: 0.9627  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [740/985]  eta: 0:03:55  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9637  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [750/985]  eta: 0:03:46  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [760/985]  eta: 0:03:36  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [770/985]  eta: 0:03:26  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [780/985]  eta: 0:03:17  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [790/985]  eta: 0:03:07  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [800/985]  eta: 0:02:57  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [810/985]  eta: 0:02:48  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [820/985]  eta: 0:02:38  lr: 0.000015  loss: 0.0199 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [830/985]  eta: 0:02:29  lr: 0.000015  loss: 0.0190 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [840/985]  eta: 0:02:19  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [850/985]  eta: 0:02:09  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [860/985]  eta: 0:02:00  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [870/985]  eta: 0:01:50  lr: 0.000015  loss: 0.0172 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [880/985]  eta: 0:01:40  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [890/985]  eta: 0:01:31  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [900/985]  eta: 0:01:21  lr: 0.000015  loss: 0.0192 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [910/985]  eta: 0:01:12  lr: 0.000015  loss: 0.0214 (0.0192)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [920/985]  eta: 0:01:02  lr: 0.000015  loss: 0.0198 (0.0192)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [930/985]  eta: 0:00:52  lr: 0.000015  loss: 0.0184 (0.0192)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [940/985]  eta: 0:00:43  lr: 0.000015  loss: 0.0186 (0.0192)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [950/985]  eta: 0:00:33  lr: 0.000015  loss: 0.0191 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [960/985]  eta: 0:00:24  lr: 0.000015  loss: 0.0193 (0.0192)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [970/985]  eta: 0:00:14  lr: 0.000015  loss: 0.0198 (0.0192)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [980/985]  eta: 0:00:04  lr: 0.000015  loss: 0.0179 (0.0192)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829]  [984/985]  eta: 0:00:00  lr: 0.000015  loss: 0.0179 (0.0192)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:829] Total time: 0:15:46 (0.9606 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.0179 (0.0192)\n",
      "Valid: [epoch:829]  [ 0/14]  eta: 0:02:57  loss: 0.0130 (0.0130)  time: 12.6806  data: 0.6694  max mem: 41892\n",
      "Valid: [epoch:829]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0145)  time: 11.9607  data: 0.0479  max mem: 41892\n",
      "Valid: [epoch:829] Total time: 0:02:47 (11.9667 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_829_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:830]  [  0/985]  eta: 1:20:04  lr: 0.000015  loss: 0.0187 (0.0187)  time: 4.8775  data: 3.8571  max mem: 41892\n",
      "Train: [epoch:830]  [ 10/985]  eta: 0:21:16  lr: 0.000015  loss: 0.0187 (0.0192)  time: 1.3096  data: 0.3508  max mem: 41892\n",
      "Train: [epoch:830]  [ 20/985]  eta: 0:18:13  lr: 0.000015  loss: 0.0188 (0.0193)  time: 0.9458  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:830]  [ 30/985]  eta: 0:17:02  lr: 0.000015  loss: 0.0188 (0.0194)  time: 0.9396  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [ 40/985]  eta: 0:16:25  lr: 0.000015  loss: 0.0176 (0.0186)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [ 50/985]  eta: 0:15:57  lr: 0.000015  loss: 0.0165 (0.0184)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [ 60/985]  eta: 0:15:35  lr: 0.000015  loss: 0.0181 (0.0186)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [ 70/985]  eta: 0:15:17  lr: 0.000015  loss: 0.0180 (0.0185)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [ 80/985]  eta: 0:15:01  lr: 0.000015  loss: 0.0167 (0.0182)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [ 90/985]  eta: 0:14:46  lr: 0.000015  loss: 0.0169 (0.0183)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [100/985]  eta: 0:14:33  lr: 0.000015  loss: 0.0179 (0.0184)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [110/985]  eta: 0:14:20  lr: 0.000015  loss: 0.0172 (0.0183)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [120/985]  eta: 0:14:08  lr: 0.000015  loss: 0.0174 (0.0184)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [130/985]  eta: 0:13:57  lr: 0.000015  loss: 0.0193 (0.0186)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [140/985]  eta: 0:13:46  lr: 0.000015  loss: 0.0196 (0.0187)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [150/985]  eta: 0:13:34  lr: 0.000015  loss: 0.0196 (0.0188)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [160/985]  eta: 0:13:24  lr: 0.000015  loss: 0.0178 (0.0188)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [170/985]  eta: 0:13:13  lr: 0.000015  loss: 0.0178 (0.0188)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [180/985]  eta: 0:13:03  lr: 0.000015  loss: 0.0199 (0.0190)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [190/985]  eta: 0:12:52  lr: 0.000015  loss: 0.0204 (0.0190)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [200/985]  eta: 0:12:42  lr: 0.000015  loss: 0.0193 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [210/985]  eta: 0:12:32  lr: 0.000015  loss: 0.0180 (0.0190)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [220/985]  eta: 0:12:22  lr: 0.000015  loss: 0.0177 (0.0189)  time: 0.9678  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [230/985]  eta: 0:12:12  lr: 0.000015  loss: 0.0170 (0.0189)  time: 0.9659  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [240/985]  eta: 0:12:02  lr: 0.000015  loss: 0.0179 (0.0189)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [250/985]  eta: 0:11:52  lr: 0.000015  loss: 0.0189 (0.0189)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [260/985]  eta: 0:11:42  lr: 0.000015  loss: 0.0177 (0.0189)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [270/985]  eta: 0:11:32  lr: 0.000015  loss: 0.0179 (0.0189)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [280/985]  eta: 0:11:22  lr: 0.000015  loss: 0.0184 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [290/985]  eta: 0:11:12  lr: 0.000015  loss: 0.0197 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [300/985]  eta: 0:11:03  lr: 0.000015  loss: 0.0197 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [310/985]  eta: 0:10:53  lr: 0.000015  loss: 0.0197 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [320/985]  eta: 0:10:43  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [330/985]  eta: 0:10:33  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [340/985]  eta: 0:10:23  lr: 0.000015  loss: 0.0193 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [350/985]  eta: 0:10:13  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [360/985]  eta: 0:10:03  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [370/985]  eta: 0:09:54  lr: 0.000015  loss: 0.0174 (0.0190)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [380/985]  eta: 0:09:44  lr: 0.000015  loss: 0.0174 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [390/985]  eta: 0:09:34  lr: 0.000015  loss: 0.0176 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [400/985]  eta: 0:09:24  lr: 0.000015  loss: 0.0190 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [410/985]  eta: 0:09:15  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [420/985]  eta: 0:09:05  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [430/985]  eta: 0:08:55  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [440/985]  eta: 0:08:45  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [450/985]  eta: 0:08:36  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [460/985]  eta: 0:08:26  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [470/985]  eta: 0:08:16  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [480/985]  eta: 0:08:06  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [490/985]  eta: 0:07:57  lr: 0.000015  loss: 0.0201 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [500/985]  eta: 0:07:47  lr: 0.000015  loss: 0.0200 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [510/985]  eta: 0:07:37  lr: 0.000015  loss: 0.0197 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [520/985]  eta: 0:07:28  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [530/985]  eta: 0:07:18  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [540/985]  eta: 0:07:08  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [550/985]  eta: 0:06:58  lr: 0.000015  loss: 0.0170 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [560/985]  eta: 0:06:49  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [570/985]  eta: 0:06:39  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [580/985]  eta: 0:06:29  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [590/985]  eta: 0:06:20  lr: 0.000015  loss: 0.0173 (0.0190)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [600/985]  eta: 0:06:10  lr: 0.000015  loss: 0.0170 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [610/985]  eta: 0:06:00  lr: 0.000015  loss: 0.0170 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [620/985]  eta: 0:05:51  lr: 0.000015  loss: 0.0180 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [630/985]  eta: 0:05:41  lr: 0.000015  loss: 0.0180 (0.0190)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [640/985]  eta: 0:05:31  lr: 0.000015  loss: 0.0158 (0.0189)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [650/985]  eta: 0:05:22  lr: 0.000015  loss: 0.0178 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [660/985]  eta: 0:05:12  lr: 0.000015  loss: 0.0186 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [670/985]  eta: 0:05:02  lr: 0.000015  loss: 0.0181 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [680/985]  eta: 0:04:53  lr: 0.000015  loss: 0.0181 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:830]  [690/985]  eta: 0:04:43  lr: 0.000015  loss: 0.0181 (0.0190)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [700/985]  eta: 0:04:33  lr: 0.000015  loss: 0.0183 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [710/985]  eta: 0:04:24  lr: 0.000015  loss: 0.0203 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [720/985]  eta: 0:04:14  lr: 0.000015  loss: 0.0190 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [730/985]  eta: 0:04:05  lr: 0.000015  loss: 0.0184 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [740/985]  eta: 0:03:55  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [750/985]  eta: 0:03:45  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [760/985]  eta: 0:03:36  lr: 0.000015  loss: 0.0188 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [770/985]  eta: 0:03:26  lr: 0.000015  loss: 0.0218 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [780/985]  eta: 0:03:16  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [790/985]  eta: 0:03:07  lr: 0.000015  loss: 0.0172 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [800/985]  eta: 0:02:57  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [810/985]  eta: 0:02:48  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [820/985]  eta: 0:02:38  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [830/985]  eta: 0:02:28  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [840/985]  eta: 0:02:19  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [850/985]  eta: 0:02:09  lr: 0.000015  loss: 0.0163 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [860/985]  eta: 0:02:00  lr: 0.000015  loss: 0.0169 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [870/985]  eta: 0:01:50  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [880/985]  eta: 0:01:40  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [890/985]  eta: 0:01:31  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [900/985]  eta: 0:01:21  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [910/985]  eta: 0:01:12  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [920/985]  eta: 0:01:02  lr: 0.000015  loss: 0.0200 (0.0191)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [930/985]  eta: 0:00:52  lr: 0.000015  loss: 0.0196 (0.0191)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [940/985]  eta: 0:00:43  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9677  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [950/985]  eta: 0:00:33  lr: 0.000015  loss: 0.0190 (0.0191)  time: 0.9648  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [960/985]  eta: 0:00:24  lr: 0.000015  loss: 0.0194 (0.0191)  time: 0.9638  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [970/985]  eta: 0:00:14  lr: 0.000015  loss: 0.0194 (0.0191)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [980/985]  eta: 0:00:04  lr: 0.000015  loss: 0.0195 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830]  [984/985]  eta: 0:00:00  lr: 0.000015  loss: 0.0194 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:830] Total time: 0:15:46 (0.9606 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.0194 (0.0191)\n",
      "Valid: [epoch:830]  [ 0/14]  eta: 0:03:00  loss: 0.0148 (0.0148)  time: 12.8937  data: 0.5798  max mem: 41892\n",
      "Valid: [epoch:830]  [13/14]  eta: 0:00:12  loss: 0.0144 (0.0143)  time: 12.0587  data: 0.0415  max mem: 41892\n",
      "Valid: [epoch:830] Total time: 0:02:48 (12.0656 s / it)\n",
      "Averaged stats: loss: 0.0144 (0.0143)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_830_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:831]  [  0/985]  eta: 1:18:59  lr: 0.000015  loss: 0.0170 (0.0170)  time: 4.8118  data: 3.8226  max mem: 41892\n",
      "Train: [epoch:831]  [ 10/985]  eta: 0:21:11  lr: 0.000015  loss: 0.0199 (0.0204)  time: 1.3041  data: 0.3476  max mem: 41892\n",
      "Train: [epoch:831]  [ 20/985]  eta: 0:18:12  lr: 0.000015  loss: 0.0183 (0.0193)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [ 30/985]  eta: 0:17:01  lr: 0.000015  loss: 0.0183 (0.0195)  time: 0.9414  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [ 40/985]  eta: 0:16:23  lr: 0.000015  loss: 0.0184 (0.0189)  time: 0.9450  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [ 50/985]  eta: 0:15:56  lr: 0.000015  loss: 0.0156 (0.0183)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [ 60/985]  eta: 0:15:34  lr: 0.000015  loss: 0.0168 (0.0185)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [ 70/985]  eta: 0:15:16  lr: 0.000015  loss: 0.0177 (0.0183)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [ 80/985]  eta: 0:15:00  lr: 0.000015  loss: 0.0171 (0.0183)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [ 90/985]  eta: 0:14:47  lr: 0.000015  loss: 0.0180 (0.0183)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [100/985]  eta: 0:14:33  lr: 0.000015  loss: 0.0186 (0.0185)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [110/985]  eta: 0:14:21  lr: 0.000015  loss: 0.0189 (0.0186)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [120/985]  eta: 0:14:09  lr: 0.000015  loss: 0.0196 (0.0188)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [130/985]  eta: 0:13:58  lr: 0.000015  loss: 0.0194 (0.0188)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [140/985]  eta: 0:13:47  lr: 0.000015  loss: 0.0187 (0.0189)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [150/985]  eta: 0:13:36  lr: 0.000015  loss: 0.0187 (0.0190)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [160/985]  eta: 0:13:25  lr: 0.000015  loss: 0.0189 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [170/985]  eta: 0:13:14  lr: 0.000015  loss: 0.0180 (0.0189)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [180/985]  eta: 0:13:04  lr: 0.000015  loss: 0.0175 (0.0189)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [190/985]  eta: 0:12:53  lr: 0.000015  loss: 0.0174 (0.0189)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [200/985]  eta: 0:12:42  lr: 0.000015  loss: 0.0174 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [210/985]  eta: 0:12:32  lr: 0.000015  loss: 0.0179 (0.0189)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [220/985]  eta: 0:12:22  lr: 0.000015  loss: 0.0176 (0.0189)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [230/985]  eta: 0:12:12  lr: 0.000015  loss: 0.0177 (0.0189)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [240/985]  eta: 0:12:02  lr: 0.000015  loss: 0.0180 (0.0189)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [250/985]  eta: 0:11:52  lr: 0.000015  loss: 0.0178 (0.0189)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [260/985]  eta: 0:11:42  lr: 0.000015  loss: 0.0171 (0.0188)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [270/985]  eta: 0:11:32  lr: 0.000015  loss: 0.0171 (0.0188)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [280/985]  eta: 0:11:22  lr: 0.000015  loss: 0.0171 (0.0188)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [290/985]  eta: 0:11:12  lr: 0.000015  loss: 0.0180 (0.0188)  time: 0.9586  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:831]  [300/985]  eta: 0:11:02  lr: 0.000015  loss: 0.0191 (0.0188)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [310/985]  eta: 0:10:52  lr: 0.000015  loss: 0.0177 (0.0188)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [320/985]  eta: 0:10:42  lr: 0.000015  loss: 0.0187 (0.0189)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [330/985]  eta: 0:10:32  lr: 0.000015  loss: 0.0196 (0.0189)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [340/985]  eta: 0:10:23  lr: 0.000015  loss: 0.0195 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [350/985]  eta: 0:10:13  lr: 0.000015  loss: 0.0184 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [360/985]  eta: 0:10:03  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [370/985]  eta: 0:09:53  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9626  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [380/985]  eta: 0:09:44  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9642  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [390/985]  eta: 0:09:34  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9642  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [400/985]  eta: 0:09:24  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [410/985]  eta: 0:09:14  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [420/985]  eta: 0:09:05  lr: 0.000015  loss: 0.0183 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [430/985]  eta: 0:08:55  lr: 0.000015  loss: 0.0183 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [440/985]  eta: 0:08:45  lr: 0.000015  loss: 0.0174 (0.0190)  time: 0.9627  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [450/985]  eta: 0:08:35  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [460/985]  eta: 0:08:26  lr: 0.000015  loss: 0.0188 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [470/985]  eta: 0:08:16  lr: 0.000015  loss: 0.0201 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [480/985]  eta: 0:08:06  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [490/985]  eta: 0:07:56  lr: 0.000015  loss: 0.0190 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [500/985]  eta: 0:07:47  lr: 0.000015  loss: 0.0195 (0.0191)  time: 0.9542  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:831]  [510/985]  eta: 0:07:37  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9595  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:831]  [520/985]  eta: 0:07:27  lr: 0.000015  loss: 0.0194 (0.0192)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [530/985]  eta: 0:07:18  lr: 0.000015  loss: 0.0210 (0.0192)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [540/985]  eta: 0:07:08  lr: 0.000015  loss: 0.0187 (0.0192)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [550/985]  eta: 0:06:58  lr: 0.000015  loss: 0.0192 (0.0192)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [560/985]  eta: 0:06:49  lr: 0.000015  loss: 0.0183 (0.0192)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [570/985]  eta: 0:06:39  lr: 0.000015  loss: 0.0169 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [580/985]  eta: 0:06:29  lr: 0.000015  loss: 0.0184 (0.0192)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [590/985]  eta: 0:06:20  lr: 0.000015  loss: 0.0185 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [600/985]  eta: 0:06:10  lr: 0.000015  loss: 0.0183 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [610/985]  eta: 0:06:00  lr: 0.000015  loss: 0.0179 (0.0192)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [620/985]  eta: 0:05:51  lr: 0.000015  loss: 0.0186 (0.0192)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [630/985]  eta: 0:05:41  lr: 0.000015  loss: 0.0211 (0.0193)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [640/985]  eta: 0:05:31  lr: 0.000015  loss: 0.0202 (0.0193)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [650/985]  eta: 0:05:22  lr: 0.000015  loss: 0.0196 (0.0193)  time: 0.9640  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [660/985]  eta: 0:05:12  lr: 0.000015  loss: 0.0180 (0.0193)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [670/985]  eta: 0:05:02  lr: 0.000015  loss: 0.0168 (0.0193)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [680/985]  eta: 0:04:53  lr: 0.000015  loss: 0.0170 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [690/985]  eta: 0:04:43  lr: 0.000015  loss: 0.0167 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [700/985]  eta: 0:04:34  lr: 0.000015  loss: 0.0170 (0.0192)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [710/985]  eta: 0:04:24  lr: 0.000015  loss: 0.0180 (0.0192)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [720/985]  eta: 0:04:14  lr: 0.000015  loss: 0.0198 (0.0192)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [730/985]  eta: 0:04:05  lr: 0.000015  loss: 0.0183 (0.0192)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [740/985]  eta: 0:03:55  lr: 0.000015  loss: 0.0176 (0.0192)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [750/985]  eta: 0:03:45  lr: 0.000015  loss: 0.0182 (0.0192)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [760/985]  eta: 0:03:36  lr: 0.000015  loss: 0.0182 (0.0192)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [770/985]  eta: 0:03:26  lr: 0.000015  loss: 0.0175 (0.0192)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [780/985]  eta: 0:03:17  lr: 0.000015  loss: 0.0175 (0.0192)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [790/985]  eta: 0:03:07  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [800/985]  eta: 0:02:57  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [810/985]  eta: 0:02:48  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [820/985]  eta: 0:02:38  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [830/985]  eta: 0:02:28  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [840/985]  eta: 0:02:19  lr: 0.000015  loss: 0.0172 (0.0191)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [850/985]  eta: 0:02:09  lr: 0.000015  loss: 0.0166 (0.0191)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [860/985]  eta: 0:02:00  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [870/985]  eta: 0:01:50  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [880/985]  eta: 0:01:40  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [890/985]  eta: 0:01:31  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [900/985]  eta: 0:01:21  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [910/985]  eta: 0:01:12  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [920/985]  eta: 0:01:02  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [930/985]  eta: 0:00:52  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [940/985]  eta: 0:00:43  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [950/985]  eta: 0:00:33  lr: 0.000015  loss: 0.0193 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:831]  [960/985]  eta: 0:00:24  lr: 0.000015  loss: 0.0203 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [970/985]  eta: 0:00:14  lr: 0.000015  loss: 0.0190 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [980/985]  eta: 0:00:04  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831]  [984/985]  eta: 0:00:00  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:831] Total time: 0:15:46 (0.9607 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.0182 (0.0191)\n",
      "Valid: [epoch:831]  [ 0/14]  eta: 0:03:00  loss: 0.0148 (0.0148)  time: 12.8808  data: 0.6372  max mem: 41892\n",
      "Valid: [epoch:831]  [13/14]  eta: 0:00:12  loss: 0.0145 (0.0145)  time: 12.1628  data: 0.0456  max mem: 41892\n",
      "Valid: [epoch:831] Total time: 0:02:50 (12.1692 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_831_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:832]  [  0/985]  eta: 1:03:56  lr: 0.000015  loss: 0.0202 (0.0202)  time: 3.8949  data: 2.9003  max mem: 41892\n",
      "Train: [epoch:832]  [ 10/985]  eta: 0:19:46  lr: 0.000015  loss: 0.0181 (0.0177)  time: 1.2169  data: 0.2638  max mem: 41892\n",
      "Train: [epoch:832]  [ 20/985]  eta: 0:17:27  lr: 0.000015  loss: 0.0182 (0.0184)  time: 0.9447  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [ 30/985]  eta: 0:16:33  lr: 0.000015  loss: 0.0182 (0.0185)  time: 0.9432  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [ 40/985]  eta: 0:16:00  lr: 0.000015  loss: 0.0175 (0.0182)  time: 0.9443  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [ 50/985]  eta: 0:15:37  lr: 0.000015  loss: 0.0162 (0.0179)  time: 0.9449  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [ 60/985]  eta: 0:15:19  lr: 0.000015  loss: 0.0173 (0.0181)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [ 70/985]  eta: 0:15:03  lr: 0.000015  loss: 0.0173 (0.0179)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [ 80/985]  eta: 0:14:49  lr: 0.000015  loss: 0.0156 (0.0178)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [ 90/985]  eta: 0:14:38  lr: 0.000015  loss: 0.0163 (0.0178)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [100/985]  eta: 0:14:25  lr: 0.000015  loss: 0.0174 (0.0178)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [110/985]  eta: 0:14:14  lr: 0.000015  loss: 0.0184 (0.0180)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [120/985]  eta: 0:14:02  lr: 0.000015  loss: 0.0187 (0.0181)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [130/985]  eta: 0:13:51  lr: 0.000015  loss: 0.0187 (0.0184)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [140/985]  eta: 0:13:40  lr: 0.000015  loss: 0.0192 (0.0185)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [150/985]  eta: 0:13:30  lr: 0.000015  loss: 0.0188 (0.0185)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [160/985]  eta: 0:13:19  lr: 0.000015  loss: 0.0171 (0.0185)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [170/985]  eta: 0:13:09  lr: 0.000015  loss: 0.0170 (0.0184)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [180/985]  eta: 0:12:58  lr: 0.000015  loss: 0.0170 (0.0185)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [190/985]  eta: 0:12:49  lr: 0.000015  loss: 0.0176 (0.0185)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [200/985]  eta: 0:12:39  lr: 0.000015  loss: 0.0190 (0.0186)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [210/985]  eta: 0:12:28  lr: 0.000015  loss: 0.0185 (0.0185)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [220/985]  eta: 0:12:19  lr: 0.000015  loss: 0.0175 (0.0185)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [230/985]  eta: 0:12:09  lr: 0.000015  loss: 0.0166 (0.0184)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [240/985]  eta: 0:11:59  lr: 0.000015  loss: 0.0174 (0.0185)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [250/985]  eta: 0:11:49  lr: 0.000015  loss: 0.0177 (0.0185)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [260/985]  eta: 0:11:39  lr: 0.000015  loss: 0.0177 (0.0185)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [270/985]  eta: 0:11:29  lr: 0.000015  loss: 0.0183 (0.0185)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [280/985]  eta: 0:11:19  lr: 0.000015  loss: 0.0180 (0.0185)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [290/985]  eta: 0:11:09  lr: 0.000015  loss: 0.0184 (0.0186)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [300/985]  eta: 0:11:00  lr: 0.000015  loss: 0.0196 (0.0187)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [310/985]  eta: 0:10:50  lr: 0.000015  loss: 0.0193 (0.0187)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [320/985]  eta: 0:10:40  lr: 0.000015  loss: 0.0188 (0.0188)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [330/985]  eta: 0:10:30  lr: 0.000015  loss: 0.0183 (0.0188)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [340/985]  eta: 0:10:21  lr: 0.000015  loss: 0.0183 (0.0188)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [350/985]  eta: 0:10:11  lr: 0.000015  loss: 0.0183 (0.0188)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [360/985]  eta: 0:10:01  lr: 0.000015  loss: 0.0177 (0.0188)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [370/985]  eta: 0:09:51  lr: 0.000015  loss: 0.0179 (0.0188)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [380/985]  eta: 0:09:42  lr: 0.000015  loss: 0.0179 (0.0188)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [390/985]  eta: 0:09:32  lr: 0.000015  loss: 0.0183 (0.0188)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [400/985]  eta: 0:09:22  lr: 0.000015  loss: 0.0189 (0.0188)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [410/985]  eta: 0:09:13  lr: 0.000015  loss: 0.0197 (0.0188)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [420/985]  eta: 0:09:03  lr: 0.000015  loss: 0.0197 (0.0188)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [430/985]  eta: 0:08:53  lr: 0.000015  loss: 0.0194 (0.0189)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [440/985]  eta: 0:08:44  lr: 0.000015  loss: 0.0210 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [450/985]  eta: 0:08:34  lr: 0.000015  loss: 0.0220 (0.0190)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [460/985]  eta: 0:08:24  lr: 0.000015  loss: 0.0188 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [470/985]  eta: 0:08:15  lr: 0.000015  loss: 0.0184 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [480/985]  eta: 0:08:05  lr: 0.000015  loss: 0.0178 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [490/985]  eta: 0:07:55  lr: 0.000015  loss: 0.0179 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [500/985]  eta: 0:07:46  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [510/985]  eta: 0:07:36  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [520/985]  eta: 0:07:26  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [530/985]  eta: 0:07:17  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [540/985]  eta: 0:07:07  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [550/985]  eta: 0:06:58  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [560/985]  eta: 0:06:48  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:832]  [570/985]  eta: 0:06:38  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [580/985]  eta: 0:06:29  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [590/985]  eta: 0:06:19  lr: 0.000015  loss: 0.0179 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [600/985]  eta: 0:06:09  lr: 0.000015  loss: 0.0173 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [610/985]  eta: 0:06:00  lr: 0.000015  loss: 0.0175 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [620/985]  eta: 0:05:50  lr: 0.000015  loss: 0.0186 (0.0190)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [630/985]  eta: 0:05:40  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [640/985]  eta: 0:05:31  lr: 0.000015  loss: 0.0181 (0.0190)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [650/985]  eta: 0:05:21  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [660/985]  eta: 0:05:12  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [670/985]  eta: 0:05:02  lr: 0.000015  loss: 0.0175 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [680/985]  eta: 0:04:52  lr: 0.000015  loss: 0.0173 (0.0190)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [690/985]  eta: 0:04:43  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [700/985]  eta: 0:04:33  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [710/985]  eta: 0:04:24  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [720/985]  eta: 0:04:14  lr: 0.000015  loss: 0.0194 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [730/985]  eta: 0:04:04  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [740/985]  eta: 0:03:55  lr: 0.000015  loss: 0.0161 (0.0190)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [750/985]  eta: 0:03:45  lr: 0.000015  loss: 0.0183 (0.0190)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [760/985]  eta: 0:03:36  lr: 0.000015  loss: 0.0187 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [770/985]  eta: 0:03:26  lr: 0.000015  loss: 0.0176 (0.0190)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [780/985]  eta: 0:03:16  lr: 0.000015  loss: 0.0180 (0.0190)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [790/985]  eta: 0:03:07  lr: 0.000015  loss: 0.0182 (0.0190)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [800/985]  eta: 0:02:57  lr: 0.000015  loss: 0.0187 (0.0190)  time: 0.9643  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [810/985]  eta: 0:02:48  lr: 0.000015  loss: 0.0193 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [820/985]  eta: 0:02:38  lr: 0.000015  loss: 0.0204 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [830/985]  eta: 0:02:28  lr: 0.000015  loss: 0.0205 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [840/985]  eta: 0:02:19  lr: 0.000015  loss: 0.0175 (0.0190)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [850/985]  eta: 0:02:09  lr: 0.000015  loss: 0.0175 (0.0190)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [860/985]  eta: 0:02:00  lr: 0.000015  loss: 0.0182 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [870/985]  eta: 0:01:50  lr: 0.000015  loss: 0.0188 (0.0190)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [880/985]  eta: 0:01:40  lr: 0.000015  loss: 0.0186 (0.0190)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [890/985]  eta: 0:01:31  lr: 0.000015  loss: 0.0187 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [900/985]  eta: 0:01:21  lr: 0.000015  loss: 0.0208 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [910/985]  eta: 0:01:12  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [920/985]  eta: 0:01:02  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [930/985]  eta: 0:00:52  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [940/985]  eta: 0:00:43  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [950/985]  eta: 0:00:33  lr: 0.000015  loss: 0.0189 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [960/985]  eta: 0:00:23  lr: 0.000015  loss: 0.0195 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [970/985]  eta: 0:00:14  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [980/985]  eta: 0:00:04  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832]  [984/985]  eta: 0:00:00  lr: 0.000015  loss: 0.0201 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:832] Total time: 0:15:45 (0.9599 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.0201 (0.0191)\n",
      "Valid: [epoch:832]  [ 0/14]  eta: 0:02:57  loss: 0.0136 (0.0136)  time: 12.6645  data: 0.5918  max mem: 41892\n",
      "Valid: [epoch:832]  [13/14]  eta: 0:00:11  loss: 0.0144 (0.0144)  time: 11.9309  data: 0.0424  max mem: 41892\n",
      "Valid: [epoch:832] Total time: 0:02:47 (11.9371 s / it)\n",
      "Averaged stats: loss: 0.0144 (0.0144)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_832_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:833]  [  0/985]  eta: 0:53:26  lr: 0.000015  loss: 0.0170 (0.0170)  time: 3.2552  data: 2.2365  max mem: 41892\n",
      "Train: [epoch:833]  [ 10/985]  eta: 0:19:11  lr: 0.000015  loss: 0.0170 (0.0181)  time: 1.1811  data: 0.2035  max mem: 41892\n",
      "Train: [epoch:833]  [ 20/985]  eta: 0:17:08  lr: 0.000015  loss: 0.0180 (0.0198)  time: 0.9561  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:833]  [ 30/985]  eta: 0:16:22  lr: 0.000015  loss: 0.0183 (0.0194)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [ 40/985]  eta: 0:15:53  lr: 0.000015  loss: 0.0174 (0.0189)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [ 50/985]  eta: 0:15:31  lr: 0.000015  loss: 0.0158 (0.0185)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [ 60/985]  eta: 0:15:14  lr: 0.000015  loss: 0.0177 (0.0189)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [ 70/985]  eta: 0:14:59  lr: 0.000015  loss: 0.0177 (0.0187)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [ 80/985]  eta: 0:14:47  lr: 0.000015  loss: 0.0158 (0.0185)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [ 90/985]  eta: 0:14:35  lr: 0.000015  loss: 0.0158 (0.0183)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [100/985]  eta: 0:14:23  lr: 0.000015  loss: 0.0183 (0.0185)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [110/985]  eta: 0:14:12  lr: 0.000015  loss: 0.0182 (0.0185)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [120/985]  eta: 0:14:01  lr: 0.000015  loss: 0.0184 (0.0188)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [130/985]  eta: 0:13:50  lr: 0.000015  loss: 0.0187 (0.0188)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [140/985]  eta: 0:13:40  lr: 0.000015  loss: 0.0184 (0.0189)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [150/985]  eta: 0:13:29  lr: 0.000015  loss: 0.0197 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [160/985]  eta: 0:13:19  lr: 0.000015  loss: 0.0192 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [170/985]  eta: 0:13:09  lr: 0.000015  loss: 0.0174 (0.0190)  time: 0.9645  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:833]  [180/985]  eta: 0:12:59  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9667  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [190/985]  eta: 0:12:49  lr: 0.000015  loss: 0.0191 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [200/985]  eta: 0:12:39  lr: 0.000015  loss: 0.0192 (0.0192)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [210/985]  eta: 0:12:29  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [220/985]  eta: 0:12:19  lr: 0.000015  loss: 0.0178 (0.0190)  time: 0.9648  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [230/985]  eta: 0:12:09  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [240/985]  eta: 0:11:59  lr: 0.000015  loss: 0.0193 (0.0192)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [250/985]  eta: 0:11:49  lr: 0.000015  loss: 0.0189 (0.0191)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [260/985]  eta: 0:11:39  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [270/985]  eta: 0:11:30  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [280/985]  eta: 0:11:20  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [290/985]  eta: 0:11:10  lr: 0.000015  loss: 0.0173 (0.0190)  time: 0.9658  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [300/985]  eta: 0:11:00  lr: 0.000015  loss: 0.0175 (0.0190)  time: 0.9649  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [310/985]  eta: 0:10:51  lr: 0.000015  loss: 0.0166 (0.0190)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [320/985]  eta: 0:10:41  lr: 0.000015  loss: 0.0166 (0.0190)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [330/985]  eta: 0:10:31  lr: 0.000015  loss: 0.0175 (0.0190)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [340/985]  eta: 0:10:22  lr: 0.000015  loss: 0.0177 (0.0189)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [350/985]  eta: 0:10:12  lr: 0.000015  loss: 0.0177 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [360/985]  eta: 0:10:02  lr: 0.000015  loss: 0.0179 (0.0190)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [370/985]  eta: 0:09:52  lr: 0.000015  loss: 0.0197 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [380/985]  eta: 0:09:43  lr: 0.000015  loss: 0.0196 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [390/985]  eta: 0:09:33  lr: 0.000015  loss: 0.0176 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [400/985]  eta: 0:09:23  lr: 0.000015  loss: 0.0174 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [410/985]  eta: 0:09:13  lr: 0.000015  loss: 0.0200 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [420/985]  eta: 0:09:04  lr: 0.000015  loss: 0.0190 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [430/985]  eta: 0:08:54  lr: 0.000015  loss: 0.0170 (0.0190)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [440/985]  eta: 0:08:44  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [450/985]  eta: 0:08:35  lr: 0.000015  loss: 0.0184 (0.0190)  time: 0.9630  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [460/985]  eta: 0:08:25  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [470/985]  eta: 0:08:15  lr: 0.000015  loss: 0.0177 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [480/985]  eta: 0:08:06  lr: 0.000015  loss: 0.0170 (0.0190)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [490/985]  eta: 0:07:56  lr: 0.000015  loss: 0.0183 (0.0190)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [500/985]  eta: 0:07:46  lr: 0.000015  loss: 0.0185 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [510/985]  eta: 0:07:37  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [520/985]  eta: 0:07:27  lr: 0.000015  loss: 0.0190 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [530/985]  eta: 0:07:17  lr: 0.000015  loss: 0.0190 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [540/985]  eta: 0:07:08  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [550/985]  eta: 0:06:58  lr: 0.000015  loss: 0.0172 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [560/985]  eta: 0:06:48  lr: 0.000015  loss: 0.0172 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [570/985]  eta: 0:06:39  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [580/985]  eta: 0:06:29  lr: 0.000015  loss: 0.0183 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [590/985]  eta: 0:06:19  lr: 0.000015  loss: 0.0193 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [600/985]  eta: 0:06:10  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [610/985]  eta: 0:06:00  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [620/985]  eta: 0:05:51  lr: 0.000015  loss: 0.0188 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [630/985]  eta: 0:05:41  lr: 0.000015  loss: 0.0200 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [640/985]  eta: 0:05:31  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [650/985]  eta: 0:05:22  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9636  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [660/985]  eta: 0:05:12  lr: 0.000015  loss: 0.0177 (0.0191)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [670/985]  eta: 0:05:02  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [680/985]  eta: 0:04:53  lr: 0.000015  loss: 0.0176 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [690/985]  eta: 0:04:43  lr: 0.000015  loss: 0.0170 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [700/985]  eta: 0:04:33  lr: 0.000015  loss: 0.0173 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [710/985]  eta: 0:04:24  lr: 0.000015  loss: 0.0187 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [720/985]  eta: 0:04:14  lr: 0.000015  loss: 0.0192 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [730/985]  eta: 0:04:05  lr: 0.000015  loss: 0.0192 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [740/985]  eta: 0:03:55  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [750/985]  eta: 0:03:45  lr: 0.000015  loss: 0.0184 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [760/985]  eta: 0:03:36  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [770/985]  eta: 0:03:26  lr: 0.000015  loss: 0.0179 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [780/985]  eta: 0:03:16  lr: 0.000015  loss: 0.0185 (0.0191)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [790/985]  eta: 0:03:07  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [800/985]  eta: 0:02:57  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [810/985]  eta: 0:02:48  lr: 0.000015  loss: 0.0201 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [820/985]  eta: 0:02:38  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [830/985]  eta: 0:02:28  lr: 0.000015  loss: 0.0174 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:833]  [840/985]  eta: 0:02:19  lr: 0.000015  loss: 0.0178 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [850/985]  eta: 0:02:09  lr: 0.000015  loss: 0.0171 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [860/985]  eta: 0:02:00  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [870/985]  eta: 0:01:50  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [880/985]  eta: 0:01:40  lr: 0.000015  loss: 0.0188 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [890/985]  eta: 0:01:31  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [900/985]  eta: 0:01:21  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [910/985]  eta: 0:01:12  lr: 0.000015  loss: 0.0193 (0.0191)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [920/985]  eta: 0:01:02  lr: 0.000015  loss: 0.0182 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [930/985]  eta: 0:00:52  lr: 0.000015  loss: 0.0171 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [940/985]  eta: 0:00:43  lr: 0.000015  loss: 0.0180 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [950/985]  eta: 0:00:33  lr: 0.000015  loss: 0.0186 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [960/985]  eta: 0:00:24  lr: 0.000015  loss: 0.0201 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [970/985]  eta: 0:00:14  lr: 0.000015  loss: 0.0181 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [980/985]  eta: 0:00:04  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833]  [984/985]  eta: 0:00:00  lr: 0.000015  loss: 0.0175 (0.0191)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:833] Total time: 0:15:45 (0.9603 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.0175 (0.0191)\n",
      "Valid: [epoch:833]  [ 0/14]  eta: 0:02:57  loss: 0.0128 (0.0128)  time: 12.7114  data: 0.5641  max mem: 41892\n",
      "Valid: [epoch:833]  [13/14]  eta: 0:00:11  loss: 0.0145 (0.0145)  time: 11.9483  data: 0.0404  max mem: 41892\n",
      "Valid: [epoch:833] Total time: 0:02:47 (11.9547 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_833_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:834]  [  0/985]  eta: 1:21:33  lr: 0.000014  loss: 0.0230 (0.0230)  time: 4.9684  data: 3.9394  max mem: 41892\n",
      "Train: [epoch:834]  [ 10/985]  eta: 0:21:32  lr: 0.000014  loss: 0.0180 (0.0191)  time: 1.3252  data: 0.3583  max mem: 41892\n",
      "Train: [epoch:834]  [ 20/985]  eta: 0:18:21  lr: 0.000014  loss: 0.0182 (0.0196)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [ 30/985]  eta: 0:17:08  lr: 0.000014  loss: 0.0187 (0.0196)  time: 0.9404  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [ 40/985]  eta: 0:16:27  lr: 0.000014  loss: 0.0182 (0.0192)  time: 0.9428  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [ 50/985]  eta: 0:15:58  lr: 0.000014  loss: 0.0173 (0.0188)  time: 0.9455  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [ 60/985]  eta: 0:15:36  lr: 0.000014  loss: 0.0183 (0.0199)  time: 0.9469  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [ 70/985]  eta: 0:15:20  lr: 0.000014  loss: 0.0231 (0.0198)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [ 80/985]  eta: 0:15:04  lr: 0.000014  loss: 0.0152 (0.0192)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [ 90/985]  eta: 0:14:50  lr: 0.000014  loss: 0.0153 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [100/985]  eta: 0:14:36  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [110/985]  eta: 0:14:24  lr: 0.000014  loss: 0.0176 (0.0189)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [120/985]  eta: 0:14:11  lr: 0.000014  loss: 0.0190 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [130/985]  eta: 0:13:59  lr: 0.000014  loss: 0.0194 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [140/985]  eta: 0:13:49  lr: 0.000014  loss: 0.0170 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [150/985]  eta: 0:13:37  lr: 0.000014  loss: 0.0172 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [160/985]  eta: 0:13:27  lr: 0.000014  loss: 0.0187 (0.0193)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [170/985]  eta: 0:13:16  lr: 0.000014  loss: 0.0194 (0.0193)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [180/985]  eta: 0:13:05  lr: 0.000014  loss: 0.0196 (0.0193)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [190/985]  eta: 0:12:55  lr: 0.000014  loss: 0.0189 (0.0193)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [200/985]  eta: 0:12:44  lr: 0.000014  loss: 0.0184 (0.0193)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [210/985]  eta: 0:12:34  lr: 0.000014  loss: 0.0175 (0.0193)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [220/985]  eta: 0:12:24  lr: 0.000014  loss: 0.0178 (0.0193)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [230/985]  eta: 0:12:13  lr: 0.000014  loss: 0.0183 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [240/985]  eta: 0:12:03  lr: 0.000014  loss: 0.0180 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [250/985]  eta: 0:11:53  lr: 0.000014  loss: 0.0173 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [260/985]  eta: 0:11:43  lr: 0.000014  loss: 0.0173 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [270/985]  eta: 0:11:33  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [280/985]  eta: 0:11:23  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [290/985]  eta: 0:11:13  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [300/985]  eta: 0:11:03  lr: 0.000014  loss: 0.0188 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [310/985]  eta: 0:10:53  lr: 0.000014  loss: 0.0175 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [320/985]  eta: 0:10:44  lr: 0.000014  loss: 0.0168 (0.0191)  time: 0.9656  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [330/985]  eta: 0:10:34  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [340/985]  eta: 0:10:24  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [350/985]  eta: 0:10:14  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [360/985]  eta: 0:10:04  lr: 0.000014  loss: 0.0195 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [370/985]  eta: 0:09:54  lr: 0.000014  loss: 0.0198 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [380/985]  eta: 0:09:45  lr: 0.000014  loss: 0.0187 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [390/985]  eta: 0:09:35  lr: 0.000014  loss: 0.0177 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [400/985]  eta: 0:09:25  lr: 0.000014  loss: 0.0187 (0.0192)  time: 0.9643  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [410/985]  eta: 0:09:15  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9636  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [420/985]  eta: 0:09:05  lr: 0.000014  loss: 0.0177 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [430/985]  eta: 0:08:56  lr: 0.000014  loss: 0.0192 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [440/985]  eta: 0:08:46  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:834]  [450/985]  eta: 0:08:36  lr: 0.000014  loss: 0.0175 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [460/985]  eta: 0:08:26  lr: 0.000014  loss: 0.0187 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [470/985]  eta: 0:08:17  lr: 0.000014  loss: 0.0198 (0.0192)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [480/985]  eta: 0:08:07  lr: 0.000014  loss: 0.0197 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [490/985]  eta: 0:07:57  lr: 0.000014  loss: 0.0205 (0.0193)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [500/985]  eta: 0:07:47  lr: 0.000014  loss: 0.0203 (0.0193)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [510/985]  eta: 0:07:38  lr: 0.000014  loss: 0.0184 (0.0193)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [520/985]  eta: 0:07:28  lr: 0.000014  loss: 0.0187 (0.0192)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [530/985]  eta: 0:07:18  lr: 0.000014  loss: 0.0190 (0.0193)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [540/985]  eta: 0:07:09  lr: 0.000014  loss: 0.0181 (0.0192)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [550/985]  eta: 0:06:59  lr: 0.000014  loss: 0.0182 (0.0192)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [560/985]  eta: 0:06:49  lr: 0.000014  loss: 0.0185 (0.0192)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [570/985]  eta: 0:06:40  lr: 0.000014  loss: 0.0173 (0.0192)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [580/985]  eta: 0:06:30  lr: 0.000014  loss: 0.0174 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [590/985]  eta: 0:06:20  lr: 0.000014  loss: 0.0190 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [600/985]  eta: 0:06:10  lr: 0.000014  loss: 0.0176 (0.0192)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [610/985]  eta: 0:06:01  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [620/985]  eta: 0:05:51  lr: 0.000014  loss: 0.0182 (0.0192)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [630/985]  eta: 0:05:41  lr: 0.000014  loss: 0.0182 (0.0192)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [640/985]  eta: 0:05:32  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [650/985]  eta: 0:05:22  lr: 0.000014  loss: 0.0184 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [660/985]  eta: 0:05:12  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [670/985]  eta: 0:05:03  lr: 0.000014  loss: 0.0170 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [680/985]  eta: 0:04:53  lr: 0.000014  loss: 0.0174 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [690/985]  eta: 0:04:43  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [700/985]  eta: 0:04:34  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [710/985]  eta: 0:04:24  lr: 0.000014  loss: 0.0201 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [720/985]  eta: 0:04:15  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [730/985]  eta: 0:04:05  lr: 0.000014  loss: 0.0173 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [740/985]  eta: 0:03:55  lr: 0.000014  loss: 0.0181 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [750/985]  eta: 0:03:46  lr: 0.000014  loss: 0.0181 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [760/985]  eta: 0:03:36  lr: 0.000014  loss: 0.0168 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [770/985]  eta: 0:03:26  lr: 0.000014  loss: 0.0166 (0.0191)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [780/985]  eta: 0:03:17  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [790/985]  eta: 0:03:07  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [800/985]  eta: 0:02:57  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [810/985]  eta: 0:02:48  lr: 0.000014  loss: 0.0188 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [820/985]  eta: 0:02:38  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [830/985]  eta: 0:02:29  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [840/985]  eta: 0:02:19  lr: 0.000014  loss: 0.0175 (0.0191)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [850/985]  eta: 0:02:09  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [860/985]  eta: 0:02:00  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [870/985]  eta: 0:01:50  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [880/985]  eta: 0:01:40  lr: 0.000014  loss: 0.0172 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [890/985]  eta: 0:01:31  lr: 0.000014  loss: 0.0171 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [900/985]  eta: 0:01:21  lr: 0.000014  loss: 0.0177 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [910/985]  eta: 0:01:12  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [920/985]  eta: 0:01:02  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [930/985]  eta: 0:00:52  lr: 0.000014  loss: 0.0174 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [940/985]  eta: 0:00:43  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [950/985]  eta: 0:00:33  lr: 0.000014  loss: 0.0189 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [960/985]  eta: 0:00:24  lr: 0.000014  loss: 0.0192 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [970/985]  eta: 0:00:14  lr: 0.000014  loss: 0.0191 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [980/985]  eta: 0:00:04  lr: 0.000014  loss: 0.0191 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834]  [984/985]  eta: 0:00:00  lr: 0.000014  loss: 0.0191 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:834] Total time: 0:15:46 (0.9612 s / it)\n",
      "Averaged stats: lr: 0.000014  loss: 0.0191 (0.0191)\n",
      "Valid: [epoch:834]  [ 0/14]  eta: 0:03:31  loss: 0.0146 (0.0146)  time: 15.0813  data: 0.7242  max mem: 41892\n",
      "Valid: [epoch:834]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0146)  time: 12.8556  data: 0.0518  max mem: 41892\n",
      "Valid: [epoch:834] Total time: 0:03:00 (12.8627 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_834_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:835]  [  0/985]  eta: 1:25:42  lr: 0.000014  loss: 0.0177 (0.0177)  time: 5.2205  data: 4.1980  max mem: 41892\n",
      "Train: [epoch:835]  [ 10/985]  eta: 0:21:44  lr: 0.000014  loss: 0.0197 (0.0208)  time: 1.3377  data: 0.3818  max mem: 41892\n",
      "Train: [epoch:835]  [ 20/985]  eta: 0:18:35  lr: 0.000014  loss: 0.0194 (0.0199)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [ 30/985]  eta: 0:17:17  lr: 0.000014  loss: 0.0189 (0.0198)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [ 40/985]  eta: 0:16:36  lr: 0.000014  loss: 0.0176 (0.0189)  time: 0.9479  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [ 50/985]  eta: 0:16:05  lr: 0.000014  loss: 0.0164 (0.0188)  time: 0.9483  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:835]  [ 60/985]  eta: 0:15:42  lr: 0.000014  loss: 0.0191 (0.0189)  time: 0.9449  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [ 70/985]  eta: 0:15:23  lr: 0.000014  loss: 0.0179 (0.0188)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [ 80/985]  eta: 0:15:06  lr: 0.000014  loss: 0.0177 (0.0188)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [ 90/985]  eta: 0:14:51  lr: 0.000014  loss: 0.0193 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [100/985]  eta: 0:14:37  lr: 0.000014  loss: 0.0192 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [110/985]  eta: 0:14:25  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [120/985]  eta: 0:14:13  lr: 0.000014  loss: 0.0193 (0.0193)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [130/985]  eta: 0:14:01  lr: 0.000014  loss: 0.0209 (0.0195)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [140/985]  eta: 0:13:49  lr: 0.000014  loss: 0.0195 (0.0195)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [150/985]  eta: 0:13:38  lr: 0.000014  loss: 0.0178 (0.0194)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [160/985]  eta: 0:13:27  lr: 0.000014  loss: 0.0178 (0.0194)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [170/985]  eta: 0:13:16  lr: 0.000014  loss: 0.0181 (0.0194)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [180/985]  eta: 0:13:05  lr: 0.000014  loss: 0.0188 (0.0194)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [190/985]  eta: 0:12:55  lr: 0.000014  loss: 0.0188 (0.0194)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [200/985]  eta: 0:12:44  lr: 0.000014  loss: 0.0181 (0.0193)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [210/985]  eta: 0:12:34  lr: 0.000014  loss: 0.0169 (0.0193)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [220/985]  eta: 0:12:24  lr: 0.000014  loss: 0.0165 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [230/985]  eta: 0:12:14  lr: 0.000014  loss: 0.0174 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [240/985]  eta: 0:12:03  lr: 0.000014  loss: 0.0190 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [250/985]  eta: 0:11:53  lr: 0.000014  loss: 0.0181 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [260/985]  eta: 0:11:43  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [270/985]  eta: 0:11:33  lr: 0.000014  loss: 0.0174 (0.0189)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [280/985]  eta: 0:11:23  lr: 0.000014  loss: 0.0185 (0.0190)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [290/985]  eta: 0:11:13  lr: 0.000014  loss: 0.0192 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [300/985]  eta: 0:11:03  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [310/985]  eta: 0:10:53  lr: 0.000014  loss: 0.0178 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [320/985]  eta: 0:10:43  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [330/985]  eta: 0:10:33  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [340/985]  eta: 0:10:23  lr: 0.000014  loss: 0.0182 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [350/985]  eta: 0:10:14  lr: 0.000014  loss: 0.0170 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [360/985]  eta: 0:10:04  lr: 0.000014  loss: 0.0188 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [370/985]  eta: 0:09:54  lr: 0.000014  loss: 0.0188 (0.0190)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [380/985]  eta: 0:09:44  lr: 0.000014  loss: 0.0187 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [390/985]  eta: 0:09:34  lr: 0.000014  loss: 0.0199 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [400/985]  eta: 0:09:25  lr: 0.000014  loss: 0.0192 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [410/985]  eta: 0:09:15  lr: 0.000014  loss: 0.0192 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [420/985]  eta: 0:09:05  lr: 0.000014  loss: 0.0188 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [430/985]  eta: 0:08:55  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [440/985]  eta: 0:08:45  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [450/985]  eta: 0:08:36  lr: 0.000014  loss: 0.0171 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [460/985]  eta: 0:08:26  lr: 0.000014  loss: 0.0187 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [470/985]  eta: 0:08:16  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [480/985]  eta: 0:08:07  lr: 0.000014  loss: 0.0174 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [490/985]  eta: 0:07:57  lr: 0.000014  loss: 0.0187 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [500/985]  eta: 0:07:47  lr: 0.000014  loss: 0.0192 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [510/985]  eta: 0:07:37  lr: 0.000014  loss: 0.0186 (0.0192)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [520/985]  eta: 0:07:28  lr: 0.000014  loss: 0.0186 (0.0192)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [530/985]  eta: 0:07:18  lr: 0.000014  loss: 0.0186 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [540/985]  eta: 0:07:08  lr: 0.000014  loss: 0.0170 (0.0192)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [550/985]  eta: 0:06:59  lr: 0.000014  loss: 0.0170 (0.0191)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [560/985]  eta: 0:06:49  lr: 0.000014  loss: 0.0173 (0.0191)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [570/985]  eta: 0:06:39  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [580/985]  eta: 0:06:30  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [590/985]  eta: 0:06:20  lr: 0.000014  loss: 0.0192 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [600/985]  eta: 0:06:10  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [610/985]  eta: 0:06:01  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [620/985]  eta: 0:05:51  lr: 0.000014  loss: 0.0217 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [630/985]  eta: 0:05:41  lr: 0.000014  loss: 0.0188 (0.0192)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [640/985]  eta: 0:05:32  lr: 0.000014  loss: 0.0170 (0.0192)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [650/985]  eta: 0:05:22  lr: 0.000014  loss: 0.0178 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [660/985]  eta: 0:05:12  lr: 0.000014  loss: 0.0179 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [670/985]  eta: 0:05:03  lr: 0.000014  loss: 0.0179 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [680/985]  eta: 0:04:53  lr: 0.000014  loss: 0.0173 (0.0191)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [690/985]  eta: 0:04:43  lr: 0.000014  loss: 0.0185 (0.0192)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [700/985]  eta: 0:04:34  lr: 0.000014  loss: 0.0188 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [710/985]  eta: 0:04:24  lr: 0.000014  loss: 0.0188 (0.0192)  time: 0.9599  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:835]  [720/985]  eta: 0:04:15  lr: 0.000014  loss: 0.0198 (0.0192)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [730/985]  eta: 0:04:05  lr: 0.000014  loss: 0.0174 (0.0192)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [740/985]  eta: 0:03:55  lr: 0.000014  loss: 0.0173 (0.0192)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [750/985]  eta: 0:03:46  lr: 0.000014  loss: 0.0183 (0.0192)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [760/985]  eta: 0:03:36  lr: 0.000014  loss: 0.0195 (0.0192)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [770/985]  eta: 0:03:26  lr: 0.000014  loss: 0.0195 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [780/985]  eta: 0:03:17  lr: 0.000014  loss: 0.0177 (0.0192)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [790/985]  eta: 0:03:07  lr: 0.000014  loss: 0.0181 (0.0192)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [800/985]  eta: 0:02:57  lr: 0.000014  loss: 0.0181 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [810/985]  eta: 0:02:48  lr: 0.000014  loss: 0.0181 (0.0192)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [820/985]  eta: 0:02:38  lr: 0.000014  loss: 0.0196 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [830/985]  eta: 0:02:29  lr: 0.000014  loss: 0.0198 (0.0192)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [840/985]  eta: 0:02:19  lr: 0.000014  loss: 0.0182 (0.0192)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [850/985]  eta: 0:02:09  lr: 0.000014  loss: 0.0174 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [860/985]  eta: 0:02:00  lr: 0.000014  loss: 0.0174 (0.0192)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [870/985]  eta: 0:01:50  lr: 0.000014  loss: 0.0172 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [880/985]  eta: 0:01:40  lr: 0.000014  loss: 0.0169 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [890/985]  eta: 0:01:31  lr: 0.000014  loss: 0.0184 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [900/985]  eta: 0:01:21  lr: 0.000014  loss: 0.0188 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [910/985]  eta: 0:01:12  lr: 0.000014  loss: 0.0182 (0.0192)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [920/985]  eta: 0:01:02  lr: 0.000014  loss: 0.0181 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [930/985]  eta: 0:00:52  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [940/985]  eta: 0:00:43  lr: 0.000014  loss: 0.0175 (0.0192)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [950/985]  eta: 0:00:33  lr: 0.000014  loss: 0.0190 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [960/985]  eta: 0:00:24  lr: 0.000014  loss: 0.0191 (0.0192)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [970/985]  eta: 0:00:14  lr: 0.000014  loss: 0.0183 (0.0192)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [980/985]  eta: 0:00:04  lr: 0.000014  loss: 0.0183 (0.0192)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835]  [984/985]  eta: 0:00:00  lr: 0.000014  loss: 0.0181 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:835] Total time: 0:15:46 (0.9606 s / it)\n",
      "Averaged stats: lr: 0.000014  loss: 0.0181 (0.0192)\n",
      "Valid: [epoch:835]  [ 0/14]  eta: 0:03:01  loss: 0.0133 (0.0133)  time: 12.9893  data: 0.6564  max mem: 41892\n",
      "Valid: [epoch:835]  [13/14]  eta: 0:00:11  loss: 0.0144 (0.0144)  time: 11.9759  data: 0.0470  max mem: 41892\n",
      "Valid: [epoch:835] Total time: 0:02:47 (11.9825 s / it)\n",
      "Averaged stats: loss: 0.0144 (0.0144)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_835_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:836]  [  0/985]  eta: 1:12:12  lr: 0.000014  loss: 0.0283 (0.0283)  time: 4.3989  data: 3.4067  max mem: 41892\n",
      "Train: [epoch:836]  [ 10/985]  eta: 0:20:28  lr: 0.000014  loss: 0.0189 (0.0218)  time: 1.2602  data: 0.3098  max mem: 41892\n",
      "Train: [epoch:836]  [ 20/985]  eta: 0:17:48  lr: 0.000014  loss: 0.0186 (0.0206)  time: 0.9423  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [ 30/985]  eta: 0:16:45  lr: 0.000014  loss: 0.0178 (0.0199)  time: 0.9390  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [ 40/985]  eta: 0:16:09  lr: 0.000014  loss: 0.0157 (0.0190)  time: 0.9408  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [ 50/985]  eta: 0:15:46  lr: 0.000014  loss: 0.0168 (0.0187)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [ 60/985]  eta: 0:15:25  lr: 0.000014  loss: 0.0190 (0.0189)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [ 70/985]  eta: 0:15:10  lr: 0.000014  loss: 0.0190 (0.0188)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [ 80/985]  eta: 0:14:55  lr: 0.000014  loss: 0.0172 (0.0187)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [ 90/985]  eta: 0:14:42  lr: 0.000014  loss: 0.0181 (0.0187)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [100/985]  eta: 0:14:28  lr: 0.000014  loss: 0.0189 (0.0188)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [110/985]  eta: 0:14:16  lr: 0.000014  loss: 0.0185 (0.0188)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [120/985]  eta: 0:14:05  lr: 0.000014  loss: 0.0186 (0.0189)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [130/985]  eta: 0:13:54  lr: 0.000014  loss: 0.0189 (0.0189)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [140/985]  eta: 0:13:43  lr: 0.000014  loss: 0.0188 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [150/985]  eta: 0:13:32  lr: 0.000014  loss: 0.0186 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [160/985]  eta: 0:13:21  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [170/985]  eta: 0:13:10  lr: 0.000014  loss: 0.0175 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [180/985]  eta: 0:13:00  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [190/985]  eta: 0:12:50  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [200/985]  eta: 0:12:40  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [210/985]  eta: 0:12:29  lr: 0.000014  loss: 0.0167 (0.0189)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [220/985]  eta: 0:12:19  lr: 0.000014  loss: 0.0168 (0.0189)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [230/985]  eta: 0:12:09  lr: 0.000014  loss: 0.0176 (0.0189)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [240/985]  eta: 0:11:59  lr: 0.000014  loss: 0.0190 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [250/985]  eta: 0:11:49  lr: 0.000014  loss: 0.0181 (0.0189)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [260/985]  eta: 0:11:40  lr: 0.000014  loss: 0.0174 (0.0189)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [270/985]  eta: 0:11:30  lr: 0.000014  loss: 0.0175 (0.0189)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [280/985]  eta: 0:11:20  lr: 0.000014  loss: 0.0179 (0.0189)  time: 0.9664  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [290/985]  eta: 0:11:10  lr: 0.000014  loss: 0.0178 (0.0188)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [300/985]  eta: 0:11:00  lr: 0.000014  loss: 0.0178 (0.0189)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [310/985]  eta: 0:10:51  lr: 0.000014  loss: 0.0183 (0.0189)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [320/985]  eta: 0:10:41  lr: 0.000014  loss: 0.0177 (0.0188)  time: 0.9598  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:836]  [330/985]  eta: 0:10:31  lr: 0.000014  loss: 0.0173 (0.0188)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [340/985]  eta: 0:10:21  lr: 0.000014  loss: 0.0173 (0.0188)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [350/985]  eta: 0:10:12  lr: 0.000014  loss: 0.0177 (0.0188)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [360/985]  eta: 0:10:02  lr: 0.000014  loss: 0.0189 (0.0188)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [370/985]  eta: 0:09:52  lr: 0.000014  loss: 0.0190 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [380/985]  eta: 0:09:42  lr: 0.000014  loss: 0.0184 (0.0189)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [390/985]  eta: 0:09:33  lr: 0.000014  loss: 0.0184 (0.0189)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [400/985]  eta: 0:09:23  lr: 0.000014  loss: 0.0180 (0.0189)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [410/985]  eta: 0:09:13  lr: 0.000014  loss: 0.0181 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [420/985]  eta: 0:09:03  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [430/985]  eta: 0:08:54  lr: 0.000014  loss: 0.0195 (0.0190)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [440/985]  eta: 0:08:44  lr: 0.000014  loss: 0.0187 (0.0190)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [450/985]  eta: 0:08:34  lr: 0.000014  loss: 0.0185 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [460/985]  eta: 0:08:25  lr: 0.000014  loss: 0.0185 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [470/985]  eta: 0:08:15  lr: 0.000014  loss: 0.0176 (0.0190)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [480/985]  eta: 0:08:05  lr: 0.000014  loss: 0.0176 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [490/985]  eta: 0:07:56  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [500/985]  eta: 0:07:46  lr: 0.000014  loss: 0.0189 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [510/985]  eta: 0:07:36  lr: 0.000014  loss: 0.0189 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [520/985]  eta: 0:07:27  lr: 0.000014  loss: 0.0186 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [530/985]  eta: 0:07:17  lr: 0.000014  loss: 0.0184 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [540/985]  eta: 0:07:08  lr: 0.000014  loss: 0.0168 (0.0190)  time: 0.9629  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [550/985]  eta: 0:06:58  lr: 0.000014  loss: 0.0167 (0.0189)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [560/985]  eta: 0:06:48  lr: 0.000014  loss: 0.0175 (0.0189)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [570/985]  eta: 0:06:39  lr: 0.000014  loss: 0.0181 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [580/985]  eta: 0:06:29  lr: 0.000014  loss: 0.0190 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [590/985]  eta: 0:06:19  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [600/985]  eta: 0:06:10  lr: 0.000014  loss: 0.0181 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [610/985]  eta: 0:06:00  lr: 0.000014  loss: 0.0186 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [620/985]  eta: 0:05:50  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [630/985]  eta: 0:05:41  lr: 0.000014  loss: 0.0172 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [640/985]  eta: 0:05:31  lr: 0.000014  loss: 0.0159 (0.0189)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [650/985]  eta: 0:05:21  lr: 0.000014  loss: 0.0175 (0.0189)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [660/985]  eta: 0:05:12  lr: 0.000014  loss: 0.0186 (0.0189)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [670/985]  eta: 0:05:02  lr: 0.000014  loss: 0.0185 (0.0189)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [680/985]  eta: 0:04:52  lr: 0.000014  loss: 0.0169 (0.0189)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [690/985]  eta: 0:04:43  lr: 0.000014  loss: 0.0176 (0.0189)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [700/985]  eta: 0:04:33  lr: 0.000014  loss: 0.0182 (0.0189)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [710/985]  eta: 0:04:24  lr: 0.000014  loss: 0.0188 (0.0189)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [720/985]  eta: 0:04:14  lr: 0.000014  loss: 0.0188 (0.0189)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [730/985]  eta: 0:04:04  lr: 0.000014  loss: 0.0184 (0.0189)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [740/985]  eta: 0:03:55  lr: 0.000014  loss: 0.0210 (0.0190)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [750/985]  eta: 0:03:45  lr: 0.000014  loss: 0.0190 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [760/985]  eta: 0:03:36  lr: 0.000014  loss: 0.0188 (0.0190)  time: 0.9667  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [770/985]  eta: 0:03:26  lr: 0.000014  loss: 0.0190 (0.0190)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [780/985]  eta: 0:03:16  lr: 0.000014  loss: 0.0176 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [790/985]  eta: 0:03:07  lr: 0.000014  loss: 0.0166 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [800/985]  eta: 0:02:57  lr: 0.000014  loss: 0.0174 (0.0190)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [810/985]  eta: 0:02:47  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [820/985]  eta: 0:02:38  lr: 0.000014  loss: 0.0192 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [830/985]  eta: 0:02:28  lr: 0.000014  loss: 0.0198 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [840/985]  eta: 0:02:19  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [850/985]  eta: 0:02:09  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [860/985]  eta: 0:01:59  lr: 0.000014  loss: 0.0177 (0.0190)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [870/985]  eta: 0:01:50  lr: 0.000014  loss: 0.0177 (0.0190)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [880/985]  eta: 0:01:40  lr: 0.000014  loss: 0.0174 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [890/985]  eta: 0:01:31  lr: 0.000014  loss: 0.0199 (0.0190)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [900/985]  eta: 0:01:21  lr: 0.000014  loss: 0.0199 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [910/985]  eta: 0:01:11  lr: 0.000014  loss: 0.0194 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [920/985]  eta: 0:01:02  lr: 0.000014  loss: 0.0189 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [930/985]  eta: 0:00:52  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [940/985]  eta: 0:00:43  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [950/985]  eta: 0:00:33  lr: 0.000014  loss: 0.0190 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [960/985]  eta: 0:00:23  lr: 0.000014  loss: 0.0193 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [970/985]  eta: 0:00:14  lr: 0.000014  loss: 0.0193 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836]  [980/985]  eta: 0:00:04  lr: 0.000014  loss: 0.0191 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:836]  [984/985]  eta: 0:00:00  lr: 0.000014  loss: 0.0195 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:836] Total time: 0:15:44 (0.9594 s / it)\n",
      "Averaged stats: lr: 0.000014  loss: 0.0195 (0.0191)\n",
      "Valid: [epoch:836]  [ 0/14]  eta: 0:02:57  loss: 0.0150 (0.0150)  time: 12.6932  data: 0.5801  max mem: 41892\n",
      "Valid: [epoch:836]  [13/14]  eta: 0:00:11  loss: 0.0147 (0.0147)  time: 11.9631  data: 0.0415  max mem: 41892\n",
      "Valid: [epoch:836] Total time: 0:02:47 (11.9691 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_836_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:837]  [  0/985]  eta: 1:14:14  lr: 0.000014  loss: 0.0327 (0.0327)  time: 4.5222  data: 3.5423  max mem: 41892\n",
      "Train: [epoch:837]  [ 10/985]  eta: 0:20:37  lr: 0.000014  loss: 0.0190 (0.0219)  time: 1.2689  data: 0.3221  max mem: 41892\n",
      "Train: [epoch:837]  [ 20/985]  eta: 0:17:51  lr: 0.000014  loss: 0.0182 (0.0204)  time: 0.9400  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [ 30/985]  eta: 0:16:48  lr: 0.000014  loss: 0.0180 (0.0198)  time: 0.9383  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [ 40/985]  eta: 0:16:13  lr: 0.000014  loss: 0.0175 (0.0193)  time: 0.9450  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [ 50/985]  eta: 0:15:46  lr: 0.000014  loss: 0.0164 (0.0189)  time: 0.9460  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [ 60/985]  eta: 0:15:26  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9425  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [ 70/985]  eta: 0:15:09  lr: 0.000014  loss: 0.0180 (0.0188)  time: 0.9449  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [ 80/985]  eta: 0:14:54  lr: 0.000014  loss: 0.0156 (0.0185)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [ 90/985]  eta: 0:14:40  lr: 0.000014  loss: 0.0165 (0.0187)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [100/985]  eta: 0:14:27  lr: 0.000014  loss: 0.0167 (0.0187)  time: 0.9490  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [110/985]  eta: 0:14:16  lr: 0.000014  loss: 0.0162 (0.0185)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [120/985]  eta: 0:14:04  lr: 0.000014  loss: 0.0172 (0.0186)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [130/985]  eta: 0:13:53  lr: 0.000014  loss: 0.0198 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [140/985]  eta: 0:13:42  lr: 0.000014  loss: 0.0209 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [150/985]  eta: 0:13:32  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [160/985]  eta: 0:13:21  lr: 0.000014  loss: 0.0187 (0.0192)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [170/985]  eta: 0:13:10  lr: 0.000014  loss: 0.0184 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [180/985]  eta: 0:13:00  lr: 0.000014  loss: 0.0181 (0.0193)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [190/985]  eta: 0:12:49  lr: 0.000014  loss: 0.0200 (0.0193)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [200/985]  eta: 0:12:39  lr: 0.000014  loss: 0.0188 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [210/985]  eta: 0:12:29  lr: 0.000014  loss: 0.0182 (0.0193)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [220/985]  eta: 0:12:19  lr: 0.000014  loss: 0.0163 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [230/985]  eta: 0:12:09  lr: 0.000014  loss: 0.0161 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [240/985]  eta: 0:11:59  lr: 0.000014  loss: 0.0195 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [250/985]  eta: 0:11:49  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [260/985]  eta: 0:11:39  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [270/985]  eta: 0:11:29  lr: 0.000014  loss: 0.0174 (0.0190)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [280/985]  eta: 0:11:19  lr: 0.000014  loss: 0.0178 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [290/985]  eta: 0:11:09  lr: 0.000014  loss: 0.0173 (0.0189)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [300/985]  eta: 0:10:59  lr: 0.000014  loss: 0.0179 (0.0189)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [310/985]  eta: 0:10:50  lr: 0.000014  loss: 0.0187 (0.0190)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [320/985]  eta: 0:10:40  lr: 0.000014  loss: 0.0170 (0.0189)  time: 0.9657  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [330/985]  eta: 0:10:30  lr: 0.000014  loss: 0.0173 (0.0189)  time: 0.9647  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [340/985]  eta: 0:10:21  lr: 0.000014  loss: 0.0181 (0.0189)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [350/985]  eta: 0:10:11  lr: 0.000014  loss: 0.0178 (0.0189)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [360/985]  eta: 0:10:01  lr: 0.000014  loss: 0.0187 (0.0189)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [370/985]  eta: 0:09:52  lr: 0.000014  loss: 0.0193 (0.0190)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [380/985]  eta: 0:09:42  lr: 0.000014  loss: 0.0182 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [390/985]  eta: 0:09:32  lr: 0.000014  loss: 0.0174 (0.0190)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [400/985]  eta: 0:09:23  lr: 0.000014  loss: 0.0181 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [410/985]  eta: 0:09:13  lr: 0.000014  loss: 0.0176 (0.0190)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [420/985]  eta: 0:09:03  lr: 0.000014  loss: 0.0178 (0.0190)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [430/985]  eta: 0:08:53  lr: 0.000014  loss: 0.0189 (0.0190)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [440/985]  eta: 0:08:44  lr: 0.000014  loss: 0.0189 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [450/985]  eta: 0:08:34  lr: 0.000014  loss: 0.0182 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [460/985]  eta: 0:08:24  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [470/985]  eta: 0:08:15  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [480/985]  eta: 0:08:05  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [490/985]  eta: 0:07:55  lr: 0.000014  loss: 0.0180 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [500/985]  eta: 0:07:46  lr: 0.000014  loss: 0.0184 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [510/985]  eta: 0:07:36  lr: 0.000014  loss: 0.0205 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [520/985]  eta: 0:07:26  lr: 0.000014  loss: 0.0195 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [530/985]  eta: 0:07:17  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [540/985]  eta: 0:07:07  lr: 0.000014  loss: 0.0165 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [550/985]  eta: 0:06:57  lr: 0.000014  loss: 0.0166 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [560/985]  eta: 0:06:48  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [570/985]  eta: 0:06:38  lr: 0.000014  loss: 0.0173 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [580/985]  eta: 0:06:28  lr: 0.000014  loss: 0.0187 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [590/985]  eta: 0:06:19  lr: 0.000014  loss: 0.0207 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:837]  [600/985]  eta: 0:06:09  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [610/985]  eta: 0:06:00  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [620/985]  eta: 0:05:50  lr: 0.000014  loss: 0.0190 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [630/985]  eta: 0:05:40  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [640/985]  eta: 0:05:31  lr: 0.000014  loss: 0.0177 (0.0191)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [650/985]  eta: 0:05:21  lr: 0.000014  loss: 0.0177 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [660/985]  eta: 0:05:11  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [670/985]  eta: 0:05:02  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [680/985]  eta: 0:04:52  lr: 0.000014  loss: 0.0175 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [690/985]  eta: 0:04:43  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [700/985]  eta: 0:04:33  lr: 0.000014  loss: 0.0195 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [710/985]  eta: 0:04:23  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [720/985]  eta: 0:04:14  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [730/985]  eta: 0:04:04  lr: 0.000014  loss: 0.0198 (0.0192)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [740/985]  eta: 0:03:55  lr: 0.000014  loss: 0.0192 (0.0192)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [750/985]  eta: 0:03:45  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [760/985]  eta: 0:03:35  lr: 0.000014  loss: 0.0168 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [770/985]  eta: 0:03:26  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [780/985]  eta: 0:03:16  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [790/985]  eta: 0:03:07  lr: 0.000014  loss: 0.0164 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [800/985]  eta: 0:02:57  lr: 0.000014  loss: 0.0172 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [810/985]  eta: 0:02:47  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [820/985]  eta: 0:02:38  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [830/985]  eta: 0:02:28  lr: 0.000014  loss: 0.0190 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [840/985]  eta: 0:02:19  lr: 0.000014  loss: 0.0172 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [850/985]  eta: 0:02:09  lr: 0.000014  loss: 0.0170 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [860/985]  eta: 0:01:59  lr: 0.000014  loss: 0.0169 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [870/985]  eta: 0:01:50  lr: 0.000014  loss: 0.0168 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [880/985]  eta: 0:01:40  lr: 0.000014  loss: 0.0169 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [890/985]  eta: 0:01:31  lr: 0.000014  loss: 0.0175 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [900/985]  eta: 0:01:21  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [910/985]  eta: 0:01:11  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [920/985]  eta: 0:01:02  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [930/985]  eta: 0:00:52  lr: 0.000014  loss: 0.0174 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [940/985]  eta: 0:00:43  lr: 0.000014  loss: 0.0189 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [950/985]  eta: 0:00:33  lr: 0.000014  loss: 0.0199 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [960/985]  eta: 0:00:23  lr: 0.000014  loss: 0.0188 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [970/985]  eta: 0:00:14  lr: 0.000014  loss: 0.0188 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [980/985]  eta: 0:00:04  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837]  [984/985]  eta: 0:00:00  lr: 0.000014  loss: 0.0185 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:837] Total time: 0:15:44 (0.9589 s / it)\n",
      "Averaged stats: lr: 0.000014  loss: 0.0185 (0.0191)\n",
      "Valid: [epoch:837]  [ 0/14]  eta: 0:02:59  loss: 0.0142 (0.0142)  time: 12.8138  data: 0.5667  max mem: 41892\n",
      "Valid: [epoch:837]  [13/14]  eta: 0:00:12  loss: 0.0147 (0.0147)  time: 12.0038  data: 0.0406  max mem: 41892\n",
      "Valid: [epoch:837] Total time: 0:02:48 (12.0098 s / it)\n",
      "Averaged stats: loss: 0.0147 (0.0147)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_837_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:838]  [  0/985]  eta: 1:01:33  lr: 0.000014  loss: 0.0243 (0.0243)  time: 3.7502  data: 2.7568  max mem: 41892\n",
      "Train: [epoch:838]  [ 10/985]  eta: 0:19:33  lr: 0.000014  loss: 0.0198 (0.0204)  time: 1.2037  data: 0.2507  max mem: 41892\n",
      "Train: [epoch:838]  [ 20/985]  eta: 0:17:23  lr: 0.000014  loss: 0.0185 (0.0202)  time: 0.9480  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [ 30/985]  eta: 0:16:32  lr: 0.000014  loss: 0.0185 (0.0196)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [ 40/985]  eta: 0:16:01  lr: 0.000014  loss: 0.0173 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [ 50/985]  eta: 0:15:37  lr: 0.000014  loss: 0.0163 (0.0189)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [ 60/985]  eta: 0:15:18  lr: 0.000014  loss: 0.0178 (0.0190)  time: 0.9437  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [ 70/985]  eta: 0:15:04  lr: 0.000014  loss: 0.0166 (0.0187)  time: 0.9503  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [ 80/985]  eta: 0:14:50  lr: 0.000014  loss: 0.0159 (0.0185)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [ 90/985]  eta: 0:14:36  lr: 0.000014  loss: 0.0163 (0.0184)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [100/985]  eta: 0:14:24  lr: 0.000014  loss: 0.0184 (0.0184)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [110/985]  eta: 0:14:12  lr: 0.000014  loss: 0.0181 (0.0184)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [120/985]  eta: 0:14:01  lr: 0.000014  loss: 0.0185 (0.0187)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [130/985]  eta: 0:13:50  lr: 0.000014  loss: 0.0210 (0.0189)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [140/985]  eta: 0:13:40  lr: 0.000014  loss: 0.0191 (0.0188)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [150/985]  eta: 0:13:29  lr: 0.000014  loss: 0.0188 (0.0189)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [160/985]  eta: 0:13:19  lr: 0.000014  loss: 0.0189 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [170/985]  eta: 0:13:08  lr: 0.000014  loss: 0.0195 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [180/985]  eta: 0:12:58  lr: 0.000014  loss: 0.0195 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [190/985]  eta: 0:12:48  lr: 0.000014  loss: 0.0178 (0.0191)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [200/985]  eta: 0:12:38  lr: 0.000014  loss: 0.0178 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:838]  [210/985]  eta: 0:12:28  lr: 0.000014  loss: 0.0178 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [220/985]  eta: 0:12:18  lr: 0.000014  loss: 0.0168 (0.0189)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [230/985]  eta: 0:12:08  lr: 0.000014  loss: 0.0167 (0.0189)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [240/985]  eta: 0:11:58  lr: 0.000014  loss: 0.0188 (0.0189)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [250/985]  eta: 0:11:48  lr: 0.000014  loss: 0.0188 (0.0190)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [260/985]  eta: 0:11:38  lr: 0.000014  loss: 0.0188 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [270/985]  eta: 0:11:29  lr: 0.000014  loss: 0.0180 (0.0189)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [280/985]  eta: 0:11:19  lr: 0.000014  loss: 0.0190 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [290/985]  eta: 0:11:09  lr: 0.000014  loss: 0.0194 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [300/985]  eta: 0:10:59  lr: 0.000014  loss: 0.0185 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [310/985]  eta: 0:10:50  lr: 0.000014  loss: 0.0183 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [320/985]  eta: 0:10:40  lr: 0.000014  loss: 0.0185 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [330/985]  eta: 0:10:30  lr: 0.000014  loss: 0.0181 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [340/985]  eta: 0:10:20  lr: 0.000014  loss: 0.0182 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [350/985]  eta: 0:10:10  lr: 0.000014  loss: 0.0188 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [360/985]  eta: 0:10:01  lr: 0.000014  loss: 0.0187 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [370/985]  eta: 0:09:51  lr: 0.000014  loss: 0.0184 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [380/985]  eta: 0:09:41  lr: 0.000014  loss: 0.0184 (0.0191)  time: 0.9637  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [390/985]  eta: 0:09:32  lr: 0.000014  loss: 0.0183 (0.0191)  time: 0.9643  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [400/985]  eta: 0:09:22  lr: 0.000014  loss: 0.0195 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [410/985]  eta: 0:09:12  lr: 0.000014  loss: 0.0189 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [420/985]  eta: 0:09:03  lr: 0.000014  loss: 0.0180 (0.0192)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [430/985]  eta: 0:08:53  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [440/985]  eta: 0:08:43  lr: 0.000014  loss: 0.0189 (0.0192)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [450/985]  eta: 0:08:33  lr: 0.000014  loss: 0.0189 (0.0192)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [460/985]  eta: 0:08:24  lr: 0.000014  loss: 0.0188 (0.0192)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [470/985]  eta: 0:08:14  lr: 0.000014  loss: 0.0185 (0.0192)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [480/985]  eta: 0:08:05  lr: 0.000014  loss: 0.0176 (0.0192)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [490/985]  eta: 0:07:55  lr: 0.000014  loss: 0.0169 (0.0192)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [500/985]  eta: 0:07:45  lr: 0.000014  loss: 0.0174 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [510/985]  eta: 0:07:36  lr: 0.000014  loss: 0.0192 (0.0193)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [520/985]  eta: 0:07:26  lr: 0.000014  loss: 0.0186 (0.0192)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [530/985]  eta: 0:07:16  lr: 0.000014  loss: 0.0185 (0.0193)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [540/985]  eta: 0:07:07  lr: 0.000014  loss: 0.0186 (0.0193)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [550/985]  eta: 0:06:57  lr: 0.000014  loss: 0.0172 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [560/985]  eta: 0:06:48  lr: 0.000014  loss: 0.0173 (0.0192)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [570/985]  eta: 0:06:38  lr: 0.000014  loss: 0.0189 (0.0192)  time: 0.9655  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [580/985]  eta: 0:06:28  lr: 0.000014  loss: 0.0189 (0.0192)  time: 0.9641  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [590/985]  eta: 0:06:19  lr: 0.000014  loss: 0.0185 (0.0192)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [600/985]  eta: 0:06:09  lr: 0.000014  loss: 0.0183 (0.0192)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [610/985]  eta: 0:06:00  lr: 0.000014  loss: 0.0180 (0.0192)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [620/985]  eta: 0:05:50  lr: 0.000014  loss: 0.0190 (0.0193)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [630/985]  eta: 0:05:40  lr: 0.000014  loss: 0.0186 (0.0193)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [640/985]  eta: 0:05:31  lr: 0.000014  loss: 0.0170 (0.0193)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [650/985]  eta: 0:05:21  lr: 0.000014  loss: 0.0203 (0.0193)  time: 0.9645  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [660/985]  eta: 0:05:12  lr: 0.000014  loss: 0.0179 (0.0193)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [670/985]  eta: 0:05:02  lr: 0.000014  loss: 0.0169 (0.0192)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [680/985]  eta: 0:04:52  lr: 0.000014  loss: 0.0163 (0.0192)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [690/985]  eta: 0:04:43  lr: 0.000014  loss: 0.0174 (0.0192)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [700/985]  eta: 0:04:33  lr: 0.000014  loss: 0.0175 (0.0192)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [710/985]  eta: 0:04:23  lr: 0.000014  loss: 0.0182 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [720/985]  eta: 0:04:14  lr: 0.000014  loss: 0.0186 (0.0192)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [730/985]  eta: 0:04:04  lr: 0.000014  loss: 0.0188 (0.0192)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [740/985]  eta: 0:03:55  lr: 0.000014  loss: 0.0181 (0.0192)  time: 0.9630  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [750/985]  eta: 0:03:45  lr: 0.000014  loss: 0.0177 (0.0192)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [760/985]  eta: 0:03:35  lr: 0.000014  loss: 0.0175 (0.0192)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [770/985]  eta: 0:03:26  lr: 0.000014  loss: 0.0179 (0.0192)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [780/985]  eta: 0:03:16  lr: 0.000014  loss: 0.0177 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [790/985]  eta: 0:03:07  lr: 0.000014  loss: 0.0175 (0.0192)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [800/985]  eta: 0:02:57  lr: 0.000014  loss: 0.0174 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [810/985]  eta: 0:02:47  lr: 0.000014  loss: 0.0179 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [820/985]  eta: 0:02:38  lr: 0.000014  loss: 0.0178 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [830/985]  eta: 0:02:28  lr: 0.000014  loss: 0.0175 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [840/985]  eta: 0:02:19  lr: 0.000014  loss: 0.0175 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [850/985]  eta: 0:02:09  lr: 0.000014  loss: 0.0175 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [860/985]  eta: 0:01:59  lr: 0.000014  loss: 0.0181 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:838]  [870/985]  eta: 0:01:50  lr: 0.000014  loss: 0.0181 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [880/985]  eta: 0:01:40  lr: 0.000014  loss: 0.0169 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [890/985]  eta: 0:01:31  lr: 0.000014  loss: 0.0176 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [900/985]  eta: 0:01:21  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [910/985]  eta: 0:01:11  lr: 0.000014  loss: 0.0184 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [920/985]  eta: 0:01:02  lr: 0.000014  loss: 0.0180 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [930/985]  eta: 0:00:52  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [940/985]  eta: 0:00:43  lr: 0.000014  loss: 0.0186 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [950/985]  eta: 0:00:33  lr: 0.000014  loss: 0.0184 (0.0191)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [960/985]  eta: 0:00:23  lr: 0.000014  loss: 0.0181 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [970/985]  eta: 0:00:14  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9508  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [980/985]  eta: 0:00:04  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838]  [984/985]  eta: 0:00:00  lr: 0.000014  loss: 0.0182 (0.0191)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:838] Total time: 0:15:44 (0.9592 s / it)\n",
      "Averaged stats: lr: 0.000014  loss: 0.0182 (0.0191)\n",
      "Valid: [epoch:838]  [ 0/14]  eta: 0:02:52  loss: 0.0154 (0.0154)  time: 12.3538  data: 0.5852  max mem: 41892\n",
      "Valid: [epoch:838]  [13/14]  eta: 0:00:11  loss: 0.0149 (0.0149)  time: 11.9152  data: 0.0419  max mem: 41892\n",
      "Valid: [epoch:838] Total time: 0:02:46 (11.9223 s / it)\n",
      "Averaged stats: loss: 0.0149 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_838_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:839]  [  0/985]  eta: 1:12:22  lr: 0.000013  loss: 0.0260 (0.0260)  time: 4.4089  data: 3.4033  max mem: 41892\n",
      "Train: [epoch:839]  [ 10/985]  eta: 0:20:31  lr: 0.000013  loss: 0.0197 (0.0202)  time: 1.2628  data: 0.3095  max mem: 41892\n",
      "Train: [epoch:839]  [ 20/985]  eta: 0:17:50  lr: 0.000013  loss: 0.0186 (0.0195)  time: 0.9438  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [ 30/985]  eta: 0:16:51  lr: 0.000013  loss: 0.0184 (0.0198)  time: 0.9468  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [ 40/985]  eta: 0:16:16  lr: 0.000013  loss: 0.0172 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [ 50/985]  eta: 0:15:50  lr: 0.000013  loss: 0.0164 (0.0188)  time: 0.9504  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [ 60/985]  eta: 0:15:29  lr: 0.000013  loss: 0.0180 (0.0193)  time: 0.9482  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [ 70/985]  eta: 0:15:12  lr: 0.000013  loss: 0.0180 (0.0189)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [ 80/985]  eta: 0:14:58  lr: 0.000013  loss: 0.0164 (0.0188)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [ 90/985]  eta: 0:14:43  lr: 0.000013  loss: 0.0174 (0.0188)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [100/985]  eta: 0:14:31  lr: 0.000013  loss: 0.0170 (0.0188)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [110/985]  eta: 0:14:19  lr: 0.000013  loss: 0.0170 (0.0189)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [120/985]  eta: 0:14:07  lr: 0.000013  loss: 0.0180 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [130/985]  eta: 0:13:56  lr: 0.000013  loss: 0.0180 (0.0189)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [140/985]  eta: 0:13:46  lr: 0.000013  loss: 0.0184 (0.0189)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [150/985]  eta: 0:13:34  lr: 0.000013  loss: 0.0182 (0.0188)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [160/985]  eta: 0:13:24  lr: 0.000013  loss: 0.0177 (0.0188)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [170/985]  eta: 0:13:13  lr: 0.000013  loss: 0.0177 (0.0188)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [180/985]  eta: 0:13:03  lr: 0.000013  loss: 0.0182 (0.0189)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [190/985]  eta: 0:12:52  lr: 0.000013  loss: 0.0192 (0.0189)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [200/985]  eta: 0:12:42  lr: 0.000013  loss: 0.0186 (0.0189)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [210/985]  eta: 0:12:32  lr: 0.000013  loss: 0.0175 (0.0188)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [220/985]  eta: 0:12:21  lr: 0.000013  loss: 0.0171 (0.0187)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [230/985]  eta: 0:12:11  lr: 0.000013  loss: 0.0167 (0.0188)  time: 0.9517  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [240/985]  eta: 0:12:01  lr: 0.000013  loss: 0.0189 (0.0188)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [250/985]  eta: 0:11:51  lr: 0.000013  loss: 0.0170 (0.0188)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [260/985]  eta: 0:11:41  lr: 0.000013  loss: 0.0190 (0.0189)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [270/985]  eta: 0:11:31  lr: 0.000013  loss: 0.0195 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [280/985]  eta: 0:11:21  lr: 0.000013  loss: 0.0191 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [290/985]  eta: 0:11:11  lr: 0.000013  loss: 0.0182 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [300/985]  eta: 0:11:01  lr: 0.000013  loss: 0.0182 (0.0190)  time: 0.9643  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [310/985]  eta: 0:10:52  lr: 0.000013  loss: 0.0194 (0.0191)  time: 0.9657  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [320/985]  eta: 0:10:42  lr: 0.000013  loss: 0.0198 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [330/985]  eta: 0:10:32  lr: 0.000013  loss: 0.0174 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [340/985]  eta: 0:10:22  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [350/985]  eta: 0:10:12  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [360/985]  eta: 0:10:03  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [370/985]  eta: 0:09:53  lr: 0.000013  loss: 0.0190 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [380/985]  eta: 0:09:43  lr: 0.000013  loss: 0.0193 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [390/985]  eta: 0:09:33  lr: 0.000013  loss: 0.0200 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [400/985]  eta: 0:09:24  lr: 0.000013  loss: 0.0195 (0.0192)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [410/985]  eta: 0:09:14  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [420/985]  eta: 0:09:04  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [430/985]  eta: 0:08:54  lr: 0.000013  loss: 0.0185 (0.0192)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [440/985]  eta: 0:08:45  lr: 0.000013  loss: 0.0186 (0.0192)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [450/985]  eta: 0:08:35  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [460/985]  eta: 0:08:25  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [470/985]  eta: 0:08:16  lr: 0.000013  loss: 0.0190 (0.0192)  time: 0.9540  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:839]  [480/985]  eta: 0:08:06  lr: 0.000013  loss: 0.0186 (0.0192)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [490/985]  eta: 0:07:56  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [500/985]  eta: 0:07:46  lr: 0.000013  loss: 0.0198 (0.0192)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [510/985]  eta: 0:07:37  lr: 0.000013  loss: 0.0190 (0.0192)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [520/985]  eta: 0:07:27  lr: 0.000013  loss: 0.0186 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [530/985]  eta: 0:07:17  lr: 0.000013  loss: 0.0195 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [540/985]  eta: 0:07:08  lr: 0.000013  loss: 0.0187 (0.0192)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [550/985]  eta: 0:06:58  lr: 0.000013  loss: 0.0188 (0.0192)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [560/985]  eta: 0:06:48  lr: 0.000013  loss: 0.0188 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [570/985]  eta: 0:06:39  lr: 0.000013  loss: 0.0189 (0.0192)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [580/985]  eta: 0:06:29  lr: 0.000013  loss: 0.0182 (0.0192)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [590/985]  eta: 0:06:19  lr: 0.000013  loss: 0.0179 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [600/985]  eta: 0:06:10  lr: 0.000013  loss: 0.0175 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [610/985]  eta: 0:06:00  lr: 0.000013  loss: 0.0170 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [620/985]  eta: 0:05:50  lr: 0.000013  loss: 0.0176 (0.0192)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [630/985]  eta: 0:05:41  lr: 0.000013  loss: 0.0187 (0.0192)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [640/985]  eta: 0:05:31  lr: 0.000013  loss: 0.0187 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [650/985]  eta: 0:05:21  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [660/985]  eta: 0:05:12  lr: 0.000013  loss: 0.0176 (0.0192)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [670/985]  eta: 0:05:02  lr: 0.000013  loss: 0.0170 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [680/985]  eta: 0:04:53  lr: 0.000013  loss: 0.0170 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [690/985]  eta: 0:04:43  lr: 0.000013  loss: 0.0183 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [700/985]  eta: 0:04:33  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [710/985]  eta: 0:04:24  lr: 0.000013  loss: 0.0176 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [720/985]  eta: 0:04:14  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [730/985]  eta: 0:04:05  lr: 0.000013  loss: 0.0172 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [740/985]  eta: 0:03:55  lr: 0.000013  loss: 0.0163 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [750/985]  eta: 0:03:45  lr: 0.000013  loss: 0.0173 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [760/985]  eta: 0:03:36  lr: 0.000013  loss: 0.0173 (0.0190)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [770/985]  eta: 0:03:26  lr: 0.000013  loss: 0.0172 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [780/985]  eta: 0:03:16  lr: 0.000013  loss: 0.0187 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [790/985]  eta: 0:03:07  lr: 0.000013  loss: 0.0189 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [800/985]  eta: 0:02:57  lr: 0.000013  loss: 0.0191 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [810/985]  eta: 0:02:48  lr: 0.000013  loss: 0.0188 (0.0190)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [820/985]  eta: 0:02:38  lr: 0.000013  loss: 0.0190 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [830/985]  eta: 0:02:28  lr: 0.000013  loss: 0.0192 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [840/985]  eta: 0:02:19  lr: 0.000013  loss: 0.0177 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [850/985]  eta: 0:02:09  lr: 0.000013  loss: 0.0177 (0.0191)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [860/985]  eta: 0:02:00  lr: 0.000013  loss: 0.0184 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [870/985]  eta: 0:01:50  lr: 0.000013  loss: 0.0172 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [880/985]  eta: 0:01:40  lr: 0.000013  loss: 0.0169 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [890/985]  eta: 0:01:31  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [900/985]  eta: 0:01:21  lr: 0.000013  loss: 0.0214 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [910/985]  eta: 0:01:12  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [920/985]  eta: 0:01:02  lr: 0.000013  loss: 0.0185 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [930/985]  eta: 0:00:52  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [940/985]  eta: 0:00:43  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [950/985]  eta: 0:00:33  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [960/985]  eta: 0:00:23  lr: 0.000013  loss: 0.0186 (0.0191)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [970/985]  eta: 0:00:14  lr: 0.000013  loss: 0.0193 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [980/985]  eta: 0:00:04  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839]  [984/985]  eta: 0:00:00  lr: 0.000013  loss: 0.0184 (0.0191)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:839] Total time: 0:15:45 (0.9600 s / it)\n",
      "Averaged stats: lr: 0.000013  loss: 0.0184 (0.0191)\n",
      "Valid: [epoch:839]  [ 0/14]  eta: 0:02:54  loss: 0.0151 (0.0151)  time: 12.4800  data: 0.6065  max mem: 41892\n",
      "Valid: [epoch:839]  [13/14]  eta: 0:00:11  loss: 0.0145 (0.0145)  time: 11.8143  data: 0.0434  max mem: 41892\n",
      "Valid: [epoch:839] Total time: 0:02:45 (11.8207 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_839_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:840]  [  0/985]  eta: 1:17:34  lr: 0.000013  loss: 0.0217 (0.0217)  time: 4.7254  data: 3.7264  max mem: 41892\n",
      "Train: [epoch:840]  [ 10/985]  eta: 0:20:56  lr: 0.000013  loss: 0.0183 (0.0200)  time: 1.2887  data: 0.3389  max mem: 41892\n",
      "Train: [epoch:840]  [ 20/985]  eta: 0:18:02  lr: 0.000013  loss: 0.0183 (0.0200)  time: 0.9411  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [ 30/985]  eta: 0:16:54  lr: 0.000013  loss: 0.0174 (0.0188)  time: 0.9379  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [ 40/985]  eta: 0:16:17  lr: 0.000013  loss: 0.0158 (0.0182)  time: 0.9439  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [ 50/985]  eta: 0:15:51  lr: 0.000013  loss: 0.0157 (0.0177)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [ 60/985]  eta: 0:15:30  lr: 0.000013  loss: 0.0179 (0.0181)  time: 0.9462  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [ 70/985]  eta: 0:15:12  lr: 0.000013  loss: 0.0183 (0.0181)  time: 0.9463  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [ 80/985]  eta: 0:14:57  lr: 0.000013  loss: 0.0163 (0.0179)  time: 0.9502  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:840]  [ 90/985]  eta: 0:14:43  lr: 0.000013  loss: 0.0171 (0.0181)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [100/985]  eta: 0:14:30  lr: 0.000013  loss: 0.0181 (0.0181)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [110/985]  eta: 0:14:17  lr: 0.000013  loss: 0.0178 (0.0182)  time: 0.9485  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [120/985]  eta: 0:14:06  lr: 0.000013  loss: 0.0177 (0.0183)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [130/985]  eta: 0:13:54  lr: 0.000013  loss: 0.0185 (0.0185)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [140/985]  eta: 0:13:43  lr: 0.000013  loss: 0.0194 (0.0185)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [150/985]  eta: 0:13:32  lr: 0.000013  loss: 0.0182 (0.0185)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [160/985]  eta: 0:13:22  lr: 0.000013  loss: 0.0175 (0.0185)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [170/985]  eta: 0:13:11  lr: 0.000013  loss: 0.0175 (0.0185)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [180/985]  eta: 0:13:01  lr: 0.000013  loss: 0.0189 (0.0187)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [190/985]  eta: 0:12:50  lr: 0.000013  loss: 0.0195 (0.0187)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [200/985]  eta: 0:12:40  lr: 0.000013  loss: 0.0195 (0.0188)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [210/985]  eta: 0:12:30  lr: 0.000013  loss: 0.0176 (0.0187)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [220/985]  eta: 0:12:20  lr: 0.000013  loss: 0.0175 (0.0187)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [230/985]  eta: 0:12:10  lr: 0.000013  loss: 0.0183 (0.0187)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [240/985]  eta: 0:12:00  lr: 0.000013  loss: 0.0188 (0.0188)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [250/985]  eta: 0:11:50  lr: 0.000013  loss: 0.0180 (0.0187)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [260/985]  eta: 0:11:40  lr: 0.000013  loss: 0.0171 (0.0187)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [270/985]  eta: 0:11:30  lr: 0.000013  loss: 0.0178 (0.0188)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [280/985]  eta: 0:11:21  lr: 0.000013  loss: 0.0188 (0.0187)  time: 0.9672  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [290/985]  eta: 0:11:11  lr: 0.000013  loss: 0.0188 (0.0188)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [300/985]  eta: 0:11:01  lr: 0.000013  loss: 0.0186 (0.0188)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [310/985]  eta: 0:10:51  lr: 0.000013  loss: 0.0183 (0.0189)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [320/985]  eta: 0:10:41  lr: 0.000013  loss: 0.0183 (0.0189)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [330/985]  eta: 0:10:31  lr: 0.000013  loss: 0.0174 (0.0188)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [340/985]  eta: 0:10:21  lr: 0.000013  loss: 0.0180 (0.0189)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [350/985]  eta: 0:10:12  lr: 0.000013  loss: 0.0184 (0.0189)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [360/985]  eta: 0:10:02  lr: 0.000013  loss: 0.0174 (0.0189)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [370/985]  eta: 0:09:52  lr: 0.000013  loss: 0.0172 (0.0188)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [380/985]  eta: 0:09:42  lr: 0.000013  loss: 0.0175 (0.0189)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [390/985]  eta: 0:09:33  lr: 0.000013  loss: 0.0186 (0.0188)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [400/985]  eta: 0:09:23  lr: 0.000013  loss: 0.0186 (0.0188)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [410/985]  eta: 0:09:13  lr: 0.000013  loss: 0.0186 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [420/985]  eta: 0:09:03  lr: 0.000013  loss: 0.0176 (0.0188)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [430/985]  eta: 0:08:54  lr: 0.000013  loss: 0.0187 (0.0188)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [440/985]  eta: 0:08:44  lr: 0.000013  loss: 0.0185 (0.0188)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [450/985]  eta: 0:08:34  lr: 0.000013  loss: 0.0186 (0.0188)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [460/985]  eta: 0:08:25  lr: 0.000013  loss: 0.0183 (0.0188)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [470/985]  eta: 0:08:15  lr: 0.000013  loss: 0.0182 (0.0188)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [480/985]  eta: 0:08:05  lr: 0.000013  loss: 0.0213 (0.0189)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [490/985]  eta: 0:07:55  lr: 0.000013  loss: 0.0199 (0.0189)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [500/985]  eta: 0:07:46  lr: 0.000013  loss: 0.0190 (0.0190)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [510/985]  eta: 0:07:36  lr: 0.000013  loss: 0.0193 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [520/985]  eta: 0:07:27  lr: 0.000013  loss: 0.0195 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [530/985]  eta: 0:07:17  lr: 0.000013  loss: 0.0197 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [540/985]  eta: 0:07:07  lr: 0.000013  loss: 0.0191 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [550/985]  eta: 0:06:58  lr: 0.000013  loss: 0.0196 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [560/985]  eta: 0:06:48  lr: 0.000013  loss: 0.0189 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [570/985]  eta: 0:06:38  lr: 0.000013  loss: 0.0180 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [580/985]  eta: 0:06:29  lr: 0.000013  loss: 0.0175 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [590/985]  eta: 0:06:19  lr: 0.000013  loss: 0.0179 (0.0190)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [600/985]  eta: 0:06:09  lr: 0.000013  loss: 0.0185 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [610/985]  eta: 0:06:00  lr: 0.000013  loss: 0.0183 (0.0191)  time: 0.9646  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [620/985]  eta: 0:05:50  lr: 0.000013  loss: 0.0196 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [630/985]  eta: 0:05:41  lr: 0.000013  loss: 0.0198 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [640/985]  eta: 0:05:31  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [650/985]  eta: 0:05:21  lr: 0.000013  loss: 0.0191 (0.0191)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [660/985]  eta: 0:05:12  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [670/985]  eta: 0:05:02  lr: 0.000013  loss: 0.0180 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [680/985]  eta: 0:04:52  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [690/985]  eta: 0:04:43  lr: 0.000013  loss: 0.0174 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [700/985]  eta: 0:04:33  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [710/985]  eta: 0:04:24  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [720/985]  eta: 0:04:14  lr: 0.000013  loss: 0.0181 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [730/985]  eta: 0:04:04  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [740/985]  eta: 0:03:55  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9626  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:840]  [750/985]  eta: 0:03:45  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [760/985]  eta: 0:03:36  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [770/985]  eta: 0:03:26  lr: 0.000013  loss: 0.0179 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [780/985]  eta: 0:03:16  lr: 0.000013  loss: 0.0183 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [790/985]  eta: 0:03:07  lr: 0.000013  loss: 0.0191 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [800/985]  eta: 0:02:57  lr: 0.000013  loss: 0.0191 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [810/985]  eta: 0:02:47  lr: 0.000013  loss: 0.0186 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [820/985]  eta: 0:02:38  lr: 0.000013  loss: 0.0182 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [830/985]  eta: 0:02:28  lr: 0.000013  loss: 0.0195 (0.0192)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [840/985]  eta: 0:02:19  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [850/985]  eta: 0:02:09  lr: 0.000013  loss: 0.0170 (0.0192)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [860/985]  eta: 0:01:59  lr: 0.000013  loss: 0.0168 (0.0192)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [870/985]  eta: 0:01:50  lr: 0.000013  loss: 0.0170 (0.0191)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [880/985]  eta: 0:01:40  lr: 0.000013  loss: 0.0159 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [890/985]  eta: 0:01:31  lr: 0.000013  loss: 0.0177 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [900/985]  eta: 0:01:21  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [910/985]  eta: 0:01:11  lr: 0.000013  loss: 0.0180 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [920/985]  eta: 0:01:02  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [930/985]  eta: 0:00:52  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [940/985]  eta: 0:00:43  lr: 0.000013  loss: 0.0188 (0.0191)  time: 0.9618  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [950/985]  eta: 0:00:33  lr: 0.000013  loss: 0.0200 (0.0191)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [960/985]  eta: 0:00:23  lr: 0.000013  loss: 0.0206 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [970/985]  eta: 0:00:14  lr: 0.000013  loss: 0.0186 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [980/985]  eta: 0:00:04  lr: 0.000013  loss: 0.0167 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840]  [984/985]  eta: 0:00:00  lr: 0.000013  loss: 0.0167 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:840] Total time: 0:15:45 (0.9597 s / it)\n",
      "Averaged stats: lr: 0.000013  loss: 0.0167 (0.0191)\n",
      "Valid: [epoch:840]  [ 0/14]  eta: 0:02:55  loss: 0.0144 (0.0144)  time: 12.5162  data: 0.6034  max mem: 41892\n",
      "Valid: [epoch:840]  [13/14]  eta: 0:00:11  loss: 0.0145 (0.0145)  time: 11.8214  data: 0.0432  max mem: 41892\n",
      "Valid: [epoch:840] Total time: 0:02:45 (11.8281 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_840_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:841]  [  0/985]  eta: 1:18:28  lr: 0.000013  loss: 0.0234 (0.0234)  time: 4.7807  data: 3.7638  max mem: 41892\n",
      "Train: [epoch:841]  [ 10/985]  eta: 0:21:09  lr: 0.000013  loss: 0.0201 (0.0205)  time: 1.3019  data: 0.3423  max mem: 41892\n",
      "Train: [epoch:841]  [ 20/985]  eta: 0:18:09  lr: 0.000013  loss: 0.0188 (0.0202)  time: 0.9465  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [ 30/985]  eta: 0:16:59  lr: 0.000013  loss: 0.0180 (0.0196)  time: 0.9388  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [ 40/985]  eta: 0:16:20  lr: 0.000013  loss: 0.0162 (0.0189)  time: 0.9408  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [ 50/985]  eta: 0:15:52  lr: 0.000013  loss: 0.0166 (0.0189)  time: 0.9416  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [ 60/985]  eta: 0:15:30  lr: 0.000013  loss: 0.0185 (0.0193)  time: 0.9413  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [ 70/985]  eta: 0:15:14  lr: 0.000013  loss: 0.0182 (0.0193)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [ 80/985]  eta: 0:14:58  lr: 0.000013  loss: 0.0171 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [ 90/985]  eta: 0:14:44  lr: 0.000013  loss: 0.0187 (0.0192)  time: 0.9470  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [100/985]  eta: 0:14:31  lr: 0.000013  loss: 0.0180 (0.0191)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [110/985]  eta: 0:14:18  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [120/985]  eta: 0:14:07  lr: 0.000013  loss: 0.0184 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [130/985]  eta: 0:13:55  lr: 0.000013  loss: 0.0186 (0.0193)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [140/985]  eta: 0:13:45  lr: 0.000013  loss: 0.0181 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [150/985]  eta: 0:13:34  lr: 0.000013  loss: 0.0179 (0.0192)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [160/985]  eta: 0:13:23  lr: 0.000013  loss: 0.0177 (0.0192)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [170/985]  eta: 0:13:12  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [180/985]  eta: 0:13:02  lr: 0.000013  loss: 0.0195 (0.0192)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [190/985]  eta: 0:12:51  lr: 0.000013  loss: 0.0195 (0.0193)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [200/985]  eta: 0:12:41  lr: 0.000013  loss: 0.0197 (0.0193)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [210/985]  eta: 0:12:31  lr: 0.000013  loss: 0.0175 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [220/985]  eta: 0:12:21  lr: 0.000013  loss: 0.0179 (0.0192)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [230/985]  eta: 0:12:11  lr: 0.000013  loss: 0.0171 (0.0191)  time: 0.9626  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [240/985]  eta: 0:12:01  lr: 0.000013  loss: 0.0174 (0.0192)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [250/985]  eta: 0:11:51  lr: 0.000013  loss: 0.0189 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [260/985]  eta: 0:11:41  lr: 0.000013  loss: 0.0166 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [270/985]  eta: 0:11:31  lr: 0.000013  loss: 0.0168 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [280/985]  eta: 0:11:21  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [290/985]  eta: 0:11:11  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [300/985]  eta: 0:11:01  lr: 0.000013  loss: 0.0180 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [310/985]  eta: 0:10:51  lr: 0.000013  loss: 0.0192 (0.0192)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [320/985]  eta: 0:10:41  lr: 0.000013  loss: 0.0188 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [330/985]  eta: 0:10:32  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [340/985]  eta: 0:10:22  lr: 0.000013  loss: 0.0177 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [350/985]  eta: 0:10:12  lr: 0.000013  loss: 0.0177 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:841]  [360/985]  eta: 0:10:02  lr: 0.000013  loss: 0.0188 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [370/985]  eta: 0:09:52  lr: 0.000013  loss: 0.0187 (0.0192)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [380/985]  eta: 0:09:43  lr: 0.000013  loss: 0.0185 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [390/985]  eta: 0:09:33  lr: 0.000013  loss: 0.0197 (0.0192)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [400/985]  eta: 0:09:23  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [410/985]  eta: 0:09:13  lr: 0.000013  loss: 0.0179 (0.0192)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [420/985]  eta: 0:09:04  lr: 0.000013  loss: 0.0206 (0.0193)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [430/985]  eta: 0:08:54  lr: 0.000013  loss: 0.0197 (0.0193)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [440/985]  eta: 0:08:44  lr: 0.000013  loss: 0.0170 (0.0193)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [450/985]  eta: 0:08:35  lr: 0.000013  loss: 0.0165 (0.0193)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [460/985]  eta: 0:08:25  lr: 0.000013  loss: 0.0179 (0.0193)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [470/985]  eta: 0:08:15  lr: 0.000013  loss: 0.0180 (0.0193)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [480/985]  eta: 0:08:05  lr: 0.000013  loss: 0.0174 (0.0193)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [490/985]  eta: 0:07:56  lr: 0.000013  loss: 0.0179 (0.0193)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [500/985]  eta: 0:07:46  lr: 0.000013  loss: 0.0181 (0.0193)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [510/985]  eta: 0:07:36  lr: 0.000013  loss: 0.0178 (0.0193)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [520/985]  eta: 0:07:27  lr: 0.000013  loss: 0.0177 (0.0193)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [530/985]  eta: 0:07:17  lr: 0.000013  loss: 0.0192 (0.0193)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [540/985]  eta: 0:07:07  lr: 0.000013  loss: 0.0179 (0.0193)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [550/985]  eta: 0:06:58  lr: 0.000013  loss: 0.0165 (0.0192)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [560/985]  eta: 0:06:48  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [570/985]  eta: 0:06:38  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [580/985]  eta: 0:06:29  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [590/985]  eta: 0:06:19  lr: 0.000013  loss: 0.0193 (0.0193)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [600/985]  eta: 0:06:10  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9627  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [610/985]  eta: 0:06:00  lr: 0.000013  loss: 0.0179 (0.0192)  time: 0.9625  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [620/985]  eta: 0:05:50  lr: 0.000013  loss: 0.0190 (0.0193)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [630/985]  eta: 0:05:41  lr: 0.000013  loss: 0.0188 (0.0192)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [640/985]  eta: 0:05:31  lr: 0.000013  loss: 0.0177 (0.0192)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [650/985]  eta: 0:05:21  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [660/985]  eta: 0:05:12  lr: 0.000013  loss: 0.0192 (0.0192)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [670/985]  eta: 0:05:02  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [680/985]  eta: 0:04:52  lr: 0.000013  loss: 0.0163 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [690/985]  eta: 0:04:43  lr: 0.000013  loss: 0.0163 (0.0192)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [700/985]  eta: 0:04:33  lr: 0.000013  loss: 0.0167 (0.0192)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [710/985]  eta: 0:04:24  lr: 0.000013  loss: 0.0184 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [720/985]  eta: 0:04:14  lr: 0.000013  loss: 0.0172 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [730/985]  eta: 0:04:04  lr: 0.000013  loss: 0.0165 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [740/985]  eta: 0:03:55  lr: 0.000013  loss: 0.0179 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [750/985]  eta: 0:03:45  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [760/985]  eta: 0:03:35  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [770/985]  eta: 0:03:26  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [780/985]  eta: 0:03:16  lr: 0.000013  loss: 0.0180 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [790/985]  eta: 0:03:07  lr: 0.000013  loss: 0.0189 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [800/985]  eta: 0:02:57  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [810/985]  eta: 0:02:47  lr: 0.000013  loss: 0.0186 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [820/985]  eta: 0:02:38  lr: 0.000013  loss: 0.0184 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [830/985]  eta: 0:02:28  lr: 0.000013  loss: 0.0180 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [840/985]  eta: 0:02:19  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [850/985]  eta: 0:02:09  lr: 0.000013  loss: 0.0183 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [860/985]  eta: 0:01:59  lr: 0.000013  loss: 0.0183 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [870/985]  eta: 0:01:50  lr: 0.000013  loss: 0.0169 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [880/985]  eta: 0:01:40  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [890/985]  eta: 0:01:31  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [900/985]  eta: 0:01:21  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [910/985]  eta: 0:01:11  lr: 0.000013  loss: 0.0193 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [920/985]  eta: 0:01:02  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [930/985]  eta: 0:00:52  lr: 0.000013  loss: 0.0200 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [940/985]  eta: 0:00:43  lr: 0.000013  loss: 0.0196 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [950/985]  eta: 0:00:33  lr: 0.000013  loss: 0.0201 (0.0192)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [960/985]  eta: 0:00:23  lr: 0.000013  loss: 0.0209 (0.0192)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [970/985]  eta: 0:00:14  lr: 0.000013  loss: 0.0184 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [980/985]  eta: 0:00:04  lr: 0.000013  loss: 0.0174 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841]  [984/985]  eta: 0:00:00  lr: 0.000013  loss: 0.0180 (0.0191)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:841] Total time: 0:15:44 (0.9591 s / it)\n",
      "Averaged stats: lr: 0.000013  loss: 0.0180 (0.0191)\n",
      "Valid: [epoch:841]  [ 0/14]  eta: 0:02:54  loss: 0.0151 (0.0151)  time: 12.4783  data: 0.5912  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:841]  [13/14]  eta: 0:00:12  loss: 0.0146 (0.0146)  time: 12.0870  data: 0.0423  max mem: 41892\n",
      "Valid: [epoch:841] Total time: 0:02:49 (12.0939 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_841_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:842]  [  0/985]  eta: 0:42:33  lr: 0.000013  loss: 0.0145 (0.0145)  time: 2.5928  data: 1.6191  max mem: 41892\n",
      "Train: [epoch:842]  [ 10/985]  eta: 0:18:38  lr: 0.000013  loss: 0.0198 (0.0198)  time: 1.1472  data: 0.1837  max mem: 41892\n",
      "Train: [epoch:842]  [ 20/985]  eta: 0:16:53  lr: 0.000013  loss: 0.0178 (0.0196)  time: 0.9730  data: 0.0201  max mem: 41892\n",
      "Train: [epoch:842]  [ 30/985]  eta: 0:16:12  lr: 0.000013  loss: 0.0178 (0.0200)  time: 0.9477  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [ 40/985]  eta: 0:15:45  lr: 0.000013  loss: 0.0181 (0.0195)  time: 0.9481  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [ 50/985]  eta: 0:15:25  lr: 0.000013  loss: 0.0183 (0.0196)  time: 0.9451  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [ 60/985]  eta: 0:15:09  lr: 0.000013  loss: 0.0184 (0.0195)  time: 0.9489  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [ 70/985]  eta: 0:14:55  lr: 0.000013  loss: 0.0178 (0.0194)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [ 80/985]  eta: 0:14:42  lr: 0.000013  loss: 0.0166 (0.0190)  time: 0.9478  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [ 90/985]  eta: 0:14:30  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [100/985]  eta: 0:14:19  lr: 0.000013  loss: 0.0179 (0.0190)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [110/985]  eta: 0:14:07  lr: 0.000013  loss: 0.0173 (0.0189)  time: 0.9493  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [120/985]  eta: 0:13:56  lr: 0.000013  loss: 0.0174 (0.0190)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [130/985]  eta: 0:13:46  lr: 0.000013  loss: 0.0194 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [140/985]  eta: 0:13:36  lr: 0.000013  loss: 0.0187 (0.0192)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [150/985]  eta: 0:13:25  lr: 0.000013  loss: 0.0182 (0.0192)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [160/985]  eta: 0:13:16  lr: 0.000013  loss: 0.0184 (0.0192)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [170/985]  eta: 0:13:05  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [180/985]  eta: 0:12:55  lr: 0.000013  loss: 0.0188 (0.0193)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [190/985]  eta: 0:12:45  lr: 0.000013  loss: 0.0201 (0.0194)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [200/985]  eta: 0:12:35  lr: 0.000013  loss: 0.0196 (0.0194)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [210/985]  eta: 0:12:25  lr: 0.000013  loss: 0.0180 (0.0193)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [220/985]  eta: 0:12:15  lr: 0.000013  loss: 0.0174 (0.0192)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [230/985]  eta: 0:12:05  lr: 0.000013  loss: 0.0165 (0.0192)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [240/985]  eta: 0:11:56  lr: 0.000013  loss: 0.0181 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [250/985]  eta: 0:11:46  lr: 0.000013  loss: 0.0181 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [260/985]  eta: 0:11:36  lr: 0.000013  loss: 0.0171 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [270/985]  eta: 0:11:26  lr: 0.000013  loss: 0.0174 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [280/985]  eta: 0:11:16  lr: 0.000013  loss: 0.0177 (0.0190)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [290/985]  eta: 0:11:07  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [300/985]  eta: 0:10:57  lr: 0.000013  loss: 0.0197 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [310/985]  eta: 0:10:48  lr: 0.000013  loss: 0.0188 (0.0191)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [320/985]  eta: 0:10:38  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [330/985]  eta: 0:10:28  lr: 0.000013  loss: 0.0178 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [340/985]  eta: 0:10:19  lr: 0.000013  loss: 0.0184 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [350/985]  eta: 0:10:09  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [360/985]  eta: 0:09:59  lr: 0.000013  loss: 0.0177 (0.0190)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [370/985]  eta: 0:09:50  lr: 0.000013  loss: 0.0178 (0.0190)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [380/985]  eta: 0:09:40  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [390/985]  eta: 0:09:30  lr: 0.000013  loss: 0.0193 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [400/985]  eta: 0:09:21  lr: 0.000013  loss: 0.0190 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [410/985]  eta: 0:09:11  lr: 0.000013  loss: 0.0191 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [420/985]  eta: 0:09:02  lr: 0.000013  loss: 0.0172 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [430/985]  eta: 0:08:52  lr: 0.000013  loss: 0.0177 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [440/985]  eta: 0:08:42  lr: 0.000013  loss: 0.0193 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [450/985]  eta: 0:08:33  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [460/985]  eta: 0:08:23  lr: 0.000013  loss: 0.0184 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [470/985]  eta: 0:08:14  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [480/985]  eta: 0:08:04  lr: 0.000013  loss: 0.0179 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [490/985]  eta: 0:07:54  lr: 0.000013  loss: 0.0202 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [500/985]  eta: 0:07:45  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [510/985]  eta: 0:07:35  lr: 0.000013  loss: 0.0195 (0.0192)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [520/985]  eta: 0:07:26  lr: 0.000013  loss: 0.0181 (0.0192)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [530/985]  eta: 0:07:16  lr: 0.000013  loss: 0.0192 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [540/985]  eta: 0:07:06  lr: 0.000013  loss: 0.0193 (0.0192)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [550/985]  eta: 0:06:57  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [560/985]  eta: 0:06:47  lr: 0.000013  loss: 0.0180 (0.0192)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [570/985]  eta: 0:06:38  lr: 0.000013  loss: 0.0179 (0.0192)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [580/985]  eta: 0:06:28  lr: 0.000013  loss: 0.0183 (0.0192)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [590/985]  eta: 0:06:18  lr: 0.000013  loss: 0.0183 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [600/985]  eta: 0:06:09  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [610/985]  eta: 0:05:59  lr: 0.000013  loss: 0.0169 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [620/985]  eta: 0:05:49  lr: 0.000013  loss: 0.0183 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:842]  [630/985]  eta: 0:05:40  lr: 0.000013  loss: 0.0185 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [640/985]  eta: 0:05:30  lr: 0.000013  loss: 0.0172 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [650/985]  eta: 0:05:21  lr: 0.000013  loss: 0.0171 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [660/985]  eta: 0:05:11  lr: 0.000013  loss: 0.0181 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [670/985]  eta: 0:05:02  lr: 0.000013  loss: 0.0165 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [680/985]  eta: 0:04:52  lr: 0.000013  loss: 0.0168 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [690/985]  eta: 0:04:42  lr: 0.000013  loss: 0.0184 (0.0190)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [700/985]  eta: 0:04:33  lr: 0.000013  loss: 0.0189 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [710/985]  eta: 0:04:23  lr: 0.000013  loss: 0.0197 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [720/985]  eta: 0:04:14  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [730/985]  eta: 0:04:04  lr: 0.000013  loss: 0.0177 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [740/985]  eta: 0:03:54  lr: 0.000013  loss: 0.0174 (0.0190)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [750/985]  eta: 0:03:45  lr: 0.000013  loss: 0.0179 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [760/985]  eta: 0:03:35  lr: 0.000013  loss: 0.0182 (0.0190)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [770/985]  eta: 0:03:26  lr: 0.000013  loss: 0.0180 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [780/985]  eta: 0:03:16  lr: 0.000013  loss: 0.0185 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [790/985]  eta: 0:03:06  lr: 0.000013  loss: 0.0184 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [800/985]  eta: 0:02:57  lr: 0.000013  loss: 0.0186 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [810/985]  eta: 0:02:47  lr: 0.000013  loss: 0.0179 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [820/985]  eta: 0:02:38  lr: 0.000013  loss: 0.0174 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [830/985]  eta: 0:02:28  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [840/985]  eta: 0:02:18  lr: 0.000013  loss: 0.0184 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [850/985]  eta: 0:02:09  lr: 0.000013  loss: 0.0176 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [860/985]  eta: 0:01:59  lr: 0.000013  loss: 0.0171 (0.0191)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [870/985]  eta: 0:01:50  lr: 0.000013  loss: 0.0173 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [880/985]  eta: 0:01:40  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [890/985]  eta: 0:01:31  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [900/985]  eta: 0:01:21  lr: 0.000013  loss: 0.0187 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [910/985]  eta: 0:01:11  lr: 0.000013  loss: 0.0191 (0.0191)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [920/985]  eta: 0:01:02  lr: 0.000013  loss: 0.0185 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [930/985]  eta: 0:00:52  lr: 0.000013  loss: 0.0159 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [940/985]  eta: 0:00:43  lr: 0.000013  loss: 0.0175 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [950/985]  eta: 0:00:33  lr: 0.000013  loss: 0.0179 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [960/985]  eta: 0:00:23  lr: 0.000013  loss: 0.0179 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [970/985]  eta: 0:00:14  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9500  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [980/985]  eta: 0:00:04  lr: 0.000013  loss: 0.0182 (0.0191)  time: 0.9499  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842]  [984/985]  eta: 0:00:00  lr: 0.000013  loss: 0.0176 (0.0191)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:842] Total time: 0:15:43 (0.9579 s / it)\n",
      "Averaged stats: lr: 0.000013  loss: 0.0176 (0.0191)\n",
      "Valid: [epoch:842]  [ 0/14]  eta: 0:02:59  loss: 0.0147 (0.0147)  time: 12.7974  data: 0.6030  max mem: 41892\n",
      "Valid: [epoch:842]  [13/14]  eta: 0:00:12  loss: 0.0145 (0.0144)  time: 12.0239  data: 0.0432  max mem: 41892\n",
      "Valid: [epoch:842] Total time: 0:02:48 (12.0303 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0144)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_842_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:843]  [  0/985]  eta: 1:17:16  lr: 0.000012  loss: 0.0281 (0.0281)  time: 4.7071  data: 3.7318  max mem: 41892\n",
      "Train: [epoch:843]  [ 10/985]  eta: 0:21:00  lr: 0.000012  loss: 0.0191 (0.0205)  time: 1.2931  data: 0.3394  max mem: 41892\n",
      "Train: [epoch:843]  [ 20/985]  eta: 0:18:03  lr: 0.000012  loss: 0.0168 (0.0194)  time: 0.9435  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [ 30/985]  eta: 0:16:56  lr: 0.000012  loss: 0.0192 (0.0205)  time: 0.9381  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [ 40/985]  eta: 0:16:17  lr: 0.000012  loss: 0.0188 (0.0195)  time: 0.9413  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [ 50/985]  eta: 0:15:50  lr: 0.000012  loss: 0.0164 (0.0193)  time: 0.9421  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [ 60/985]  eta: 0:15:28  lr: 0.000012  loss: 0.0190 (0.0195)  time: 0.9420  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [ 70/985]  eta: 0:15:11  lr: 0.000012  loss: 0.0191 (0.0194)  time: 0.9442  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [ 80/985]  eta: 0:14:55  lr: 0.000012  loss: 0.0161 (0.0191)  time: 0.9475  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [ 90/985]  eta: 0:14:41  lr: 0.000012  loss: 0.0181 (0.0192)  time: 0.9474  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [100/985]  eta: 0:14:28  lr: 0.000012  loss: 0.0180 (0.0190)  time: 0.9488  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [110/985]  eta: 0:14:16  lr: 0.000012  loss: 0.0180 (0.0190)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [120/985]  eta: 0:14:04  lr: 0.000012  loss: 0.0184 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [130/985]  eta: 0:13:53  lr: 0.000012  loss: 0.0184 (0.0192)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [140/985]  eta: 0:13:42  lr: 0.000012  loss: 0.0191 (0.0194)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [150/985]  eta: 0:13:31  lr: 0.000012  loss: 0.0211 (0.0195)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [160/985]  eta: 0:13:21  lr: 0.000012  loss: 0.0203 (0.0196)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [170/985]  eta: 0:13:11  lr: 0.000012  loss: 0.0177 (0.0195)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [180/985]  eta: 0:13:01  lr: 0.000012  loss: 0.0184 (0.0195)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [190/985]  eta: 0:12:50  lr: 0.000012  loss: 0.0198 (0.0196)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [200/985]  eta: 0:12:40  lr: 0.000012  loss: 0.0192 (0.0196)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [210/985]  eta: 0:12:30  lr: 0.000012  loss: 0.0182 (0.0195)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [220/985]  eta: 0:12:20  lr: 0.000012  loss: 0.0180 (0.0195)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [230/985]  eta: 0:12:10  lr: 0.000012  loss: 0.0180 (0.0194)  time: 0.9575  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:843]  [240/985]  eta: 0:12:00  lr: 0.000012  loss: 0.0173 (0.0194)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [250/985]  eta: 0:11:50  lr: 0.000012  loss: 0.0190 (0.0194)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [260/985]  eta: 0:11:40  lr: 0.000012  loss: 0.0179 (0.0194)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [270/985]  eta: 0:11:30  lr: 0.000012  loss: 0.0179 (0.0193)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [280/985]  eta: 0:11:20  lr: 0.000012  loss: 0.0189 (0.0193)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [290/985]  eta: 0:11:10  lr: 0.000012  loss: 0.0190 (0.0193)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [300/985]  eta: 0:11:01  lr: 0.000012  loss: 0.0192 (0.0193)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [310/985]  eta: 0:10:51  lr: 0.000012  loss: 0.0198 (0.0194)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [320/985]  eta: 0:10:41  lr: 0.000012  loss: 0.0195 (0.0194)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [330/985]  eta: 0:10:31  lr: 0.000012  loss: 0.0177 (0.0194)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [340/985]  eta: 0:10:21  lr: 0.000012  loss: 0.0172 (0.0193)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [350/985]  eta: 0:10:12  lr: 0.000012  loss: 0.0173 (0.0193)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [360/985]  eta: 0:10:02  lr: 0.000012  loss: 0.0182 (0.0193)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [370/985]  eta: 0:09:52  lr: 0.000012  loss: 0.0174 (0.0192)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [380/985]  eta: 0:09:42  lr: 0.000012  loss: 0.0159 (0.0192)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [390/985]  eta: 0:09:33  lr: 0.000012  loss: 0.0174 (0.0192)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [400/985]  eta: 0:09:23  lr: 0.000012  loss: 0.0199 (0.0192)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [410/985]  eta: 0:09:13  lr: 0.000012  loss: 0.0189 (0.0192)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [420/985]  eta: 0:09:03  lr: 0.000012  loss: 0.0189 (0.0193)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [430/985]  eta: 0:08:54  lr: 0.000012  loss: 0.0196 (0.0193)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [440/985]  eta: 0:08:44  lr: 0.000012  loss: 0.0191 (0.0193)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [450/985]  eta: 0:08:34  lr: 0.000012  loss: 0.0183 (0.0192)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [460/985]  eta: 0:08:25  lr: 0.000012  loss: 0.0169 (0.0192)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [470/985]  eta: 0:08:15  lr: 0.000012  loss: 0.0171 (0.0192)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [480/985]  eta: 0:08:05  lr: 0.000012  loss: 0.0180 (0.0192)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [490/985]  eta: 0:07:55  lr: 0.000012  loss: 0.0187 (0.0192)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [500/985]  eta: 0:07:46  lr: 0.000012  loss: 0.0175 (0.0192)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [510/985]  eta: 0:07:36  lr: 0.000012  loss: 0.0178 (0.0192)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [520/985]  eta: 0:07:27  lr: 0.000012  loss: 0.0187 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [530/985]  eta: 0:07:17  lr: 0.000012  loss: 0.0189 (0.0192)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [540/985]  eta: 0:07:07  lr: 0.000012  loss: 0.0183 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [550/985]  eta: 0:06:58  lr: 0.000012  loss: 0.0183 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [560/985]  eta: 0:06:48  lr: 0.000012  loss: 0.0173 (0.0192)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [570/985]  eta: 0:06:38  lr: 0.000012  loss: 0.0172 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [580/985]  eta: 0:06:29  lr: 0.000012  loss: 0.0188 (0.0192)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [590/985]  eta: 0:06:19  lr: 0.000012  loss: 0.0199 (0.0192)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [600/985]  eta: 0:06:09  lr: 0.000012  loss: 0.0176 (0.0191)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [610/985]  eta: 0:06:00  lr: 0.000012  loss: 0.0171 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [620/985]  eta: 0:05:50  lr: 0.000012  loss: 0.0179 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [630/985]  eta: 0:05:40  lr: 0.000012  loss: 0.0196 (0.0192)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [640/985]  eta: 0:05:31  lr: 0.000012  loss: 0.0173 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [650/985]  eta: 0:05:21  lr: 0.000012  loss: 0.0173 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [660/985]  eta: 0:05:12  lr: 0.000012  loss: 0.0173 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [670/985]  eta: 0:05:02  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [680/985]  eta: 0:04:52  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [690/985]  eta: 0:04:43  lr: 0.000012  loss: 0.0187 (0.0192)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [700/985]  eta: 0:04:33  lr: 0.000012  loss: 0.0173 (0.0191)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [710/985]  eta: 0:04:23  lr: 0.000012  loss: 0.0185 (0.0192)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [720/985]  eta: 0:04:14  lr: 0.000012  loss: 0.0193 (0.0192)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [730/985]  eta: 0:04:04  lr: 0.000012  loss: 0.0183 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [740/985]  eta: 0:03:55  lr: 0.000012  loss: 0.0181 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [750/985]  eta: 0:03:45  lr: 0.000012  loss: 0.0181 (0.0191)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [760/985]  eta: 0:03:35  lr: 0.000012  loss: 0.0194 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [770/985]  eta: 0:03:26  lr: 0.000012  loss: 0.0179 (0.0191)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [780/985]  eta: 0:03:16  lr: 0.000012  loss: 0.0176 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [790/985]  eta: 0:03:07  lr: 0.000012  loss: 0.0176 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [800/985]  eta: 0:02:57  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [810/985]  eta: 0:02:47  lr: 0.000012  loss: 0.0175 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [820/985]  eta: 0:02:38  lr: 0.000012  loss: 0.0178 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [830/985]  eta: 0:02:28  lr: 0.000012  loss: 0.0176 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [840/985]  eta: 0:02:19  lr: 0.000012  loss: 0.0175 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [850/985]  eta: 0:02:09  lr: 0.000012  loss: 0.0175 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [860/985]  eta: 0:01:59  lr: 0.000012  loss: 0.0180 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [870/985]  eta: 0:01:50  lr: 0.000012  loss: 0.0177 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [880/985]  eta: 0:01:40  lr: 0.000012  loss: 0.0172 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [890/985]  eta: 0:01:31  lr: 0.000012  loss: 0.0186 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:843]  [900/985]  eta: 0:01:21  lr: 0.000012  loss: 0.0201 (0.0191)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [910/985]  eta: 0:01:11  lr: 0.000012  loss: 0.0209 (0.0192)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [920/985]  eta: 0:01:02  lr: 0.000012  loss: 0.0196 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [930/985]  eta: 0:00:52  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [940/985]  eta: 0:00:43  lr: 0.000012  loss: 0.0177 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [950/985]  eta: 0:00:33  lr: 0.000012  loss: 0.0173 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [960/985]  eta: 0:00:23  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [970/985]  eta: 0:00:14  lr: 0.000012  loss: 0.0177 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [980/985]  eta: 0:00:04  lr: 0.000012  loss: 0.0180 (0.0191)  time: 0.9502  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843]  [984/985]  eta: 0:00:00  lr: 0.000012  loss: 0.0181 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:843] Total time: 0:15:44 (0.9591 s / it)\n",
      "Averaged stats: lr: 0.000012  loss: 0.0181 (0.0191)\n",
      "Valid: [epoch:843]  [ 0/14]  eta: 0:03:00  loss: 0.0147 (0.0147)  time: 12.8731  data: 0.6990  max mem: 41892\n",
      "Valid: [epoch:843]  [13/14]  eta: 0:00:12  loss: 0.0145 (0.0144)  time: 12.0677  data: 0.0500  max mem: 41892\n",
      "Valid: [epoch:843] Total time: 0:02:49 (12.0744 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0144)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_843_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:844]  [  0/985]  eta: 1:01:46  lr: 0.000012  loss: 0.0156 (0.0156)  time: 3.7630  data: 2.7548  max mem: 41892\n",
      "Train: [epoch:844]  [ 10/985]  eta: 0:19:36  lr: 0.000012  loss: 0.0169 (0.0173)  time: 1.2070  data: 0.2506  max mem: 41892\n",
      "Train: [epoch:844]  [ 20/985]  eta: 0:17:21  lr: 0.000012  loss: 0.0184 (0.0179)  time: 0.9455  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:844]  [ 30/985]  eta: 0:16:28  lr: 0.000012  loss: 0.0181 (0.0180)  time: 0.9412  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [ 40/985]  eta: 0:16:02  lr: 0.000012  loss: 0.0168 (0.0181)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [ 50/985]  eta: 0:15:39  lr: 0.000012  loss: 0.0174 (0.0184)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [ 60/985]  eta: 0:15:22  lr: 0.000012  loss: 0.0182 (0.0189)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [ 70/985]  eta: 0:15:06  lr: 0.000012  loss: 0.0188 (0.0189)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [ 80/985]  eta: 0:14:52  lr: 0.000012  loss: 0.0169 (0.0189)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [ 90/985]  eta: 0:14:40  lr: 0.000012  loss: 0.0178 (0.0190)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [100/985]  eta: 0:14:28  lr: 0.000012  loss: 0.0180 (0.0191)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [110/985]  eta: 0:14:17  lr: 0.000012  loss: 0.0182 (0.0190)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [120/985]  eta: 0:14:05  lr: 0.000012  loss: 0.0193 (0.0192)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [130/985]  eta: 0:13:55  lr: 0.000012  loss: 0.0201 (0.0194)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [140/985]  eta: 0:13:44  lr: 0.000012  loss: 0.0177 (0.0194)  time: 0.9635  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [150/985]  eta: 0:13:33  lr: 0.000012  loss: 0.0181 (0.0194)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [160/985]  eta: 0:13:22  lr: 0.000012  loss: 0.0187 (0.0193)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [170/985]  eta: 0:13:12  lr: 0.000012  loss: 0.0172 (0.0192)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [180/985]  eta: 0:13:01  lr: 0.000012  loss: 0.0172 (0.0192)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [190/985]  eta: 0:12:51  lr: 0.000012  loss: 0.0184 (0.0193)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [200/985]  eta: 0:12:41  lr: 0.000012  loss: 0.0181 (0.0192)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [210/985]  eta: 0:12:31  lr: 0.000012  loss: 0.0181 (0.0192)  time: 0.9637  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [220/985]  eta: 0:12:21  lr: 0.000012  loss: 0.0180 (0.0192)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [230/985]  eta: 0:12:11  lr: 0.000012  loss: 0.0180 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [240/985]  eta: 0:12:01  lr: 0.000012  loss: 0.0184 (0.0191)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [250/985]  eta: 0:11:51  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [260/985]  eta: 0:11:41  lr: 0.000012  loss: 0.0169 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [270/985]  eta: 0:11:31  lr: 0.000012  loss: 0.0171 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [280/985]  eta: 0:11:21  lr: 0.000012  loss: 0.0177 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [290/985]  eta: 0:11:11  lr: 0.000012  loss: 0.0184 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [300/985]  eta: 0:11:01  lr: 0.000012  loss: 0.0185 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [310/985]  eta: 0:10:51  lr: 0.000012  loss: 0.0189 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [320/985]  eta: 0:10:41  lr: 0.000012  loss: 0.0176 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [330/985]  eta: 0:10:32  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [340/985]  eta: 0:10:22  lr: 0.000012  loss: 0.0183 (0.0191)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [350/985]  eta: 0:10:12  lr: 0.000012  loss: 0.0179 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [360/985]  eta: 0:10:02  lr: 0.000012  loss: 0.0179 (0.0191)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [370/985]  eta: 0:09:52  lr: 0.000012  loss: 0.0177 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [380/985]  eta: 0:09:43  lr: 0.000012  loss: 0.0177 (0.0190)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [390/985]  eta: 0:09:33  lr: 0.000012  loss: 0.0184 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [400/985]  eta: 0:09:23  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [410/985]  eta: 0:09:14  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [420/985]  eta: 0:09:04  lr: 0.000012  loss: 0.0178 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [430/985]  eta: 0:08:54  lr: 0.000012  loss: 0.0176 (0.0190)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [440/985]  eta: 0:08:44  lr: 0.000012  loss: 0.0179 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [450/985]  eta: 0:08:35  lr: 0.000012  loss: 0.0179 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [460/985]  eta: 0:08:25  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [470/985]  eta: 0:08:15  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [480/985]  eta: 0:08:06  lr: 0.000012  loss: 0.0177 (0.0190)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [490/985]  eta: 0:07:56  lr: 0.000012  loss: 0.0190 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [500/985]  eta: 0:07:46  lr: 0.000012  loss: 0.0209 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:844]  [510/985]  eta: 0:07:36  lr: 0.000012  loss: 0.0199 (0.0191)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [520/985]  eta: 0:07:27  lr: 0.000012  loss: 0.0190 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [530/985]  eta: 0:07:17  lr: 0.000012  loss: 0.0171 (0.0191)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [540/985]  eta: 0:07:08  lr: 0.000012  loss: 0.0171 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [550/985]  eta: 0:06:58  lr: 0.000012  loss: 0.0186 (0.0191)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [560/985]  eta: 0:06:48  lr: 0.000012  loss: 0.0190 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [570/985]  eta: 0:06:38  lr: 0.000012  loss: 0.0181 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [580/985]  eta: 0:06:29  lr: 0.000012  loss: 0.0181 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [590/985]  eta: 0:06:19  lr: 0.000012  loss: 0.0183 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [600/985]  eta: 0:06:09  lr: 0.000012  loss: 0.0171 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [610/985]  eta: 0:06:00  lr: 0.000012  loss: 0.0163 (0.0190)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [620/985]  eta: 0:05:50  lr: 0.000012  loss: 0.0175 (0.0191)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [630/985]  eta: 0:05:41  lr: 0.000012  loss: 0.0192 (0.0190)  time: 0.9583  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [640/985]  eta: 0:05:31  lr: 0.000012  loss: 0.0172 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [650/985]  eta: 0:05:21  lr: 0.000012  loss: 0.0173 (0.0190)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [660/985]  eta: 0:05:12  lr: 0.000012  loss: 0.0173 (0.0190)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [670/985]  eta: 0:05:02  lr: 0.000012  loss: 0.0181 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [680/985]  eta: 0:04:52  lr: 0.000012  loss: 0.0178 (0.0190)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [690/985]  eta: 0:04:43  lr: 0.000012  loss: 0.0179 (0.0190)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [700/985]  eta: 0:04:33  lr: 0.000012  loss: 0.0182 (0.0190)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [710/985]  eta: 0:04:24  lr: 0.000012  loss: 0.0182 (0.0190)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [720/985]  eta: 0:04:14  lr: 0.000012  loss: 0.0188 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [730/985]  eta: 0:04:04  lr: 0.000012  loss: 0.0187 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [740/985]  eta: 0:03:55  lr: 0.000012  loss: 0.0195 (0.0190)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [750/985]  eta: 0:03:45  lr: 0.000012  loss: 0.0195 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [760/985]  eta: 0:03:35  lr: 0.000012  loss: 0.0184 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [770/985]  eta: 0:03:26  lr: 0.000012  loss: 0.0176 (0.0190)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [780/985]  eta: 0:03:16  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [790/985]  eta: 0:03:07  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [800/985]  eta: 0:02:57  lr: 0.000012  loss: 0.0170 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [810/985]  eta: 0:02:47  lr: 0.000012  loss: 0.0180 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [820/985]  eta: 0:02:38  lr: 0.000012  loss: 0.0186 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [830/985]  eta: 0:02:28  lr: 0.000012  loss: 0.0194 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [840/985]  eta: 0:02:19  lr: 0.000012  loss: 0.0177 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [850/985]  eta: 0:02:09  lr: 0.000012  loss: 0.0162 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [860/985]  eta: 0:01:59  lr: 0.000012  loss: 0.0172 (0.0191)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [870/985]  eta: 0:01:50  lr: 0.000012  loss: 0.0180 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [880/985]  eta: 0:01:40  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [890/985]  eta: 0:01:31  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [900/985]  eta: 0:01:21  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [910/985]  eta: 0:01:11  lr: 0.000012  loss: 0.0192 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [920/985]  eta: 0:01:02  lr: 0.000012  loss: 0.0193 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [930/985]  eta: 0:00:52  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [940/985]  eta: 0:00:43  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [950/985]  eta: 0:00:33  lr: 0.000012  loss: 0.0190 (0.0191)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [960/985]  eta: 0:00:23  lr: 0.000012  loss: 0.0200 (0.0191)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [970/985]  eta: 0:00:14  lr: 0.000012  loss: 0.0197 (0.0191)  time: 0.9501  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [980/985]  eta: 0:00:04  lr: 0.000012  loss: 0.0199 (0.0191)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844]  [984/985]  eta: 0:00:00  lr: 0.000012  loss: 0.0197 (0.0191)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:844] Total time: 0:15:44 (0.9586 s / it)\n",
      "Averaged stats: lr: 0.000012  loss: 0.0197 (0.0191)\n",
      "Valid: [epoch:844]  [ 0/14]  eta: 0:02:54  loss: 0.0151 (0.0151)  time: 12.4668  data: 0.5448  max mem: 41892\n",
      "Valid: [epoch:844]  [13/14]  eta: 0:00:11  loss: 0.0145 (0.0145)  time: 11.8933  data: 0.0390  max mem: 41892\n",
      "Valid: [epoch:844] Total time: 0:02:46 (11.9000 s / it)\n",
      "Averaged stats: loss: 0.0145 (0.0145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_844_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:845]  [  0/985]  eta: 1:09:24  lr: 0.000012  loss: 0.0183 (0.0183)  time: 4.2275  data: 3.2259  max mem: 41892\n",
      "Train: [epoch:845]  [ 10/985]  eta: 0:20:29  lr: 0.000012  loss: 0.0183 (0.0193)  time: 1.2608  data: 0.2934  max mem: 41892\n",
      "Train: [epoch:845]  [ 20/985]  eta: 0:17:47  lr: 0.000012  loss: 0.0176 (0.0186)  time: 0.9505  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [ 30/985]  eta: 0:16:45  lr: 0.000012  loss: 0.0186 (0.0190)  time: 0.9378  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [ 40/985]  eta: 0:16:09  lr: 0.000012  loss: 0.0175 (0.0185)  time: 0.9403  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [ 50/985]  eta: 0:15:43  lr: 0.000012  loss: 0.0164 (0.0184)  time: 0.9431  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [ 60/985]  eta: 0:15:24  lr: 0.000012  loss: 0.0180 (0.0187)  time: 0.9454  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [ 70/985]  eta: 0:15:08  lr: 0.000012  loss: 0.0182 (0.0185)  time: 0.9514  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [ 80/985]  eta: 0:14:53  lr: 0.000012  loss: 0.0167 (0.0184)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [ 90/985]  eta: 0:14:41  lr: 0.000012  loss: 0.0173 (0.0183)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [100/985]  eta: 0:14:28  lr: 0.000012  loss: 0.0182 (0.0185)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [110/985]  eta: 0:14:15  lr: 0.000012  loss: 0.0184 (0.0186)  time: 0.9501  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:845]  [120/985]  eta: 0:14:04  lr: 0.000012  loss: 0.0187 (0.0187)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [130/985]  eta: 0:13:53  lr: 0.000012  loss: 0.0184 (0.0187)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [140/985]  eta: 0:13:43  lr: 0.000012  loss: 0.0173 (0.0188)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [150/985]  eta: 0:13:32  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [160/985]  eta: 0:13:21  lr: 0.000012  loss: 0.0190 (0.0189)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [170/985]  eta: 0:13:11  lr: 0.000012  loss: 0.0185 (0.0188)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [180/985]  eta: 0:13:01  lr: 0.000012  loss: 0.0184 (0.0188)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [190/985]  eta: 0:12:50  lr: 0.000012  loss: 0.0188 (0.0189)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [200/985]  eta: 0:12:40  lr: 0.000012  loss: 0.0183 (0.0189)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [210/985]  eta: 0:12:30  lr: 0.000012  loss: 0.0193 (0.0190)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [220/985]  eta: 0:12:20  lr: 0.000012  loss: 0.0197 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [230/985]  eta: 0:12:10  lr: 0.000012  loss: 0.0181 (0.0190)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [240/985]  eta: 0:12:00  lr: 0.000012  loss: 0.0169 (0.0190)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [250/985]  eta: 0:11:50  lr: 0.000012  loss: 0.0174 (0.0189)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [260/985]  eta: 0:11:40  lr: 0.000012  loss: 0.0167 (0.0188)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [270/985]  eta: 0:11:30  lr: 0.000012  loss: 0.0169 (0.0188)  time: 0.9628  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [280/985]  eta: 0:11:21  lr: 0.000012  loss: 0.0170 (0.0188)  time: 0.9651  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [290/985]  eta: 0:11:11  lr: 0.000012  loss: 0.0176 (0.0188)  time: 0.9645  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [300/985]  eta: 0:11:01  lr: 0.000012  loss: 0.0183 (0.0188)  time: 0.9655  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [310/985]  eta: 0:10:51  lr: 0.000012  loss: 0.0179 (0.0188)  time: 0.9630  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [320/985]  eta: 0:10:42  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9573  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [330/985]  eta: 0:10:32  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [340/985]  eta: 0:10:22  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [350/985]  eta: 0:10:12  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [360/985]  eta: 0:10:02  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [370/985]  eta: 0:09:53  lr: 0.000012  loss: 0.0172 (0.0187)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [380/985]  eta: 0:09:43  lr: 0.000012  loss: 0.0176 (0.0187)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [390/985]  eta: 0:09:33  lr: 0.000012  loss: 0.0184 (0.0187)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [400/985]  eta: 0:09:23  lr: 0.000012  loss: 0.0192 (0.0188)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [410/985]  eta: 0:09:14  lr: 0.000012  loss: 0.0180 (0.0188)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [420/985]  eta: 0:09:04  lr: 0.000012  loss: 0.0189 (0.0188)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [430/985]  eta: 0:08:54  lr: 0.000012  loss: 0.0193 (0.0188)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [440/985]  eta: 0:08:44  lr: 0.000012  loss: 0.0181 (0.0189)  time: 0.9608  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [450/985]  eta: 0:08:35  lr: 0.000012  loss: 0.0189 (0.0189)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [460/985]  eta: 0:08:25  lr: 0.000012  loss: 0.0200 (0.0189)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [470/985]  eta: 0:08:15  lr: 0.000012  loss: 0.0183 (0.0189)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [480/985]  eta: 0:08:06  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9575  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [490/985]  eta: 0:07:56  lr: 0.000012  loss: 0.0186 (0.0190)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [500/985]  eta: 0:07:46  lr: 0.000012  loss: 0.0206 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [510/985]  eta: 0:07:37  lr: 0.000012  loss: 0.0212 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [520/985]  eta: 0:07:27  lr: 0.000012  loss: 0.0182 (0.0190)  time: 0.9521  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [530/985]  eta: 0:07:17  lr: 0.000012  loss: 0.0188 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [540/985]  eta: 0:07:08  lr: 0.000012  loss: 0.0184 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [550/985]  eta: 0:06:58  lr: 0.000012  loss: 0.0172 (0.0190)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [560/985]  eta: 0:06:48  lr: 0.000012  loss: 0.0178 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [570/985]  eta: 0:06:39  lr: 0.000012  loss: 0.0188 (0.0190)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [580/985]  eta: 0:06:29  lr: 0.000012  loss: 0.0188 (0.0190)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [590/985]  eta: 0:06:19  lr: 0.000012  loss: 0.0194 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [600/985]  eta: 0:06:10  lr: 0.000012  loss: 0.0194 (0.0191)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [610/985]  eta: 0:06:00  lr: 0.000012  loss: 0.0191 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [620/985]  eta: 0:05:50  lr: 0.000012  loss: 0.0193 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [630/985]  eta: 0:05:41  lr: 0.000012  loss: 0.0197 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [640/985]  eta: 0:05:31  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [650/985]  eta: 0:05:21  lr: 0.000012  loss: 0.0179 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [660/985]  eta: 0:05:12  lr: 0.000012  loss: 0.0189 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [670/985]  eta: 0:05:02  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [680/985]  eta: 0:04:52  lr: 0.000012  loss: 0.0169 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [690/985]  eta: 0:04:43  lr: 0.000012  loss: 0.0178 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [700/985]  eta: 0:04:33  lr: 0.000012  loss: 0.0198 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [710/985]  eta: 0:04:24  lr: 0.000012  loss: 0.0191 (0.0192)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [720/985]  eta: 0:04:14  lr: 0.000012  loss: 0.0205 (0.0192)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [730/985]  eta: 0:04:04  lr: 0.000012  loss: 0.0167 (0.0192)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [740/985]  eta: 0:03:55  lr: 0.000012  loss: 0.0167 (0.0192)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [750/985]  eta: 0:03:45  lr: 0.000012  loss: 0.0184 (0.0192)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [760/985]  eta: 0:03:35  lr: 0.000012  loss: 0.0184 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [770/985]  eta: 0:03:26  lr: 0.000012  loss: 0.0179 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:845]  [780/985]  eta: 0:03:16  lr: 0.000012  loss: 0.0179 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [790/985]  eta: 0:03:07  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [800/985]  eta: 0:02:57  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [810/985]  eta: 0:02:47  lr: 0.000012  loss: 0.0176 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [820/985]  eta: 0:02:38  lr: 0.000012  loss: 0.0173 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [830/985]  eta: 0:02:28  lr: 0.000012  loss: 0.0172 (0.0191)  time: 0.9657  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [840/985]  eta: 0:02:19  lr: 0.000012  loss: 0.0176 (0.0191)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [850/985]  eta: 0:02:09  lr: 0.000012  loss: 0.0176 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [860/985]  eta: 0:01:59  lr: 0.000012  loss: 0.0171 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [870/985]  eta: 0:01:50  lr: 0.000012  loss: 0.0171 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [880/985]  eta: 0:01:40  lr: 0.000012  loss: 0.0173 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [890/985]  eta: 0:01:31  lr: 0.000012  loss: 0.0174 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [900/985]  eta: 0:01:21  lr: 0.000012  loss: 0.0195 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [910/985]  eta: 0:01:11  lr: 0.000012  loss: 0.0191 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [920/985]  eta: 0:01:02  lr: 0.000012  loss: 0.0186 (0.0191)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [930/985]  eta: 0:00:52  lr: 0.000012  loss: 0.0172 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [940/985]  eta: 0:00:43  lr: 0.000012  loss: 0.0183 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [950/985]  eta: 0:00:33  lr: 0.000012  loss: 0.0187 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [960/985]  eta: 0:00:23  lr: 0.000012  loss: 0.0203 (0.0191)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [970/985]  eta: 0:00:14  lr: 0.000012  loss: 0.0190 (0.0191)  time: 0.9507  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [980/985]  eta: 0:00:04  lr: 0.000012  loss: 0.0178 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845]  [984/985]  eta: 0:00:00  lr: 0.000012  loss: 0.0183 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:845] Total time: 0:15:44 (0.9589 s / it)\n",
      "Averaged stats: lr: 0.000012  loss: 0.0183 (0.0191)\n",
      "Valid: [epoch:845]  [ 0/14]  eta: 0:02:54  loss: 0.0170 (0.0170)  time: 12.4559  data: 0.5890  max mem: 41892\n",
      "Valid: [epoch:845]  [13/14]  eta: 0:00:11  loss: 0.0144 (0.0144)  time: 11.8292  data: 0.0422  max mem: 41892\n",
      "Valid: [epoch:845] Total time: 0:02:45 (11.8357 s / it)\n",
      "Averaged stats: loss: 0.0144 (0.0144)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_845_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.014%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:846]  [  0/985]  eta: 1:19:44  lr: 0.000012  loss: 0.0172 (0.0172)  time: 4.8578  data: 3.8771  max mem: 41892\n",
      "Train: [epoch:846]  [ 10/985]  eta: 0:21:03  lr: 0.000012  loss: 0.0177 (0.0178)  time: 1.2963  data: 0.3526  max mem: 41892\n",
      "Train: [epoch:846]  [ 20/985]  eta: 0:18:07  lr: 0.000012  loss: 0.0177 (0.0182)  time: 0.9404  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [ 30/985]  eta: 0:17:01  lr: 0.000012  loss: 0.0188 (0.0189)  time: 0.9449  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [ 40/985]  eta: 0:16:21  lr: 0.000012  loss: 0.0180 (0.0185)  time: 0.9462  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [ 50/985]  eta: 0:15:54  lr: 0.000012  loss: 0.0157 (0.0181)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [ 60/985]  eta: 0:15:32  lr: 0.000012  loss: 0.0179 (0.0184)  time: 0.9454  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [ 70/985]  eta: 0:15:14  lr: 0.000012  loss: 0.0192 (0.0186)  time: 0.9456  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [ 80/985]  eta: 0:15:00  lr: 0.000012  loss: 0.0180 (0.0186)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [ 90/985]  eta: 0:14:45  lr: 0.000012  loss: 0.0176 (0.0186)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [100/985]  eta: 0:14:32  lr: 0.000012  loss: 0.0176 (0.0187)  time: 0.9510  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [110/985]  eta: 0:14:20  lr: 0.000012  loss: 0.0173 (0.0185)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [120/985]  eta: 0:14:08  lr: 0.000012  loss: 0.0180 (0.0186)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [130/985]  eta: 0:13:56  lr: 0.000012  loss: 0.0179 (0.0185)  time: 0.9506  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [140/985]  eta: 0:13:45  lr: 0.000012  loss: 0.0172 (0.0186)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [150/985]  eta: 0:13:35  lr: 0.000012  loss: 0.0191 (0.0188)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [160/985]  eta: 0:13:24  lr: 0.000012  loss: 0.0196 (0.0189)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [170/985]  eta: 0:13:13  lr: 0.000012  loss: 0.0179 (0.0189)  time: 0.9546  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [180/985]  eta: 0:13:02  lr: 0.000012  loss: 0.0179 (0.0189)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [190/985]  eta: 0:12:52  lr: 0.000012  loss: 0.0184 (0.0189)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [200/985]  eta: 0:12:42  lr: 0.000012  loss: 0.0182 (0.0189)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [210/985]  eta: 0:12:32  lr: 0.000012  loss: 0.0183 (0.0188)  time: 0.9622  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [220/985]  eta: 0:12:22  lr: 0.000012  loss: 0.0183 (0.0188)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [230/985]  eta: 0:12:12  lr: 0.000012  loss: 0.0173 (0.0188)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [240/985]  eta: 0:12:02  lr: 0.000012  loss: 0.0187 (0.0189)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [250/985]  eta: 0:11:52  lr: 0.000012  loss: 0.0183 (0.0189)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [260/985]  eta: 0:11:42  lr: 0.000012  loss: 0.0163 (0.0188)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [270/985]  eta: 0:11:32  lr: 0.000012  loss: 0.0170 (0.0188)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [280/985]  eta: 0:11:22  lr: 0.000012  loss: 0.0188 (0.0188)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [290/985]  eta: 0:11:12  lr: 0.000012  loss: 0.0188 (0.0188)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [300/985]  eta: 0:11:02  lr: 0.000012  loss: 0.0191 (0.0189)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [310/985]  eta: 0:10:52  lr: 0.000012  loss: 0.0188 (0.0189)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [320/985]  eta: 0:10:42  lr: 0.000012  loss: 0.0180 (0.0189)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [330/985]  eta: 0:10:32  lr: 0.000012  loss: 0.0170 (0.0188)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [340/985]  eta: 0:10:23  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [350/985]  eta: 0:10:13  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [360/985]  eta: 0:10:03  lr: 0.000012  loss: 0.0177 (0.0188)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [370/985]  eta: 0:09:53  lr: 0.000012  loss: 0.0191 (0.0189)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [380/985]  eta: 0:09:43  lr: 0.000012  loss: 0.0195 (0.0189)  time: 0.9606  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:846]  [390/985]  eta: 0:09:34  lr: 0.000012  loss: 0.0180 (0.0190)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [400/985]  eta: 0:09:24  lr: 0.000012  loss: 0.0180 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [410/985]  eta: 0:09:14  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [420/985]  eta: 0:09:04  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [430/985]  eta: 0:08:55  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [440/985]  eta: 0:08:45  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [450/985]  eta: 0:08:35  lr: 0.000012  loss: 0.0191 (0.0190)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [460/985]  eta: 0:08:25  lr: 0.000012  loss: 0.0191 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [470/985]  eta: 0:08:16  lr: 0.000012  loss: 0.0192 (0.0190)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [480/985]  eta: 0:08:06  lr: 0.000012  loss: 0.0194 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [490/985]  eta: 0:07:56  lr: 0.000012  loss: 0.0212 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [500/985]  eta: 0:07:47  lr: 0.000012  loss: 0.0196 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [510/985]  eta: 0:07:37  lr: 0.000012  loss: 0.0186 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [520/985]  eta: 0:07:27  lr: 0.000012  loss: 0.0183 (0.0192)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [530/985]  eta: 0:07:18  lr: 0.000012  loss: 0.0186 (0.0192)  time: 0.9615  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [540/985]  eta: 0:07:08  lr: 0.000012  loss: 0.0191 (0.0192)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [550/985]  eta: 0:06:58  lr: 0.000012  loss: 0.0190 (0.0192)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [560/985]  eta: 0:06:49  lr: 0.000012  loss: 0.0172 (0.0192)  time: 0.9544  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [570/985]  eta: 0:06:39  lr: 0.000012  loss: 0.0169 (0.0192)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [580/985]  eta: 0:06:29  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [590/985]  eta: 0:06:20  lr: 0.000012  loss: 0.0177 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [600/985]  eta: 0:06:10  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [610/985]  eta: 0:06:00  lr: 0.000012  loss: 0.0172 (0.0191)  time: 0.9630  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [620/985]  eta: 0:05:51  lr: 0.000012  loss: 0.0185 (0.0192)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [630/985]  eta: 0:05:41  lr: 0.000012  loss: 0.0191 (0.0192)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [640/985]  eta: 0:05:31  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [650/985]  eta: 0:05:22  lr: 0.000012  loss: 0.0184 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [660/985]  eta: 0:05:12  lr: 0.000012  loss: 0.0176 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [670/985]  eta: 0:05:02  lr: 0.000012  loss: 0.0168 (0.0191)  time: 0.9566  data: 0.0002  max mem: 41892\n",
      "Train: [epoch:846]  [680/985]  eta: 0:04:53  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [690/985]  eta: 0:04:43  lr: 0.000012  loss: 0.0174 (0.0191)  time: 0.9584  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [700/985]  eta: 0:04:34  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [710/985]  eta: 0:04:24  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [720/985]  eta: 0:04:14  lr: 0.000012  loss: 0.0178 (0.0191)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [730/985]  eta: 0:04:05  lr: 0.000012  loss: 0.0178 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [740/985]  eta: 0:03:55  lr: 0.000012  loss: 0.0172 (0.0191)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [750/985]  eta: 0:03:45  lr: 0.000012  loss: 0.0182 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [760/985]  eta: 0:03:36  lr: 0.000012  loss: 0.0180 (0.0191)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [770/985]  eta: 0:03:26  lr: 0.000012  loss: 0.0172 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [780/985]  eta: 0:03:17  lr: 0.000012  loss: 0.0167 (0.0191)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [790/985]  eta: 0:03:07  lr: 0.000012  loss: 0.0168 (0.0191)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [800/985]  eta: 0:02:57  lr: 0.000012  loss: 0.0178 (0.0190)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [810/985]  eta: 0:02:48  lr: 0.000012  loss: 0.0181 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [820/985]  eta: 0:02:38  lr: 0.000012  loss: 0.0183 (0.0191)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [830/985]  eta: 0:02:28  lr: 0.000012  loss: 0.0206 (0.0191)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [840/985]  eta: 0:02:19  lr: 0.000012  loss: 0.0200 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [850/985]  eta: 0:02:09  lr: 0.000012  loss: 0.0165 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [860/985]  eta: 0:02:00  lr: 0.000012  loss: 0.0173 (0.0191)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [870/985]  eta: 0:01:50  lr: 0.000012  loss: 0.0177 (0.0191)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [880/985]  eta: 0:01:40  lr: 0.000012  loss: 0.0172 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [890/985]  eta: 0:01:31  lr: 0.000012  loss: 0.0172 (0.0190)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [900/985]  eta: 0:01:21  lr: 0.000012  loss: 0.0185 (0.0190)  time: 0.9614  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [910/985]  eta: 0:01:12  lr: 0.000012  loss: 0.0183 (0.0190)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [920/985]  eta: 0:01:02  lr: 0.000012  loss: 0.0175 (0.0190)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [930/985]  eta: 0:00:52  lr: 0.000012  loss: 0.0173 (0.0190)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [940/985]  eta: 0:00:43  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [950/985]  eta: 0:00:33  lr: 0.000012  loss: 0.0190 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [960/985]  eta: 0:00:24  lr: 0.000012  loss: 0.0187 (0.0191)  time: 0.9534  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [970/985]  eta: 0:00:14  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [980/985]  eta: 0:00:04  lr: 0.000012  loss: 0.0185 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846]  [984/985]  eta: 0:00:00  lr: 0.000012  loss: 0.0213 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:846] Total time: 0:15:46 (0.9607 s / it)\n",
      "Averaged stats: lr: 0.000012  loss: 0.0213 (0.0191)\n",
      "Valid: [epoch:846]  [ 0/14]  eta: 0:03:00  loss: 0.0134 (0.0134)  time: 12.9256  data: 0.5659  max mem: 41892\n",
      "Valid: [epoch:846]  [13/14]  eta: 0:00:12  loss: 0.0150 (0.0150)  time: 12.1278  data: 0.0405  max mem: 41892\n",
      "Valid: [epoch:846] Total time: 0:02:49 (12.1343 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0150)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_846_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:847]  [  0/985]  eta: 1:19:07  lr: 0.000011  loss: 0.0198 (0.0198)  time: 4.8194  data: 3.8130  max mem: 41892\n",
      "Train: [epoch:847]  [ 10/985]  eta: 0:21:10  lr: 0.000011  loss: 0.0198 (0.0202)  time: 1.3032  data: 0.3468  max mem: 41892\n",
      "Train: [epoch:847]  [ 20/985]  eta: 0:18:09  lr: 0.000011  loss: 0.0183 (0.0203)  time: 0.9446  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [ 30/985]  eta: 0:16:58  lr: 0.000011  loss: 0.0181 (0.0197)  time: 0.9370  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [ 40/985]  eta: 0:16:21  lr: 0.000011  loss: 0.0169 (0.0193)  time: 0.9441  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [ 50/985]  eta: 0:15:53  lr: 0.000011  loss: 0.0165 (0.0189)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [ 60/985]  eta: 0:15:33  lr: 0.000011  loss: 0.0176 (0.0190)  time: 0.9471  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [ 70/985]  eta: 0:15:14  lr: 0.000011  loss: 0.0180 (0.0189)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [ 80/985]  eta: 0:14:59  lr: 0.000011  loss: 0.0180 (0.0187)  time: 0.9486  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [ 90/985]  eta: 0:14:46  lr: 0.000011  loss: 0.0178 (0.0187)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [100/985]  eta: 0:14:32  lr: 0.000011  loss: 0.0178 (0.0187)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [110/985]  eta: 0:14:20  lr: 0.000011  loss: 0.0185 (0.0187)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [120/985]  eta: 0:14:08  lr: 0.000011  loss: 0.0187 (0.0188)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [130/985]  eta: 0:13:56  lr: 0.000011  loss: 0.0186 (0.0189)  time: 0.9518  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [140/985]  eta: 0:13:45  lr: 0.000011  loss: 0.0186 (0.0189)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [150/985]  eta: 0:13:34  lr: 0.000011  loss: 0.0190 (0.0190)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [160/985]  eta: 0:13:23  lr: 0.000011  loss: 0.0184 (0.0189)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [170/985]  eta: 0:13:13  lr: 0.000011  loss: 0.0166 (0.0188)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [180/985]  eta: 0:13:02  lr: 0.000011  loss: 0.0169 (0.0188)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [190/985]  eta: 0:12:52  lr: 0.000011  loss: 0.0182 (0.0189)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [200/985]  eta: 0:12:42  lr: 0.000011  loss: 0.0185 (0.0189)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [210/985]  eta: 0:12:31  lr: 0.000011  loss: 0.0184 (0.0189)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [220/985]  eta: 0:12:21  lr: 0.000011  loss: 0.0175 (0.0189)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [230/985]  eta: 0:12:11  lr: 0.000011  loss: 0.0175 (0.0189)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [240/985]  eta: 0:12:01  lr: 0.000011  loss: 0.0187 (0.0189)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [250/985]  eta: 0:11:51  lr: 0.000011  loss: 0.0187 (0.0189)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [260/985]  eta: 0:11:41  lr: 0.000011  loss: 0.0171 (0.0189)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [270/985]  eta: 0:11:31  lr: 0.000011  loss: 0.0176 (0.0189)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [280/985]  eta: 0:11:21  lr: 0.000011  loss: 0.0189 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [290/985]  eta: 0:11:11  lr: 0.000011  loss: 0.0186 (0.0189)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [300/985]  eta: 0:11:01  lr: 0.000011  loss: 0.0186 (0.0190)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [310/985]  eta: 0:10:51  lr: 0.000011  loss: 0.0177 (0.0191)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [320/985]  eta: 0:10:41  lr: 0.000011  loss: 0.0176 (0.0190)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [330/985]  eta: 0:10:32  lr: 0.000011  loss: 0.0178 (0.0190)  time: 0.9565  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [340/985]  eta: 0:10:22  lr: 0.000011  loss: 0.0184 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [350/985]  eta: 0:10:12  lr: 0.000011  loss: 0.0177 (0.0190)  time: 0.9587  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [360/985]  eta: 0:10:02  lr: 0.000011  loss: 0.0178 (0.0190)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [370/985]  eta: 0:09:53  lr: 0.000011  loss: 0.0172 (0.0190)  time: 0.9582  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [380/985]  eta: 0:09:43  lr: 0.000011  loss: 0.0170 (0.0189)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [390/985]  eta: 0:09:33  lr: 0.000011  loss: 0.0180 (0.0190)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [400/985]  eta: 0:09:23  lr: 0.000011  loss: 0.0180 (0.0190)  time: 0.9562  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [410/985]  eta: 0:09:14  lr: 0.000011  loss: 0.0179 (0.0189)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [420/985]  eta: 0:09:04  lr: 0.000011  loss: 0.0183 (0.0190)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [430/985]  eta: 0:08:54  lr: 0.000011  loss: 0.0193 (0.0190)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [440/985]  eta: 0:08:44  lr: 0.000011  loss: 0.0184 (0.0190)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [450/985]  eta: 0:08:35  lr: 0.000011  loss: 0.0178 (0.0190)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [460/985]  eta: 0:08:25  lr: 0.000011  loss: 0.0192 (0.0191)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [470/985]  eta: 0:08:15  lr: 0.000011  loss: 0.0187 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [480/985]  eta: 0:08:06  lr: 0.000011  loss: 0.0177 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [490/985]  eta: 0:07:56  lr: 0.000011  loss: 0.0191 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [500/985]  eta: 0:07:46  lr: 0.000011  loss: 0.0190 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [510/985]  eta: 0:07:36  lr: 0.000011  loss: 0.0176 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [520/985]  eta: 0:07:27  lr: 0.000011  loss: 0.0180 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [530/985]  eta: 0:07:17  lr: 0.000011  loss: 0.0175 (0.0191)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [540/985]  eta: 0:07:08  lr: 0.000011  loss: 0.0171 (0.0191)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [550/985]  eta: 0:06:58  lr: 0.000011  loss: 0.0179 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [560/985]  eta: 0:06:48  lr: 0.000011  loss: 0.0190 (0.0191)  time: 0.9524  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [570/985]  eta: 0:06:39  lr: 0.000011  loss: 0.0188 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [580/985]  eta: 0:06:29  lr: 0.000011  loss: 0.0184 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [590/985]  eta: 0:06:19  lr: 0.000011  loss: 0.0200 (0.0191)  time: 0.9610  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [600/985]  eta: 0:06:10  lr: 0.000011  loss: 0.0200 (0.0191)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [610/985]  eta: 0:06:00  lr: 0.000011  loss: 0.0177 (0.0190)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [620/985]  eta: 0:05:50  lr: 0.000011  loss: 0.0179 (0.0191)  time: 0.9529  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [630/985]  eta: 0:05:41  lr: 0.000011  loss: 0.0184 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [640/985]  eta: 0:05:31  lr: 0.000011  loss: 0.0191 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [650/985]  eta: 0:05:21  lr: 0.000011  loss: 0.0193 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:847]  [660/985]  eta: 0:05:12  lr: 0.000011  loss: 0.0203 (0.0191)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [670/985]  eta: 0:05:02  lr: 0.000011  loss: 0.0174 (0.0191)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [680/985]  eta: 0:04:52  lr: 0.000011  loss: 0.0177 (0.0191)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [690/985]  eta: 0:04:43  lr: 0.000011  loss: 0.0171 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [700/985]  eta: 0:04:33  lr: 0.000011  loss: 0.0175 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [710/985]  eta: 0:04:24  lr: 0.000011  loss: 0.0186 (0.0191)  time: 0.9528  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [720/985]  eta: 0:04:14  lr: 0.000011  loss: 0.0184 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [730/985]  eta: 0:04:04  lr: 0.000011  loss: 0.0178 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [740/985]  eta: 0:03:55  lr: 0.000011  loss: 0.0176 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [750/985]  eta: 0:03:45  lr: 0.000011  loss: 0.0188 (0.0191)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [760/985]  eta: 0:03:35  lr: 0.000011  loss: 0.0188 (0.0191)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [770/985]  eta: 0:03:26  lr: 0.000011  loss: 0.0184 (0.0191)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [780/985]  eta: 0:03:16  lr: 0.000011  loss: 0.0171 (0.0191)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [790/985]  eta: 0:03:07  lr: 0.000011  loss: 0.0176 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [800/985]  eta: 0:02:57  lr: 0.000011  loss: 0.0191 (0.0191)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [810/985]  eta: 0:02:47  lr: 0.000011  loss: 0.0188 (0.0191)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [820/985]  eta: 0:02:38  lr: 0.000011  loss: 0.0198 (0.0191)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [830/985]  eta: 0:02:28  lr: 0.000011  loss: 0.0186 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [840/985]  eta: 0:02:19  lr: 0.000011  loss: 0.0173 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [850/985]  eta: 0:02:09  lr: 0.000011  loss: 0.0180 (0.0191)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [860/985]  eta: 0:01:59  lr: 0.000011  loss: 0.0191 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [870/985]  eta: 0:01:50  lr: 0.000011  loss: 0.0191 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [880/985]  eta: 0:01:40  lr: 0.000011  loss: 0.0175 (0.0191)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [890/985]  eta: 0:01:31  lr: 0.000011  loss: 0.0174 (0.0191)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [900/985]  eta: 0:01:21  lr: 0.000011  loss: 0.0192 (0.0191)  time: 0.9655  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [910/985]  eta: 0:01:11  lr: 0.000011  loss: 0.0187 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [920/985]  eta: 0:01:02  lr: 0.000011  loss: 0.0177 (0.0191)  time: 0.9548  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [930/985]  eta: 0:00:52  lr: 0.000011  loss: 0.0175 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [940/985]  eta: 0:00:43  lr: 0.000011  loss: 0.0185 (0.0191)  time: 0.9541  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [950/985]  eta: 0:00:33  lr: 0.000011  loss: 0.0187 (0.0191)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [960/985]  eta: 0:00:23  lr: 0.000011  loss: 0.0187 (0.0191)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [970/985]  eta: 0:00:14  lr: 0.000011  loss: 0.0180 (0.0191)  time: 0.9513  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [980/985]  eta: 0:00:04  lr: 0.000011  loss: 0.0180 (0.0191)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847]  [984/985]  eta: 0:00:00  lr: 0.000011  loss: 0.0178 (0.0191)  time: 0.9511  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:847] Total time: 0:15:44 (0.9592 s / it)\n",
      "Averaged stats: lr: 0.000011  loss: 0.0178 (0.0191)\n",
      "Valid: [epoch:847]  [ 0/14]  eta: 0:02:54  loss: 0.0149 (0.0149)  time: 12.4560  data: 0.5814  max mem: 41892\n",
      "Valid: [epoch:847]  [13/14]  eta: 0:00:11  loss: 0.0146 (0.0146)  time: 11.7038  data: 0.0416  max mem: 41892\n",
      "Valid: [epoch:847] Total time: 0:02:43 (11.7109 s / it)\n",
      "Averaged stats: loss: 0.0146 (0.0146)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_847_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:848]  [  0/985]  eta: 1:13:11  lr: 0.000011  loss: 0.0195 (0.0195)  time: 4.4581  data: 3.4633  max mem: 41892\n",
      "Train: [epoch:848]  [ 10/985]  eta: 0:20:44  lr: 0.000011  loss: 0.0192 (0.0196)  time: 1.2763  data: 0.3150  max mem: 41892\n",
      "Train: [epoch:848]  [ 20/985]  eta: 0:17:57  lr: 0.000011  loss: 0.0192 (0.0196)  time: 0.9494  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [ 30/985]  eta: 0:16:53  lr: 0.000011  loss: 0.0190 (0.0194)  time: 0.9428  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [ 40/985]  eta: 0:16:19  lr: 0.000011  loss: 0.0172 (0.0190)  time: 0.9519  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [ 50/985]  eta: 0:15:52  lr: 0.000011  loss: 0.0189 (0.0192)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [ 60/985]  eta: 0:15:32  lr: 0.000011  loss: 0.0180 (0.0188)  time: 0.9497  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [ 70/985]  eta: 0:15:16  lr: 0.000011  loss: 0.0167 (0.0185)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [ 80/985]  eta: 0:15:00  lr: 0.000011  loss: 0.0162 (0.0183)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [ 90/985]  eta: 0:14:46  lr: 0.000011  loss: 0.0175 (0.0184)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [100/985]  eta: 0:14:33  lr: 0.000011  loss: 0.0192 (0.0185)  time: 0.9543  data: 0.0003  max mem: 41892\n",
      "Train: [epoch:848]  [110/985]  eta: 0:14:21  lr: 0.000011  loss: 0.0182 (0.0184)  time: 0.9571  data: 0.0003  max mem: 41892\n",
      "Train: [epoch:848]  [120/985]  eta: 0:14:09  lr: 0.000011  loss: 0.0187 (0.0187)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [130/985]  eta: 0:13:58  lr: 0.000011  loss: 0.0186 (0.0188)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [140/985]  eta: 0:13:47  lr: 0.000011  loss: 0.0177 (0.0187)  time: 0.9605  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [150/985]  eta: 0:13:36  lr: 0.000011  loss: 0.0177 (0.0187)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [160/985]  eta: 0:13:25  lr: 0.000011  loss: 0.0183 (0.0187)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [170/985]  eta: 0:13:14  lr: 0.000011  loss: 0.0186 (0.0188)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [180/985]  eta: 0:13:04  lr: 0.000011  loss: 0.0181 (0.0188)  time: 0.9581  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [190/985]  eta: 0:12:53  lr: 0.000011  loss: 0.0181 (0.0188)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [200/985]  eta: 0:12:43  lr: 0.000011  loss: 0.0179 (0.0188)  time: 0.9526  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [210/985]  eta: 0:12:32  lr: 0.000011  loss: 0.0166 (0.0187)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [220/985]  eta: 0:12:22  lr: 0.000011  loss: 0.0158 (0.0186)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [230/985]  eta: 0:12:12  lr: 0.000011  loss: 0.0173 (0.0186)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [240/985]  eta: 0:12:02  lr: 0.000011  loss: 0.0185 (0.0186)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [250/985]  eta: 0:11:52  lr: 0.000011  loss: 0.0182 (0.0186)  time: 0.9574  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [260/985]  eta: 0:11:42  lr: 0.000011  loss: 0.0182 (0.0186)  time: 0.9630  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:848]  [270/985]  eta: 0:11:32  lr: 0.000011  loss: 0.0188 (0.0186)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [280/985]  eta: 0:11:22  lr: 0.000011  loss: 0.0187 (0.0187)  time: 0.9594  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [290/985]  eta: 0:11:12  lr: 0.000011  loss: 0.0179 (0.0187)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [300/985]  eta: 0:11:02  lr: 0.000011  loss: 0.0182 (0.0187)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [310/985]  eta: 0:10:52  lr: 0.000011  loss: 0.0188 (0.0188)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [320/985]  eta: 0:10:43  lr: 0.000011  loss: 0.0180 (0.0188)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [330/985]  eta: 0:10:33  lr: 0.000011  loss: 0.0180 (0.0188)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [340/985]  eta: 0:10:23  lr: 0.000011  loss: 0.0181 (0.0188)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [350/985]  eta: 0:10:13  lr: 0.000011  loss: 0.0173 (0.0188)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [360/985]  eta: 0:10:03  lr: 0.000011  loss: 0.0170 (0.0187)  time: 0.9545  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [370/985]  eta: 0:09:53  lr: 0.000011  loss: 0.0169 (0.0187)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [380/985]  eta: 0:09:43  lr: 0.000011  loss: 0.0185 (0.0188)  time: 0.9536  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [390/985]  eta: 0:09:33  lr: 0.000011  loss: 0.0185 (0.0188)  time: 0.9530  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [400/985]  eta: 0:09:24  lr: 0.000011  loss: 0.0183 (0.0188)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [410/985]  eta: 0:09:14  lr: 0.000011  loss: 0.0198 (0.0188)  time: 0.9539  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [420/985]  eta: 0:09:04  lr: 0.000011  loss: 0.0193 (0.0189)  time: 0.9537  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [430/985]  eta: 0:08:54  lr: 0.000011  loss: 0.0191 (0.0189)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [440/985]  eta: 0:08:45  lr: 0.000011  loss: 0.0175 (0.0189)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [450/985]  eta: 0:08:35  lr: 0.000011  loss: 0.0177 (0.0189)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [460/985]  eta: 0:08:25  lr: 0.000011  loss: 0.0178 (0.0189)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [470/985]  eta: 0:08:15  lr: 0.000011  loss: 0.0179 (0.0189)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [480/985]  eta: 0:08:06  lr: 0.000011  loss: 0.0185 (0.0189)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [490/985]  eta: 0:07:56  lr: 0.000011  loss: 0.0200 (0.0189)  time: 0.9568  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [500/985]  eta: 0:07:46  lr: 0.000011  loss: 0.0193 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [510/985]  eta: 0:07:37  lr: 0.000011  loss: 0.0190 (0.0190)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [520/985]  eta: 0:07:27  lr: 0.000011  loss: 0.0187 (0.0190)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [530/985]  eta: 0:07:17  lr: 0.000011  loss: 0.0186 (0.0190)  time: 0.9570  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [540/985]  eta: 0:07:08  lr: 0.000011  loss: 0.0188 (0.0190)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [550/985]  eta: 0:06:58  lr: 0.000011  loss: 0.0179 (0.0190)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [560/985]  eta: 0:06:48  lr: 0.000011  loss: 0.0188 (0.0190)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [570/985]  eta: 0:06:39  lr: 0.000011  loss: 0.0192 (0.0190)  time: 0.9590  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [580/985]  eta: 0:06:29  lr: 0.000011  loss: 0.0179 (0.0190)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [590/985]  eta: 0:06:19  lr: 0.000011  loss: 0.0179 (0.0191)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [600/985]  eta: 0:06:10  lr: 0.000011  loss: 0.0184 (0.0190)  time: 0.9595  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [610/985]  eta: 0:06:00  lr: 0.000011  loss: 0.0190 (0.0191)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [620/985]  eta: 0:05:51  lr: 0.000011  loss: 0.0191 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [630/985]  eta: 0:05:41  lr: 0.000011  loss: 0.0181 (0.0191)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [640/985]  eta: 0:05:31  lr: 0.000011  loss: 0.0174 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [650/985]  eta: 0:05:22  lr: 0.000011  loss: 0.0176 (0.0190)  time: 0.9630  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [660/985]  eta: 0:05:12  lr: 0.000011  loss: 0.0176 (0.0190)  time: 0.9681  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [670/985]  eta: 0:05:03  lr: 0.000011  loss: 0.0179 (0.0190)  time: 0.9692  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [680/985]  eta: 0:04:53  lr: 0.000011  loss: 0.0178 (0.0190)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [690/985]  eta: 0:04:43  lr: 0.000011  loss: 0.0178 (0.0190)  time: 0.9576  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [700/985]  eta: 0:04:34  lr: 0.000011  loss: 0.0177 (0.0190)  time: 0.9557  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [710/985]  eta: 0:04:24  lr: 0.000011  loss: 0.0180 (0.0190)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [720/985]  eta: 0:04:14  lr: 0.000011  loss: 0.0184 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [730/985]  eta: 0:04:05  lr: 0.000011  loss: 0.0178 (0.0190)  time: 0.9577  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [740/985]  eta: 0:03:55  lr: 0.000011  loss: 0.0167 (0.0190)  time: 0.9579  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [750/985]  eta: 0:03:45  lr: 0.000011  loss: 0.0178 (0.0190)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [760/985]  eta: 0:03:36  lr: 0.000011  loss: 0.0181 (0.0190)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [770/985]  eta: 0:03:26  lr: 0.000011  loss: 0.0171 (0.0190)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [780/985]  eta: 0:03:17  lr: 0.000011  loss: 0.0177 (0.0190)  time: 0.9616  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [790/985]  eta: 0:03:07  lr: 0.000011  loss: 0.0175 (0.0190)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [800/985]  eta: 0:02:57  lr: 0.000011  loss: 0.0181 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [810/985]  eta: 0:02:48  lr: 0.000011  loss: 0.0194 (0.0190)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [820/985]  eta: 0:02:38  lr: 0.000011  loss: 0.0180 (0.0190)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [830/985]  eta: 0:02:29  lr: 0.000011  loss: 0.0180 (0.0190)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [840/985]  eta: 0:02:19  lr: 0.000011  loss: 0.0191 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [850/985]  eta: 0:02:09  lr: 0.000011  loss: 0.0187 (0.0190)  time: 0.9551  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [860/985]  eta: 0:02:00  lr: 0.000011  loss: 0.0169 (0.0190)  time: 0.9543  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [870/985]  eta: 0:01:50  lr: 0.000011  loss: 0.0175 (0.0190)  time: 0.9552  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [880/985]  eta: 0:01:40  lr: 0.000011  loss: 0.0176 (0.0190)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [890/985]  eta: 0:01:31  lr: 0.000011  loss: 0.0179 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [900/985]  eta: 0:01:21  lr: 0.000011  loss: 0.0184 (0.0190)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [910/985]  eta: 0:01:12  lr: 0.000011  loss: 0.0202 (0.0190)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [920/985]  eta: 0:01:02  lr: 0.000011  loss: 0.0199 (0.0190)  time: 0.9614  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:848]  [930/985]  eta: 0:00:52  lr: 0.000011  loss: 0.0199 (0.0190)  time: 0.9621  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [940/985]  eta: 0:00:43  lr: 0.000011  loss: 0.0183 (0.0190)  time: 0.9578  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [950/985]  eta: 0:00:33  lr: 0.000011  loss: 0.0185 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [960/985]  eta: 0:00:24  lr: 0.000011  loss: 0.0189 (0.0190)  time: 0.9532  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [970/985]  eta: 0:00:14  lr: 0.000011  loss: 0.0193 (0.0190)  time: 0.9520  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [980/985]  eta: 0:00:04  lr: 0.000011  loss: 0.0187 (0.0191)  time: 0.9516  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848]  [984/985]  eta: 0:00:00  lr: 0.000011  loss: 0.0187 (0.0191)  time: 0.9515  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:848] Total time: 0:15:46 (0.9608 s / it)\n",
      "Averaged stats: lr: 0.000011  loss: 0.0187 (0.0191)\n",
      "Valid: [epoch:848]  [ 0/14]  eta: 0:03:28  loss: 0.0150 (0.0150)  time: 14.9229  data: 0.5877  max mem: 41892\n",
      "Valid: [epoch:848]  [13/14]  eta: 0:00:13  loss: 0.0150 (0.0149)  time: 13.3188  data: 0.0421  max mem: 41892\n",
      "Valid: [epoch:848] Total time: 0:03:06 (13.3252 s / it)\n",
      "Averaged stats: loss: 0.0150 (0.0149)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/epoch_848_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.015%\n",
      "Min loss: 0.014\n",
      "Best Epoch: 817.000\n",
      "Train: [epoch:849]  [  0/985]  eta: 1:19:00  lr: 0.000011  loss: 0.0179 (0.0179)  time: 4.8129  data: 3.8339  max mem: 41892\n",
      "Train: [epoch:849]  [ 10/985]  eta: 0:21:13  lr: 0.000011  loss: 0.0179 (0.0181)  time: 1.3065  data: 0.3487  max mem: 41892\n",
      "Train: [epoch:849]  [ 20/985]  eta: 0:18:12  lr: 0.000011  loss: 0.0183 (0.0185)  time: 0.9476  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [ 30/985]  eta: 0:17:00  lr: 0.000011  loss: 0.0185 (0.0185)  time: 0.9385  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [ 40/985]  eta: 0:16:23  lr: 0.000011  loss: 0.0167 (0.0180)  time: 0.9458  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [ 50/985]  eta: 0:15:55  lr: 0.000011  loss: 0.0166 (0.0181)  time: 0.9495  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [ 60/985]  eta: 0:15:34  lr: 0.000011  loss: 0.0179 (0.0183)  time: 0.9472  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [ 70/985]  eta: 0:15:17  lr: 0.000011  loss: 0.0179 (0.0182)  time: 0.9512  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [ 80/985]  eta: 0:15:01  lr: 0.000011  loss: 0.0155 (0.0180)  time: 0.9509  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [ 90/985]  eta: 0:14:46  lr: 0.000011  loss: 0.0172 (0.0182)  time: 0.9496  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [100/985]  eta: 0:14:33  lr: 0.000011  loss: 0.0175 (0.0181)  time: 0.9498  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [110/985]  eta: 0:14:20  lr: 0.000011  loss: 0.0167 (0.0182)  time: 0.9525  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [120/985]  eta: 0:14:08  lr: 0.000011  loss: 0.0189 (0.0185)  time: 0.9523  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [130/985]  eta: 0:13:58  lr: 0.000011  loss: 0.0202 (0.0187)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [140/985]  eta: 0:13:46  lr: 0.000011  loss: 0.0194 (0.0187)  time: 0.9638  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [150/985]  eta: 0:13:35  lr: 0.000011  loss: 0.0189 (0.0187)  time: 0.9527  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [160/985]  eta: 0:13:24  lr: 0.000011  loss: 0.0175 (0.0187)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [170/985]  eta: 0:13:13  lr: 0.000011  loss: 0.0173 (0.0186)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [180/985]  eta: 0:13:03  lr: 0.000011  loss: 0.0172 (0.0186)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [190/985]  eta: 0:12:53  lr: 0.000011  loss: 0.0188 (0.0187)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [200/985]  eta: 0:12:42  lr: 0.000011  loss: 0.0192 (0.0187)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [210/985]  eta: 0:12:32  lr: 0.000011  loss: 0.0181 (0.0187)  time: 0.9535  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [220/985]  eta: 0:12:21  lr: 0.000011  loss: 0.0163 (0.0187)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [230/985]  eta: 0:12:11  lr: 0.000011  loss: 0.0174 (0.0187)  time: 0.9558  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [240/985]  eta: 0:12:01  lr: 0.000011  loss: 0.0174 (0.0186)  time: 0.9540  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [250/985]  eta: 0:11:51  lr: 0.000011  loss: 0.0173 (0.0186)  time: 0.9522  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [260/985]  eta: 0:11:41  lr: 0.000011  loss: 0.0181 (0.0186)  time: 0.9572  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [270/985]  eta: 0:11:31  lr: 0.000011  loss: 0.0185 (0.0187)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [280/985]  eta: 0:11:21  lr: 0.000011  loss: 0.0188 (0.0187)  time: 0.9599  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [290/985]  eta: 0:11:11  lr: 0.000011  loss: 0.0182 (0.0187)  time: 0.9604  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [300/985]  eta: 0:11:02  lr: 0.000011  loss: 0.0182 (0.0188)  time: 0.9580  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [310/985]  eta: 0:10:52  lr: 0.000011  loss: 0.0182 (0.0188)  time: 0.9567  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [320/985]  eta: 0:10:42  lr: 0.000011  loss: 0.0171 (0.0187)  time: 0.9531  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [330/985]  eta: 0:10:32  lr: 0.000011  loss: 0.0174 (0.0188)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [340/985]  eta: 0:10:22  lr: 0.000011  loss: 0.0174 (0.0188)  time: 0.9609  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [350/985]  eta: 0:10:13  lr: 0.000011  loss: 0.0169 (0.0188)  time: 0.9601  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [360/985]  eta: 0:10:03  lr: 0.000011  loss: 0.0179 (0.0188)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [370/985]  eta: 0:09:53  lr: 0.000011  loss: 0.0173 (0.0187)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [380/985]  eta: 0:09:43  lr: 0.000011  loss: 0.0170 (0.0188)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [390/985]  eta: 0:09:33  lr: 0.000011  loss: 0.0205 (0.0188)  time: 0.9555  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [400/985]  eta: 0:09:24  lr: 0.000011  loss: 0.0198 (0.0189)  time: 0.9611  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [410/985]  eta: 0:09:14  lr: 0.000011  loss: 0.0198 (0.0189)  time: 0.9602  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [420/985]  eta: 0:09:04  lr: 0.000011  loss: 0.0182 (0.0189)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [430/985]  eta: 0:08:55  lr: 0.000011  loss: 0.0191 (0.0190)  time: 0.9596  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [440/985]  eta: 0:08:45  lr: 0.000011  loss: 0.0195 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [450/985]  eta: 0:08:35  lr: 0.000011  loss: 0.0184 (0.0190)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [460/985]  eta: 0:08:25  lr: 0.000011  loss: 0.0189 (0.0191)  time: 0.9592  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [470/985]  eta: 0:08:16  lr: 0.000011  loss: 0.0198 (0.0191)  time: 0.9585  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [480/985]  eta: 0:08:06  lr: 0.000011  loss: 0.0180 (0.0191)  time: 0.9553  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [490/985]  eta: 0:07:56  lr: 0.000011  loss: 0.0177 (0.0191)  time: 0.9559  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [500/985]  eta: 0:07:47  lr: 0.000011  loss: 0.0177 (0.0191)  time: 0.9547  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [510/985]  eta: 0:07:37  lr: 0.000011  loss: 0.0187 (0.0191)  time: 0.9563  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [520/985]  eta: 0:07:27  lr: 0.000011  loss: 0.0184 (0.0191)  time: 0.9571  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [530/985]  eta: 0:07:18  lr: 0.000011  loss: 0.0183 (0.0191)  time: 0.9565  data: 0.0001  max mem: 41892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:849]  [540/985]  eta: 0:07:08  lr: 0.000011  loss: 0.0174 (0.0191)  time: 0.9624  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [550/985]  eta: 0:06:58  lr: 0.000011  loss: 0.0172 (0.0191)  time: 0.9617  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [560/985]  eta: 0:06:49  lr: 0.000011  loss: 0.0180 (0.0191)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [570/985]  eta: 0:06:39  lr: 0.000011  loss: 0.0185 (0.0191)  time: 0.9586  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [580/985]  eta: 0:06:29  lr: 0.000011  loss: 0.0199 (0.0192)  time: 0.9589  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [590/985]  eta: 0:06:20  lr: 0.000011  loss: 0.0190 (0.0192)  time: 0.9607  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [600/985]  eta: 0:06:10  lr: 0.000011  loss: 0.0185 (0.0192)  time: 0.9591  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [610/985]  eta: 0:06:00  lr: 0.000011  loss: 0.0176 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [620/985]  eta: 0:05:51  lr: 0.000011  loss: 0.0193 (0.0192)  time: 0.9564  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [630/985]  eta: 0:05:41  lr: 0.000011  loss: 0.0193 (0.0192)  time: 0.9566  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [640/985]  eta: 0:05:31  lr: 0.000011  loss: 0.0179 (0.0192)  time: 0.9634  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [650/985]  eta: 0:05:22  lr: 0.000011  loss: 0.0179 (0.0192)  time: 0.9627  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [660/985]  eta: 0:05:12  lr: 0.000011  loss: 0.0183 (0.0192)  time: 0.9606  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [670/985]  eta: 0:05:03  lr: 0.000011  loss: 0.0188 (0.0192)  time: 0.9620  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [680/985]  eta: 0:04:53  lr: 0.000011  loss: 0.0180 (0.0192)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [690/985]  eta: 0:04:43  lr: 0.000011  loss: 0.0176 (0.0192)  time: 0.9619  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [700/985]  eta: 0:04:34  lr: 0.000011  loss: 0.0178 (0.0192)  time: 0.9556  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [710/985]  eta: 0:04:24  lr: 0.000011  loss: 0.0179 (0.0192)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [720/985]  eta: 0:04:14  lr: 0.000011  loss: 0.0177 (0.0191)  time: 0.9554  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [730/985]  eta: 0:04:05  lr: 0.000011  loss: 0.0177 (0.0191)  time: 0.9598  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [740/985]  eta: 0:03:55  lr: 0.000011  loss: 0.0175 (0.0191)  time: 0.9597  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [750/985]  eta: 0:03:46  lr: 0.000011  loss: 0.0175 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [760/985]  eta: 0:03:36  lr: 0.000011  loss: 0.0176 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [770/985]  eta: 0:03:26  lr: 0.000011  loss: 0.0181 (0.0191)  time: 0.9603  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [780/985]  eta: 0:03:17  lr: 0.000011  loss: 0.0181 (0.0191)  time: 0.9593  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [790/985]  eta: 0:03:07  lr: 0.000011  loss: 0.0169 (0.0191)  time: 0.9569  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [800/985]  eta: 0:02:57  lr: 0.000011  loss: 0.0183 (0.0191)  time: 0.9631  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [810/985]  eta: 0:02:48  lr: 0.000011  loss: 0.0200 (0.0192)  time: 0.9623  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [820/985]  eta: 0:02:38  lr: 0.000011  loss: 0.0197 (0.0192)  time: 0.9632  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [830/985]  eta: 0:02:29  lr: 0.000011  loss: 0.0179 (0.0192)  time: 0.9649  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [840/985]  eta: 0:02:19  lr: 0.000011  loss: 0.0172 (0.0192)  time: 0.9588  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [850/985]  eta: 0:02:09  lr: 0.000011  loss: 0.0172 (0.0192)  time: 0.9560  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [860/985]  eta: 0:02:00  lr: 0.000011  loss: 0.0181 (0.0192)  time: 0.9561  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [870/985]  eta: 0:01:50  lr: 0.000011  loss: 0.0171 (0.0191)  time: 0.9613  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [880/985]  eta: 0:01:40  lr: 0.000011  loss: 0.0160 (0.0191)  time: 0.9600  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [890/985]  eta: 0:01:31  lr: 0.000011  loss: 0.0173 (0.0191)  time: 0.9542  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [900/985]  eta: 0:01:21  lr: 0.000011  loss: 0.0183 (0.0191)  time: 0.9550  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [910/985]  eta: 0:01:12  lr: 0.000011  loss: 0.0193 (0.0191)  time: 0.9549  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [920/985]  eta: 0:01:02  lr: 0.000011  loss: 0.0193 (0.0191)  time: 0.9533  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [930/985]  eta: 0:00:52  lr: 0.000011  loss: 0.0171 (0.0191)  time: 0.9538  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [940/985]  eta: 0:00:43  lr: 0.000011  loss: 0.0178 (0.0191)  time: 0.9612  data: 0.0001  max mem: 41892\n",
      "Train: [epoch:849]  [950/985]  eta: 0:00:33  lr: 0.000011  loss: 0.0173 (0.0191)  time: 0.9619  data: 0.0001  max mem: 41892\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--batch-size 7 \\\n",
    "--epochs 1000 \\\n",
    "--lr_scheduler \"cosine_annealing_warm_restart\" \\\n",
    "--lr 1e-6 \\\n",
    "--data-set 'Sinogram_DCM' \\\n",
    "--model-name 'Restormer' \\\n",
    "--criterion 'Perceptual+L1 Loss' \\\n",
    "--output_dir '/workspace/sunggu/4.Dose_img2img/model/[Previous]Restormer' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Previous]Restormer/low2high/' \\\n",
    "--validate-every 2 \\\n",
    "--num_workers 16 \\\n",
    "--criterion_mode 'single label' \\\n",
    "--multiple_GT \"False\" \\\n",
    "--patch_training \"True\" \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--resume '/workspace/sunggu/4.Dose_img2img/model/[Previous]Restormer/epoch_780_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list = read_log(path = '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/log.txt')\n",
    "\n",
    "train_lr   = [ log_list[i]['train_lr'] for i in range(len(log_list)) ]\n",
    "train_loss = [ log_list[i]['train_loss'] for i in range(len(log_list)) ]\n",
    "valid_loss = [ log_list[i]['valid_loss'] for i in range(len(log_list)) ]\n",
    "epoch      = [ log_list[i]['epoch'] for i in range(len(log_list)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(train_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(valid_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(np.argsort(valid_loss)[:10]) & set(np.argsort(train_loss)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py \\\n",
    "--training-mode 'sinogram' \\\n",
    "--data-set 'TEST_Sinogram_DCM' \\\n",
    "--model-name 'ED_CNN' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Test/png/[Privious]ED_CNN/epoch_999/' \\\n",
    "--num_workers 4 \\\n",
    "--pin-mem \\\n",
    "--range-minus1-plus1 'False' \\\n",
    "--teacher_forcing \"False\" \\\n",
    "--resume '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/epoch_999_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 978 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Original === \n",
    "PSNR avg: 54.4628 \n",
    "SSIM avg: 0.9956 \n",
    "RMSE avg: 7.9607\n",
    "\n",
    "\n",
    "Predictions === \n",
    "PSNR avg: 57.6190 \n",
    "SSIM avg: 0.9980 \n",
    "RMSE avg: 5.5423\n",
    "***********************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
