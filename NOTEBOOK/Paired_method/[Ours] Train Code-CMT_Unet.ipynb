{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        cor = torch.matmul(k_x, q_x)                               # (B, D,    D)\n",
    "        cor = F.softmax(cor, dim=1)                                # (B, s(D), D)\n",
    "\n",
    "        v_x = v_x.transpose(1, 2).view(B, D, -1)              # (B, D, C*H*W)        \n",
    "        v_x = v_x.view(B, D, C*H*W)                           # (B, D, C*H*W)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 64, 64))\n",
    "b = torch.ones((2, 64, 3*64*64))\n",
    "\n",
    "\n",
    "c = torch.ones((2, 1, 64*64, 64*64))\n",
    "d = torch.ones((2, 1, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4096, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 4096])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4096, 4096])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 12288])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x   = torch.matmul(a, b)                          # (B, C, D, H*W, H*W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39326/3049917083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m                          \u001b[0;31m# (B, C, D, H*W, H*W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "x   = torch.matmul(c, d)                          # (B, C, D, H*W, H*W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4096, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels==0.7.4\n",
    "# !pip install efficientnet-pytorch==0.6.3\n",
    "# !pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CUDA 11.1\n",
    "# !pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE -> MAE Loss 꿀팁!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  4 06:14:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:18:00.0 Off |                  Off |\n",
      "| 33%   27C    P8    26W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:3B:00.0 Off |                  Off |\n",
      "| 33%   27C    P8    30W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     On   | 00000000:86:00.0 Off |                  Off |\n",
      "| 33%   23C    P8     6W / 260W |      1MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:AF:00.0 Off |                  Off |\n",
      "| 33%   23C    P8    18W / 260W |      1MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2, 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/4.Dose_img2img/scripts study\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/4.Dose_img2img/scripts study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 갯수 =  48\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"CPU 갯수 = \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMT_Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inintializing...!\n",
      "inintializing...!\n",
      "adopt performer encoder for tokens-to-token\n",
      "adopt performer encoder for tokens-to-token\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "inintializing...!\n",
      "***********************************************\n",
      "***********************************************\n",
      "Dataset Name:  Sinogram_DCM\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output To:  /workspace/sunggu/4.Dose_img2img/model/[Ours]CMT_Unet_L1_x1000\n",
      "Save   To:  /workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0002\n",
      "Weight Decay:  0.05\n",
      "Batchsize:  16\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6899\n",
      "Valid [Total]  number =  14\n",
      "Creating model: CMT_Unet\n",
      "Number of Learnable Params: 76860861\n",
      "CMT_Unet(\n",
      "  (encoder): CMT(\n",
      "    (stem): CMTStem(\n",
      "      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (gelu1): GELU()\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gelu2): GELU()\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (gelu3): GELU()\n",
      "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (patch1): Patch_Aggregate(\n",
      "      (conv): Conv2x2(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (patch2): Patch_Aggregate(\n",
      "      (conv): Conv2x2(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (patch3): Patch_Aggregate(\n",
      "      (conv): Conv2x2(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (patch4): Patch_Aggregate(\n",
      "      (conv): Conv2x2(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (stage1): Sequential(\n",
      "      (0): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(8, 8), padding=(1, 1), groups=64)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(8, 8), padding=(1, 1), groups=64)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            )\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(8, 8), padding=(1, 1), groups=64)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(8, 8), padding=(1, 1), groups=64)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            )\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(8, 8), padding=(1, 1), groups=64)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(8, 8), padding=(1, 1), groups=64)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            )\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage2): Sequential(\n",
      "      (0): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=128)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=128)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "            )\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=128)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=128)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "            )\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=128)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=128)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "            )\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage3): Sequential(\n",
      "      (0): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "            )\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage4): Sequential(\n",
      "      (0): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "            )\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "            )\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): CMTBlock(\n",
      "        (lpu): LPU(\n",
      "          (DWConv): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "        )\n",
      "        (lmhsa): LMHSA(\n",
      "          (dwconv_k): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (dwconv_v): DWCONV(\n",
      "            (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (irffn): IRFFN(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (dwconv): Sequential(\n",
      "            (0): DWCONV(\n",
      "              (depthwise): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "            )\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): GELU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=1280, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      "  (upsample1): UpsampleBlock(\n",
      "    (conv): Conv2d(512, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
      "    (prelu): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (decoder_block1): ConvBlock(\n",
      "    (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (upsample2): UpsampleBlock(\n",
      "    (conv): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
      "    (prelu): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (decoder_block2): ConvBlock(\n",
      "    (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (upsample3): UpsampleBlock(\n",
      "    (conv): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
      "    (prelu): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (decoder_block3): ConvBlock(\n",
      "    (conv1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (upsample4): UpsampleBlock(\n",
      "    (conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
      "    (prelu): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (decoder_block4): ConvBlock(\n",
      "    (conv1): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (upsample5): UpsampleBlock(\n",
      "    (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
      "    (prelu): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (decoder_block5): ConvBlock(\n",
      "    (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (head): OutputProj(\n",
      "    (conv1): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Start training for 1000 epochs\n",
      "Train: [epoch:0]  [  0/431]  eta: 1:17:36  lr: 0.000001  loss: 5756159.0000 (5756159.0000)  time: 10.8046  data: 2.4895  max mem: 15039\n",
      "Train: [epoch:0]  [ 10/431]  eta: 0:13:31  lr: 0.000001  loss: 5584813.0000 (5583230.9091)  time: 1.9274  data: 0.2265  max mem: 15925\n",
      "Train: [epoch:0]  [ 20/431]  eta: 0:10:13  lr: 0.000001  loss: 5389788.0000 (5437120.5952)  time: 1.0279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [ 30/431]  eta: 0:08:59  lr: 0.000001  loss: 5115934.0000 (5300665.9355)  time: 1.0234  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [ 40/431]  eta: 0:08:16  lr: 0.000001  loss: 4890796.0000 (5172702.8780)  time: 1.0354  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [ 50/431]  eta: 0:07:48  lr: 0.000001  loss: 4641094.0000 (5052899.4902)  time: 1.0506  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [ 60/431]  eta: 0:07:25  lr: 0.000001  loss: 4411429.5000 (4932845.4426)  time: 1.0592  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [ 70/431]  eta: 0:07:08  lr: 0.000001  loss: 4199507.0000 (4815812.9577)  time: 1.0793  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [ 80/431]  eta: 0:06:53  lr: 0.000001  loss: 3990623.0000 (4707185.7438)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [ 90/431]  eta: 0:06:39  lr: 0.000001  loss: 3826916.0000 (4605682.4890)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [100/431]  eta: 0:06:25  lr: 0.000001  loss: 3709130.7500 (4508907.0000)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [110/431]  eta: 0:06:13  lr: 0.000001  loss: 3541945.2500 (4416045.1171)  time: 1.1276  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [120/431]  eta: 0:06:00  lr: 0.000001  loss: 3415478.7500 (4327646.7727)  time: 1.1289  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [130/431]  eta: 0:05:48  lr: 0.000001  loss: 3270748.0000 (4242425.6279)  time: 1.1320  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [140/431]  eta: 0:05:36  lr: 0.000001  loss: 3156910.7500 (4162476.5798)  time: 1.1289  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [150/431]  eta: 0:05:24  lr: 0.000001  loss: 3022016.7500 (4084641.0315)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [160/431]  eta: 0:05:12  lr: 0.000001  loss: 2931757.2500 (4010596.0870)  time: 1.1372  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [170/431]  eta: 0:05:00  lr: 0.000001  loss: 2839970.2500 (3940625.6564)  time: 1.1336  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [180/431]  eta: 0:04:48  lr: 0.000001  loss: 2756116.2500 (3871753.0552)  time: 1.1311  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [190/431]  eta: 0:04:37  lr: 0.000001  loss: 2653386.7500 (3806038.3665)  time: 1.1438  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [200/431]  eta: 0:04:26  lr: 0.000001  loss: 2579478.0000 (3743299.8694)  time: 1.1621  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [210/431]  eta: 0:04:14  lr: 0.000001  loss: 2508275.5000 (3683486.3839)  time: 1.1610  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [220/431]  eta: 0:04:03  lr: 0.000001  loss: 2419083.5000 (3625093.9106)  time: 1.1504  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [230/431]  eta: 0:03:51  lr: 0.000001  loss: 2354505.7500 (3568710.8907)  time: 1.1498  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [240/431]  eta: 0:03:39  lr: 0.000001  loss: 2282735.2500 (3514299.1909)  time: 1.1526  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [250/431]  eta: 0:03:28  lr: 0.000001  loss: 2225699.2500 (3461861.8675)  time: 1.1512  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [260/431]  eta: 0:03:16  lr: 0.000001  loss: 2174608.7500 (3411526.1849)  time: 1.1446  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [270/431]  eta: 0:03:05  lr: 0.000001  loss: 2109987.2500 (3362393.4553)  time: 1.1315  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [280/431]  eta: 0:02:53  lr: 0.000001  loss: 2058434.1250 (3315543.1530)  time: 1.1323  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [290/431]  eta: 0:02:42  lr: 0.000001  loss: 2007881.7500 (3270095.1499)  time: 1.1328  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [300/431]  eta: 0:02:30  lr: 0.000001  loss: 1972776.2500 (3226380.5673)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [310/431]  eta: 0:02:18  lr: 0.000001  loss: 1916052.8750 (3183626.2998)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [320/431]  eta: 0:02:07  lr: 0.000001  loss: 1874404.2500 (3142246.5666)  time: 1.1310  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [330/431]  eta: 0:01:55  lr: 0.000001  loss: 1821284.0000 (3101533.3803)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [340/431]  eta: 0:01:44  lr: 0.000001  loss: 1779887.7500 (3062105.3207)  time: 1.1321  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [350/431]  eta: 0:01:32  lr: 0.000001  loss: 1733824.7500 (3023650.9697)  time: 1.1434  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [360/431]  eta: 0:01:21  lr: 0.000001  loss: 1695639.1250 (2986595.6330)  time: 1.1347  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [370/431]  eta: 0:01:09  lr: 0.000001  loss: 1670887.2500 (2950541.3885)  time: 1.1361  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [380/431]  eta: 0:00:58  lr: 0.000001  loss: 1634783.6250 (2915800.9793)  time: 1.1410  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [390/431]  eta: 0:00:46  lr: 0.000001  loss: 1597774.5000 (2881764.6714)  time: 1.1437  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [400/431]  eta: 0:00:35  lr: 0.000001  loss: 1576437.7500 (2848860.0764)  time: 1.1321  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [410/431]  eta: 0:00:24  lr: 0.000001  loss: 1534128.8750 (2816498.7406)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:0]  [420/431]  eta: 0:00:12  lr: 0.000001  loss: 1492751.0000 (2784879.5772)  time: 1.1354  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:0]  [430/431]  eta: 0:00:01  lr: 0.000001  loss: 1470952.8750 (2753921.0142)  time: 1.1322  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:0] Total time: 0:08:13 (1.1444 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 1470952.8750 (2753921.0142)\n",
      "Valid: [epoch:0]  [ 0/14]  eta: 0:00:32  loss: 1432310.2500 (1432310.2500)  time: 2.3567  data: 2.1932  max mem: 15925\n",
      "Valid: [epoch:0]  [13/14]  eta: 0:00:00  loss: 1465909.0000 (1457145.7500)  time: 0.2660  data: 0.1567  max mem: 15925\n",
      "Valid: [epoch:0] Total time: 0:00:03 (0.2807 s / it)\n",
      "Averaged stats: loss: 1465909.0000 (1457145.7500)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_0_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1457145.750%\n",
      "Min loss: 1457145.750\n",
      "Best Epoch: 0.000\n",
      "/home/sunggu/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Train: [epoch:1]  [  0/431]  eta: 0:31:21  lr: 0.000001  loss: 1439482.2500 (1439482.2500)  time: 4.3657  data: 3.1755  max mem: 15925\n",
      "Train: [epoch:1]  [ 10/431]  eta: 0:09:36  lr: 0.000001  loss: 1419672.5000 (1413891.0682)  time: 1.3688  data: 0.2889  max mem: 15925\n",
      "Train: [epoch:1]  [ 20/431]  eta: 0:08:23  lr: 0.000001  loss: 1409165.8750 (1406075.7500)  time: 1.0675  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [ 30/431]  eta: 0:07:54  lr: 0.000001  loss: 1378211.0000 (1394966.1129)  time: 1.0817  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:1]  [ 40/431]  eta: 0:07:36  lr: 0.000001  loss: 1364316.5000 (1386035.0640)  time: 1.1055  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:1]  [ 50/431]  eta: 0:07:21  lr: 0.000001  loss: 1344099.0000 (1375154.7328)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [ 60/431]  eta: 0:07:07  lr: 0.000001  loss: 1320650.6250 (1363728.5389)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [ 70/431]  eta: 0:06:53  lr: 0.000001  loss: 1287260.0000 (1351433.9313)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [ 80/431]  eta: 0:06:41  lr: 0.000001  loss: 1265806.1250 (1339221.7469)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [ 90/431]  eta: 0:06:28  lr: 0.000001  loss: 1243471.7500 (1327271.0701)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [100/431]  eta: 0:06:16  lr: 0.000001  loss: 1216194.7500 (1316034.8713)  time: 1.1144  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:1]  [110/431]  eta: 0:06:05  lr: 0.000001  loss: 1196908.7500 (1304552.5203)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [120/431]  eta: 0:05:53  lr: 0.000001  loss: 1176210.2500 (1292670.1209)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [130/431]  eta: 0:05:41  lr: 0.000001  loss: 1148829.5000 (1281190.1880)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [140/431]  eta: 0:05:30  lr: 0.000001  loss: 1130080.6250 (1269728.5931)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [150/431]  eta: 0:05:18  lr: 0.000001  loss: 1109786.2500 (1258617.8493)  time: 1.1303  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [160/431]  eta: 0:05:07  lr: 0.000001  loss: 1089193.1250 (1247599.4767)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [170/431]  eta: 0:04:55  lr: 0.000001  loss: 1072569.0000 (1237045.8882)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [180/431]  eta: 0:04:44  lr: 0.000001  loss: 1055663.2500 (1226564.3726)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [190/431]  eta: 0:04:32  lr: 0.000001  loss: 1034421.1250 (1215878.6587)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [200/431]  eta: 0:04:21  lr: 0.000001  loss: 1017684.8750 (1205907.0662)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [210/431]  eta: 0:04:09  lr: 0.000001  loss: 1005204.4375 (1195876.5992)  time: 1.1346  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [220/431]  eta: 0:03:58  lr: 0.000001  loss: 980462.3750 (1185743.6473)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [230/431]  eta: 0:03:46  lr: 0.000001  loss: 967003.8750 (1175937.6261)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [240/431]  eta: 0:03:35  lr: 0.000001  loss: 947168.1875 (1166260.0451)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [250/431]  eta: 0:03:24  lr: 0.000001  loss: 935205.3125 (1156732.6424)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [260/431]  eta: 0:03:12  lr: 0.000001  loss: 917381.8125 (1147301.8590)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [270/431]  eta: 0:03:01  lr: 0.000001  loss: 900927.0625 (1137965.0450)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [280/431]  eta: 0:02:50  lr: 0.000001  loss: 891675.9375 (1129060.5254)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [290/431]  eta: 0:02:38  lr: 0.000001  loss: 880070.0000 (1120291.9502)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [300/431]  eta: 0:02:27  lr: 0.000001  loss: 864849.3750 (1111686.8216)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [310/431]  eta: 0:02:16  lr: 0.000001  loss: 857297.1875 (1103435.2203)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [320/431]  eta: 0:02:04  lr: 0.000001  loss: 848761.5000 (1095218.0590)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [330/431]  eta: 0:01:53  lr: 0.000001  loss: 828263.6250 (1086775.1883)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [340/431]  eta: 0:01:42  lr: 0.000001  loss: 804694.8125 (1078341.4382)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [350/431]  eta: 0:01:31  lr: 0.000001  loss: 791348.0000 (1069965.7532)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [360/431]  eta: 0:01:19  lr: 0.000001  loss: 780994.1250 (1061959.1991)  time: 1.1149  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [370/431]  eta: 0:01:08  lr: 0.000001  loss: 776645.5625 (1054185.9648)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [380/431]  eta: 0:00:57  lr: 0.000001  loss: 765064.3125 (1046439.3192)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [390/431]  eta: 0:00:46  lr: 0.000001  loss: 754305.9375 (1038914.5764)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [400/431]  eta: 0:00:34  lr: 0.000001  loss: 745381.9375 (1031443.6252)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [410/431]  eta: 0:00:23  lr: 0.000001  loss: 732638.6875 (1024064.4886)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:1]  [420/431]  eta: 0:00:12  lr: 0.000001  loss: 726369.2500 (1016892.6188)  time: 1.1224  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:1]  [430/431]  eta: 0:00:01  lr: 0.000001  loss: 714070.6875 (1009732.0390)  time: 1.1249  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:1] Total time: 0:08:04 (1.1244 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 714070.6875 (1009732.0390)\n",
      "Valid: [epoch:1]  [ 0/14]  eta: 0:00:30  loss: 664485.6875 (664485.6875)  time: 2.1779  data: 1.9869  max mem: 15925\n",
      "Valid: [epoch:1]  [13/14]  eta: 0:00:00  loss: 711373.1250 (712499.7321)  time: 0.2406  data: 0.1420  max mem: 15925\n",
      "Valid: [epoch:1] Total time: 0:00:03 (0.2560 s / it)\n",
      "Averaged stats: loss: 711373.1250 (712499.7321)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_1_input_n_20.png\n",
      "loss of the network on the 14 valid images: 712499.732%\n",
      "Min loss: 712499.732\n",
      "Best Epoch: 1.000\n",
      "Train: [epoch:2]  [  0/431]  eta: 0:33:33  lr: 0.000020  loss: 712342.4375 (712342.4375)  time: 4.6721  data: 3.4770  max mem: 15925\n",
      "Train: [epoch:2]  [ 10/431]  eta: 0:09:40  lr: 0.000020  loss: 599376.8125 (604182.9943)  time: 1.3780  data: 0.3163  max mem: 15925\n",
      "Train: [epoch:2]  [ 20/431]  eta: 0:08:25  lr: 0.000020  loss: 517403.1875 (536459.2530)  time: 1.0587  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [ 30/431]  eta: 0:07:57  lr: 0.000020  loss: 406752.9375 (482259.9738)  time: 1.0881  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [ 40/431]  eta: 0:07:35  lr: 0.000020  loss: 327811.4062 (437507.5412)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [ 50/431]  eta: 0:07:20  lr: 0.000020  loss: 265006.8438 (399527.4905)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [ 60/431]  eta: 0:07:07  lr: 0.000020  loss: 217736.8125 (366770.0138)  time: 1.1294  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [ 70/431]  eta: 0:06:54  lr: 0.000020  loss: 180753.4531 (338178.3856)  time: 1.1289  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [ 80/431]  eta: 0:06:42  lr: 0.000020  loss: 146694.8906 (312854.6977)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [ 90/431]  eta: 0:06:30  lr: 0.000020  loss: 121291.7344 (290280.2230)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [100/431]  eta: 0:06:17  lr: 0.000020  loss: 92932.4062 (269765.0514)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [110/431]  eta: 0:06:05  lr: 0.000020  loss: 71596.6953 (251007.5034)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [120/431]  eta: 0:05:53  lr: 0.000020  loss: 51870.6250 (233830.4823)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [130/431]  eta: 0:05:42  lr: 0.000020  loss: 34246.7305 (218249.1575)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [140/431]  eta: 0:05:30  lr: 0.000020  loss: 24338.6504 (204262.0979)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [150/431]  eta: 0:05:18  lr: 0.000020  loss: 18330.8496 (191826.9947)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [160/431]  eta: 0:05:07  lr: 0.000020  loss: 14990.5762 (180792.0305)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [170/431]  eta: 0:04:55  lr: 0.000020  loss: 13502.8242 (170985.5203)  time: 1.1167  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:2]  [180/431]  eta: 0:04:44  lr: 0.000020  loss: 12512.2305 (162213.1624)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [190/431]  eta: 0:04:32  lr: 0.000020  loss: 11768.9023 (154324.5887)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [200/431]  eta: 0:04:21  lr: 0.000020  loss: 11154.2188 (147195.1870)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [210/431]  eta: 0:04:09  lr: 0.000020  loss: 10761.3896 (140719.8175)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [220/431]  eta: 0:03:58  lr: 0.000020  loss: 10360.1914 (134813.5415)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [230/431]  eta: 0:03:46  lr: 0.000020  loss: 9959.7041 (129402.6021)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [240/431]  eta: 0:03:35  lr: 0.000020  loss: 9637.7471 (124427.6176)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [250/431]  eta: 0:03:24  lr: 0.000020  loss: 9306.8242 (119834.8033)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [260/431]  eta: 0:03:12  lr: 0.000020  loss: 8976.0537 (115582.4663)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [270/431]  eta: 0:03:01  lr: 0.000020  loss: 8703.3379 (111634.2294)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [280/431]  eta: 0:02:50  lr: 0.000020  loss: 8440.3574 (107960.3185)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [290/431]  eta: 0:02:38  lr: 0.000020  loss: 8317.2090 (104531.2564)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [300/431]  eta: 0:02:27  lr: 0.000020  loss: 8010.9712 (101319.5608)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [310/431]  eta: 0:02:16  lr: 0.000020  loss: 7765.0098 (98309.7833)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [320/431]  eta: 0:02:05  lr: 0.000020  loss: 7597.5972 (95481.7289)  time: 1.1341  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [330/431]  eta: 0:01:53  lr: 0.000020  loss: 7385.9956 (92815.8750)  time: 1.1324  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [340/431]  eta: 0:01:42  lr: 0.000020  loss: 7148.1929 (90301.6586)  time: 1.1352  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [350/431]  eta: 0:01:31  lr: 0.000020  loss: 6945.8784 (87924.6190)  time: 1.1307  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [360/431]  eta: 0:01:19  lr: 0.000020  loss: 6826.0083 (85677.5805)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [370/431]  eta: 0:01:08  lr: 0.000020  loss: 6733.1953 (83546.7542)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [380/431]  eta: 0:00:57  lr: 0.000020  loss: 6489.0962 (81523.4073)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [390/431]  eta: 0:00:46  lr: 0.000020  loss: 6369.5835 (79600.5311)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [400/431]  eta: 0:00:34  lr: 0.000020  loss: 6289.8472 (77770.6947)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [410/431]  eta: 0:00:23  lr: 0.000020  loss: 6088.8003 (76025.0188)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:2]  [420/431]  eta: 0:00:12  lr: 0.000020  loss: 5950.8618 (74359.0350)  time: 1.1088  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:2]  [430/431]  eta: 0:00:01  lr: 0.000020  loss: 5805.3047 (72767.0841)  time: 1.1275  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:2] Total time: 0:08:05 (1.1261 s / it)\n",
      "Averaged stats: lr: 0.000020  loss: 5805.3047 (72767.0841)\n",
      "Valid: [epoch:2]  [ 0/14]  eta: 0:00:29  loss: 6157.1045 (6157.1045)  time: 2.1248  data: 1.9764  max mem: 15925\n",
      "Valid: [epoch:2]  [13/14]  eta: 0:00:00  loss: 5797.8789 (5784.2386)  time: 0.2314  data: 0.1412  max mem: 15925\n",
      "Valid: [epoch:2] Total time: 0:00:03 (0.2480 s / it)\n",
      "Averaged stats: loss: 5797.8789 (5784.2386)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_2_input_n_20.png\n",
      "loss of the network on the 14 valid images: 5784.239%\n",
      "Min loss: 5784.239\n",
      "Best Epoch: 2.000\n",
      "Train: [epoch:3]  [  0/431]  eta: 0:33:48  lr: 0.000040  loss: 5633.9619 (5633.9619)  time: 4.7056  data: 3.5039  max mem: 15925\n",
      "Train: [epoch:3]  [ 10/431]  eta: 0:09:41  lr: 0.000040  loss: 5508.7520 (5513.6999)  time: 1.3801  data: 0.3188  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 20/431]  eta: 0:08:26  lr: 0.000040  loss: 5401.1929 (5434.3002)  time: 1.0597  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [ 30/431]  eta: 0:07:53  lr: 0.000040  loss: 5263.5698 (5354.3316)  time: 1.0700  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [ 40/431]  eta: 0:07:32  lr: 0.000040  loss: 5082.6211 (5259.5197)  time: 1.0780  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [ 50/431]  eta: 0:07:18  lr: 0.000040  loss: 4835.0913 (5160.2374)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [ 60/431]  eta: 0:07:04  lr: 0.000040  loss: 4639.2163 (5068.0175)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [ 70/431]  eta: 0:06:51  lr: 0.000040  loss: 4486.0898 (4975.0410)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [ 80/431]  eta: 0:06:38  lr: 0.000040  loss: 4310.5366 (4884.7923)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [ 90/431]  eta: 0:06:26  lr: 0.000040  loss: 4202.0835 (4801.3550)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [100/431]  eta: 0:06:15  lr: 0.000040  loss: 4049.3330 (4717.7785)  time: 1.1255  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:3]  [110/431]  eta: 0:06:03  lr: 0.000040  loss: 3906.8342 (4642.5146)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [120/431]  eta: 0:05:51  lr: 0.000040  loss: 3802.8550 (4568.3026)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [130/431]  eta: 0:05:40  lr: 0.000040  loss: 3639.4998 (4493.2194)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [140/431]  eta: 0:05:28  lr: 0.000040  loss: 3545.5183 (4422.0615)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [150/431]  eta: 0:05:17  lr: 0.000040  loss: 3428.4028 (4353.0831)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [160/431]  eta: 0:05:05  lr: 0.000040  loss: 3269.0129 (4283.2575)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [170/431]  eta: 0:04:54  lr: 0.000040  loss: 3195.5884 (4218.1510)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [180/431]  eta: 0:04:42  lr: 0.000040  loss: 3125.3569 (4154.1682)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [190/431]  eta: 0:04:31  lr: 0.000040  loss: 3003.9744 (4092.3326)  time: 1.1265  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [200/431]  eta: 0:04:20  lr: 0.000040  loss: 2935.7297 (4032.3899)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [210/431]  eta: 0:04:08  lr: 0.000040  loss: 2840.2395 (3973.1859)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [220/431]  eta: 0:03:57  lr: 0.000040  loss: 2736.4617 (3915.4458)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [230/431]  eta: 0:03:46  lr: 0.000040  loss: 2674.0591 (3859.7017)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [240/431]  eta: 0:03:34  lr: 0.000040  loss: 2544.8914 (3803.3519)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [250/431]  eta: 0:03:23  lr: 0.000040  loss: 2469.3953 (3748.8993)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [260/431]  eta: 0:03:12  lr: 0.000040  loss: 2396.0151 (3695.4467)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [270/431]  eta: 0:03:00  lr: 0.000040  loss: 2302.5312 (3643.3539)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [280/431]  eta: 0:02:49  lr: 0.000040  loss: 2260.6030 (3592.6634)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [290/431]  eta: 0:02:38  lr: 0.000040  loss: 2190.7251 (3543.0399)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [300/431]  eta: 0:02:26  lr: 0.000040  loss: 2094.3838 (3493.5589)  time: 1.1102  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:3]  [310/431]  eta: 0:02:15  lr: 0.000040  loss: 2024.8809 (3445.4240)  time: 1.1080  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:3]  [320/431]  eta: 0:02:04  lr: 0.000040  loss: 1948.9496 (3397.9565)  time: 1.1086  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:3]  [330/431]  eta: 0:01:53  lr: 0.000040  loss: 1868.9614 (3351.1070)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [340/431]  eta: 0:01:41  lr: 0.000040  loss: 1819.8373 (3304.9984)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [350/431]  eta: 0:01:30  lr: 0.000040  loss: 1719.7819 (3259.4315)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [360/431]  eta: 0:01:19  lr: 0.000040  loss: 1692.5253 (3214.8783)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [370/431]  eta: 0:01:08  lr: 0.000040  loss: 1610.2207 (3170.9083)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [380/431]  eta: 0:00:57  lr: 0.000040  loss: 1569.7289 (3127.6674)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [390/431]  eta: 0:00:45  lr: 0.000040  loss: 1488.9847 (3085.1301)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [400/431]  eta: 0:00:34  lr: 0.000040  loss: 1439.5624 (3042.9978)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [410/431]  eta: 0:00:23  lr: 0.000040  loss: 1365.2183 (3001.8326)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:3]  [420/431]  eta: 0:00:12  lr: 0.000040  loss: 1319.6464 (2961.1158)  time: 1.1247  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:3]  [430/431]  eta: 0:00:01  lr: 0.000040  loss: 1249.3115 (2921.0299)  time: 1.1167  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:3] Total time: 0:08:02 (1.1203 s / it)\n",
      "Averaged stats: lr: 0.000040  loss: 1249.3115 (2921.0299)\n",
      "Valid: [epoch:3]  [ 0/14]  eta: 0:00:33  loss: 1226.9373 (1226.9373)  time: 2.3670  data: 2.2581  max mem: 15925\n",
      "Valid: [epoch:3]  [13/14]  eta: 0:00:00  loss: 1210.8324 (1208.3395)  time: 0.2453  data: 0.1614  max mem: 15925\n",
      "Valid: [epoch:3] Total time: 0:00:03 (0.2615 s / it)\n",
      "Averaged stats: loss: 1210.8324 (1208.3395)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_3_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1208.340%\n",
      "Min loss: 1208.340\n",
      "Best Epoch: 3.000\n",
      "Train: [epoch:4]  [  0/431]  eta: 0:34:44  lr: 0.000060  loss: 1199.2609 (1199.2609)  time: 4.8357  data: 3.5394  max mem: 15925\n",
      "Train: [epoch:4]  [ 10/431]  eta: 0:09:45  lr: 0.000060  loss: 1158.8948 (1159.1480)  time: 1.3905  data: 0.3220  max mem: 15925\n",
      "Train: [epoch:4]  [ 20/431]  eta: 0:08:26  lr: 0.000060  loss: 1122.1838 (1120.2474)  time: 1.0531  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [ 30/431]  eta: 0:07:58  lr: 0.000060  loss: 1026.7457 (1080.7750)  time: 1.0845  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:4]  [ 40/431]  eta: 0:07:36  lr: 0.000060  loss: 955.8939 (1042.4397)  time: 1.0998  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:4]  [ 50/431]  eta: 0:07:21  lr: 0.000060  loss: 905.1393 (1009.0652)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [ 60/431]  eta: 0:07:07  lr: 0.000060  loss: 831.0201 (976.3110)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [ 70/431]  eta: 0:06:54  lr: 0.000060  loss: 781.8421 (944.5432)  time: 1.1244  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [ 80/431]  eta: 0:06:42  lr: 0.000060  loss: 721.0186 (915.0413)  time: 1.1345  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [ 90/431]  eta: 0:06:31  lr: 0.000060  loss: 672.0358 (885.6058)  time: 1.1423  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [100/431]  eta: 0:06:18  lr: 0.000060  loss: 617.3461 (857.3274)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [110/431]  eta: 0:06:06  lr: 0.000060  loss: 566.9867 (829.6124)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [120/431]  eta: 0:05:55  lr: 0.000060  loss: 527.4947 (803.5566)  time: 1.1321  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [130/431]  eta: 0:05:43  lr: 0.000060  loss: 502.9140 (779.1084)  time: 1.1369  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [140/431]  eta: 0:05:31  lr: 0.000060  loss: 462.9581 (756.5353)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [150/431]  eta: 0:05:19  lr: 0.000060  loss: 442.4220 (734.8465)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [160/431]  eta: 0:05:07  lr: 0.000060  loss: 417.0579 (714.8550)  time: 1.1184  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [170/431]  eta: 0:04:56  lr: 0.000060  loss: 402.9851 (695.5068)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [180/431]  eta: 0:04:44  lr: 0.000060  loss: 381.4423 (677.9646)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [190/431]  eta: 0:04:32  lr: 0.000060  loss: 369.6537 (661.3382)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [200/431]  eta: 0:04:21  lr: 0.000060  loss: 349.0430 (645.6355)  time: 1.1302  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [210/431]  eta: 0:04:10  lr: 0.000060  loss: 344.9889 (631.2679)  time: 1.1324  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [220/431]  eta: 0:03:58  lr: 0.000060  loss: 332.0983 (617.3780)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [230/431]  eta: 0:03:47  lr: 0.000060  loss: 320.5181 (604.5613)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [240/431]  eta: 0:03:35  lr: 0.000060  loss: 315.7571 (592.4142)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [250/431]  eta: 0:03:24  lr: 0.000060  loss: 308.8315 (580.6509)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [260/431]  eta: 0:03:13  lr: 0.000060  loss: 302.7511 (570.0299)  time: 1.1276  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [270/431]  eta: 0:03:01  lr: 0.000060  loss: 300.7005 (560.0193)  time: 1.1152  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:4]  [280/431]  eta: 0:02:50  lr: 0.000060  loss: 291.4846 (550.2438)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [290/431]  eta: 0:02:39  lr: 0.000060  loss: 283.2722 (540.9482)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [300/431]  eta: 0:02:27  lr: 0.000060  loss: 279.8843 (532.3986)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [310/431]  eta: 0:02:16  lr: 0.000060  loss: 275.0365 (523.8322)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [320/431]  eta: 0:02:05  lr: 0.000060  loss: 265.4828 (515.7761)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [330/431]  eta: 0:01:53  lr: 0.000060  loss: 269.7678 (508.5498)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [340/431]  eta: 0:01:42  lr: 0.000060  loss: 276.9603 (501.6751)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [350/431]  eta: 0:01:31  lr: 0.000060  loss: 276.1778 (494.8497)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [360/431]  eta: 0:01:20  lr: 0.000060  loss: 251.0594 (488.0557)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [370/431]  eta: 0:01:08  lr: 0.000060  loss: 251.0594 (481.5664)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [380/431]  eta: 0:00:57  lr: 0.000060  loss: 252.7831 (475.6514)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [390/431]  eta: 0:00:46  lr: 0.000060  loss: 252.9287 (469.8438)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [400/431]  eta: 0:00:34  lr: 0.000060  loss: 246.9027 (464.2533)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:4]  [410/431]  eta: 0:00:23  lr: 0.000060  loss: 245.9315 (458.8973)  time: 1.1114  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:4]  [420/431]  eta: 0:00:12  lr: 0.000060  loss: 242.8504 (453.8046)  time: 1.1126  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:4]  [430/431]  eta: 0:00:01  lr: 0.000060  loss: 240.2810 (448.7651)  time: 1.1186  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:4] Total time: 0:08:05 (1.1263 s / it)\n",
      "Averaged stats: lr: 0.000060  loss: 240.2810 (448.7651)\n",
      "Valid: [epoch:4]  [ 0/14]  eta: 0:00:30  loss: 248.6782 (248.6782)  time: 2.1613  data: 2.0206  max mem: 15925\n",
      "Valid: [epoch:4]  [13/14]  eta: 0:00:00  loss: 248.6782 (246.6158)  time: 0.2314  data: 0.1444  max mem: 15925\n",
      "Valid: [epoch:4] Total time: 0:00:03 (0.2481 s / it)\n",
      "Averaged stats: loss: 248.6782 (246.6158)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_4_input_n_20.png\n",
      "loss of the network on the 14 valid images: 246.616%\n",
      "Min loss: 246.616\n",
      "Best Epoch: 4.000\n",
      "Train: [epoch:5]  [  0/431]  eta: 0:32:26  lr: 0.000080  loss: 269.5204 (269.5204)  time: 4.5173  data: 3.4147  max mem: 15925\n",
      "Train: [epoch:5]  [ 10/431]  eta: 0:09:36  lr: 0.000080  loss: 250.3048 (249.4832)  time: 1.3697  data: 0.3106  max mem: 15925\n",
      "Train: [epoch:5]  [ 20/431]  eta: 0:08:23  lr: 0.000080  loss: 240.1939 (243.0169)  time: 1.0601  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [ 30/431]  eta: 0:07:54  lr: 0.000080  loss: 228.5573 (238.9916)  time: 1.0803  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [ 40/431]  eta: 0:07:35  lr: 0.000080  loss: 222.7890 (234.2912)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [ 50/431]  eta: 0:07:18  lr: 0.000080  loss: 225.3673 (233.3506)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [ 60/431]  eta: 0:07:04  lr: 0.000080  loss: 229.9762 (232.3401)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [ 70/431]  eta: 0:06:51  lr: 0.000080  loss: 227.0425 (231.6521)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [ 80/431]  eta: 0:06:38  lr: 0.000080  loss: 228.5217 (231.4956)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [ 90/431]  eta: 0:06:26  lr: 0.000080  loss: 227.1491 (229.8523)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [100/431]  eta: 0:06:14  lr: 0.000080  loss: 213.8571 (228.4881)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [110/431]  eta: 0:06:03  lr: 0.000080  loss: 210.2868 (226.9991)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [120/431]  eta: 0:05:51  lr: 0.000080  loss: 205.6904 (225.3167)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [130/431]  eta: 0:05:39  lr: 0.000080  loss: 210.0555 (224.3769)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [140/431]  eta: 0:05:28  lr: 0.000080  loss: 210.6050 (223.4947)  time: 1.1157  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [150/431]  eta: 0:05:16  lr: 0.000080  loss: 217.0257 (223.0406)  time: 1.1118  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [160/431]  eta: 0:05:05  lr: 0.000080  loss: 213.7357 (222.4632)  time: 1.1216  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [170/431]  eta: 0:04:53  lr: 0.000080  loss: 213.6973 (222.0394)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [180/431]  eta: 0:04:42  lr: 0.000080  loss: 210.8873 (221.1025)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [190/431]  eta: 0:04:31  lr: 0.000080  loss: 210.1794 (220.6011)  time: 1.1318  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [200/431]  eta: 0:04:20  lr: 0.000080  loss: 210.1794 (219.9396)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [210/431]  eta: 0:04:08  lr: 0.000080  loss: 206.6665 (219.2707)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [220/431]  eta: 0:03:57  lr: 0.000080  loss: 206.3470 (218.6578)  time: 1.1268  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [230/431]  eta: 0:03:46  lr: 0.000080  loss: 205.5830 (218.0590)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [240/431]  eta: 0:03:34  lr: 0.000080  loss: 208.6878 (217.8225)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [250/431]  eta: 0:03:23  lr: 0.000080  loss: 203.9455 (217.1807)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [260/431]  eta: 0:03:12  lr: 0.000080  loss: 199.1687 (216.7253)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [270/431]  eta: 0:03:00  lr: 0.000080  loss: 202.3179 (216.2957)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [280/431]  eta: 0:02:49  lr: 0.000080  loss: 201.5803 (215.7643)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [290/431]  eta: 0:02:38  lr: 0.000080  loss: 202.5634 (215.3178)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [300/431]  eta: 0:02:27  lr: 0.000080  loss: 202.7272 (214.7929)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [310/431]  eta: 0:02:15  lr: 0.000080  loss: 198.5425 (214.1011)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [320/431]  eta: 0:02:04  lr: 0.000080  loss: 194.8509 (213.4963)  time: 1.1182  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [330/431]  eta: 0:01:53  lr: 0.000080  loss: 199.0894 (213.2516)  time: 1.1081  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [340/431]  eta: 0:01:42  lr: 0.000080  loss: 205.2408 (212.8914)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [350/431]  eta: 0:01:30  lr: 0.000080  loss: 198.7604 (212.5581)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [360/431]  eta: 0:01:19  lr: 0.000080  loss: 199.1984 (212.1791)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [370/431]  eta: 0:01:08  lr: 0.000080  loss: 197.2569 (211.6011)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [380/431]  eta: 0:00:57  lr: 0.000080  loss: 191.6706 (210.8838)  time: 1.1165  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [390/431]  eta: 0:00:45  lr: 0.000080  loss: 191.8270 (210.3028)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [400/431]  eta: 0:00:34  lr: 0.000080  loss: 192.2271 (209.8345)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:5]  [410/431]  eta: 0:00:23  lr: 0.000080  loss: 194.3842 (209.5231)  time: 1.1145  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [420/431]  eta: 0:00:12  lr: 0.000080  loss: 191.2977 (208.8877)  time: 1.1146  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5]  [430/431]  eta: 0:00:01  lr: 0.000080  loss: 189.0636 (208.4824)  time: 1.1149  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:5] Total time: 0:08:03 (1.1216 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 189.0636 (208.4824)\n",
      "Valid: [epoch:5]  [ 0/14]  eta: 0:00:30  loss: 201.1595 (201.1595)  time: 2.1880  data: 2.0634  max mem: 15925\n",
      "Valid: [epoch:5]  [13/14]  eta: 0:00:00  loss: 187.8397 (194.1536)  time: 0.2444  data: 0.1475  max mem: 15925\n",
      "Valid: [epoch:5] Total time: 0:00:03 (0.2598 s / it)\n",
      "Averaged stats: loss: 187.8397 (194.1536)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_5_input_n_20.png\n",
      "loss of the network on the 14 valid images: 194.154%\n",
      "Min loss: 194.154\n",
      "Best Epoch: 5.000\n",
      "Train: [epoch:6]  [  0/431]  eta: 0:31:34  lr: 0.000100  loss: 195.1597 (195.1597)  time: 4.3945  data: 3.1289  max mem: 15925\n",
      "Train: [epoch:6]  [ 10/431]  eta: 0:09:42  lr: 0.000100  loss: 202.5688 (202.4279)  time: 1.3837  data: 0.2847  max mem: 15925\n",
      "Train: [epoch:6]  [ 20/431]  eta: 0:08:27  lr: 0.000100  loss: 196.9130 (198.2340)  time: 1.0776  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [ 30/431]  eta: 0:07:55  lr: 0.000100  loss: 186.4148 (193.0416)  time: 1.0777  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6]  [ 40/431]  eta: 0:07:35  lr: 0.000100  loss: 180.0755 (189.5466)  time: 1.0923  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6]  [ 50/431]  eta: 0:07:18  lr: 0.000100  loss: 178.4797 (186.7503)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [ 60/431]  eta: 0:07:04  lr: 0.000100  loss: 177.3468 (183.9903)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [ 70/431]  eta: 0:06:51  lr: 0.000100  loss: 179.2329 (184.1867)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [ 80/431]  eta: 0:06:39  lr: 0.000100  loss: 188.3383 (184.9550)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [ 90/431]  eta: 0:06:26  lr: 0.000100  loss: 183.5280 (184.2852)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [100/431]  eta: 0:06:14  lr: 0.000100  loss: 176.4749 (183.4335)  time: 1.1087  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6]  [110/431]  eta: 0:06:02  lr: 0.000100  loss: 169.4551 (181.7655)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [120/431]  eta: 0:05:51  lr: 0.000100  loss: 170.8774 (181.8687)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [130/431]  eta: 0:05:39  lr: 0.000100  loss: 183.7782 (181.6222)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [140/431]  eta: 0:05:27  lr: 0.000100  loss: 181.3018 (181.5976)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [150/431]  eta: 0:05:16  lr: 0.000100  loss: 183.2831 (181.7949)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [160/431]  eta: 0:05:05  lr: 0.000100  loss: 183.2112 (181.6506)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [170/431]  eta: 0:04:53  lr: 0.000100  loss: 169.7107 (180.9602)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [180/431]  eta: 0:04:42  lr: 0.000100  loss: 175.3578 (180.7961)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [190/431]  eta: 0:04:31  lr: 0.000100  loss: 177.4268 (180.7227)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [200/431]  eta: 0:04:19  lr: 0.000100  loss: 173.7434 (180.3404)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [210/431]  eta: 0:04:08  lr: 0.000100  loss: 173.6205 (179.8678)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [220/431]  eta: 0:03:57  lr: 0.000100  loss: 177.5157 (179.8101)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [230/431]  eta: 0:03:45  lr: 0.000100  loss: 180.1409 (179.7216)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [240/431]  eta: 0:03:34  lr: 0.000100  loss: 176.8483 (179.5322)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [250/431]  eta: 0:03:23  lr: 0.000100  loss: 173.5212 (179.1194)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [260/431]  eta: 0:03:11  lr: 0.000100  loss: 174.7336 (178.9594)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [270/431]  eta: 0:03:00  lr: 0.000100  loss: 174.0243 (178.7382)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [280/431]  eta: 0:02:49  lr: 0.000100  loss: 173.8985 (178.5974)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [290/431]  eta: 0:02:37  lr: 0.000100  loss: 173.8985 (178.4085)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [300/431]  eta: 0:02:26  lr: 0.000100  loss: 174.5526 (178.2966)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [310/431]  eta: 0:02:15  lr: 0.000100  loss: 168.0016 (177.8964)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [320/431]  eta: 0:02:04  lr: 0.000100  loss: 157.7108 (177.2259)  time: 1.1103  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6]  [330/431]  eta: 0:01:53  lr: 0.000100  loss: 165.4503 (177.1986)  time: 1.1180  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6]  [340/431]  eta: 0:01:41  lr: 0.000100  loss: 177.0177 (177.0780)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [350/431]  eta: 0:01:30  lr: 0.000100  loss: 172.6720 (176.9936)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [360/431]  eta: 0:01:19  lr: 0.000100  loss: 171.7011 (176.7909)  time: 1.1056  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6]  [370/431]  eta: 0:01:08  lr: 0.000100  loss: 171.8861 (176.6610)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [380/431]  eta: 0:00:57  lr: 0.000100  loss: 171.0204 (176.4231)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [390/431]  eta: 0:00:45  lr: 0.000100  loss: 167.5994 (176.0328)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [400/431]  eta: 0:00:34  lr: 0.000100  loss: 164.0829 (175.8143)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:6]  [410/431]  eta: 0:00:23  lr: 0.000100  loss: 167.6306 (175.5866)  time: 1.1005  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6]  [420/431]  eta: 0:00:12  lr: 0.000100  loss: 166.5210 (175.2964)  time: 1.0971  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6]  [430/431]  eta: 0:00:01  lr: 0.000100  loss: 169.6282 (175.2993)  time: 1.1030  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:6] Total time: 0:08:01 (1.1170 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 169.6282 (175.2993)\n",
      "Valid: [epoch:6]  [ 0/14]  eta: 0:00:30  loss: 189.1208 (189.1208)  time: 2.1677  data: 2.0220  max mem: 15925\n",
      "Valid: [epoch:6]  [13/14]  eta: 0:00:00  loss: 167.4435 (167.8109)  time: 0.2464  data: 0.1445  max mem: 15925\n",
      "Valid: [epoch:6] Total time: 0:00:03 (0.2628 s / it)\n",
      "Averaged stats: loss: 167.4435 (167.8109)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_6_input_n_20.png\n",
      "loss of the network on the 14 valid images: 167.811%\n",
      "Min loss: 167.811\n",
      "Best Epoch: 6.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [  0/431]  eta: 0:33:15  lr: 0.000120  loss: 202.6971 (202.6971)  time: 4.6302  data: 3.4326  max mem: 15925\n",
      "Train: [epoch:7]  [ 10/431]  eta: 0:09:45  lr: 0.000120  loss: 181.7546 (182.9660)  time: 1.3911  data: 0.3122  max mem: 15925\n",
      "Train: [epoch:7]  [ 20/431]  eta: 0:08:25  lr: 0.000120  loss: 176.3374 (174.0258)  time: 1.0605  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [ 30/431]  eta: 0:07:51  lr: 0.000120  loss: 160.8710 (169.4585)  time: 1.0580  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [ 40/431]  eta: 0:07:32  lr: 0.000120  loss: 154.2988 (166.0822)  time: 1.0811  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [ 50/431]  eta: 0:07:16  lr: 0.000120  loss: 157.8765 (164.7191)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [ 60/431]  eta: 0:07:01  lr: 0.000120  loss: 160.6062 (164.0962)  time: 1.0949  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [ 70/431]  eta: 0:06:48  lr: 0.000120  loss: 162.3438 (163.9521)  time: 1.0953  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [ 80/431]  eta: 0:06:36  lr: 0.000120  loss: 163.9191 (164.0542)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [ 90/431]  eta: 0:06:24  lr: 0.000120  loss: 160.4621 (163.2813)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [100/431]  eta: 0:06:12  lr: 0.000120  loss: 159.2430 (162.8250)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [110/431]  eta: 0:06:01  lr: 0.000120  loss: 158.5022 (162.1557)  time: 1.1166  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [120/431]  eta: 0:05:50  lr: 0.000120  loss: 155.7700 (161.6225)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [130/431]  eta: 0:05:38  lr: 0.000120  loss: 152.7237 (160.6618)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [140/431]  eta: 0:05:26  lr: 0.000120  loss: 152.4698 (160.3041)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [150/431]  eta: 0:05:14  lr: 0.000120  loss: 151.4194 (159.9127)  time: 1.0936  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [160/431]  eta: 0:05:03  lr: 0.000120  loss: 157.4047 (160.0277)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [170/431]  eta: 0:04:52  lr: 0.000120  loss: 157.9565 (159.7378)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [180/431]  eta: 0:04:40  lr: 0.000120  loss: 157.2854 (159.7785)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [190/431]  eta: 0:04:29  lr: 0.000120  loss: 158.9173 (159.8427)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [200/431]  eta: 0:04:18  lr: 0.000120  loss: 158.9173 (159.7790)  time: 1.1094  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [210/431]  eta: 0:04:06  lr: 0.000120  loss: 156.1474 (159.4630)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [220/431]  eta: 0:03:55  lr: 0.000120  loss: 160.8939 (159.4972)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [230/431]  eta: 0:03:44  lr: 0.000120  loss: 162.8166 (159.6144)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [240/431]  eta: 0:03:33  lr: 0.000120  loss: 162.7784 (159.6461)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [250/431]  eta: 0:03:21  lr: 0.000120  loss: 162.7784 (159.6655)  time: 1.1131  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [260/431]  eta: 0:03:10  lr: 0.000120  loss: 165.1368 (159.9356)  time: 1.1053  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [270/431]  eta: 0:02:59  lr: 0.000120  loss: 166.0458 (160.0581)  time: 1.1101  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [280/431]  eta: 0:02:48  lr: 0.000120  loss: 156.9091 (159.7967)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [290/431]  eta: 0:02:37  lr: 0.000120  loss: 149.4921 (159.4103)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [300/431]  eta: 0:02:26  lr: 0.000120  loss: 149.4921 (159.3109)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [310/431]  eta: 0:02:14  lr: 0.000120  loss: 149.9411 (159.0253)  time: 1.1152  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [320/431]  eta: 0:02:03  lr: 0.000120  loss: 149.1295 (158.6412)  time: 1.1150  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [330/431]  eta: 0:01:52  lr: 0.000120  loss: 154.9470 (158.8166)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [340/431]  eta: 0:01:41  lr: 0.000120  loss: 165.0331 (159.0515)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [350/431]  eta: 0:01:30  lr: 0.000120  loss: 165.1966 (159.0259)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [360/431]  eta: 0:01:19  lr: 0.000120  loss: 159.1712 (158.9292)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [370/431]  eta: 0:01:08  lr: 0.000120  loss: 153.1365 (158.7215)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [380/431]  eta: 0:00:56  lr: 0.000120  loss: 153.1365 (158.5876)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [390/431]  eta: 0:00:45  lr: 0.000120  loss: 152.0411 (158.3769)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [400/431]  eta: 0:00:34  lr: 0.000120  loss: 149.4302 (158.2633)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [410/431]  eta: 0:00:23  lr: 0.000120  loss: 150.8547 (158.0504)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:7]  [420/431]  eta: 0:00:12  lr: 0.000120  loss: 150.8547 (157.9497)  time: 1.1123  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7]  [430/431]  eta: 0:00:01  lr: 0.000120  loss: 156.0817 (158.0475)  time: 1.1164  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:7] Total time: 0:08:01 (1.1162 s / it)\n",
      "Averaged stats: lr: 0.000120  loss: 156.0817 (158.0475)\n",
      "Valid: [epoch:7]  [ 0/14]  eta: 0:00:32  loss: 126.8121 (126.8121)  time: 2.2942  data: 2.1463  max mem: 15925\n",
      "Valid: [epoch:7]  [13/14]  eta: 0:00:00  loss: 152.7003 (157.0475)  time: 0.2672  data: 0.1534  max mem: 15925\n",
      "Valid: [epoch:7] Total time: 0:00:03 (0.2822 s / it)\n",
      "Averaged stats: loss: 152.7003 (157.0475)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_7_input_n_20.png\n",
      "loss of the network on the 14 valid images: 157.047%\n",
      "Min loss: 157.047\n",
      "Best Epoch: 7.000\n",
      "Train: [epoch:8]  [  0/431]  eta: 0:27:13  lr: 0.000140  loss: 159.2939 (159.2939)  time: 3.7899  data: 2.5766  max mem: 15925\n",
      "Train: [epoch:8]  [ 10/431]  eta: 0:09:04  lr: 0.000140  loss: 170.1554 (167.0037)  time: 1.2925  data: 0.2345  max mem: 15925\n",
      "Train: [epoch:8]  [ 20/431]  eta: 0:08:11  lr: 0.000140  loss: 165.9044 (160.6456)  time: 1.0667  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [ 30/431]  eta: 0:07:44  lr: 0.000140  loss: 147.1896 (155.8910)  time: 1.0864  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [ 40/431]  eta: 0:07:28  lr: 0.000140  loss: 145.2228 (153.7919)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [ 50/431]  eta: 0:07:14  lr: 0.000140  loss: 144.4634 (151.3138)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [ 60/431]  eta: 0:07:01  lr: 0.000140  loss: 138.9534 (149.1599)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [ 70/431]  eta: 0:06:49  lr: 0.000140  loss: 140.5481 (149.1584)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [ 80/431]  eta: 0:06:36  lr: 0.000140  loss: 154.3138 (149.9438)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [ 90/431]  eta: 0:06:25  lr: 0.000140  loss: 145.6664 (148.8522)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [100/431]  eta: 0:06:13  lr: 0.000140  loss: 141.6890 (148.3069)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [110/431]  eta: 0:06:01  lr: 0.000140  loss: 141.8849 (147.6438)  time: 1.1025  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8]  [120/431]  eta: 0:05:49  lr: 0.000140  loss: 142.5497 (147.3607)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [130/431]  eta: 0:05:38  lr: 0.000140  loss: 146.3683 (147.5305)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [140/431]  eta: 0:05:27  lr: 0.000140  loss: 146.0161 (146.9558)  time: 1.1181  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8]  [150/431]  eta: 0:05:15  lr: 0.000140  loss: 142.1880 (147.0418)  time: 1.1184  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [160/431]  eta: 0:05:04  lr: 0.000140  loss: 149.2945 (147.2274)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [170/431]  eta: 0:04:52  lr: 0.000140  loss: 145.4427 (147.0273)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [180/431]  eta: 0:04:41  lr: 0.000140  loss: 144.1938 (146.8672)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [190/431]  eta: 0:04:30  lr: 0.000140  loss: 147.6880 (147.0513)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [200/431]  eta: 0:04:18  lr: 0.000140  loss: 146.7062 (146.7446)  time: 1.1175  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8]  [210/431]  eta: 0:04:07  lr: 0.000140  loss: 144.1157 (146.6987)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [220/431]  eta: 0:03:56  lr: 0.000140  loss: 146.9755 (146.7872)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [230/431]  eta: 0:03:45  lr: 0.000140  loss: 148.2254 (146.9438)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [240/431]  eta: 0:03:34  lr: 0.000140  loss: 148.2254 (146.7802)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [250/431]  eta: 0:03:22  lr: 0.000140  loss: 146.9594 (146.9843)  time: 1.1033  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8]  [260/431]  eta: 0:03:11  lr: 0.000140  loss: 151.1037 (147.0474)  time: 1.1124  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8]  [270/431]  eta: 0:03:00  lr: 0.000140  loss: 151.7030 (147.2735)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [280/431]  eta: 0:02:49  lr: 0.000140  loss: 148.8498 (147.2537)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [290/431]  eta: 0:02:37  lr: 0.000140  loss: 147.6974 (147.3318)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [300/431]  eta: 0:02:26  lr: 0.000140  loss: 157.5225 (147.7241)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [310/431]  eta: 0:02:15  lr: 0.000140  loss: 149.6659 (147.4028)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [320/431]  eta: 0:02:04  lr: 0.000140  loss: 138.8004 (147.1425)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [330/431]  eta: 0:01:52  lr: 0.000140  loss: 146.2566 (147.4599)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [340/431]  eta: 0:01:41  lr: 0.000140  loss: 154.5180 (147.6722)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [350/431]  eta: 0:01:30  lr: 0.000140  loss: 152.9503 (147.8432)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [360/431]  eta: 0:01:19  lr: 0.000140  loss: 142.1535 (147.6275)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [370/431]  eta: 0:01:08  lr: 0.000140  loss: 143.4111 (147.6340)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [380/431]  eta: 0:00:57  lr: 0.000140  loss: 148.5985 (147.6521)  time: 1.1162  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8]  [390/431]  eta: 0:00:45  lr: 0.000140  loss: 147.4631 (147.6256)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [400/431]  eta: 0:00:34  lr: 0.000140  loss: 147.3703 (147.6129)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:8]  [410/431]  eta: 0:00:23  lr: 0.000140  loss: 144.8586 (147.5787)  time: 1.1102  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8]  [420/431]  eta: 0:00:12  lr: 0.000140  loss: 140.4876 (147.4462)  time: 1.1100  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8]  [430/431]  eta: 0:00:01  lr: 0.000140  loss: 142.7414 (147.4021)  time: 1.1139  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:8] Total time: 0:08:01 (1.1181 s / it)\n",
      "Averaged stats: lr: 0.000140  loss: 142.7414 (147.4021)\n",
      "Valid: [epoch:8]  [ 0/14]  eta: 0:00:30  loss: 166.9052 (166.9052)  time: 2.1930  data: 2.0119  max mem: 15925\n",
      "Valid: [epoch:8]  [13/14]  eta: 0:00:00  loss: 141.9977 (148.1632)  time: 0.2442  data: 0.1438  max mem: 15925\n",
      "Valid: [epoch:8] Total time: 0:00:03 (0.2593 s / it)\n",
      "Averaged stats: loss: 141.9977 (148.1632)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_8_input_n_20.png\n",
      "loss of the network on the 14 valid images: 148.163%\n",
      "Min loss: 148.163\n",
      "Best Epoch: 8.000\n",
      "Train: [epoch:9]  [  0/431]  eta: 0:29:12  lr: 0.000160  loss: 179.3180 (179.3180)  time: 4.0662  data: 2.6352  max mem: 15925\n",
      "Train: [epoch:9]  [ 10/431]  eta: 0:09:10  lr: 0.000160  loss: 145.9616 (148.4556)  time: 1.3079  data: 0.2397  max mem: 15925\n",
      "Train: [epoch:9]  [ 20/431]  eta: 0:08:12  lr: 0.000160  loss: 145.7668 (148.0826)  time: 1.0538  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [ 30/431]  eta: 0:07:47  lr: 0.000160  loss: 140.6898 (144.5841)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [ 40/431]  eta: 0:07:31  lr: 0.000160  loss: 140.6898 (143.5318)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [ 50/431]  eta: 0:07:14  lr: 0.000160  loss: 141.2997 (141.8595)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [ 60/431]  eta: 0:07:02  lr: 0.000160  loss: 129.1985 (139.6339)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [ 70/431]  eta: 0:06:49  lr: 0.000160  loss: 136.1129 (140.2539)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [ 80/431]  eta: 0:06:36  lr: 0.000160  loss: 145.8387 (141.1240)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [ 90/431]  eta: 0:06:24  lr: 0.000160  loss: 140.6956 (140.0593)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [100/431]  eta: 0:06:12  lr: 0.000160  loss: 128.7756 (139.8700)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [110/431]  eta: 0:06:00  lr: 0.000160  loss: 128.7756 (138.7259)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [120/431]  eta: 0:05:48  lr: 0.000160  loss: 132.5388 (138.4375)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [130/431]  eta: 0:05:37  lr: 0.000160  loss: 138.8356 (138.7233)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [140/431]  eta: 0:05:26  lr: 0.000160  loss: 138.2146 (138.5083)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [150/431]  eta: 0:05:14  lr: 0.000160  loss: 135.1992 (138.4030)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [160/431]  eta: 0:05:03  lr: 0.000160  loss: 141.8002 (138.8760)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [170/431]  eta: 0:04:52  lr: 0.000160  loss: 141.8002 (138.7088)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [180/431]  eta: 0:04:41  lr: 0.000160  loss: 134.5410 (138.5192)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [190/431]  eta: 0:04:29  lr: 0.000160  loss: 143.8228 (139.3819)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [200/431]  eta: 0:04:18  lr: 0.000160  loss: 143.2861 (139.1447)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [210/431]  eta: 0:04:07  lr: 0.000160  loss: 137.5934 (139.0816)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [220/431]  eta: 0:03:56  lr: 0.000160  loss: 134.2857 (138.9281)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [230/431]  eta: 0:03:44  lr: 0.000160  loss: 140.4642 (139.3252)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [240/431]  eta: 0:03:33  lr: 0.000160  loss: 146.0474 (139.6035)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [250/431]  eta: 0:03:22  lr: 0.000160  loss: 140.8189 (139.6163)  time: 1.1037  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:9]  [260/431]  eta: 0:03:10  lr: 0.000160  loss: 142.3451 (139.8595)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [270/431]  eta: 0:02:59  lr: 0.000160  loss: 144.2337 (140.1207)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [280/431]  eta: 0:02:48  lr: 0.000160  loss: 138.0312 (140.0066)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [290/431]  eta: 0:02:37  lr: 0.000160  loss: 138.0622 (140.1399)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [300/431]  eta: 0:02:26  lr: 0.000160  loss: 146.5413 (140.4172)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [310/431]  eta: 0:02:15  lr: 0.000160  loss: 142.0863 (140.3223)  time: 1.1188  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [320/431]  eta: 0:02:03  lr: 0.000160  loss: 134.5919 (140.0907)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [330/431]  eta: 0:01:52  lr: 0.000160  loss: 139.7523 (140.3869)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [340/431]  eta: 0:01:41  lr: 0.000160  loss: 149.6195 (140.5918)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [350/431]  eta: 0:01:30  lr: 0.000160  loss: 146.4002 (140.8269)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [360/431]  eta: 0:01:19  lr: 0.000160  loss: 143.0235 (140.7271)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [370/431]  eta: 0:01:08  lr: 0.000160  loss: 136.1263 (140.7250)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [380/431]  eta: 0:00:56  lr: 0.000160  loss: 142.4869 (140.8197)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [390/431]  eta: 0:00:45  lr: 0.000160  loss: 142.4869 (140.7651)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [400/431]  eta: 0:00:34  lr: 0.000160  loss: 138.4120 (140.6283)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:9]  [410/431]  eta: 0:00:23  lr: 0.000160  loss: 137.4608 (140.5858)  time: 1.1186  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:9]  [420/431]  eta: 0:00:12  lr: 0.000160  loss: 141.0895 (140.6370)  time: 1.1267  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:9]  [430/431]  eta: 0:00:01  lr: 0.000160  loss: 143.4493 (140.6509)  time: 1.1132  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:9] Total time: 0:08:00 (1.1159 s / it)\n",
      "Averaged stats: lr: 0.000160  loss: 143.4493 (140.6509)\n",
      "Valid: [epoch:9]  [ 0/14]  eta: 0:00:31  loss: 164.8777 (164.8777)  time: 2.2565  data: 2.1320  max mem: 15925\n",
      "Valid: [epoch:9]  [13/14]  eta: 0:00:00  loss: 134.1212 (142.2515)  time: 0.2389  data: 0.1523  max mem: 15925\n",
      "Valid: [epoch:9] Total time: 0:00:03 (0.2528 s / it)\n",
      "Averaged stats: loss: 134.1212 (142.2515)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_9_input_n_20.png\n",
      "loss of the network on the 14 valid images: 142.251%\n",
      "Min loss: 142.251\n",
      "Best Epoch: 9.000\n",
      "Train: [epoch:10]  [  0/431]  eta: 0:26:43  lr: 0.000180  loss: 134.7628 (134.7628)  time: 3.7200  data: 2.4783  max mem: 15925\n",
      "Train: [epoch:10]  [ 10/431]  eta: 0:09:11  lr: 0.000180  loss: 155.6186 (153.0472)  time: 1.3102  data: 0.2255  max mem: 15925\n",
      "Train: [epoch:10]  [ 20/431]  eta: 0:08:11  lr: 0.000180  loss: 147.0747 (146.1407)  time: 1.0695  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [ 30/431]  eta: 0:07:45  lr: 0.000180  loss: 135.6940 (141.9252)  time: 1.0802  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [ 40/431]  eta: 0:07:28  lr: 0.000180  loss: 133.8047 (139.4888)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [ 50/431]  eta: 0:07:13  lr: 0.000180  loss: 132.0458 (137.5394)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [ 60/431]  eta: 0:06:59  lr: 0.000180  loss: 129.4913 (135.5237)  time: 1.0974  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [ 70/431]  eta: 0:06:47  lr: 0.000180  loss: 132.5712 (136.5149)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [ 80/431]  eta: 0:06:35  lr: 0.000180  loss: 138.5957 (135.9009)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [ 90/431]  eta: 0:06:23  lr: 0.000180  loss: 131.7280 (135.0007)  time: 1.1036  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [100/431]  eta: 0:06:11  lr: 0.000180  loss: 132.8880 (134.5741)  time: 1.1105  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [110/431]  eta: 0:05:59  lr: 0.000180  loss: 132.8880 (133.9561)  time: 1.1090  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [120/431]  eta: 0:05:48  lr: 0.000180  loss: 135.7364 (134.1645)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [130/431]  eta: 0:05:36  lr: 0.000180  loss: 138.2351 (134.0308)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [140/431]  eta: 0:05:25  lr: 0.000180  loss: 132.1674 (133.8295)  time: 1.1178  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [150/431]  eta: 0:05:14  lr: 0.000180  loss: 137.8155 (134.3494)  time: 1.1070  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [160/431]  eta: 0:05:03  lr: 0.000180  loss: 141.3881 (134.5868)  time: 1.1082  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [170/431]  eta: 0:04:51  lr: 0.000180  loss: 138.5018 (134.7482)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [180/431]  eta: 0:04:40  lr: 0.000180  loss: 138.4084 (134.9874)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [190/431]  eta: 0:04:29  lr: 0.000180  loss: 141.0300 (135.4108)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [200/431]  eta: 0:04:18  lr: 0.000180  loss: 140.6391 (135.3563)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [210/431]  eta: 0:04:06  lr: 0.000180  loss: 132.2699 (135.3345)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [220/431]  eta: 0:03:55  lr: 0.000180  loss: 133.1639 (135.3574)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [230/431]  eta: 0:03:44  lr: 0.000180  loss: 136.3816 (135.5336)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [240/431]  eta: 0:03:33  lr: 0.000180  loss: 145.0132 (135.9698)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [250/431]  eta: 0:03:22  lr: 0.000180  loss: 140.2138 (135.7696)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [260/431]  eta: 0:03:10  lr: 0.000180  loss: 141.0060 (136.2770)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [270/431]  eta: 0:02:59  lr: 0.000180  loss: 147.3678 (136.5521)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [280/431]  eta: 0:02:48  lr: 0.000180  loss: 138.9576 (136.6320)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [290/431]  eta: 0:02:37  lr: 0.000180  loss: 137.9526 (136.7221)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [300/431]  eta: 0:02:26  lr: 0.000180  loss: 138.5047 (136.7601)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [310/431]  eta: 0:02:14  lr: 0.000180  loss: 138.5107 (136.6916)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [320/431]  eta: 0:02:03  lr: 0.000180  loss: 128.9564 (136.4870)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [330/431]  eta: 0:01:52  lr: 0.000180  loss: 141.8064 (136.9298)  time: 1.1160  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:10]  [340/431]  eta: 0:01:41  lr: 0.000180  loss: 145.8360 (137.3446)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [350/431]  eta: 0:01:30  lr: 0.000180  loss: 144.9820 (137.4749)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [360/431]  eta: 0:01:19  lr: 0.000180  loss: 134.0075 (137.4432)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [370/431]  eta: 0:01:08  lr: 0.000180  loss: 131.1291 (137.2112)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [380/431]  eta: 0:00:56  lr: 0.000180  loss: 135.6487 (137.2304)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [390/431]  eta: 0:00:45  lr: 0.000180  loss: 141.1050 (137.3227)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [400/431]  eta: 0:00:34  lr: 0.000180  loss: 135.9460 (137.2636)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:10]  [410/431]  eta: 0:00:23  lr: 0.000180  loss: 136.4810 (137.2970)  time: 1.1113  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [420/431]  eta: 0:00:12  lr: 0.000180  loss: 136.4246 (137.2599)  time: 1.1192  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10]  [430/431]  eta: 0:00:01  lr: 0.000180  loss: 140.5490 (137.3592)  time: 1.1068  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:10] Total time: 0:08:00 (1.1149 s / it)\n",
      "Averaged stats: lr: 0.000180  loss: 140.5490 (137.3592)\n",
      "Valid: [epoch:10]  [ 0/14]  eta: 0:00:31  loss: 159.7414 (159.7414)  time: 2.2319  data: 2.0826  max mem: 15925\n",
      "Valid: [epoch:10]  [13/14]  eta: 0:00:00  loss: 133.5488 (141.2190)  time: 0.2402  data: 0.1489  max mem: 15925\n",
      "Valid: [epoch:10] Total time: 0:00:03 (0.2568 s / it)\n",
      "Averaged stats: loss: 133.5488 (141.2190)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_10_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 141.219%\n",
      "Min loss: 141.219\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:11]  [  0/431]  eta: 0:34:49  lr: 0.000200  loss: 142.6008 (142.6008)  time: 4.8470  data: 3.6432  max mem: 15925\n",
      "Train: [epoch:11]  [ 10/431]  eta: 0:09:44  lr: 0.000200  loss: 149.9745 (145.6827)  time: 1.3889  data: 0.3315  max mem: 15925\n",
      "Train: [epoch:11]  [ 20/431]  eta: 0:08:24  lr: 0.000200  loss: 140.6336 (142.3868)  time: 1.0455  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [ 30/431]  eta: 0:07:55  lr: 0.000200  loss: 130.9291 (136.4883)  time: 1.0737  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [ 40/431]  eta: 0:07:34  lr: 0.000200  loss: 126.0251 (134.6736)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [ 50/431]  eta: 0:07:19  lr: 0.000200  loss: 126.7883 (132.7746)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [ 60/431]  eta: 0:07:04  lr: 0.000200  loss: 129.1645 (132.9499)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 134.5101 (133.3609)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 135.3640 (133.3465)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 134.2154 (133.2485)  time: 1.1142  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:11]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 126.8480 (132.4914)  time: 1.1116  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:11]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 124.6351 (131.9400)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 128.3757 (131.7756)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 131.8624 (131.5932)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 131.4931 (131.6975)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 130.6530 (131.9843)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 133.7176 (132.3638)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 131.7737 (132.1413)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 128.7215 (132.2720)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 131.9166 (132.1847)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 137.7254 (132.7906)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 139.6533 (132.7824)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 133.9680 (133.0082)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 141.3139 (133.5184)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 141.6844 (133.8366)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 139.0094 (134.1325)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 139.2157 (134.4755)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 148.7501 (135.0249)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 139.8198 (134.8952)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 130.3203 (134.8614)  time: 1.1056  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:11]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 138.5929 (135.1271)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 135.1677 (134.9848)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 127.7395 (134.7447)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 138.8520 (135.1857)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 143.1537 (135.3875)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 147.3294 (135.8241)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 143.9648 (135.7633)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 138.4136 (135.8725)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 140.7800 (135.9505)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 136.9210 (135.9690)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 137.0097 (136.0989)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:11]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 139.2696 (136.2007)  time: 1.1097  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:11]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.8539 (136.2188)  time: 1.1149  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:11]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 137.0373 (136.2054)  time: 1.1243  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:11] Total time: 0:08:01 (1.1177 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 137.0373 (136.2054)\n",
      "Valid: [epoch:11]  [ 0/14]  eta: 0:00:30  loss: 161.7019 (161.7019)  time: 2.2051  data: 2.0617  max mem: 15925\n",
      "Valid: [epoch:11]  [13/14]  eta: 0:00:00  loss: 133.0892 (140.2234)  time: 0.2344  data: 0.1473  max mem: 15925\n",
      "Valid: [epoch:11] Total time: 0:00:03 (0.2497 s / it)\n",
      "Averaged stats: loss: 133.0892 (140.2234)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_11_input_n_20.png\n",
      "loss of the network on the 14 valid images: 140.223%\n",
      "Min loss: 140.223\n",
      "Best Epoch: 11.000\n",
      "Train: [epoch:12]  [  0/431]  eta: 0:35:01  lr: 0.000200  loss: 157.6961 (157.6961)  time: 4.8749  data: 3.6976  max mem: 15925\n",
      "Train: [epoch:12]  [ 10/431]  eta: 0:09:52  lr: 0.000200  loss: 150.5726 (149.9046)  time: 1.4065  data: 0.3364  max mem: 15925\n",
      "Train: [epoch:12]  [ 20/431]  eta: 0:08:31  lr: 0.000200  loss: 145.2144 (142.6424)  time: 1.0630  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [ 30/431]  eta: 0:07:58  lr: 0.000200  loss: 131.7372 (138.9878)  time: 1.0741  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [ 40/431]  eta: 0:07:36  lr: 0.000200  loss: 131.7372 (137.4159)  time: 1.0870  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:12]  [ 50/431]  eta: 0:07:21  lr: 0.000200  loss: 125.8019 (134.2809)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [ 60/431]  eta: 0:07:07  lr: 0.000200  loss: 123.1851 (133.2045)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [ 70/431]  eta: 0:06:53  lr: 0.000200  loss: 132.4358 (133.9099)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 140.8309 (134.7508)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 132.4205 (133.9780)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 129.9833 (133.7957)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 134.0896 (133.5515)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 129.7229 (133.0143)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 129.7229 (133.0464)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [140/431]  eta: 0:05:28  lr: 0.000200  loss: 137.9587 (132.9131)  time: 1.1207  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 139.0407 (133.4973)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 140.7521 (133.8422)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 134.3363 (133.8270)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 137.5340 (134.1099)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 143.3237 (134.6165)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 141.7692 (134.6536)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 131.9478 (134.3839)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 129.5629 (134.3525)  time: 1.1196  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:12]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 128.6126 (134.3766)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 132.1801 (134.2120)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 133.7385 (134.2843)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 142.4559 (134.8077)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 147.7924 (135.0636)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 134.5318 (135.0530)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 134.5318 (135.1102)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 136.9816 (135.3602)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 139.4651 (135.2903)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 130.6523 (134.9119)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 130.8832 (135.1553)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 141.0880 (135.3928)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 141.0880 (135.6003)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 136.3244 (135.5769)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 136.5846 (135.6203)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 136.5728 (135.6481)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 134.9320 (135.5985)  time: 1.1310  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 139.6618 (135.6828)  time: 1.1267  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 135.9607 (135.6982)  time: 1.1260  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:12]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 132.0403 (135.6836)  time: 1.1324  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:12]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 134.4431 (135.5726)  time: 1.1223  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:12] Total time: 0:08:03 (1.1213 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 134.4431 (135.5726)\n",
      "Valid: [epoch:12]  [ 0/14]  eta: 0:00:32  loss: 129.0478 (129.0478)  time: 2.3299  data: 2.1751  max mem: 15925\n",
      "Valid: [epoch:12]  [13/14]  eta: 0:00:00  loss: 133.1343 (139.8026)  time: 0.2431  data: 0.1554  max mem: 15925\n",
      "Valid: [epoch:12] Total time: 0:00:03 (0.2583 s / it)\n",
      "Averaged stats: loss: 133.1343 (139.8026)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_12_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.803%\n",
      "Min loss: 139.803\n",
      "Best Epoch: 12.000\n",
      "Train: [epoch:13]  [  0/431]  eta: 0:29:33  lr: 0.000200  loss: 134.8877 (134.8877)  time: 4.1158  data: 2.8572  max mem: 15925\n",
      "Train: [epoch:13]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 144.4828 (145.1498)  time: 1.3398  data: 0.2599  max mem: 15925\n",
      "Train: [epoch:13]  [ 20/431]  eta: 0:08:15  lr: 0.000200  loss: 138.3427 (140.7581)  time: 1.0592  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [ 30/431]  eta: 0:07:47  lr: 0.000200  loss: 136.0810 (137.2841)  time: 1.0718  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:13]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 129.3071 (135.5723)  time: 1.0925  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 125.1296 (133.4214)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 125.0789 (132.0549)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 133.0548 (132.7675)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [ 80/431]  eta: 0:06:38  lr: 0.000200  loss: 133.3801 (132.6752)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 130.9382 (132.5566)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 135.1313 (132.9494)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 128.7746 (132.0666)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 132.5249 (132.5400)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 134.4510 (132.3733)  time: 1.1308  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [140/431]  eta: 0:05:29  lr: 0.000200  loss: 129.6100 (132.2080)  time: 1.1392  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [150/431]  eta: 0:05:17  lr: 0.000200  loss: 134.7223 (132.7259)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [160/431]  eta: 0:05:06  lr: 0.000200  loss: 135.5548 (132.8910)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [170/431]  eta: 0:04:55  lr: 0.000200  loss: 132.3589 (132.6466)  time: 1.1343  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [180/431]  eta: 0:04:43  lr: 0.000200  loss: 132.3589 (132.6428)  time: 1.1379  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [190/431]  eta: 0:04:32  lr: 0.000200  loss: 129.2199 (132.3474)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [200/431]  eta: 0:04:21  lr: 0.000200  loss: 129.2199 (132.3317)  time: 1.1327  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [210/431]  eta: 0:04:09  lr: 0.000200  loss: 133.7484 (132.4046)  time: 1.1330  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [220/431]  eta: 0:03:58  lr: 0.000200  loss: 133.7484 (132.6181)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [230/431]  eta: 0:03:46  lr: 0.000200  loss: 134.3285 (132.7237)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [240/431]  eta: 0:03:35  lr: 0.000200  loss: 140.6089 (133.1377)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [250/431]  eta: 0:03:24  lr: 0.000200  loss: 140.6089 (133.3050)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 137.2806 (133.5688)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [270/431]  eta: 0:03:01  lr: 0.000200  loss: 149.1661 (134.2655)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [280/431]  eta: 0:02:50  lr: 0.000200  loss: 142.3779 (134.2607)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 134.3639 (134.1260)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [300/431]  eta: 0:02:27  lr: 0.000200  loss: 135.9915 (134.3679)  time: 1.1109  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [310/431]  eta: 0:02:16  lr: 0.000200  loss: 133.6789 (134.2241)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 129.0676 (133.8762)  time: 1.1279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 135.4995 (134.3337)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 152.3446 (134.7148)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [350/431]  eta: 0:01:31  lr: 0.000200  loss: 151.1160 (135.0622)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 139.9441 (135.1194)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 137.2764 (135.2263)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 137.6008 (135.3710)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [390/431]  eta: 0:00:46  lr: 0.000200  loss: 137.5469 (135.3169)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 129.0428 (135.2071)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:13]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 129.0878 (135.2658)  time: 1.1211  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:13]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 140.4763 (135.3149)  time: 1.1184  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:13]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 140.4763 (135.4486)  time: 1.1153  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:13] Total time: 0:08:04 (1.1244 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 140.4763 (135.4486)\n",
      "Valid: [epoch:13]  [ 0/14]  eta: 0:00:31  loss: 161.5066 (161.5066)  time: 2.2302  data: 2.0655  max mem: 15925\n",
      "Valid: [epoch:13]  [13/14]  eta: 0:00:00  loss: 133.2528 (139.8119)  time: 0.2424  data: 0.1476  max mem: 15925\n",
      "Valid: [epoch:13] Total time: 0:00:03 (0.2586 s / it)\n",
      "Averaged stats: loss: 133.2528 (139.8119)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_13_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.812%\n",
      "Min loss: 139.803\n",
      "Best Epoch: 12.000\n",
      "Train: [epoch:14]  [  0/431]  eta: 0:33:16  lr: 0.000200  loss: 157.3875 (157.3875)  time: 4.6334  data: 3.4949  max mem: 15925\n",
      "Train: [epoch:14]  [ 10/431]  eta: 0:09:35  lr: 0.000200  loss: 152.8445 (149.8195)  time: 1.3679  data: 0.3179  max mem: 15925\n",
      "Train: [epoch:14]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 145.8722 (144.1606)  time: 1.0478  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 131.3432 (139.7381)  time: 1.0691  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:14]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 127.0983 (136.4859)  time: 1.0923  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:14]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 124.4533 (133.7344)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 119.3446 (131.3046)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 127.6596 (131.8046)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 138.9199 (133.3434)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 140.0260 (133.5520)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 133.5144 (133.1592)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 128.8485 (132.1174)  time: 1.1072  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:14]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 130.5747 (132.2719)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 136.6764 (132.4548)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 136.1359 (132.5584)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 137.5969 (133.0114)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 140.2870 (133.4289)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 127.1162 (132.8958)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 126.1111 (132.9790)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 137.5341 (133.5458)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 136.8136 (133.5056)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 134.5325 (133.4598)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 135.7141 (133.7358)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 139.1017 (133.8343)  time: 1.1245  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 141.3266 (134.2139)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 141.3266 (134.4108)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 139.4209 (134.6327)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 143.2181 (134.9788)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 133.4161 (134.8169)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 128.1686 (134.6820)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 139.4615 (134.9039)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 135.8435 (134.8488)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 128.0837 (134.6213)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 136.9079 (135.0737)  time: 1.1303  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 149.2455 (135.4952)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 145.7995 (135.7075)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 135.5290 (135.7174)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 135.3016 (135.7423)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 137.6973 (135.9224)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 140.9726 (135.9248)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 132.2379 (135.8489)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 133.4021 (135.8459)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:14]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 133.7830 (135.8980)  time: 1.1230  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:14]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 130.5792 (135.8585)  time: 1.1123  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:14] Total time: 0:08:03 (1.1207 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 130.5792 (135.8585)\n",
      "Valid: [epoch:14]  [ 0/14]  eta: 0:00:33  loss: 161.1376 (161.1376)  time: 2.3952  data: 2.2605  max mem: 15925\n",
      "Valid: [epoch:14]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7769)  time: 0.2772  data: 0.1616  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:14] Total time: 0:00:04 (0.2922 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7769)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_14_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.777%\n",
      "Min loss: 139.777\n",
      "Best Epoch: 14.000\n",
      "Train: [epoch:15]  [  0/431]  eta: 0:34:06  lr: 0.000200  loss: 154.0790 (154.0790)  time: 4.7489  data: 3.4564  max mem: 15925\n",
      "Train: [epoch:15]  [ 10/431]  eta: 0:09:40  lr: 0.000200  loss: 150.6215 (150.7131)  time: 1.3799  data: 0.3144  max mem: 15925\n",
      "Train: [epoch:15]  [ 20/431]  eta: 0:08:29  lr: 0.000200  loss: 143.5360 (143.6504)  time: 1.0652  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [ 30/431]  eta: 0:07:57  lr: 0.000200  loss: 130.1169 (138.1779)  time: 1.0858  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [ 40/431]  eta: 0:07:37  lr: 0.000200  loss: 124.5844 (134.3246)  time: 1.0968  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:15]  [ 50/431]  eta: 0:07:21  lr: 0.000200  loss: 127.2772 (133.1228)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [ 60/431]  eta: 0:07:06  lr: 0.000200  loss: 130.8777 (133.3410)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [ 70/431]  eta: 0:06:53  lr: 0.000200  loss: 133.8455 (133.6463)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [ 80/431]  eta: 0:06:40  lr: 0.000200  loss: 136.0883 (134.4757)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [ 90/431]  eta: 0:06:28  lr: 0.000200  loss: 136.2174 (134.2820)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [100/431]  eta: 0:06:16  lr: 0.000200  loss: 130.9200 (133.7021)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [110/431]  eta: 0:06:04  lr: 0.000200  loss: 126.8915 (133.2482)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [120/431]  eta: 0:05:52  lr: 0.000200  loss: 130.8927 (133.0345)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 133.2606 (133.0006)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [140/431]  eta: 0:05:29  lr: 0.000200  loss: 136.1030 (133.5688)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [150/431]  eta: 0:05:17  lr: 0.000200  loss: 136.1315 (133.4592)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [160/431]  eta: 0:05:06  lr: 0.000200  loss: 134.7427 (133.6958)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [170/431]  eta: 0:04:54  lr: 0.000200  loss: 135.0568 (133.8358)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 136.9987 (134.0192)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 134.7130 (134.0541)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [200/431]  eta: 0:04:20  lr: 0.000200  loss: 131.4406 (133.9880)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 131.6121 (133.6347)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 134.1630 (133.9267)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [230/431]  eta: 0:03:46  lr: 0.000200  loss: 139.9032 (134.1689)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 137.7993 (134.2983)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 138.1454 (134.4511)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 138.1454 (134.6834)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [270/431]  eta: 0:03:01  lr: 0.000200  loss: 139.2412 (135.1827)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 133.1034 (135.0321)  time: 1.1244  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 129.3949 (135.0825)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [300/431]  eta: 0:02:27  lr: 0.000200  loss: 142.8242 (135.3479)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [310/431]  eta: 0:02:16  lr: 0.000200  loss: 140.7921 (135.1185)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 123.2228 (134.8301)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 135.8052 (135.1946)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 146.4173 (135.4498)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 143.7667 (135.5557)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 140.5953 (135.4827)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 134.3514 (135.4199)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 133.1841 (135.3582)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 135.8331 (135.4464)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.0071 (135.4157)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.9498 (135.5189)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:15]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.9498 (135.4187)  time: 1.1203  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:15]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 134.0100 (135.3993)  time: 1.1163  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:15] Total time: 0:08:03 (1.1215 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 134.0100 (135.3993)\n",
      "Valid: [epoch:15]  [ 0/14]  eta: 0:00:31  loss: 161.1376 (161.1376)  time: 2.2818  data: 2.1291  max mem: 15925\n",
      "Valid: [epoch:15]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7275)  time: 0.2412  data: 0.1521  max mem: 15925\n",
      "Valid: [epoch:15] Total time: 0:00:03 (0.2597 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7275)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_15_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.728%\n",
      "Min loss: 139.728\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:16]  [  0/431]  eta: 0:31:05  lr: 0.000200  loss: 133.1857 (133.1857)  time: 4.3290  data: 3.1273  max mem: 15925\n",
      "Train: [epoch:16]  [ 10/431]  eta: 0:09:31  lr: 0.000200  loss: 148.4185 (150.8223)  time: 1.3568  data: 0.2845  max mem: 15925\n",
      "Train: [epoch:16]  [ 20/431]  eta: 0:08:22  lr: 0.000200  loss: 147.5285 (144.8102)  time: 1.0676  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 132.6604 (140.2554)  time: 1.0838  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 131.3973 (137.4943)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [ 50/431]  eta: 0:07:18  lr: 0.000200  loss: 128.6365 (136.0835)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [ 60/431]  eta: 0:07:04  lr: 0.000200  loss: 127.1009 (134.0293)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 128.2053 (134.3353)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [ 80/431]  eta: 0:06:38  lr: 0.000200  loss: 133.8653 (133.9181)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 132.0332 (133.8529)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 132.0691 (133.7496)  time: 1.1006  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 132.5683 (133.4997)  time: 1.1029  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 132.9458 (133.4609)  time: 1.1172  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 133.8710 (133.5861)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 136.5041 (133.5958)  time: 1.1150  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 133.5849 (133.7953)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 137.0705 (133.9730)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 134.8540 (133.8945)  time: 1.1225  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 134.8540 (134.0238)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 138.1392 (134.3112)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 137.6593 (134.3840)  time: 1.1118  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 136.8554 (134.3450)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 131.3785 (134.1305)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 131.9049 (134.2698)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 138.5381 (134.5694)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 142.0722 (134.7345)  time: 1.1042  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 134.7975 (134.8445)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 145.5551 (135.4053)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 139.1284 (135.2037)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 130.5185 (135.2548)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 137.1113 (135.3242)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 135.7678 (135.1028)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 126.2452 (134.8490)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 137.1410 (135.1982)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 145.6572 (135.4711)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 146.6251 (135.7348)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 146.6004 (135.8240)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 139.6611 (135.9395)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 137.3182 (135.8720)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 132.2972 (135.8142)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 134.5424 (135.8924)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:16]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 133.8421 (135.7107)  time: 1.1139  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 130.4664 (135.6003)  time: 1.1152  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 134.7182 (135.5634)  time: 1.1240  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:16] Total time: 0:08:01 (1.1180 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 134.7182 (135.5634)\n",
      "Valid: [epoch:16]  [ 0/14]  eta: 0:00:33  loss: 108.0685 (108.0685)  time: 2.3982  data: 2.2524  max mem: 15925\n",
      "Valid: [epoch:16]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7269)  time: 0.2692  data: 0.1610  max mem: 15925\n",
      "Valid: [epoch:16] Total time: 0:00:03 (0.2846 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7269)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_16_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.727%\n",
      "Min loss: 139.727\n",
      "Best Epoch: 16.000\n",
      "Train: [epoch:17]  [  0/431]  eta: 0:35:18  lr: 0.000200  loss: 159.1228 (159.1228)  time: 4.9146  data: 3.5186  max mem: 15925\n",
      "Train: [epoch:17]  [ 10/431]  eta: 0:09:40  lr: 0.000200  loss: 150.8996 (153.4279)  time: 1.3793  data: 0.3201  max mem: 15925\n",
      "Train: [epoch:17]  [ 20/431]  eta: 0:08:24  lr: 0.000200  loss: 147.4169 (147.7693)  time: 1.0422  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 138.8195 (143.1427)  time: 1.0710  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 127.3607 (139.2771)  time: 1.0746  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 127.3607 (137.7852)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 129.7356 (136.4620)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 132.0706 (136.5125)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 138.2990 (136.8194)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 136.8396 (135.7830)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 130.2335 (134.9773)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 127.4558 (134.4572)  time: 1.1021  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 128.3719 (134.2061)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 127.8055 (133.9661)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 126.6693 (133.7129)  time: 1.1045  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 134.7786 (134.1404)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 135.3135 (134.2453)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 131.4937 (134.0870)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 130.6858 (134.1522)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 136.9810 (134.6090)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 137.3651 (134.6020)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 128.3567 (134.4455)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 129.2596 (134.5093)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 142.1115 (134.8212)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 135.9730 (134.7577)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 134.2003 (134.7433)  time: 1.0937  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 142.6481 (135.1225)  time: 1.0978  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 144.8313 (135.4013)  time: 1.1059  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 140.4241 (135.2610)  time: 1.1053  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 133.1594 (135.2868)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 137.4941 (135.5227)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 135.2354 (135.2397)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 123.6500 (134.9691)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 137.0122 (135.4153)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 147.1225 (135.5874)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 145.3748 (135.8721)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 142.9949 (135.8956)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 128.5840 (135.6089)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 126.7780 (135.5944)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 135.0544 (135.4821)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 132.8555 (135.4550)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:17]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 132.8555 (135.4919)  time: 1.0977  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 140.7375 (135.6221)  time: 1.0998  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 140.7385 (135.7836)  time: 1.1083  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:17] Total time: 0:07:58 (1.1100 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 140.7385 (135.7836)\n",
      "Valid: [epoch:17]  [ 0/14]  eta: 0:00:31  loss: 162.5863 (162.5863)  time: 2.2427  data: 2.1124  max mem: 15925\n",
      "Valid: [epoch:17]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.8085)  time: 0.2526  data: 0.1509  max mem: 15925\n",
      "Valid: [epoch:17] Total time: 0:00:03 (0.2691 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.8085)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_17_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.808%\n",
      "Min loss: 139.727\n",
      "Best Epoch: 16.000\n",
      "Train: [epoch:18]  [  0/431]  eta: 0:31:07  lr: 0.000200  loss: 124.4248 (124.4248)  time: 4.3322  data: 3.1693  max mem: 15925\n",
      "Train: [epoch:18]  [ 10/431]  eta: 0:09:23  lr: 0.000200  loss: 147.6421 (146.8870)  time: 1.3380  data: 0.2883  max mem: 15925\n",
      "Train: [epoch:18]  [ 20/431]  eta: 0:08:12  lr: 0.000200  loss: 143.6638 (140.8972)  time: 1.0423  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [ 30/431]  eta: 0:07:45  lr: 0.000200  loss: 132.8962 (137.8240)  time: 1.0624  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [ 40/431]  eta: 0:07:26  lr: 0.000200  loss: 130.8856 (135.3243)  time: 1.0814  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [ 50/431]  eta: 0:07:11  lr: 0.000200  loss: 126.9836 (132.3609)  time: 1.0925  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 125.2683 (131.3111)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [ 70/431]  eta: 0:06:46  lr: 0.000200  loss: 131.4240 (131.9674)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 138.3238 (132.8765)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 136.5024 (133.1206)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 131.7271 (132.3647)  time: 1.1027  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:18]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 125.8138 (132.2523)  time: 1.1051  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:18]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 132.6130 (132.4939)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 135.6102 (132.8321)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 133.0383 (132.6669)  time: 1.1051  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:18]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 134.7597 (133.2362)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 141.9268 (133.5697)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 134.2181 (133.6589)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 131.4004 (133.4585)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 134.5066 (133.8365)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 137.9626 (133.9674)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 141.5117 (134.2289)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 138.0567 (134.2863)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 134.3071 (134.2791)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 140.7076 (134.5507)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 140.7168 (134.6347)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 137.1358 (134.8211)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 143.2262 (134.9899)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 133.0199 (134.8172)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 128.8286 (134.7773)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 140.6154 (135.0691)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 139.6499 (134.9224)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 126.3882 (134.7680)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 131.0569 (134.9438)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 140.0433 (135.1970)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 141.9871 (135.4595)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 138.6308 (135.4074)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 134.1357 (135.4043)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 132.4915 (135.3505)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 135.4067 (135.4249)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.4067 (135.3694)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:18]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 138.4105 (135.4724)  time: 1.0969  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:18]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 138.7630 (135.4890)  time: 1.1055  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:18]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 137.5913 (135.5666)  time: 1.1107  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:18] Total time: 0:07:58 (1.1112 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 137.5913 (135.5666)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:18]  [ 0/14]  eta: 0:00:32  loss: 160.6569 (160.6569)  time: 2.3125  data: 2.1659  max mem: 15925\n",
      "Valid: [epoch:18]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7140)  time: 0.2443  data: 0.1548  max mem: 15925\n",
      "Valid: [epoch:18] Total time: 0:00:03 (0.2620 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7140)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_18_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.714%\n",
      "Min loss: 139.714\n",
      "Best Epoch: 18.000\n",
      "Train: [epoch:19]  [  0/431]  eta: 0:29:44  lr: 0.000200  loss: 140.2858 (140.2858)  time: 4.1392  data: 3.0120  max mem: 15925\n",
      "Train: [epoch:19]  [ 10/431]  eta: 0:09:16  lr: 0.000200  loss: 150.8111 (151.9014)  time: 1.3224  data: 0.2740  max mem: 15925\n",
      "Train: [epoch:19]  [ 20/431]  eta: 0:08:11  lr: 0.000200  loss: 144.4733 (144.2271)  time: 1.0478  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 130.5336 (139.0212)  time: 1.0756  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 131.3026 (136.1803)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [ 50/431]  eta: 0:07:12  lr: 0.000200  loss: 130.7708 (135.3370)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [ 60/431]  eta: 0:06:58  lr: 0.000200  loss: 129.2359 (134.5382)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [ 70/431]  eta: 0:06:44  lr: 0.000200  loss: 135.3504 (135.2907)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 140.3958 (136.0887)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 129.5716 (134.6899)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [100/431]  eta: 0:06:09  lr: 0.000200  loss: 125.9076 (134.0582)  time: 1.1016  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:19]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 126.6430 (133.8662)  time: 1.1080  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:19]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 125.1748 (133.0779)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 124.0402 (133.0535)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 134.9693 (133.1019)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 135.0454 (133.2756)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 136.8614 (133.3128)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 132.4925 (132.9407)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 130.1868 (133.0526)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 137.9534 (133.6592)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 141.0303 (133.9059)  time: 1.1005  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:19]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 137.4665 (134.0231)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 132.2924 (134.0048)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 132.2924 (133.9968)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 137.6895 (134.2135)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 137.8860 (134.3049)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 140.3122 (134.4418)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 141.3689 (134.8057)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 139.1107 (134.7756)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 134.4083 (134.8010)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 140.3949 (135.1664)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 138.8399 (135.0857)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 131.3289 (135.0323)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 138.8670 (135.3720)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 145.0799 (135.7671)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 142.3283 (135.9767)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 138.1314 (135.9757)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 133.9079 (135.9360)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 134.1895 (136.0810)  time: 1.1016  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:19]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 134.1895 (135.9465)  time: 1.0902  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 130.1467 (135.7494)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:19]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 129.4342 (135.6412)  time: 1.1120  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:19]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 129.8608 (135.6036)  time: 1.1089  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:19]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 133.8879 (135.6194)  time: 1.1078  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:19] Total time: 0:07:57 (1.1088 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 133.8879 (135.6194)\n",
      "Valid: [epoch:19]  [ 0/14]  eta: 0:00:35  loss: 162.9339 (162.9339)  time: 2.5228  data: 2.3934  max mem: 15925\n",
      "Valid: [epoch:19]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2597  data: 0.1710  max mem: 15925\n",
      "Valid: [epoch:19] Total time: 0:00:03 (0.2730 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_19_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:20]  [  0/431]  eta: 0:29:34  lr: 0.000200  loss: 149.2052 (149.2052)  time: 4.1182  data: 2.8427  max mem: 15925\n",
      "Train: [epoch:20]  [ 10/431]  eta: 0:09:17  lr: 0.000200  loss: 163.9628 (157.4608)  time: 1.3253  data: 0.2586  max mem: 15925\n",
      "Train: [epoch:20]  [ 20/431]  eta: 0:08:12  lr: 0.000200  loss: 147.3480 (147.8120)  time: 1.0522  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 131.5399 (141.8019)  time: 1.0762  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 124.6247 (137.3192)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 124.6247 (135.0502)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 125.2566 (133.3970)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 133.4659 (133.9841)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 138.2070 (134.4952)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 136.6878 (134.3349)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 131.3678 (133.7871)  time: 1.0991  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 127.3679 (133.3283)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 128.8894 (133.4089)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 129.3395 (133.0013)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 129.4996 (133.1688)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 143.2765 (133.9587)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 141.0103 (134.0156)  time: 1.1077  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:20]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 132.2743 (133.8080)  time: 1.0997  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:20]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 132.2743 (133.7800)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 134.7793 (133.9721)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 134.0753 (133.8832)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 130.9078 (133.7787)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 130.9078 (133.7849)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 136.9031 (133.9169)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 135.8472 (133.9819)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 134.4379 (134.0104)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 144.3563 (134.5891)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 146.5911 (134.8849)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 135.6231 (134.8703)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 134.0957 (134.8188)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 136.1791 (135.0984)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 132.3549 (134.8124)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 126.9711 (134.5221)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 139.3414 (134.9130)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 145.7681 (135.1842)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 143.1258 (135.3350)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 137.3905 (135.3608)  time: 1.1085  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:20]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 139.1205 (135.4042)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 140.7220 (135.5991)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 139.2454 (135.5986)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 136.9138 (135.6160)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:20]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.9138 (135.6339)  time: 1.0992  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:20]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 135.4540 (135.6842)  time: 1.1088  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:20]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 136.1743 (135.7703)  time: 1.1094  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:20] Total time: 0:07:58 (1.1106 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 136.1743 (135.7703)\n",
      "Valid: [epoch:20]  [ 0/14]  eta: 0:00:29  loss: 158.8652 (158.8652)  time: 2.0858  data: 1.9588  max mem: 15925\n",
      "Valid: [epoch:20]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7122)  time: 0.2430  data: 0.1400  max mem: 15925\n",
      "Valid: [epoch:20] Total time: 0:00:03 (0.2573 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7122)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_20_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.712%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:21]  [  0/431]  eta: 0:30:56  lr: 0.000200  loss: 126.4462 (126.4462)  time: 4.3069  data: 3.1318  max mem: 15925\n",
      "Train: [epoch:21]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 153.8006 (147.6097)  time: 1.3419  data: 0.2849  max mem: 15925\n",
      "Train: [epoch:21]  [ 20/431]  eta: 0:08:15  lr: 0.000200  loss: 143.9542 (141.6802)  time: 1.0507  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 132.9071 (137.3967)  time: 1.0646  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [ 40/431]  eta: 0:07:28  lr: 0.000200  loss: 131.9602 (135.9585)  time: 1.0875  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 128.2639 (133.9525)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 127.7381 (132.7764)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 129.7283 (133.7458)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 133.7713 (133.2727)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 127.1121 (132.3803)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 126.0306 (131.5810)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 123.2706 (130.8072)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 127.5916 (131.0308)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 133.1728 (131.1851)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 129.3123 (130.6541)  time: 1.1064  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 131.5236 (131.0603)  time: 1.0986  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 136.4751 (131.4731)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 134.4352 (131.4447)  time: 1.1057  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 135.4072 (131.7973)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 139.9622 (132.1730)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 139.5753 (132.3527)  time: 1.1122  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 135.6398 (132.4345)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 134.8917 (132.7426)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 135.6232 (133.0228)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 140.6550 (133.5274)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 141.5244 (133.7462)  time: 1.1061  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 144.1829 (134.2235)  time: 1.1006  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 147.6052 (134.5589)  time: 1.1057  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 135.4734 (134.3661)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 133.6961 (134.2356)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 133.2789 (134.3809)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 133.2789 (134.2008)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 128.8590 (134.1148)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 133.8166 (134.4791)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 147.2599 (134.8508)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 146.9066 (135.2337)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 143.2157 (135.2744)  time: 1.1076  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 138.4087 (135.4217)  time: 1.1139  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 138.4921 (135.3878)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 138.4921 (135.4874)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.1834 (135.4338)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:21]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 137.2338 (135.5339)  time: 1.1222  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 137.4724 (135.4776)  time: 1.1075  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 138.5332 (135.5523)  time: 1.0933  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:21] Total time: 0:08:00 (1.1139 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 138.5332 (135.5523)\n",
      "Valid: [epoch:21]  [ 0/14]  eta: 0:00:32  loss: 101.8203 (101.8203)  time: 2.3337  data: 2.2088  max mem: 15925\n",
      "Valid: [epoch:21]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7187)  time: 0.2436  data: 0.1578  max mem: 15925\n",
      "Valid: [epoch:21] Total time: 0:00:03 (0.2590 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7187)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_21_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.719%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:22]  [  0/431]  eta: 0:31:35  lr: 0.000200  loss: 149.2904 (149.2904)  time: 4.3971  data: 3.2119  max mem: 15925\n",
      "Train: [epoch:22]  [ 10/431]  eta: 0:09:30  lr: 0.000200  loss: 149.2904 (148.9676)  time: 1.3557  data: 0.2922  max mem: 15925\n",
      "Train: [epoch:22]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 148.1845 (146.8407)  time: 1.0589  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [ 30/431]  eta: 0:07:52  lr: 0.000200  loss: 134.5728 (142.0451)  time: 1.0803  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 130.3275 (138.1632)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [ 50/431]  eta: 0:07:16  lr: 0.000200  loss: 130.1426 (137.2870)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 130.7937 (135.8118)  time: 1.0971  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:22]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 133.0784 (136.5779)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 140.8636 (136.1820)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 134.5912 (135.6374)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 132.8643 (135.3725)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 132.8643 (135.1379)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 133.1021 (134.8232)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 133.8917 (135.0079)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 133.7566 (134.5574)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 133.7566 (134.7319)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 135.0886 (134.3770)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 130.8004 (134.2893)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 133.4395 (134.3153)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 134.8732 (134.4152)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 134.4916 (134.1195)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 130.4167 (133.8706)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 136.8316 (134.1143)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 138.0457 (133.8976)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 136.9530 (134.2440)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 132.2488 (134.0567)  time: 1.1276  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 136.2996 (134.4849)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 146.3482 (135.0863)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 139.8204 (134.8418)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 132.5475 (134.8087)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 135.3207 (134.9398)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 134.0279 (134.9255)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 131.5930 (134.7971)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 136.2959 (135.0427)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 145.8168 (135.3195)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 142.5506 (135.4125)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 137.0786 (135.3753)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 137.0786 (135.3442)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 140.2771 (135.4712)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 140.1552 (135.4015)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.9548 (135.4248)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:22]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.6760 (135.5087)  time: 1.0998  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:22]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 135.3567 (135.4956)  time: 1.1009  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:22]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 135.6637 (135.5505)  time: 1.0982  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:22] Total time: 0:07:59 (1.1126 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 135.6637 (135.5505)\n",
      "Valid: [epoch:22]  [ 0/14]  eta: 0:00:31  loss: 119.4266 (119.4266)  time: 2.2289  data: 2.0991  max mem: 15925\n",
      "Valid: [epoch:22]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7315)  time: 0.2437  data: 0.1500  max mem: 15925\n",
      "Valid: [epoch:22] Total time: 0:00:03 (0.2619 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7315)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_22_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.732%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:23]  [  0/431]  eta: 0:32:56  lr: 0.000200  loss: 147.0324 (147.0324)  time: 4.5859  data: 3.2708  max mem: 15925\n",
      "Train: [epoch:23]  [ 10/431]  eta: 0:09:33  lr: 0.000200  loss: 149.0412 (152.3341)  time: 1.3633  data: 0.2976  max mem: 15925\n",
      "Train: [epoch:23]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 145.8279 (147.5603)  time: 1.0496  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [ 30/431]  eta: 0:07:49  lr: 0.000200  loss: 132.0303 (141.5267)  time: 1.0668  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 129.9067 (138.8269)  time: 1.0816  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 130.1622 (136.6112)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 130.1622 (135.3446)  time: 1.1065  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 124.6713 (133.6480)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 131.9311 (134.4161)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 135.3663 (134.0027)  time: 1.1136  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 132.4773 (133.8645)  time: 1.1074  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 132.6051 (133.8682)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 131.9253 (133.7013)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 127.0028 (133.0468)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 127.3009 (132.6777)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 131.4210 (132.8969)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 145.7383 (133.5427)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 142.6744 (133.5686)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 134.2160 (133.7012)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 139.6823 (134.0132)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 134.0468 (133.6156)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 129.8477 (133.6473)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 134.9220 (133.6686)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 140.3974 (133.8510)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 136.9562 (133.9241)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 136.9562 (134.1787)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 141.2106 (134.3841)  time: 1.1012  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 144.4258 (134.7449)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 141.4178 (134.6931)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 129.5796 (134.7449)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 139.5230 (134.9845)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 135.7955 (134.7071)  time: 1.1087  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 130.1437 (134.6295)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 137.0270 (135.0819)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 150.7632 (135.5691)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 150.6555 (136.0356)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 142.4017 (135.9251)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 132.6195 (135.9059)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 134.8305 (135.9368)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 133.4808 (135.8775)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 131.1157 (135.7252)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:23]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 129.8925 (135.5846)  time: 1.0944  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 135.4901 (135.7092)  time: 1.1091  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 136.8603 (135.6766)  time: 1.1143  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:23] Total time: 0:07:56 (1.1063 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 136.8603 (135.6766)\n",
      "Valid: [epoch:23]  [ 0/14]  eta: 0:00:30  loss: 160.5304 (160.5304)  time: 2.1546  data: 1.9976  max mem: 15925\n",
      "Valid: [epoch:23]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2576  data: 0.1428  max mem: 15925\n",
      "Valid: [epoch:23] Total time: 0:00:03 (0.2735 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_23_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:24]  [  0/431]  eta: 0:34:36  lr: 0.000200  loss: 148.0427 (148.0427)  time: 4.8185  data: 3.6344  max mem: 15925\n",
      "Train: [epoch:24]  [ 10/431]  eta: 0:09:45  lr: 0.000200  loss: 150.8316 (150.7514)  time: 1.3903  data: 0.3306  max mem: 15925\n",
      "Train: [epoch:24]  [ 20/431]  eta: 0:08:27  lr: 0.000200  loss: 147.3943 (144.3342)  time: 1.0550  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [ 30/431]  eta: 0:07:54  lr: 0.000200  loss: 132.9553 (138.1088)  time: 1.0704  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 132.6171 (137.1292)  time: 1.0794  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [ 50/431]  eta: 0:07:16  lr: 0.000200  loss: 132.6171 (135.6706)  time: 1.0890  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 124.6744 (133.2370)  time: 1.0956  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:24]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 124.6744 (133.7492)  time: 1.1010  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:24]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 133.1674 (133.6675)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 129.5714 (132.9643)  time: 1.0980  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:24]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 127.2859 (131.7477)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 127.1033 (131.4074)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 128.7235 (131.6012)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 133.5974 (131.8820)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 133.5974 (132.1314)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 138.6463 (132.5259)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 139.9913 (132.8649)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 133.1501 (132.6972)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 135.1366 (132.9591)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 139.1205 (133.5772)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 135.6567 (133.5353)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 134.7854 (133.7978)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 137.0929 (133.9421)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 138.5214 (134.3725)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 143.7393 (134.6295)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 136.2488 (134.7395)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 134.1421 (134.9414)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 139.6119 (135.3243)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 138.1150 (135.1694)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 131.8082 (135.1464)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 136.4701 (135.3168)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 136.6275 (135.2764)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 130.7224 (134.9223)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 131.1617 (135.2812)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 146.3833 (135.4391)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 139.3890 (135.6078)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 134.9989 (135.5532)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 133.8697 (135.5055)  time: 1.0938  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:24]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 137.4898 (135.6795)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 138.8743 (135.7367)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 137.4465 (135.7046)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:24]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 131.1772 (135.5998)  time: 1.1000  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:24]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 133.5896 (135.6575)  time: 1.0999  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:24]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 135.8372 (135.6265)  time: 1.1062  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:24] Total time: 0:07:57 (1.1080 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 135.8372 (135.6265)\n",
      "Valid: [epoch:24]  [ 0/14]  eta: 0:00:35  loss: 159.8465 (159.8465)  time: 2.5220  data: 2.3551  max mem: 15925\n",
      "Valid: [epoch:24]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2802  data: 0.1683  max mem: 15925\n",
      "Valid: [epoch:24] Total time: 0:00:04 (0.2947 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_24_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:25]  [  0/431]  eta: 0:30:35  lr: 0.000200  loss: 120.6118 (120.6118)  time: 4.2589  data: 3.1274  max mem: 15925\n",
      "Train: [epoch:25]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 146.8708 (141.6035)  time: 1.3416  data: 0.2845  max mem: 15925\n",
      "Train: [epoch:25]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 146.8708 (140.8166)  time: 1.0570  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [ 30/431]  eta: 0:07:48  lr: 0.000200  loss: 131.7316 (135.9975)  time: 1.0712  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [ 40/431]  eta: 0:07:28  lr: 0.000200  loss: 129.4140 (135.2563)  time: 1.0823  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [ 50/431]  eta: 0:07:12  lr: 0.000200  loss: 130.7689 (134.3457)  time: 1.0883  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 129.8529 (133.7742)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 130.0576 (133.9070)  time: 1.1244  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:25]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 132.8036 (134.2581)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:25]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 132.0805 (133.6042)  time: 1.1204  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:25]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 127.7381 (132.8031)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 130.5472 (132.5238)  time: 1.1136  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:25]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 130.5472 (132.2305)  time: 1.1260  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:25]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 129.1971 (132.3143)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 128.7170 (131.7716)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 131.8482 (132.5822)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 140.8279 (133.0764)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 138.0037 (132.9480)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 138.0037 (133.4490)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 141.4062 (133.9594)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 139.4718 (133.9437)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 134.5993 (133.8109)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 132.8228 (133.8269)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 134.7734 (133.9311)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 133.8333 (134.1779)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 134.8123 (134.1503)  time: 1.1076  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:25]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 135.0757 (134.3398)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 142.3091 (134.7339)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 135.6330 (134.7370)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 135.5046 (134.6782)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 140.0555 (135.0995)  time: 1.1133  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:25]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 139.2096 (134.7819)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 127.2775 (134.6139)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 133.2147 (135.0478)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 151.2364 (135.5479)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 147.9657 (135.8001)  time: 1.1199  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:25]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 134.5942 (135.6405)  time: 1.1121  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:25]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 129.5650 (135.6444)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 134.8675 (135.5800)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 134.8675 (135.5142)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 133.0165 (135.4825)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 138.0311 (135.5034)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:25]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 138.9540 (135.4569)  time: 1.1032  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:25]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 138.9000 (135.5233)  time: 1.1019  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:25] Total time: 0:08:01 (1.1176 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 138.9000 (135.5233)\n",
      "Valid: [epoch:25]  [ 0/14]  eta: 0:00:34  loss: 162.9339 (162.9339)  time: 2.4998  data: 2.3649  max mem: 15925\n",
      "Valid: [epoch:25]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2592  data: 0.1690  max mem: 15925\n",
      "Valid: [epoch:25] Total time: 0:00:03 (0.2744 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_25_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:26]  [  0/431]  eta: 0:34:22  lr: 0.000200  loss: 146.0104 (146.0104)  time: 4.7850  data: 3.5956  max mem: 15925\n",
      "Train: [epoch:26]  [ 10/431]  eta: 0:09:45  lr: 0.000200  loss: 142.1680 (143.2213)  time: 1.3910  data: 0.3271  max mem: 15925\n",
      "Train: [epoch:26]  [ 20/431]  eta: 0:08:25  lr: 0.000200  loss: 141.3924 (140.9646)  time: 1.0532  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 132.9994 (138.2174)  time: 1.0636  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 129.2844 (135.6484)  time: 1.0752  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 123.9075 (132.7932)  time: 1.0873  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 127.8345 (133.0396)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 135.0808 (132.4647)  time: 1.0988  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 135.3405 (133.0443)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 136.3435 (133.0212)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 129.0257 (132.6572)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 125.1697 (132.0214)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 125.9119 (132.0140)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 129.3693 (131.8552)  time: 1.1162  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 128.0227 (131.6005)  time: 1.1168  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 128.4736 (131.8339)  time: 1.1216  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 141.0411 (132.3548)  time: 1.1151  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 132.9661 (132.1752)  time: 1.1178  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 132.7702 (132.3672)  time: 1.1201  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 136.8596 (132.6453)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 135.7593 (132.7032)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 132.3541 (132.7336)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 131.0333 (132.6101)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 136.4383 (133.0551)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 143.3870 (133.3457)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 142.1353 (133.4266)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 139.7106 (133.7129)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 140.4940 (133.9962)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 137.7317 (134.1575)  time: 1.1074  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 136.4340 (134.2752)  time: 1.1091  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 138.8002 (134.4774)  time: 1.1189  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 136.8623 (134.4939)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 130.4262 (134.2841)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 132.2780 (134.5939)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 147.8920 (135.0149)  time: 1.1170  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:26]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 146.5897 (135.2810)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 140.2492 (135.3228)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 134.7485 (135.2298)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 143.8999 (135.5012)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 141.1828 (135.5573)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.2352 (135.4587)  time: 1.1245  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 137.2064 (135.5532)  time: 1.1241  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:26]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 135.1329 (135.3830)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 131.1338 (135.4312)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:26] Total time: 0:08:01 (1.1165 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 131.1338 (135.4312)\n",
      "Valid: [epoch:26]  [ 0/14]  eta: 0:00:34  loss: 119.4266 (119.4266)  time: 2.4875  data: 2.3297  max mem: 15925\n",
      "Valid: [epoch:26]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2645  data: 0.1665  max mem: 15925\n",
      "Valid: [epoch:26] Total time: 0:00:03 (0.2802 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_26_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:27]  [  0/431]  eta: 0:28:37  lr: 0.000200  loss: 147.9050 (147.9050)  time: 3.9860  data: 2.8044  max mem: 15925\n",
      "Train: [epoch:27]  [ 10/431]  eta: 0:09:19  lr: 0.000200  loss: 147.4120 (145.4964)  time: 1.3283  data: 0.2551  max mem: 15925\n",
      "Train: [epoch:27]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 139.2411 (142.6948)  time: 1.0688  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [ 30/431]  eta: 0:07:49  lr: 0.000200  loss: 133.3703 (138.3340)  time: 1.0834  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 131.5610 (136.6940)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 130.4075 (135.0737)  time: 1.1124  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:27]  [ 60/431]  eta: 0:07:05  lr: 0.000200  loss: 126.2133 (133.9231)  time: 1.1239  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:27]  [ 70/431]  eta: 0:06:52  lr: 0.000200  loss: 132.8197 (133.9791)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:27]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 136.4653 (134.9663)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 131.0906 (133.9280)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 129.2612 (133.9804)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 130.0539 (133.4541)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 126.1076 (132.9240)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 128.6307 (132.6470)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 132.9160 (132.4680)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 126.8780 (132.2837)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 132.4793 (132.4708)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 134.3828 (132.6411)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 135.5056 (132.7461)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 138.5503 (133.3298)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 137.2324 (133.0337)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 133.6621 (133.0886)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 134.0603 (133.1851)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 141.5833 (133.7429)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 144.5219 (133.9343)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 133.0551 (133.7216)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 134.9090 (134.0279)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 141.4579 (134.4266)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 138.2205 (134.4560)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 135.1151 (134.5005)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 138.5830 (134.8106)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 140.8005 (134.7561)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 134.5864 (134.7032)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 137.9806 (134.9023)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 144.8754 (135.2787)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 143.4412 (135.4444)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 137.7145 (135.4757)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 136.2346 (135.5838)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 142.2562 (135.7651)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 141.7571 (135.7891)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.2150 (135.7120)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 129.1538 (135.5395)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:27]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 129.1538 (135.5535)  time: 1.1070  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:27]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 134.4957 (135.5878)  time: 1.1075  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:27] Total time: 0:08:00 (1.1152 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 134.4957 (135.5878)\n",
      "Valid: [epoch:27]  [ 0/14]  eta: 0:00:30  loss: 161.4333 (161.4333)  time: 2.1654  data: 1.9853  max mem: 15925\n",
      "Valid: [epoch:27]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2612  data: 0.1419  max mem: 15925\n",
      "Valid: [epoch:27] Total time: 0:00:03 (0.2767 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_27_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:28]  [  0/431]  eta: 0:31:09  lr: 0.000200  loss: 143.5678 (143.5678)  time: 4.3386  data: 3.1876  max mem: 15925\n",
      "Train: [epoch:28]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 146.1887 (148.5790)  time: 1.3410  data: 0.2900  max mem: 15925\n",
      "Train: [epoch:28]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 141.3634 (144.8323)  time: 1.0529  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [ 30/431]  eta: 0:07:47  lr: 0.000200  loss: 129.0293 (137.6981)  time: 1.0700  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [ 40/431]  eta: 0:07:28  lr: 0.000200  loss: 121.2281 (135.0339)  time: 1.0842  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:28]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 127.9760 (134.0070)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 127.9760 (132.3704)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 131.6817 (133.7710)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 138.2240 (134.5317)  time: 1.1018  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:28]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 135.8339 (134.2510)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 135.6845 (134.3095)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 126.2247 (133.0071)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 119.4177 (133.0971)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 133.3669 (132.5975)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 129.8185 (132.5310)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 132.6602 (132.6405)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 135.2352 (133.1051)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 133.8023 (132.9722)  time: 1.1079  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:28]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 134.8327 (133.3002)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 137.4621 (133.5097)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 135.2789 (133.5140)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 133.2354 (133.5330)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 135.7870 (133.6920)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 139.7882 (134.1444)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 142.3459 (134.4811)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 143.5001 (134.7726)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 144.7136 (135.2341)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 144.7136 (135.5402)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 136.7110 (135.3872)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 129.9359 (135.2870)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 137.3317 (135.4697)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 137.3317 (135.4015)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 129.9046 (135.2349)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 137.0172 (135.5968)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 146.0512 (135.9048)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 144.9798 (136.0344)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 143.2720 (136.1650)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 134.2309 (136.0660)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 131.2205 (136.0166)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 136.9333 (136.0188)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 133.7346 (135.7728)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:28]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.3756 (135.8308)  time: 1.0990  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:28]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 138.1442 (135.7813)  time: 1.1013  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:28]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 132.1816 (135.7197)  time: 1.1046  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:28] Total time: 0:07:57 (1.1082 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 132.1816 (135.7197)\n",
      "Valid: [epoch:28]  [ 0/14]  eta: 0:00:29  loss: 161.1376 (161.1376)  time: 2.1424  data: 1.9825  max mem: 15925\n",
      "Valid: [epoch:28]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2371  data: 0.1417  max mem: 15925\n",
      "Valid: [epoch:28] Total time: 0:00:03 (0.2525 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_28_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:29]  [  0/431]  eta: 0:33:53  lr: 0.000200  loss: 131.7011 (131.7011)  time: 4.7175  data: 3.5784  max mem: 15925\n",
      "Train: [epoch:29]  [ 10/431]  eta: 0:09:42  lr: 0.000200  loss: 149.9764 (151.2686)  time: 1.3845  data: 0.3255  max mem: 15925\n",
      "Train: [epoch:29]  [ 20/431]  eta: 0:08:25  lr: 0.000200  loss: 146.6862 (144.1349)  time: 1.0556  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 132.4919 (139.4698)  time: 1.0690  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 126.6878 (135.8797)  time: 1.0829  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 125.9397 (133.7433)  time: 1.0835  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 125.9397 (131.8532)  time: 1.0854  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 130.5614 (132.3942)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 138.3554 (133.1207)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 129.9505 (132.3197)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 128.7619 (132.4772)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 129.8221 (132.0924)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 129.8221 (132.1243)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 132.4255 (131.9389)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 135.2387 (132.3329)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 138.9512 (132.6665)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 135.4889 (133.0670)  time: 1.0945  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:29]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 133.6477 (132.7110)  time: 1.0897  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:29]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 134.8919 (133.1029)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 137.6441 (133.3159)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 136.7896 (133.0852)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 135.1310 (133.1589)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 140.7824 (133.6099)  time: 1.1001  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:29]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 142.4147 (133.8435)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 142.9898 (134.2641)  time: 1.1012  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:29]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 140.6017 (134.3061)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 139.5503 (134.3141)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 140.7174 (134.6801)  time: 1.1081  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:29]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 136.9365 (134.5437)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 130.2759 (134.5854)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 131.9319 (134.5775)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 131.2305 (134.4227)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 125.6576 (134.3189)  time: 1.0917  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:29]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 142.4406 (134.7442)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 149.4344 (135.2521)  time: 1.0893  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 147.7850 (135.3814)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 141.7053 (135.4848)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 141.7053 (135.6645)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 134.5754 (135.5331)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 131.7935 (135.5171)  time: 1.0883  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 127.9152 (135.3964)  time: 1.0858  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:29]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 132.8986 (135.4199)  time: 1.1018  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:29]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 139.9601 (135.4789)  time: 1.1096  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:29]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 137.0663 (135.5012)  time: 1.1062  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:29] Total time: 0:07:56 (1.1065 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 137.0663 (135.5012)\n",
      "Valid: [epoch:29]  [ 0/14]  eta: 0:00:34  loss: 101.8203 (101.8203)  time: 2.4971  data: 2.3528  max mem: 15925\n",
      "Valid: [epoch:29]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2830  data: 0.1681  max mem: 15925\n",
      "Valid: [epoch:29] Total time: 0:00:04 (0.2973 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_29_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:30]  [  0/431]  eta: 0:31:58  lr: 0.000200  loss: 164.9331 (164.9331)  time: 4.4516  data: 3.2827  max mem: 15925\n",
      "Train: [epoch:30]  [ 10/431]  eta: 0:09:34  lr: 0.000200  loss: 156.9647 (154.9874)  time: 1.3651  data: 0.2987  max mem: 15925\n",
      "Train: [epoch:30]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 145.6997 (148.1108)  time: 1.0557  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [ 30/431]  eta: 0:07:48  lr: 0.000200  loss: 131.7304 (143.2737)  time: 1.0597  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 131.6595 (140.1223)  time: 1.0829  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 130.0596 (137.6628)  time: 1.0984  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 128.8714 (136.3121)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 130.7388 (136.6492)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 136.7334 (136.6091)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 134.6456 (136.2343)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 133.9176 (135.8677)  time: 1.0997  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 128.0154 (134.6811)  time: 1.0954  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 128.0154 (134.7103)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 134.2682 (134.5995)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 129.7642 (133.9890)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 126.3424 (133.8983)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 134.0699 (133.9487)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 136.9436 (134.0628)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 137.5750 (134.2598)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 137.5750 (134.3175)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 136.7920 (134.3955)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 133.1134 (134.2827)  time: 1.0826  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 130.9290 (134.3681)  time: 1.0863  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 138.4833 (134.5590)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 138.9910 (134.7847)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 135.9885 (134.8067)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 138.9721 (135.1527)  time: 1.0902  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 144.3783 (135.5645)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 138.5188 (135.5270)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 133.7219 (135.4198)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 131.9056 (135.4183)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 131.9056 (135.3733)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 128.8176 (135.0934)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 137.1664 (135.2698)  time: 1.1019  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 140.6598 (135.3827)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 139.3462 (135.5042)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 136.6980 (135.4342)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 137.1168 (135.5947)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 142.2019 (135.6721)  time: 1.1091  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 136.8724 (135.6748)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:30]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 131.0134 (135.6093)  time: 1.1064  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:30]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.1914 (135.5585)  time: 1.1087  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 133.3213 (135.4157)  time: 1.1122  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 135.4603 (135.4872)  time: 1.0993  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:30] Total time: 0:07:57 (1.1075 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 135.4603 (135.4872)\n",
      "Valid: [epoch:30]  [ 0/14]  eta: 0:00:32  loss: 124.2619 (124.2619)  time: 2.3249  data: 2.1655  max mem: 15925\n",
      "Valid: [epoch:30]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2671  data: 0.1548  max mem: 15925\n",
      "Valid: [epoch:30] Total time: 0:00:03 (0.2820 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_30_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:31]  [  0/431]  eta: 0:34:03  lr: 0.000200  loss: 155.5946 (155.5946)  time: 4.7402  data: 3.5837  max mem: 15925\n",
      "Train: [epoch:31]  [ 10/431]  eta: 0:09:47  lr: 0.000200  loss: 151.8807 (152.2861)  time: 1.3965  data: 0.3260  max mem: 15925\n",
      "Train: [epoch:31]  [ 20/431]  eta: 0:08:30  lr: 0.000200  loss: 148.1987 (147.2069)  time: 1.0678  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [ 30/431]  eta: 0:08:00  lr: 0.000200  loss: 133.4868 (142.1135)  time: 1.0882  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [ 40/431]  eta: 0:07:39  lr: 0.000200  loss: 129.0609 (138.4638)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [ 50/431]  eta: 0:07:22  lr: 0.000200  loss: 128.2475 (136.7369)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [ 60/431]  eta: 0:07:07  lr: 0.000200  loss: 128.5336 (135.6882)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [ 70/431]  eta: 0:06:53  lr: 0.000200  loss: 135.1987 (136.1767)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [ 80/431]  eta: 0:06:40  lr: 0.000200  loss: 144.0295 (137.0197)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 135.7679 (136.3305)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 132.1884 (135.5112)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 132.7688 (135.0351)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 130.5982 (134.3783)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 128.9877 (134.3240)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 130.4675 (134.1968)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 131.6710 (133.9398)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 134.4040 (134.0267)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 132.8783 (134.0372)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 132.0619 (134.1217)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 134.8003 (134.1948)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 129.5584 (133.8983)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 133.6039 (134.2356)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 138.2379 (134.1654)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 134.9280 (134.1838)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 138.7693 (134.6501)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 138.7693 (134.6532)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 137.0295 (135.0558)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 146.8599 (135.5538)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 135.6861 (135.3460)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 133.9852 (135.3832)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 134.0457 (135.3994)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 130.4024 (135.2879)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 128.5257 (135.0219)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 130.7902 (135.1015)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 135.9172 (135.2635)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 139.5765 (135.4569)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 134.1528 (135.3623)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 136.0177 (135.3712)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 136.0177 (135.4395)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 135.8598 (135.4047)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:31]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 138.5919 (135.5730)  time: 1.1031  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:31]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.4009 (135.6189)  time: 1.1142  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:31]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 135.5374 (135.5519)  time: 1.1280  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:31]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 135.5941 (135.6090)  time: 1.1180  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:31] Total time: 0:07:59 (1.1137 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 135.5941 (135.6090)\n",
      "Valid: [epoch:31]  [ 0/14]  eta: 0:00:31  loss: 160.5304 (160.5304)  time: 2.2427  data: 2.0857  max mem: 15925\n",
      "Valid: [epoch:31]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2645  data: 0.1496  max mem: 15925\n",
      "Valid: [epoch:31] Total time: 0:00:03 (0.2821 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_31_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:32]  [  0/431]  eta: 0:30:07  lr: 0.000200  loss: 151.9276 (151.9276)  time: 4.1945  data: 2.9063  max mem: 15925\n",
      "Train: [epoch:32]  [ 10/431]  eta: 0:09:28  lr: 0.000200  loss: 151.4769 (149.9100)  time: 1.3513  data: 0.2645  max mem: 15925\n",
      "Train: [epoch:32]  [ 20/431]  eta: 0:08:15  lr: 0.000200  loss: 140.5719 (142.9577)  time: 1.0551  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [ 30/431]  eta: 0:07:48  lr: 0.000200  loss: 135.4328 (141.2303)  time: 1.0658  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 130.0778 (137.7205)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 126.2358 (135.5667)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 127.0435 (134.6985)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 134.4402 (134.7332)  time: 1.1067  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:32]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 138.6970 (135.5949)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 138.6970 (134.8257)  time: 1.1070  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:32]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 127.8068 (134.0645)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 126.5149 (133.5596)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 129.5502 (133.5343)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 133.8804 (133.4319)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 130.1651 (133.0844)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 135.0374 (133.5810)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 140.6102 (133.9750)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 133.6094 (133.7336)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 129.8111 (133.6081)  time: 1.0875  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 132.8461 (133.8522)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 136.8189 (133.5799)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 123.6079 (133.3678)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 131.1895 (133.3160)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 136.0740 (133.6054)  time: 1.0917  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 138.4885 (133.4664)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 135.2717 (133.5420)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 136.6029 (133.6571)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 136.6029 (133.8614)  time: 1.0990  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:32]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 135.7207 (133.9175)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 135.7746 (134.0446)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 145.1078 (134.5040)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 140.8779 (134.4383)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 133.4252 (134.2943)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 136.6032 (134.7216)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 146.7752 (135.0171)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 146.5150 (135.3747)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 140.2699 (135.2901)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 127.1396 (135.1530)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 135.8388 (135.2518)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 141.0195 (135.3754)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 134.9332 (135.3729)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:32]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.2513 (135.4877)  time: 1.0950  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:32]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.2513 (135.3939)  time: 1.0988  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:32]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 133.9827 (135.3502)  time: 1.1061  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:32] Total time: 0:07:56 (1.1063 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 133.9827 (135.3502)\n",
      "Valid: [epoch:32]  [ 0/14]  eta: 0:00:30  loss: 132.8970 (132.8970)  time: 2.1493  data: 1.9950  max mem: 15925\n",
      "Valid: [epoch:32]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2320  data: 0.1426  max mem: 15925\n",
      "Valid: [epoch:32] Total time: 0:00:03 (0.2477 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_32_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:33]  [  0/431]  eta: 0:32:14  lr: 0.000200  loss: 143.7126 (143.7126)  time: 4.4880  data: 3.3728  max mem: 15925\n",
      "Train: [epoch:33]  [ 10/431]  eta: 0:09:36  lr: 0.000200  loss: 152.7185 (147.7226)  time: 1.3685  data: 0.3068  max mem: 15925\n",
      "Train: [epoch:33]  [ 20/431]  eta: 0:08:24  lr: 0.000200  loss: 150.3493 (145.5993)  time: 1.0644  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 134.9351 (139.5788)  time: 1.0706  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:33]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 128.4117 (137.5073)  time: 1.0790  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 128.4574 (136.0541)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 126.8029 (134.4687)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 127.6190 (134.2639)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [ 80/431]  eta: 0:06:34  lr: 0.000200  loss: 134.1961 (134.6554)  time: 1.0861  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 139.3724 (134.8541)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 137.7751 (134.5884)  time: 1.0962  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:33]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 129.6944 (133.9126)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 125.4705 (133.3979)  time: 1.0889  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 126.5730 (133.2900)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 131.5771 (133.0992)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 136.6219 (133.6061)  time: 1.0969  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:33]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 136.6147 (133.3178)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 133.9334 (133.4223)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 135.1041 (133.5900)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 137.2971 (133.9681)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 137.2971 (133.9951)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 133.9456 (133.9002)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 136.4089 (134.1477)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 139.3371 (134.1900)  time: 1.1104  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:33]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 139.3371 (134.3220)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 139.3690 (134.4389)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 140.3260 (134.7588)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 141.6342 (135.1118)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 139.4749 (134.9912)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 134.0588 (134.9590)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 141.0515 (135.3090)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 141.0515 (135.1733)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 130.2793 (134.9528)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 137.9073 (135.2416)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 147.5464 (135.5436)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 149.2034 (135.8307)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 143.4380 (135.8612)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 132.7754 (135.7279)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 132.5727 (135.6289)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 133.6592 (135.6118)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 136.1237 (135.5940)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.4176 (135.5700)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:33]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 133.6447 (135.4678)  time: 1.1006  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:33]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 136.0636 (135.5897)  time: 1.1062  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:33] Total time: 0:07:57 (1.1074 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 136.0636 (135.5897)\n",
      "Valid: [epoch:33]  [ 0/14]  eta: 0:00:32  loss: 159.8465 (159.8465)  time: 2.3038  data: 2.1566  max mem: 15925\n",
      "Valid: [epoch:33]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2633  data: 0.1542  max mem: 15925\n",
      "Valid: [epoch:33] Total time: 0:00:03 (0.2794 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_33_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:34]  [  0/431]  eta: 0:32:04  lr: 0.000200  loss: 152.4682 (152.4682)  time: 4.4644  data: 3.2775  max mem: 15925\n",
      "Train: [epoch:34]  [ 10/431]  eta: 0:09:31  lr: 0.000200  loss: 150.9225 (147.7451)  time: 1.3581  data: 0.2982  max mem: 15925\n",
      "Train: [epoch:34]  [ 20/431]  eta: 0:08:22  lr: 0.000200  loss: 147.9879 (145.9798)  time: 1.0603  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:34]  [ 30/431]  eta: 0:07:52  lr: 0.000200  loss: 138.0629 (140.8215)  time: 1.0793  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [ 40/431]  eta: 0:07:35  lr: 0.000200  loss: 128.0162 (137.8879)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [ 50/431]  eta: 0:07:19  lr: 0.000200  loss: 127.2427 (135.4882)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [ 60/431]  eta: 0:07:05  lr: 0.000200  loss: 127.4282 (134.7832)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 135.4952 (134.9269)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 138.3415 (135.5011)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 133.4756 (135.1695)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 127.7039 (133.6980)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 117.4729 (132.4192)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 129.5968 (132.4636)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 133.4899 (132.3797)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 133.5465 (132.1631)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 136.7569 (132.5132)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 141.4416 (133.1371)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 139.9927 (133.2140)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 136.1879 (133.1986)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 137.1101 (133.6543)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 139.3442 (133.9088)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 135.8544 (133.7950)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 134.8195 (133.8873)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 131.8414 (133.8382)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 130.8702 (133.7987)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 132.4535 (133.9574)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 136.7722 (134.2689)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 140.0827 (134.7275)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 138.3209 (134.6248)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 137.5225 (134.8494)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 139.5312 (134.9902)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 135.9631 (134.9890)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 128.1065 (134.7974)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 130.2680 (134.9440)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 140.5143 (135.1193)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 144.5673 (135.4534)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 144.5673 (135.5226)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 131.1797 (135.3884)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 138.0172 (135.5891)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 138.8041 (135.5210)  time: 1.1046  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:34]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 131.8113 (135.3297)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:34]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 135.1348 (135.4210)  time: 1.1119  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:34]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 139.7306 (135.4248)  time: 1.1072  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:34]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 135.0997 (135.5399)  time: 1.0987  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:34] Total time: 0:07:58 (1.1096 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 135.0997 (135.5399)\n",
      "Valid: [epoch:34]  [ 0/14]  eta: 0:00:30  loss: 124.3029 (124.3029)  time: 2.2129  data: 2.0377  max mem: 15925\n",
      "Valid: [epoch:34]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7079)  time: 0.2408  data: 0.1456  max mem: 15925\n",
      "Valid: [epoch:34] Total time: 0:00:03 (0.2571 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7079)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_34_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.708%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:35]  [  0/431]  eta: 0:33:39  lr: 0.000200  loss: 156.7767 (156.7767)  time: 4.6846  data: 3.5195  max mem: 15925\n",
      "Train: [epoch:35]  [ 10/431]  eta: 0:09:38  lr: 0.000200  loss: 145.5647 (142.5578)  time: 1.3731  data: 0.3201  max mem: 15925\n",
      "Train: [epoch:35]  [ 20/431]  eta: 0:08:21  lr: 0.000200  loss: 141.7030 (142.3046)  time: 1.0463  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 135.3921 (138.3187)  time: 1.0669  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 126.6051 (134.3283)  time: 1.0780  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 131.5201 (134.1531)  time: 1.0826  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:35]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 131.5201 (132.1111)  time: 1.0972  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:35]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 133.1404 (132.9257)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 140.2061 (133.6931)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 132.1497 (132.7719)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 127.6470 (132.3073)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 126.6516 (131.9960)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 129.9576 (132.0847)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 132.9249 (132.1287)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 134.2516 (132.1995)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 135.6270 (132.7218)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 138.9223 (133.1243)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 137.8246 (133.4555)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 137.7663 (133.4521)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 140.4810 (134.1374)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 138.4706 (134.1378)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 130.1718 (133.9250)  time: 1.1210  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:35]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 132.0453 (133.9149)  time: 1.1212  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:35]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 135.0830 (134.1085)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 135.8016 (134.2516)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 139.4487 (134.4170)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 139.9802 (134.7228)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 142.9396 (135.0798)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 144.1208 (135.1056)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 132.3757 (135.0388)  time: 1.1114  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:35]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 139.3015 (135.4849)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 139.2264 (135.3404)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 129.5385 (135.0970)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 133.5948 (135.4679)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 146.5451 (135.5940)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 138.8304 (135.5989)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 138.8304 (135.6213)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 136.1195 (135.5162)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 140.0121 (135.6580)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 138.0193 (135.5890)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 131.5888 (135.5656)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 130.3770 (135.4989)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.9539 (135.5182)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:35]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 137.8970 (135.5286)  time: 1.1063  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:35] Total time: 0:08:00 (1.1143 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 137.8970 (135.5286)\n",
      "Valid: [epoch:35]  [ 0/14]  eta: 0:00:34  loss: 128.6310 (128.6310)  time: 2.4962  data: 2.3119  max mem: 15925\n",
      "Valid: [epoch:35]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2581  data: 0.1652  max mem: 15925\n",
      "Valid: [epoch:35] Total time: 0:00:03 (0.2742 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_35_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:36]  [  0/431]  eta: 0:31:55  lr: 0.000200  loss: 151.1295 (151.1295)  time: 4.4436  data: 3.2772  max mem: 15925\n",
      "Train: [epoch:36]  [ 10/431]  eta: 0:09:35  lr: 0.000200  loss: 150.2105 (147.2989)  time: 1.3668  data: 0.2981  max mem: 15925\n",
      "Train: [epoch:36]  [ 20/431]  eta: 0:08:25  lr: 0.000200  loss: 139.7373 (141.3423)  time: 1.0697  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 127.9191 (136.1953)  time: 1.0795  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [ 40/431]  eta: 0:07:34  lr: 0.000200  loss: 126.3393 (133.2964)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 122.8540 (131.4237)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [ 60/431]  eta: 0:07:04  lr: 0.000200  loss: 122.6613 (130.1761)  time: 1.1113  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:36]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 132.5143 (130.7692)  time: 1.1193  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 132.5482 (130.9966)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 128.8278 (130.5514)  time: 1.1251  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 126.7915 (130.2673)  time: 1.1228  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 127.0916 (130.0302)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 127.0916 (129.7227)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 128.9359 (130.2174)  time: 1.1208  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [140/431]  eta: 0:05:28  lr: 0.000200  loss: 136.6129 (130.7749)  time: 1.1234  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [150/431]  eta: 0:05:17  lr: 0.000200  loss: 137.7986 (131.1608)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 138.1106 (131.5415)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [170/431]  eta: 0:04:54  lr: 0.000200  loss: 133.8820 (131.6014)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [180/431]  eta: 0:04:43  lr: 0.000200  loss: 133.6785 (131.8467)  time: 1.1331  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 138.8114 (132.4469)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [200/431]  eta: 0:04:20  lr: 0.000200  loss: 135.7207 (132.2323)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [210/431]  eta: 0:04:09  lr: 0.000200  loss: 133.5520 (132.2939)  time: 1.1354  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [220/431]  eta: 0:03:58  lr: 0.000200  loss: 138.1493 (132.5133)  time: 1.1272  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [230/431]  eta: 0:03:46  lr: 0.000200  loss: 137.7413 (132.7668)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [240/431]  eta: 0:03:35  lr: 0.000200  loss: 141.2695 (133.1727)  time: 1.1258  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [250/431]  eta: 0:03:24  lr: 0.000200  loss: 141.2695 (133.3876)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 141.1678 (133.8014)  time: 1.1161  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [270/431]  eta: 0:03:01  lr: 0.000200  loss: 147.4242 (134.3071)  time: 1.1208  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [280/431]  eta: 0:02:50  lr: 0.000200  loss: 139.9030 (134.3412)  time: 1.1260  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 131.4989 (134.3073)  time: 1.1260  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [300/431]  eta: 0:02:27  lr: 0.000200  loss: 134.4409 (134.6688)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [310/431]  eta: 0:02:16  lr: 0.000200  loss: 141.7176 (134.8013)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [320/431]  eta: 0:02:05  lr: 0.000200  loss: 130.0860 (134.6268)  time: 1.1215  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 138.3586 (134.9844)  time: 1.1169  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 148.0975 (135.3103)  time: 1.1176  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [350/431]  eta: 0:01:31  lr: 0.000200  loss: 145.0045 (135.4964)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 139.7130 (135.5630)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 138.8966 (135.5467)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 137.2854 (135.5645)  time: 1.1251  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [390/431]  eta: 0:00:46  lr: 0.000200  loss: 138.1246 (135.6388)  time: 1.1236  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:36]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 140.5406 (135.6099)  time: 1.1274  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.9947 (135.5838)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 140.0876 (135.7274)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:36]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 140.8715 (135.6869)  time: 1.1089  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:36] Total time: 0:08:05 (1.1256 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 140.8715 (135.6869)\n",
      "Valid: [epoch:36]  [ 0/14]  eta: 0:00:36  loss: 161.4333 (161.4333)  time: 2.6158  data: 2.4577  max mem: 15925\n",
      "Valid: [epoch:36]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2895  data: 0.1756  max mem: 15925\n",
      "Valid: [epoch:36] Total time: 0:00:04 (0.3039 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_36_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:37]  [  0/431]  eta: 0:33:15  lr: 0.000200  loss: 155.1762 (155.1762)  time: 4.6298  data: 3.4432  max mem: 15925\n",
      "Train: [epoch:37]  [ 10/431]  eta: 0:09:35  lr: 0.000200  loss: 149.5214 (150.6378)  time: 1.3674  data: 0.3132  max mem: 15925\n",
      "Train: [epoch:37]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 145.8880 (145.6681)  time: 1.0461  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [ 30/431]  eta: 0:07:50  lr: 0.000200  loss: 134.5885 (141.0048)  time: 1.0669  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 123.9457 (136.6197)  time: 1.0843  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 121.0776 (134.4166)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [ 60/431]  eta: 0:07:05  lr: 0.000200  loss: 124.6447 (132.4000)  time: 1.1373  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [ 70/431]  eta: 0:06:52  lr: 0.000200  loss: 124.7943 (132.6511)  time: 1.1281  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [ 80/431]  eta: 0:06:40  lr: 0.000200  loss: 137.8905 (133.7515)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 132.8959 (132.9701)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 127.6712 (132.4339)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 128.5277 (132.2079)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 132.4774 (132.0667)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 134.3592 (132.3177)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [140/431]  eta: 0:05:28  lr: 0.000200  loss: 134.9494 (132.4005)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 139.2788 (133.1640)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 140.2890 (133.3269)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 136.8628 (133.0852)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 136.3967 (133.5106)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 140.8041 (133.8642)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 133.6220 (133.8748)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 132.6759 (133.9249)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 136.3724 (134.1228)  time: 1.1158  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:37]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 137.4291 (134.2993)  time: 1.1312  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 137.2250 (134.4625)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 135.0177 (134.5356)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 144.0952 (134.8709)  time: 1.1340  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 144.1110 (135.1497)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 134.1232 (134.8444)  time: 1.1102  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 131.3259 (134.7860)  time: 1.1202  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 142.4090 (135.0469)  time: 1.1126  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 136.6311 (134.9703)  time: 1.1151  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 129.2836 (134.7590)  time: 1.1276  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 132.7046 (134.8797)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 143.5587 (135.1910)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 142.7840 (135.2548)  time: 1.1223  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 133.8691 (135.2286)  time: 1.1194  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 136.2930 (135.3302)  time: 1.1124  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:37]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 141.6478 (135.4865)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 136.6142 (135.3960)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 133.7818 (135.3794)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.0481 (135.4313)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 137.9068 (135.4152)  time: 1.1157  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:37]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 136.1078 (135.4288)  time: 1.1244  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:37] Total time: 0:08:02 (1.1204 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 136.1078 (135.4288)\n",
      "Valid: [epoch:37]  [ 0/14]  eta: 0:00:33  loss: 108.0685 (108.0685)  time: 2.4016  data: 2.2253  max mem: 15925\n",
      "Valid: [epoch:37]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2744  data: 0.1590  max mem: 15925\n",
      "Valid: [epoch:37] Total time: 0:00:04 (0.2907 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_37_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:38]  [  0/431]  eta: 0:30:08  lr: 0.000200  loss: 150.8976 (150.8976)  time: 4.1962  data: 3.0237  max mem: 15925\n",
      "Train: [epoch:38]  [ 10/431]  eta: 0:09:26  lr: 0.000200  loss: 150.3692 (148.0028)  time: 1.3449  data: 0.2751  max mem: 15925\n",
      "Train: [epoch:38]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 144.8149 (145.2334)  time: 1.0687  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [ 30/431]  eta: 0:07:49  lr: 0.000200  loss: 133.1911 (136.8060)  time: 1.0758  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 121.1472 (134.1420)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [ 50/431]  eta: 0:07:18  lr: 0.000200  loss: 123.3383 (132.5860)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:38]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 126.4206 (132.0782)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 129.5871 (132.0443)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [ 80/431]  eta: 0:06:38  lr: 0.000200  loss: 139.3292 (133.3866)  time: 1.1129  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:38]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 137.2618 (133.4012)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 132.1819 (132.9148)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 129.0800 (132.3484)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 132.5263 (132.7600)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 138.2386 (132.6989)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 133.1352 (132.8238)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 137.5866 (133.4939)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 138.0328 (133.6401)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 126.8690 (133.2997)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 131.4299 (133.4765)  time: 1.1275  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 139.3947 (133.6306)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 134.1008 (133.6018)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 130.4319 (133.6163)  time: 1.1169  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:38]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 130.2764 (133.5827)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 129.6389 (133.5143)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 132.9415 (133.6880)  time: 1.0930  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:38]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 138.0487 (133.8608)  time: 1.1050  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:38]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 138.0487 (134.1684)  time: 1.1178  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:38]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 140.6254 (134.4454)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 136.3532 (134.3153)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 135.9699 (134.2164)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 137.0954 (134.3854)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 137.7226 (134.3652)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 135.1394 (134.3579)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 140.5648 (134.7920)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 142.6670 (134.9567)  time: 1.0992  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:38]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 141.3600 (135.1453)  time: 1.1140  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:38]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 138.7064 (135.2399)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 138.5525 (135.2818)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 135.4689 (135.2987)  time: 1.1166  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:38]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 138.3691 (135.4301)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 138.3691 (135.4067)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 129.3706 (135.2478)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:38]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 134.6871 (135.4041)  time: 1.1028  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:38]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 142.0616 (135.5775)  time: 1.0975  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:38] Total time: 0:08:00 (1.1158 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 142.0616 (135.5775)\n",
      "Valid: [epoch:38]  [ 0/14]  eta: 0:00:34  loss: 161.1376 (161.1376)  time: 2.4613  data: 2.2929  max mem: 15925\n",
      "Valid: [epoch:38]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2661  data: 0.1639  max mem: 15925\n",
      "Valid: [epoch:38] Total time: 0:00:03 (0.2828 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_38_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:39]  [  0/431]  eta: 0:33:25  lr: 0.000200  loss: 161.4083 (161.4083)  time: 4.6521  data: 3.4651  max mem: 15925\n",
      "Train: [epoch:39]  [ 10/431]  eta: 0:09:44  lr: 0.000200  loss: 151.0308 (149.0676)  time: 1.3892  data: 0.3152  max mem: 15925\n",
      "Train: [epoch:39]  [ 20/431]  eta: 0:08:26  lr: 0.000200  loss: 146.8043 (145.5196)  time: 1.0623  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [ 30/431]  eta: 0:07:55  lr: 0.000200  loss: 133.2175 (141.0230)  time: 1.0735  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [ 40/431]  eta: 0:07:34  lr: 0.000200  loss: 132.3058 (137.2071)  time: 1.0872  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [ 50/431]  eta: 0:07:18  lr: 0.000200  loss: 128.6272 (135.7159)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 125.6225 (133.6653)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 129.1976 (134.8397)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [ 80/431]  eta: 0:06:38  lr: 0.000200  loss: 140.4194 (135.5570)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 133.8615 (134.9082)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 129.6106 (134.0777)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 126.9718 (133.3073)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 123.6740 (133.0199)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 132.4507 (133.1413)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 132.1273 (132.8374)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 124.3655 (132.5391)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 132.2299 (132.8930)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 131.9398 (132.5384)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 131.3488 (132.9503)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 136.7976 (133.0238)  time: 1.1094  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:39]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 135.0403 (132.9282)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 133.9124 (133.0345)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 140.3125 (133.2601)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 140.6511 (133.5147)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 141.4272 (133.9075)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 138.5081 (133.9931)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 137.0612 (134.1633)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 146.9754 (134.6710)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 144.8807 (134.7734)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 134.0576 (134.7435)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 138.1652 (134.9898)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 139.6995 (134.8416)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 126.6502 (134.6202)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 141.8247 (135.0552)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 144.5727 (135.2882)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 143.3273 (135.4923)  time: 1.1235  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:39]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 141.9432 (135.5511)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:39]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 136.5810 (135.5412)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 137.0022 (135.6099)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 134.9824 (135.5688)  time: 1.1018  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:39]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 134.7859 (135.6588)  time: 1.1076  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:39]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.7859 (135.7282)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 134.1578 (135.6534)  time: 1.1104  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:39]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 133.0528 (135.5715)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:39] Total time: 0:08:01 (1.1166 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 133.0528 (135.5715)\n",
      "Valid: [epoch:39]  [ 0/14]  eta: 0:00:36  loss: 101.8203 (101.8203)  time: 2.5811  data: 2.4114  max mem: 15925\n",
      "Valid: [epoch:39]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2770  data: 0.1723  max mem: 15925\n",
      "Valid: [epoch:39] Total time: 0:00:04 (0.2936 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_39_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:40]  [  0/431]  eta: 0:33:04  lr: 0.000200  loss: 140.4066 (140.4066)  time: 4.6035  data: 3.4022  max mem: 15925\n",
      "Train: [epoch:40]  [ 10/431]  eta: 0:09:32  lr: 0.000200  loss: 148.7742 (142.6615)  time: 1.3606  data: 0.3095  max mem: 15925\n",
      "Train: [epoch:40]  [ 20/431]  eta: 0:08:24  lr: 0.000200  loss: 143.2726 (138.6144)  time: 1.0592  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [ 30/431]  eta: 0:07:55  lr: 0.000200  loss: 132.9724 (136.2554)  time: 1.0906  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 132.1184 (135.2369)  time: 1.0878  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 132.1184 (134.1553)  time: 1.0873  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:40]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 127.0858 (132.8778)  time: 1.1060  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 133.3137 (133.4686)  time: 1.1162  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [ 80/431]  eta: 0:06:38  lr: 0.000200  loss: 138.7061 (133.6883)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 133.8176 (133.1512)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 129.7462 (132.9714)  time: 1.1152  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 127.2638 (132.1115)  time: 1.1181  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 124.5231 (131.9833)  time: 1.1148  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 132.2270 (132.1935)  time: 1.0975  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 131.2233 (132.3279)  time: 1.1017  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 131.2233 (132.4270)  time: 1.1175  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 134.9493 (132.6449)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 138.3734 (132.5634)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 134.4820 (132.5697)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 135.6456 (132.9743)  time: 1.0911  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 136.6994 (133.0228)  time: 1.0938  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 129.4302 (132.7963)  time: 1.1006  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 130.7339 (132.6915)  time: 1.0986  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 133.0683 (132.8841)  time: 1.0946  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 138.4208 (133.1045)  time: 1.0987  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 133.2201 (132.9998)  time: 1.1025  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 134.8969 (133.2434)  time: 1.1076  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 144.6779 (133.9198)  time: 1.1161  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 141.7733 (134.0250)  time: 1.1215  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 137.1443 (134.1469)  time: 1.1161  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 140.3012 (134.5509)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 137.0636 (134.3880)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 129.6336 (134.2893)  time: 1.1120  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 138.1603 (134.6919)  time: 1.1082  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 148.7189 (134.9973)  time: 1.1005  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 143.0071 (135.1778)  time: 1.1051  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 138.8966 (135.2684)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 134.3827 (135.2257)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 134.9299 (135.4748)  time: 1.1174  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 136.9984 (135.5612)  time: 1.1151  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.4994 (135.5938)  time: 1.1084  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:40]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.2847 (135.6400)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:40]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 138.0507 (135.5912)  time: 1.1013  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:40]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 132.3956 (135.5622)  time: 1.0854  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:40] Total time: 0:07:59 (1.1117 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 132.3956 (135.5622)\n",
      "Valid: [epoch:40]  [ 0/14]  eta: 0:00:35  loss: 161.9902 (161.9902)  time: 2.5215  data: 2.3665  max mem: 15925\n",
      "Valid: [epoch:40]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7203)  time: 0.2905  data: 0.1691  max mem: 15925\n",
      "Valid: [epoch:40] Total time: 0:00:04 (0.3044 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7203)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_40_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.720%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:41]  [  0/431]  eta: 0:30:30  lr: 0.000200  loss: 164.0347 (164.0347)  time: 4.2466  data: 3.0562  max mem: 15925\n",
      "Train: [epoch:41]  [ 10/431]  eta: 0:09:33  lr: 0.000200  loss: 154.5545 (147.2181)  time: 1.3625  data: 0.2781  max mem: 15925\n",
      "Train: [epoch:41]  [ 20/431]  eta: 0:08:24  lr: 0.000200  loss: 144.7730 (146.2282)  time: 1.0763  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:41]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 135.7976 (142.0005)  time: 1.0798  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 130.2968 (137.3448)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [ 50/431]  eta: 0:07:19  lr: 0.000200  loss: 128.3016 (135.7576)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [ 60/431]  eta: 0:07:05  lr: 0.000200  loss: 128.8543 (134.3588)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [ 70/431]  eta: 0:06:53  lr: 0.000200  loss: 131.4012 (133.8732)  time: 1.1285  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:41]  [ 80/431]  eta: 0:06:41  lr: 0.000200  loss: 134.1419 (134.1633)  time: 1.1394  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:41]  [ 90/431]  eta: 0:06:29  lr: 0.000200  loss: 128.7616 (133.2348)  time: 1.1279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [100/431]  eta: 0:06:17  lr: 0.000200  loss: 126.5717 (132.9979)  time: 1.1243  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:41]  [110/431]  eta: 0:06:04  lr: 0.000200  loss: 129.5756 (132.6426)  time: 1.1139  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:41]  [120/431]  eta: 0:05:52  lr: 0.000200  loss: 129.5756 (132.2912)  time: 1.1020  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:41]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 128.0677 (131.7841)  time: 1.1083  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:41]  [140/431]  eta: 0:05:28  lr: 0.000200  loss: 129.7045 (131.8515)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [150/431]  eta: 0:05:17  lr: 0.000200  loss: 135.4505 (132.1916)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 137.4094 (132.5795)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [170/431]  eta: 0:04:54  lr: 0.000200  loss: 139.6798 (132.8527)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 141.2061 (133.3990)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 140.7514 (133.6142)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 133.9588 (133.3638)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 132.3752 (133.4006)  time: 1.0975  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:41]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 134.6186 (133.5043)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 134.7363 (133.7424)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 137.6856 (134.0124)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 140.5548 (134.2882)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 143.3347 (134.4127)  time: 1.1055  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:41]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 139.4929 (134.7061)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 138.7493 (134.8153)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 137.2980 (134.8158)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 138.4394 (134.9194)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 134.7238 (134.7036)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 124.6127 (134.3469)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 134.9985 (134.6634)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 145.6862 (134.8581)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 142.7624 (135.1450)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 142.7624 (135.1724)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 143.5288 (135.4234)  time: 1.0901  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 143.6590 (135.5368)  time: 1.0813  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 137.6432 (135.5178)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 138.1093 (135.5741)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 139.0327 (135.5991)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:41]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 137.4920 (135.6796)  time: 1.0929  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:41]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 131.8102 (135.5572)  time: 1.0977  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:41] Total time: 0:07:58 (1.1114 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 131.8102 (135.5572)\n",
      "Valid: [epoch:41]  [ 0/14]  eta: 0:00:36  loss: 162.9339 (162.9339)  time: 2.5876  data: 2.3996  max mem: 15925\n",
      "Valid: [epoch:41]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2716  data: 0.1715  max mem: 15925\n",
      "Valid: [epoch:41] Total time: 0:00:04 (0.2887 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_41_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:42]  [  0/431]  eta: 0:31:22  lr: 0.000200  loss: 161.4193 (161.4193)  time: 4.3669  data: 3.1773  max mem: 15925\n",
      "Train: [epoch:42]  [ 10/431]  eta: 0:09:23  lr: 0.000200  loss: 144.8417 (151.3415)  time: 1.3395  data: 0.2891  max mem: 15925\n",
      "Train: [epoch:42]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 140.7617 (144.2825)  time: 1.0489  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 131.9717 (139.4840)  time: 1.0681  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 130.8801 (136.2464)  time: 1.0804  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [ 50/431]  eta: 0:07:11  lr: 0.000200  loss: 125.7322 (134.9009)  time: 1.0861  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [ 60/431]  eta: 0:06:58  lr: 0.000200  loss: 128.6008 (133.7104)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [ 70/431]  eta: 0:06:45  lr: 0.000200  loss: 130.9626 (134.1941)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [ 80/431]  eta: 0:06:32  lr: 0.000200  loss: 137.3102 (134.6921)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 132.7980 (133.7547)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [100/431]  eta: 0:06:09  lr: 0.000200  loss: 129.4591 (133.7998)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [110/431]  eta: 0:05:56  lr: 0.000200  loss: 132.0168 (133.3755)  time: 1.0869  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [120/431]  eta: 0:05:45  lr: 0.000200  loss: 126.7999 (132.8665)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [130/431]  eta: 0:05:34  lr: 0.000200  loss: 133.4458 (133.0392)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [140/431]  eta: 0:05:22  lr: 0.000200  loss: 136.6060 (133.2304)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [150/431]  eta: 0:05:11  lr: 0.000200  loss: 133.2725 (133.0662)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [160/431]  eta: 0:05:00  lr: 0.000200  loss: 139.9892 (133.7699)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 136.5018 (133.6916)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 135.2619 (133.7640)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [190/431]  eta: 0:04:26  lr: 0.000200  loss: 135.1720 (133.8167)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [200/431]  eta: 0:04:15  lr: 0.000200  loss: 134.7448 (133.5824)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [210/431]  eta: 0:04:04  lr: 0.000200  loss: 126.9355 (133.4060)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 129.5116 (133.5520)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 138.1226 (133.8039)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 139.8885 (134.0310)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 138.9847 (134.0641)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 136.3355 (134.1748)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 136.6256 (134.3455)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [280/431]  eta: 0:02:46  lr: 0.000200  loss: 136.6256 (134.1854)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 137.1774 (134.3467)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 135.5344 (134.4033)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 135.3964 (134.3907)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 132.0016 (134.2574)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 137.5025 (134.5368)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 144.3106 (134.7891)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 145.0227 (135.0950)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 142.3877 (135.1346)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 135.5072 (135.0321)  time: 1.1090  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:42]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 133.6194 (135.1161)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 135.3255 (135.1104)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.9769 (135.1739)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 137.9675 (135.2511)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:42]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 134.0242 (135.2607)  time: 1.1013  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:42]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 137.9075 (135.3927)  time: 1.1036  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:42] Total time: 0:07:56 (1.1053 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 137.9075 (135.3927)\n",
      "Valid: [epoch:42]  [ 0/14]  eta: 0:00:33  loss: 160.5304 (160.5304)  time: 2.4259  data: 2.2755  max mem: 15925\n",
      "Valid: [epoch:42]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2701  data: 0.1626  max mem: 15925\n",
      "Valid: [epoch:42] Total time: 0:00:04 (0.2858 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_42_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:43]  [  0/431]  eta: 0:30:41  lr: 0.000200  loss: 159.3119 (159.3119)  time: 4.2728  data: 2.7753  max mem: 15925\n",
      "Train: [epoch:43]  [ 10/431]  eta: 0:09:20  lr: 0.000200  loss: 153.7409 (153.2029)  time: 1.3306  data: 0.2525  max mem: 15925\n",
      "Train: [epoch:43]  [ 20/431]  eta: 0:08:12  lr: 0.000200  loss: 148.7818 (146.7114)  time: 1.0457  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [ 30/431]  eta: 0:07:45  lr: 0.000200  loss: 130.9823 (139.3326)  time: 1.0665  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [ 40/431]  eta: 0:07:26  lr: 0.000200  loss: 127.1853 (135.5515)  time: 1.0804  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [ 50/431]  eta: 0:07:10  lr: 0.000200  loss: 128.0174 (133.8467)  time: 1.0825  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [ 60/431]  eta: 0:06:56  lr: 0.000200  loss: 125.4880 (131.9627)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [ 70/431]  eta: 0:06:44  lr: 0.000200  loss: 126.2413 (132.4167)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [ 80/431]  eta: 0:06:32  lr: 0.000200  loss: 133.9728 (132.6469)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [ 90/431]  eta: 0:06:20  lr: 0.000200  loss: 129.7935 (132.5232)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [100/431]  eta: 0:06:08  lr: 0.000200  loss: 129.7935 (132.1189)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [110/431]  eta: 0:05:57  lr: 0.000200  loss: 130.0359 (131.8817)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 130.9376 (131.6488)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 129.0198 (131.5640)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 127.6843 (131.5228)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 133.7178 (132.1965)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 139.3874 (132.6562)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 138.2349 (132.7874)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 135.5556 (132.7969)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 137.7518 (133.2853)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 136.9165 (133.1776)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 135.9115 (133.2340)  time: 1.1078  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:43]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 135.9115 (133.3282)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 135.5596 (133.4494)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 135.6542 (133.7212)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 139.0772 (133.8754)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 141.4816 (134.3893)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 144.5021 (134.6984)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 140.4865 (134.8987)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 136.4588 (135.0296)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 136.4588 (135.0861)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 136.0393 (135.0233)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 130.0955 (134.7668)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 137.6376 (135.2407)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 151.9345 (135.5689)  time: 1.0889  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 142.4300 (135.7668)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 139.0652 (135.8259)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 135.5407 (135.8443)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 135.5407 (135.8825)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 139.0082 (135.9963)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 138.4674 (135.9361)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 133.2826 (135.7936)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:43]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 133.2132 (135.7430)  time: 1.0966  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:43]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 133.3180 (135.6347)  time: 1.0888  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:43] Total time: 0:07:56 (1.1047 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 133.3180 (135.6347)\n",
      "Valid: [epoch:43]  [ 0/14]  eta: 0:00:35  loss: 101.8203 (101.8203)  time: 2.5647  data: 2.4329  max mem: 15925\n",
      "Valid: [epoch:43]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7076)  time: 0.2837  data: 0.1739  max mem: 15925\n",
      "Valid: [epoch:43] Total time: 0:00:04 (0.2995 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7076)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_43_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.708%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:44]  [  0/431]  eta: 0:32:21  lr: 0.000200  loss: 158.1664 (158.1664)  time: 4.5045  data: 3.2586  max mem: 15925\n",
      "Train: [epoch:44]  [ 10/431]  eta: 0:09:30  lr: 0.000200  loss: 151.1050 (149.9990)  time: 1.3552  data: 0.2964  max mem: 15925\n",
      "Train: [epoch:44]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 138.6182 (143.0609)  time: 1.0454  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 135.5064 (139.4563)  time: 1.0579  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [ 40/431]  eta: 0:07:25  lr: 0.000200  loss: 130.3888 (137.0083)  time: 1.0666  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:44]  [ 50/431]  eta: 0:07:10  lr: 0.000200  loss: 129.4782 (135.2875)  time: 1.0808  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [ 60/431]  eta: 0:06:57  lr: 0.000200  loss: 125.8284 (134.0970)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [ 70/431]  eta: 0:06:45  lr: 0.000200  loss: 129.6838 (134.0482)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [ 80/431]  eta: 0:06:31  lr: 0.000200  loss: 135.4629 (134.5477)  time: 1.0874  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [ 90/431]  eta: 0:06:20  lr: 0.000200  loss: 130.6503 (133.1261)  time: 1.0885  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [100/431]  eta: 0:06:08  lr: 0.000200  loss: 123.0787 (133.2063)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [110/431]  eta: 0:05:56  lr: 0.000200  loss: 128.9036 (132.4338)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [120/431]  eta: 0:05:45  lr: 0.000200  loss: 128.9036 (132.4791)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [130/431]  eta: 0:05:34  lr: 0.000200  loss: 131.5933 (132.3332)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 135.6008 (132.2377)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 135.3943 (132.4555)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [160/431]  eta: 0:05:00  lr: 0.000200  loss: 137.3349 (132.9739)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 133.5425 (132.9610)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 133.0169 (133.0934)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [190/431]  eta: 0:04:26  lr: 0.000200  loss: 138.5452 (133.3706)  time: 1.0896  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [200/431]  eta: 0:04:15  lr: 0.000200  loss: 136.5307 (133.1613)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [210/431]  eta: 0:04:04  lr: 0.000200  loss: 127.3242 (133.0734)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 132.5625 (133.1744)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 135.5397 (133.4851)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 140.6851 (133.7363)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 139.2270 (133.9153)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 134.7438 (134.0173)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [270/431]  eta: 0:02:57  lr: 0.000200  loss: 139.4758 (134.3679)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [280/431]  eta: 0:02:46  lr: 0.000200  loss: 134.1741 (134.0431)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 127.4377 (134.1022)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 138.7088 (134.1981)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 135.5002 (134.1266)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 132.4957 (134.1953)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 141.2128 (134.5544)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 149.6635 (134.9617)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 145.0902 (135.2419)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 143.3916 (135.3125)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 137.9649 (135.3486)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 137.9649 (135.3466)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 136.5345 (135.4226)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 133.6296 (135.3800)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 131.9646 (135.2384)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:44]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 133.7954 (135.3069)  time: 1.1014  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:44]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 140.4367 (135.4489)  time: 1.0885  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:44] Total time: 0:07:55 (1.1043 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 140.4367 (135.4489)\n",
      "Valid: [epoch:44]  [ 0/14]  eta: 0:00:35  loss: 161.4333 (161.4333)  time: 2.5424  data: 2.4212  max mem: 15925\n",
      "Valid: [epoch:44]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2683  data: 0.1730  max mem: 15925\n",
      "Valid: [epoch:44] Total time: 0:00:03 (0.2828 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_44_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:45]  [  0/431]  eta: 0:33:51  lr: 0.000200  loss: 164.1947 (164.1947)  time: 4.7127  data: 3.5070  max mem: 15925\n",
      "Train: [epoch:45]  [ 10/431]  eta: 0:09:35  lr: 0.000200  loss: 151.0170 (149.3996)  time: 1.3671  data: 0.3190  max mem: 15925\n",
      "Train: [epoch:45]  [ 20/431]  eta: 0:08:21  lr: 0.000200  loss: 146.0981 (145.4773)  time: 1.0464  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [ 30/431]  eta: 0:07:50  lr: 0.000200  loss: 134.5803 (142.2397)  time: 1.0658  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 131.0965 (138.5858)  time: 1.0785  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 126.3191 (136.3351)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 126.3191 (134.4205)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 129.0967 (134.1606)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [ 80/431]  eta: 0:06:34  lr: 0.000200  loss: 140.1126 (135.7204)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 138.3373 (134.1434)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 127.4149 (133.9031)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 131.0637 (133.3996)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 129.7249 (133.3045)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 129.8066 (132.9454)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 133.1595 (132.8201)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 138.0514 (133.1368)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 138.1863 (133.1411)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 135.1954 (133.1530)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 138.3819 (133.4304)  time: 1.1026  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:45]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 137.1689 (133.5385)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 131.9856 (133.6388)  time: 1.0927  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:45]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 131.8375 (133.4207)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 133.2199 (133.3765)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 137.0079 (133.6695)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 137.0229 (133.7050)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 137.5710 (134.0426)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 139.5533 (134.2843)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 138.7947 (134.3633)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 127.8028 (134.2628)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 128.1319 (134.3276)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 137.4899 (134.6111)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 139.9367 (134.5364)  time: 1.0918  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 132.7442 (134.4429)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 138.8964 (134.7812)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 146.6382 (135.0674)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 145.6165 (135.3243)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 136.0606 (135.1043)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 132.8083 (135.1305)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 139.5839 (135.1820)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 139.1181 (135.0700)  time: 1.0890  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 133.1866 (135.0847)  time: 1.0802  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:45]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 137.0573 (135.1552)  time: 1.0841  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:45]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 140.2498 (135.3578)  time: 1.0995  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:45]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 145.4045 (135.6175)  time: 1.0944  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:45] Total time: 0:07:55 (1.1028 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 145.4045 (135.6175)\n",
      "Valid: [epoch:45]  [ 0/14]  eta: 0:00:35  loss: 161.1376 (161.1376)  time: 2.5191  data: 2.3764  max mem: 15925\n",
      "Valid: [epoch:45]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2631  data: 0.1698  max mem: 15925\n",
      "Valid: [epoch:45] Total time: 0:00:03 (0.2778 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_45_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:46]  [  0/431]  eta: 0:34:05  lr: 0.000200  loss: 155.1147 (155.1147)  time: 4.7452  data: 3.5471  max mem: 15925\n",
      "Train: [epoch:46]  [ 10/431]  eta: 0:09:36  lr: 0.000200  loss: 144.3214 (141.8642)  time: 1.3702  data: 0.3226  max mem: 15925\n",
      "Train: [epoch:46]  [ 20/431]  eta: 0:08:19  lr: 0.000200  loss: 141.8922 (142.2119)  time: 1.0388  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [ 30/431]  eta: 0:07:47  lr: 0.000200  loss: 135.0340 (139.5908)  time: 1.0521  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 131.3068 (137.4638)  time: 1.0684  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [ 50/431]  eta: 0:07:12  lr: 0.000200  loss: 130.1746 (136.0109)  time: 1.0857  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [ 60/431]  eta: 0:06:57  lr: 0.000200  loss: 130.1445 (134.3311)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [ 70/431]  eta: 0:06:45  lr: 0.000200  loss: 134.1552 (134.8305)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 135.7565 (134.9825)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 134.2274 (134.8351)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [100/431]  eta: 0:06:09  lr: 0.000200  loss: 131.0917 (133.8013)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 121.9577 (132.8086)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 128.2224 (132.6127)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 129.4581 (132.1106)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 129.7592 (132.0586)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 130.3380 (132.1662)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 136.1212 (132.7551)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 136.1212 (132.8194)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 131.3833 (132.8424)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 131.3833 (132.9025)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [200/431]  eta: 0:04:15  lr: 0.000200  loss: 135.1588 (133.0576)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [210/431]  eta: 0:04:04  lr: 0.000200  loss: 135.1588 (133.3225)  time: 1.0874  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 134.4757 (133.3469)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 135.2570 (133.6458)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 140.0892 (133.7654)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 132.3401 (133.8829)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 140.3348 (134.4241)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 142.7479 (134.7513)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [280/431]  eta: 0:02:46  lr: 0.000200  loss: 136.7212 (134.6065)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 131.0187 (134.5204)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 134.9580 (134.6621)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 134.9580 (134.5575)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 131.2666 (134.4122)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 140.2285 (134.7764)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 147.7765 (135.2304)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 147.6384 (135.4188)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 137.8030 (135.4030)  time: 1.0984  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:46]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 134.5409 (135.4290)  time: 1.0912  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 137.4931 (135.5078)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 140.7353 (135.5148)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 134.6235 (135.4839)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.3090 (135.6072)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:46]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.6008 (135.5736)  time: 1.0941  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:46]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 138.0034 (135.6820)  time: 1.0906  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:46] Total time: 0:07:55 (1.1030 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 138.0034 (135.6820)\n",
      "Valid: [epoch:46]  [ 0/14]  eta: 0:00:35  loss: 101.8203 (101.8203)  time: 2.5261  data: 2.3976  max mem: 15925\n",
      "Valid: [epoch:46]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7142)  time: 0.2583  data: 0.1713  max mem: 15925\n",
      "Valid: [epoch:46] Total time: 0:00:03 (0.2733 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7142)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_46_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.714%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:47]  [  0/431]  eta: 0:32:51  lr: 0.000200  loss: 165.5147 (165.5147)  time: 4.5748  data: 3.3969  max mem: 15925\n",
      "Train: [epoch:47]  [ 10/431]  eta: 0:09:38  lr: 0.000200  loss: 151.2982 (148.8926)  time: 1.3739  data: 0.3090  max mem: 15925\n",
      "Train: [epoch:47]  [ 20/431]  eta: 0:08:23  lr: 0.000200  loss: 143.6941 (143.9744)  time: 1.0571  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 131.3333 (138.5070)  time: 1.0680  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 126.9736 (135.0459)  time: 1.0817  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 125.4010 (133.9001)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 126.1981 (133.0447)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 130.0558 (133.8066)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 138.4623 (134.3389)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 133.7475 (133.6791)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 125.8438 (132.5163)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 124.2949 (132.2869)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 134.8278 (132.7567)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 137.3728 (132.4485)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 130.3741 (132.3779)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 132.3982 (132.7349)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 136.8862 (132.9014)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 134.9296 (132.8169)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 134.8319 (132.9184)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 135.8292 (133.2477)  time: 1.0901  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 135.2103 (133.2957)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 131.2296 (133.4055)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 133.5752 (133.4482)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 136.8302 (133.9027)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 143.3763 (134.2885)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 142.1093 (134.5538)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 140.8140 (134.9159)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 145.1542 (135.2682)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 139.9253 (135.0827)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 132.3225 (135.0031)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 135.1879 (135.0874)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 135.1879 (135.1469)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 133.9906 (134.8321)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 137.0122 (135.3597)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 150.3230 (135.7034)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 143.2714 (135.8235)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 136.6281 (135.8294)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 136.2145 (135.8609)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 136.2145 (135.7828)  time: 1.0912  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 128.6812 (135.6931)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 134.6728 (135.7122)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.6728 (135.6907)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:47]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.6626 (135.7302)  time: 1.0964  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:47]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 129.7819 (135.6255)  time: 1.1004  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:47] Total time: 0:07:56 (1.1047 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 129.7819 (135.6255)\n",
      "Valid: [epoch:47]  [ 0/14]  eta: 0:00:35  loss: 114.0275 (114.0275)  time: 2.5493  data: 2.3734  max mem: 15925\n",
      "Valid: [epoch:47]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2779  data: 0.1696  max mem: 15925\n",
      "Valid: [epoch:47] Total time: 0:00:04 (0.2933 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_47_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:48]  [  0/431]  eta: 0:32:11  lr: 0.000200  loss: 135.3560 (135.3560)  time: 4.4809  data: 3.3679  max mem: 15925\n",
      "Train: [epoch:48]  [ 10/431]  eta: 0:09:28  lr: 0.000200  loss: 142.6753 (144.3794)  time: 1.3511  data: 0.3064  max mem: 15925\n",
      "Train: [epoch:48]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 145.4781 (142.3880)  time: 1.0476  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 137.1574 (137.5798)  time: 1.0615  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:48]  [ 40/431]  eta: 0:07:26  lr: 0.000200  loss: 129.7650 (136.2301)  time: 1.0708  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [ 50/431]  eta: 0:07:10  lr: 0.000200  loss: 128.7482 (134.6485)  time: 1.0784  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [ 60/431]  eta: 0:06:57  lr: 0.000200  loss: 127.7595 (134.0232)  time: 1.0871  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [ 70/431]  eta: 0:06:44  lr: 0.000200  loss: 129.7105 (133.7582)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [ 80/431]  eta: 0:06:32  lr: 0.000200  loss: 132.7218 (134.2073)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [ 90/431]  eta: 0:06:20  lr: 0.000200  loss: 135.8973 (133.9635)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [100/431]  eta: 0:06:08  lr: 0.000200  loss: 132.8957 (133.9476)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [110/431]  eta: 0:05:57  lr: 0.000200  loss: 130.9100 (133.5263)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [120/431]  eta: 0:05:45  lr: 0.000200  loss: 129.0202 (133.2832)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [130/431]  eta: 0:05:34  lr: 0.000200  loss: 127.8871 (132.8443)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 129.6900 (132.9536)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [150/431]  eta: 0:05:11  lr: 0.000200  loss: 136.9680 (133.2590)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [160/431]  eta: 0:05:00  lr: 0.000200  loss: 136.7558 (133.2700)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 133.3105 (133.3580)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [180/431]  eta: 0:04:37  lr: 0.000200  loss: 133.0016 (133.5709)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [190/431]  eta: 0:04:26  lr: 0.000200  loss: 138.8271 (133.9511)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [200/431]  eta: 0:04:15  lr: 0.000200  loss: 141.6667 (134.1863)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [210/431]  eta: 0:04:04  lr: 0.000200  loss: 137.6813 (134.2246)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 132.9547 (134.1667)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 132.9547 (134.1277)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 136.6907 (134.1356)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [250/431]  eta: 0:03:19  lr: 0.000200  loss: 129.9137 (133.9176)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [260/431]  eta: 0:03:08  lr: 0.000200  loss: 134.9891 (134.2336)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [270/431]  eta: 0:02:57  lr: 0.000200  loss: 144.4772 (134.5421)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [280/431]  eta: 0:02:46  lr: 0.000200  loss: 141.5453 (134.6428)  time: 1.1083  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:48]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 130.0213 (134.5827)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 130.9487 (134.6176)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 130.1863 (134.4308)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 130.1863 (134.2270)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 138.3504 (134.6140)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 144.6265 (134.9124)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 145.1904 (135.2036)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 142.2512 (135.2179)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 138.8255 (135.3448)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 138.2283 (135.3808)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 136.3569 (135.3890)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 136.3569 (135.3927)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.1734 (135.3898)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:48]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.1734 (135.4066)  time: 1.1087  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:48]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 135.2391 (135.3994)  time: 1.1090  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:48] Total time: 0:07:55 (1.1038 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 135.2391 (135.3994)\n",
      "Valid: [epoch:48]  [ 0/14]  eta: 0:00:37  loss: 160.5304 (160.5304)  time: 2.6532  data: 2.5276  max mem: 15925\n",
      "Valid: [epoch:48]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2852  data: 0.1806  max mem: 15925\n",
      "Valid: [epoch:48] Total time: 0:00:04 (0.3016 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_48_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:49]  [  0/431]  eta: 0:31:52  lr: 0.000200  loss: 175.7685 (175.7685)  time: 4.4375  data: 3.2631  max mem: 15925\n",
      "Train: [epoch:49]  [ 10/431]  eta: 0:09:29  lr: 0.000200  loss: 149.3207 (149.0946)  time: 1.3524  data: 0.2969  max mem: 15925\n",
      "Train: [epoch:49]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 144.6300 (144.7184)  time: 1.0492  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [ 30/431]  eta: 0:07:47  lr: 0.000200  loss: 134.9319 (139.5500)  time: 1.0629  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 123.3591 (136.6235)  time: 1.0751  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [ 50/431]  eta: 0:07:10  lr: 0.000200  loss: 129.6875 (135.3629)  time: 1.0771  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [ 60/431]  eta: 0:06:57  lr: 0.000200  loss: 129.5776 (134.3767)  time: 1.0857  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [ 70/431]  eta: 0:06:44  lr: 0.000200  loss: 134.1431 (135.2634)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [ 80/431]  eta: 0:06:32  lr: 0.000200  loss: 137.2330 (135.8649)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 136.8036 (134.8536)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [100/431]  eta: 0:06:09  lr: 0.000200  loss: 128.0957 (134.2233)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 125.2855 (133.1706)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 125.2855 (132.8243)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 130.2847 (132.7534)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 132.0581 (132.7764)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 136.9261 (133.1595)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 140.0024 (133.4047)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 138.3318 (133.6011)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 133.3623 (133.3214)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 135.9480 (133.4164)  time: 1.1088  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:49]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 138.3313 (133.5907)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 133.6934 (133.6402)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 135.2924 (133.7357)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 139.2227 (134.0161)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 141.2050 (134.1972)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 139.3241 (134.4586)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 137.0843 (134.6903)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 134.2559 (134.8776)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 130.5045 (134.7642)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 128.3519 (134.5133)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 136.5411 (134.7399)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 139.0411 (134.7457)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 127.6080 (134.5033)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 138.4253 (134.9406)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 150.8153 (135.3493)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 148.3744 (135.6304)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 141.7570 (135.7203)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 135.3698 (135.5676)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 137.1295 (135.6828)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 137.1295 (135.6294)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 131.0998 (135.5254)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 130.8478 (135.4683)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:49]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 134.2695 (135.5770)  time: 1.1079  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:49]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 133.1745 (135.5907)  time: 1.1059  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:49] Total time: 0:07:57 (1.1080 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 133.1745 (135.5907)\n",
      "Valid: [epoch:49]  [ 0/14]  eta: 0:00:34  loss: 160.5304 (160.5304)  time: 2.4505  data: 2.2924  max mem: 15925\n",
      "Valid: [epoch:49]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2543  data: 0.1638  max mem: 15925\n",
      "Valid: [epoch:49] Total time: 0:00:03 (0.2696 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_49_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:50]  [  0/431]  eta: 0:31:32  lr: 0.000200  loss: 157.1910 (157.1910)  time: 4.3916  data: 3.2000  max mem: 15925\n",
      "Train: [epoch:50]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 141.5468 (143.8833)  time: 1.3420  data: 0.2911  max mem: 15925\n",
      "Train: [epoch:50]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 137.7487 (138.6754)  time: 1.0491  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [ 30/431]  eta: 0:07:44  lr: 0.000200  loss: 129.1424 (133.7040)  time: 1.0589  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 122.9542 (132.3029)  time: 1.0757  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [ 50/431]  eta: 0:07:11  lr: 0.000200  loss: 125.6967 (132.0787)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [ 60/431]  eta: 0:06:57  lr: 0.000200  loss: 125.6967 (130.8375)  time: 1.0856  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [ 70/431]  eta: 0:06:44  lr: 0.000200  loss: 132.9241 (131.8338)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 136.8649 (132.6751)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 134.1051 (132.4629)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [100/431]  eta: 0:06:09  lr: 0.000200  loss: 130.7682 (132.0543)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [110/431]  eta: 0:05:57  lr: 0.000200  loss: 130.6406 (131.9645)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [120/431]  eta: 0:05:45  lr: 0.000200  loss: 134.0348 (132.3499)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 134.5207 (132.4505)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 130.6952 (132.2684)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 135.1976 (132.8183)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 136.8714 (133.1405)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 136.8714 (133.3198)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 137.2693 (133.5911)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 141.0395 (134.1538)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 140.6912 (134.2530)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 137.9382 (134.5479)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 137.9382 (134.5847)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 135.8111 (134.7126)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 138.6163 (134.9193)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 138.8130 (135.1576)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 138.8130 (135.2574)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 138.3795 (135.5340)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 135.2898 (135.4624)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 134.1174 (135.4278)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 136.7016 (135.6649)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 132.7398 (135.4439)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 126.3041 (135.2138)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 137.7592 (135.5315)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 144.2395 (135.7973)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 144.1488 (135.9256)  time: 1.1018  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:50]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 135.5217 (135.8654)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 136.4634 (135.9863)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 139.9841 (136.0752)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 131.0102 (135.8892)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 131.9164 (135.7912)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 131.9164 (135.6879)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:50]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 132.4552 (135.6312)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:50]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 132.4552 (135.5514)  time: 1.0988  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:50] Total time: 0:07:56 (1.1064 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 132.4552 (135.5514)\n",
      "Valid: [epoch:50]  [ 0/14]  eta: 0:00:36  loss: 158.8652 (158.8652)  time: 2.6148  data: 2.4380  max mem: 15925\n",
      "Valid: [epoch:50]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2659  data: 0.1742  max mem: 15925\n",
      "Valid: [epoch:50] Total time: 0:00:03 (0.2829 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_50_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:51]  [  0/431]  eta: 0:35:30  lr: 0.000200  loss: 151.9956 (151.9956)  time: 4.9422  data: 3.8500  max mem: 15925\n",
      "Train: [epoch:51]  [ 10/431]  eta: 0:09:48  lr: 0.000200  loss: 151.9956 (151.7289)  time: 1.3981  data: 0.3502  max mem: 15925\n",
      "Train: [epoch:51]  [ 20/431]  eta: 0:08:27  lr: 0.000200  loss: 145.9093 (146.7212)  time: 1.0487  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 137.0520 (142.0066)  time: 1.0610  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:51]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 131.0091 (139.2562)  time: 1.0762  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [ 50/431]  eta: 0:07:16  lr: 0.000200  loss: 129.6161 (136.7746)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 127.0059 (135.0654)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 130.1752 (135.4878)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 138.0871 (135.5516)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 131.6002 (134.4072)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 121.7605 (133.7118)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 124.9771 (133.1061)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 127.5386 (133.1764)  time: 1.0905  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 130.1609 (132.8735)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 133.5425 (133.0359)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 134.1174 (132.7769)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 138.4506 (133.3373)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 138.4506 (133.2246)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 131.5128 (133.2271)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 136.6556 (133.4493)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 135.7664 (133.3704)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 135.1701 (133.3598)  time: 1.0905  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 135.3517 (133.5859)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 135.4125 (133.8199)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 136.2323 (133.9849)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 136.2323 (134.0064)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 136.2820 (134.3138)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 152.0331 (134.7907)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 135.9866 (134.6259)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 135.2308 (134.7788)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 140.1268 (134.9808)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 138.2191 (134.8651)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 129.4741 (134.7794)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 133.8526 (134.9523)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 145.1854 (135.2597)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 143.8950 (135.4033)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 137.4239 (135.3680)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 132.8463 (135.3454)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 134.3101 (135.4634)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 134.0820 (135.3218)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 132.8648 (135.2857)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.1197 (135.3308)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:51]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 134.2671 (135.2219)  time: 1.0935  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:51]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 135.9977 (135.4110)  time: 1.0988  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:51] Total time: 0:07:56 (1.1057 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 135.9977 (135.4110)\n",
      "Valid: [epoch:51]  [ 0/14]  eta: 0:00:34  loss: 158.8652 (158.8652)  time: 2.4483  data: 2.2945  max mem: 15925\n",
      "Valid: [epoch:51]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7051)  time: 0.2685  data: 0.1640  max mem: 15925\n",
      "Valid: [epoch:51] Total time: 0:00:03 (0.2837 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7051)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_51_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:52]  [  0/431]  eta: 0:34:54  lr: 0.000200  loss: 115.3181 (115.3181)  time: 4.8601  data: 3.6361  max mem: 15925\n",
      "Train: [epoch:52]  [ 10/431]  eta: 0:09:32  lr: 0.000200  loss: 145.6717 (145.5402)  time: 1.3602  data: 0.3308  max mem: 15925\n",
      "Train: [epoch:52]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 143.7456 (143.8647)  time: 1.0352  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:52]  [ 30/431]  eta: 0:07:48  lr: 0.000200  loss: 134.6819 (139.9155)  time: 1.0645  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 128.9667 (136.4957)  time: 1.0806  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 128.8000 (136.5183)  time: 1.0905  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 129.4585 (135.3044)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 130.9543 (135.7377)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 141.3670 (136.5261)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 136.2243 (135.5549)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 129.1600 (134.9176)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 126.8174 (134.3910)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 129.4892 (134.2424)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 129.4892 (133.5317)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 128.7824 (133.4398)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 132.0251 (133.3633)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 138.6352 (134.0876)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 133.9509 (133.9992)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 131.9298 (134.3121)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 140.1163 (134.2800)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 136.5675 (134.0879)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 132.9746 (133.8226)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 137.8776 (134.0060)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 138.6606 (134.0461)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 138.6606 (134.2398)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 136.6430 (134.2477)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 137.7966 (134.5063)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 140.9402 (134.8380)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 139.2317 (134.8952)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 133.6801 (134.8993)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 136.5001 (135.0718)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 132.3996 (134.8365)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 127.1686 (134.4952)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 131.0274 (134.6704)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 145.7904 (135.1650)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 151.1850 (135.4543)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 137.6426 (135.3722)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 131.2672 (135.3664)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 131.7242 (135.4493)  time: 1.0890  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 131.7242 (135.4911)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 131.5158 (135.4743)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.0712 (135.5346)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:52]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.0092 (135.6275)  time: 1.1004  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:52]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 139.1392 (135.6651)  time: 1.1011  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:52] Total time: 0:07:57 (1.1069 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 139.1392 (135.6651)\n",
      "Valid: [epoch:52]  [ 0/14]  eta: 0:00:35  loss: 108.0685 (108.0685)  time: 2.5018  data: 2.3116  max mem: 15925\n",
      "Valid: [epoch:52]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2792  data: 0.1675  max mem: 15925\n",
      "Valid: [epoch:52] Total time: 0:00:04 (0.2961 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_52_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:53]  [  0/431]  eta: 0:29:45  lr: 0.000200  loss: 101.6140 (101.6140)  time: 4.1418  data: 2.9436  max mem: 15925\n",
      "Train: [epoch:53]  [ 10/431]  eta: 0:09:14  lr: 0.000200  loss: 142.9277 (136.6243)  time: 1.3164  data: 0.2678  max mem: 15925\n",
      "Train: [epoch:53]  [ 20/431]  eta: 0:08:09  lr: 0.000200  loss: 141.4495 (139.5802)  time: 1.0426  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [ 30/431]  eta: 0:07:42  lr: 0.000200  loss: 131.3376 (135.9803)  time: 1.0652  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [ 40/431]  eta: 0:07:23  lr: 0.000200  loss: 129.5882 (134.8178)  time: 1.0763  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [ 50/431]  eta: 0:07:09  lr: 0.000200  loss: 128.4957 (133.5941)  time: 1.0880  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [ 60/431]  eta: 0:06:57  lr: 0.000200  loss: 125.4655 (132.4812)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [ 70/431]  eta: 0:06:44  lr: 0.000200  loss: 131.1935 (132.6076)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [ 80/431]  eta: 0:06:32  lr: 0.000200  loss: 131.4171 (132.8311)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [ 90/431]  eta: 0:06:19  lr: 0.000200  loss: 131.3894 (132.7892)  time: 1.0918  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [100/431]  eta: 0:06:08  lr: 0.000200  loss: 131.3894 (132.5826)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [110/431]  eta: 0:05:57  lr: 0.000200  loss: 122.1589 (131.3099)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [120/431]  eta: 0:05:45  lr: 0.000200  loss: 125.3589 (131.5103)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [130/431]  eta: 0:05:34  lr: 0.000200  loss: 137.1995 (131.7679)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 139.9028 (132.1271)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [150/431]  eta: 0:05:11  lr: 0.000200  loss: 143.2852 (132.5907)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [160/431]  eta: 0:05:00  lr: 0.000200  loss: 144.0889 (133.3225)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 143.8786 (133.3949)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [180/431]  eta: 0:04:37  lr: 0.000200  loss: 130.5396 (133.0564)  time: 1.0990  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:53]  [190/431]  eta: 0:04:26  lr: 0.000200  loss: 130.5396 (133.3848)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [200/431]  eta: 0:04:15  lr: 0.000200  loss: 137.0942 (133.3996)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [210/431]  eta: 0:04:04  lr: 0.000200  loss: 137.0942 (133.5517)  time: 1.0909  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 136.3344 (133.7301)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [230/431]  eta: 0:03:41  lr: 0.000200  loss: 136.3344 (133.8683)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [240/431]  eta: 0:03:30  lr: 0.000200  loss: 137.8804 (133.8461)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [250/431]  eta: 0:03:19  lr: 0.000200  loss: 137.8180 (133.9979)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [260/431]  eta: 0:03:08  lr: 0.000200  loss: 140.9561 (134.2522)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [270/431]  eta: 0:02:57  lr: 0.000200  loss: 141.0284 (134.5498)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [280/431]  eta: 0:02:46  lr: 0.000200  loss: 136.1529 (134.5145)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 137.2594 (134.5370)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 139.1866 (134.8291)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 137.4265 (134.5913)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 125.1131 (134.3691)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 132.6445 (134.7254)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 144.7491 (135.1210)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 144.7491 (135.3784)  time: 1.0871  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 134.9206 (135.2642)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 132.8938 (135.1435)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 135.8975 (135.3219)  time: 1.0870  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 137.7771 (135.3358)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 135.6315 (135.3535)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 134.7017 (135.3675)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:53]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 137.0740 (135.4391)  time: 1.0985  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:53]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 143.5545 (135.5107)  time: 1.1009  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:53] Total time: 0:07:55 (1.1021 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 143.5545 (135.5107)\n",
      "Valid: [epoch:53]  [ 0/14]  eta: 0:00:34  loss: 128.6310 (128.6310)  time: 2.4963  data: 2.3221  max mem: 15925\n",
      "Valid: [epoch:53]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7055)  time: 0.2632  data: 0.1660  max mem: 15925\n",
      "Valid: [epoch:53] Total time: 0:00:03 (0.2799 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7055)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_53_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:54]  [  0/431]  eta: 0:34:46  lr: 0.000200  loss: 143.2838 (143.2838)  time: 4.8419  data: 3.7631  max mem: 15925\n",
      "Train: [epoch:54]  [ 10/431]  eta: 0:09:35  lr: 0.000200  loss: 153.7569 (151.0780)  time: 1.3672  data: 0.3423  max mem: 15925\n",
      "Train: [epoch:54]  [ 20/431]  eta: 0:08:23  lr: 0.000200  loss: 149.2884 (147.8292)  time: 1.0447  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 139.0563 (142.9551)  time: 1.0692  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 129.0751 (139.7385)  time: 1.0801  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 132.0752 (139.1332)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 128.9416 (135.8616)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 125.4579 (135.1222)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 130.6382 (135.1864)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 129.4306 (134.3440)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 131.0995 (134.2344)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 131.6217 (133.4355)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 129.9657 (133.0883)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 130.2107 (133.0379)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 132.6056 (132.8940)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 132.6056 (132.9527)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 132.6920 (133.0606)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 134.1084 (133.0522)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 136.3027 (133.4374)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 137.5342 (133.7261)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 133.1013 (133.5439)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 129.8528 (133.5291)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 138.1347 (133.9157)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 141.3513 (134.0991)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 145.3538 (134.5268)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 135.3677 (134.4764)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 135.3677 (134.7574)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 141.6988 (135.0382)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 139.5862 (135.0015)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 136.0504 (134.8755)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 136.6038 (135.0108)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 136.3120 (134.9583)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 126.7394 (134.6675)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 135.3225 (134.9145)  time: 1.0906  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 142.8977 (135.2079)  time: 1.0899  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:54]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 141.9701 (135.4146)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 133.9100 (135.1869)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 132.3998 (135.2682)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 140.5083 (135.4105)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 140.5079 (135.4259)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 133.1685 (135.3207)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 133.5942 (135.2608)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:54]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 136.1491 (135.3322)  time: 1.1010  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:54]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 140.3805 (135.3867)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:54] Total time: 0:07:57 (1.1069 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 140.3805 (135.3867)\n",
      "Valid: [epoch:54]  [ 0/14]  eta: 0:00:35  loss: 160.5304 (160.5304)  time: 2.5671  data: 2.4183  max mem: 15925\n",
      "Valid: [epoch:54]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2761  data: 0.1728  max mem: 15925\n",
      "Valid: [epoch:54] Total time: 0:00:04 (0.2947 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_54_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:55]  [  0/431]  eta: 0:31:53  lr: 0.000200  loss: 145.5586 (145.5586)  time: 4.4400  data: 3.2887  max mem: 15925\n",
      "Train: [epoch:55]  [ 10/431]  eta: 0:09:25  lr: 0.000200  loss: 149.1012 (149.9208)  time: 1.3425  data: 0.2991  max mem: 15925\n",
      "Train: [epoch:55]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 145.1051 (146.7362)  time: 1.0499  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 131.8439 (139.8532)  time: 1.0652  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 125.4922 (137.0647)  time: 1.0722  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [ 50/431]  eta: 0:07:11  lr: 0.000200  loss: 126.3402 (134.0124)  time: 1.0859  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [ 60/431]  eta: 0:06:58  lr: 0.000200  loss: 125.0763 (132.0566)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [ 70/431]  eta: 0:06:45  lr: 0.000200  loss: 125.8302 (132.2612)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 136.3744 (133.6881)  time: 1.1059  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:55]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 135.1408 (133.3637)  time: 1.1004  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:55]  [100/431]  eta: 0:06:09  lr: 0.000200  loss: 131.2328 (133.1338)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [110/431]  eta: 0:05:57  lr: 0.000200  loss: 129.2638 (132.2613)  time: 1.0983  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:55]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 122.2287 (131.9155)  time: 1.0993  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:55]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 128.2874 (131.7360)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 128.2874 (131.6285)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 135.3902 (132.3360)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [160/431]  eta: 0:05:00  lr: 0.000200  loss: 139.5284 (132.7487)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 137.5519 (132.9012)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 137.5519 (133.0930)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 136.7235 (133.3821)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 137.1119 (133.4993)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 134.9534 (133.4711)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 134.7949 (133.4418)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 135.8542 (133.6283)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 137.5666 (133.6851)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 138.2381 (133.8141)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 140.3400 (134.0667)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 143.1128 (134.5228)  time: 1.1069  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:55]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 137.0705 (134.4195)  time: 1.1036  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:55]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 134.9180 (134.5457)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 138.3224 (134.7052)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 134.6881 (134.5714)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 125.9395 (134.3347)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 133.1014 (134.5371)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 144.6365 (134.8849)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 144.6365 (135.0637)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 139.4959 (134.9249)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 139.4959 (135.2053)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 144.5141 (135.2922)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 133.3340 (135.2324)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 132.4707 (135.2588)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 136.8514 (135.3939)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:55]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 141.0426 (135.5035)  time: 1.1062  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:55]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 137.9510 (135.5311)  time: 1.1042  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:55] Total time: 0:07:57 (1.1069 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 137.9510 (135.5311)\n",
      "Valid: [epoch:55]  [ 0/14]  eta: 0:00:34  loss: 114.0275 (114.0275)  time: 2.4945  data: 2.3130  max mem: 15925\n",
      "Valid: [epoch:55]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7051)  time: 0.2751  data: 0.1653  max mem: 15925\n",
      "Valid: [epoch:55] Total time: 0:00:04 (0.2919 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7051)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_55_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:56]  [  0/431]  eta: 0:31:43  lr: 0.000200  loss: 113.9613 (113.9613)  time: 4.4155  data: 3.1328  max mem: 15925\n",
      "Train: [epoch:56]  [ 10/431]  eta: 0:09:29  lr: 0.000200  loss: 152.4310 (147.1764)  time: 1.3529  data: 0.2850  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:56]  [ 20/431]  eta: 0:08:21  lr: 0.000200  loss: 142.3717 (141.1366)  time: 1.0615  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [ 30/431]  eta: 0:07:50  lr: 0.000200  loss: 129.9456 (136.3335)  time: 1.0732  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 129.8189 (134.8423)  time: 1.0795  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 129.2891 (133.4649)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 127.2707 (132.3645)  time: 1.1124  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:56]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 127.3342 (132.3584)  time: 1.1182  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:56]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 134.3087 (132.8112)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 134.3087 (132.3756)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 131.6425 (132.5723)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 132.1083 (132.3876)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 128.7928 (131.9380)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 130.2586 (131.9943)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 133.1783 (132.1980)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 136.1182 (132.5642)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 141.5856 (133.2395)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 133.8316 (133.0177)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 129.2138 (132.9607)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 135.0935 (133.2547)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 131.9713 (133.0398)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 130.2341 (133.2106)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 134.3382 (133.2935)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 137.7936 (133.7107)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 140.0671 (133.8794)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 139.5403 (133.9004)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 141.9214 (134.3072)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 142.8427 (134.6213)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 140.4057 (134.6718)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 132.6801 (134.5360)  time: 1.1051  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:56]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 138.4713 (134.8848)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 137.3185 (134.6814)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 132.9456 (134.6713)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 139.3393 (135.0514)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 146.1939 (135.3101)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 144.2324 (135.4077)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 135.7976 (135.4206)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 135.3059 (135.4085)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 133.8737 (135.3882)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 137.0645 (135.4686)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 137.7706 (135.4928)  time: 1.1196  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:56]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 133.3582 (135.3882)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:56]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 133.3582 (135.4113)  time: 1.0965  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:56]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 136.0370 (135.5658)  time: 1.1084  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:56] Total time: 0:07:58 (1.1103 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 136.0370 (135.5658)\n",
      "Valid: [epoch:56]  [ 0/14]  eta: 0:00:36  loss: 132.8970 (132.8970)  time: 2.5981  data: 2.4538  max mem: 15925\n",
      "Valid: [epoch:56]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7052)  time: 0.2788  data: 0.1754  max mem: 15925\n",
      "Valid: [epoch:56] Total time: 0:00:04 (0.2971 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7052)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_56_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:57]  [  0/431]  eta: 0:32:28  lr: 0.000200  loss: 131.0426 (131.0426)  time: 4.5220  data: 3.3318  max mem: 15925\n",
      "Train: [epoch:57]  [ 10/431]  eta: 0:09:29  lr: 0.000200  loss: 148.7442 (147.5896)  time: 1.3518  data: 0.3031  max mem: 15925\n",
      "Train: [epoch:57]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 144.8087 (141.7712)  time: 1.0416  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 133.7812 (138.9540)  time: 1.0614  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [ 40/431]  eta: 0:07:28  lr: 0.000200  loss: 132.8976 (136.4725)  time: 1.0864  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 133.5580 (135.9915)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 130.2898 (134.4354)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 130.4444 (135.0085)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 141.1123 (136.2081)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 133.5419 (135.3517)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 126.6494 (134.8975)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 123.7820 (133.9885)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 126.5064 (133.4317)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 127.5811 (133.1837)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 130.8068 (133.4311)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 136.5043 (133.8523)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 136.5043 (133.9812)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 134.4604 (133.7941)  time: 1.1060  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:57]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 134.8057 (133.8906)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 139.2056 (134.3067)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 138.0478 (134.2236)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 132.5387 (134.2115)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 134.5266 (134.2540)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 136.5523 (134.5644)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 139.4585 (134.6499)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 139.4585 (134.8862)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 142.6378 (135.1898)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 137.3068 (135.1827)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 136.9800 (135.3510)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 132.5749 (135.3149)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 134.0870 (135.3119)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 134.0870 (135.0647)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 127.0922 (134.8960)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 134.7059 (135.2460)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 147.5644 (135.6026)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 143.9384 (135.7478)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 135.4364 (135.5481)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 136.9773 (135.5279)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 138.8062 (135.6187)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 138.8062 (135.6048)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 136.5976 (135.6552)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 132.7216 (135.5936)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:57]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 132.7216 (135.5488)  time: 1.0968  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:57]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 134.8554 (135.4802)  time: 1.1062  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:57] Total time: 0:07:59 (1.1131 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 134.8554 (135.4802)\n",
      "Valid: [epoch:57]  [ 0/14]  eta: 0:00:35  loss: 124.2619 (124.2619)  time: 2.5515  data: 2.4064  max mem: 15925\n",
      "Valid: [epoch:57]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7054)  time: 0.2781  data: 0.1720  max mem: 15925\n",
      "Valid: [epoch:57] Total time: 0:00:04 (0.2972 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7054)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_57_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:58]  [  0/431]  eta: 0:31:22  lr: 0.000200  loss: 163.5706 (163.5706)  time: 4.3680  data: 3.0500  max mem: 15925\n",
      "Train: [epoch:58]  [ 10/431]  eta: 0:09:27  lr: 0.000200  loss: 155.4155 (154.9281)  time: 1.3469  data: 0.2774  max mem: 15925\n",
      "Train: [epoch:58]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 148.1922 (148.4478)  time: 1.0594  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [ 30/431]  eta: 0:07:49  lr: 0.000200  loss: 130.3481 (139.7918)  time: 1.0751  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 124.5566 (137.0168)  time: 1.0793  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 124.9937 (134.8007)  time: 1.0893  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 129.2435 (134.0621)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 133.4778 (133.7782)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 135.2214 (134.1870)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 131.9109 (133.3737)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 126.9795 (132.6128)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 127.5547 (132.4518)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 127.5547 (131.8023)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 130.7694 (132.1115)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 132.5499 (131.9987)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 138.5022 (132.7059)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 140.3099 (133.0297)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 130.7256 (132.5764)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 130.7256 (132.9227)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 138.5953 (133.4387)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 137.2256 (133.3310)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 134.9986 (133.7037)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 138.4513 (133.9577)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 138.8173 (134.2178)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 143.0049 (134.6035)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 141.3453 (134.6299)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 135.9469 (134.5378)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 135.9469 (134.8635)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 132.5391 (134.7432)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 129.6022 (134.5999)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 135.0037 (134.7846)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 132.4692 (134.6699)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 124.7943 (134.4169)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 129.5310 (134.8343)  time: 1.1060  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:58]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 147.6801 (135.1116)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 144.9566 (135.4969)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 139.7160 (135.5387)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 134.8429 (135.4951)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 135.1415 (135.4391)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 136.0902 (135.4107)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 128.2966 (135.1473)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:58]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 127.3389 (135.1475)  time: 1.0932  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:58]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 139.1926 (135.2984)  time: 1.0932  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:58]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 140.0863 (135.4200)  time: 1.0998  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:58] Total time: 0:07:59 (1.1118 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 140.0863 (135.4200)\n",
      "Valid: [epoch:58]  [ 0/14]  eta: 0:00:37  loss: 161.1376 (161.1376)  time: 2.6552  data: 2.5153  max mem: 15925\n",
      "Valid: [epoch:58]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2695  data: 0.1797  max mem: 15925\n",
      "Valid: [epoch:58] Total time: 0:00:04 (0.2891 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_58_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:59]  [  0/431]  eta: 0:34:41  lr: 0.000200  loss: 125.7988 (125.7988)  time: 4.8296  data: 3.7050  max mem: 15925\n",
      "Train: [epoch:59]  [ 10/431]  eta: 0:09:35  lr: 0.000200  loss: 154.6884 (149.9292)  time: 1.3670  data: 0.3370  max mem: 15925\n",
      "Train: [epoch:59]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 142.6845 (146.6094)  time: 1.0300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [ 30/431]  eta: 0:07:47  lr: 0.000200  loss: 134.1923 (140.1251)  time: 1.0555  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 123.6837 (136.4835)  time: 1.0756  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 122.2718 (134.7289)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 128.5349 (133.5158)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 135.1563 (134.6183)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 139.2334 (134.9826)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 134.9062 (134.8354)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 130.3685 (134.3037)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 125.4104 (133.3761)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 127.6030 (133.4047)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 133.6893 (133.2053)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 125.7811 (132.6765)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 133.0462 (133.0196)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 135.3880 (132.8173)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 132.3545 (132.9509)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 132.7896 (133.1114)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 140.8994 (133.5267)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 140.8994 (133.8959)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 134.1201 (133.4647)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 125.5566 (133.4103)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 135.1477 (133.6674)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 140.0246 (133.9016)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 134.0835 (133.9180)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 137.8822 (134.2279)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 141.8770 (134.2954)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 139.9839 (134.4722)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 134.5811 (134.5024)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 138.0310 (134.7356)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 135.5453 (134.7341)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 131.7812 (134.6583)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 138.9640 (135.0423)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 144.9035 (135.4405)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 143.7505 (135.6684)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 139.3026 (135.5303)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 130.9057 (135.5078)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 132.5531 (135.5023)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 130.6778 (135.4942)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 131.0314 (135.4330)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 133.6799 (135.3713)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:59]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 135.2022 (135.4141)  time: 1.1020  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:59]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 136.9385 (135.5077)  time: 1.0996  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:59] Total time: 0:07:57 (1.1075 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 136.9385 (135.5077)\n",
      "Valid: [epoch:59]  [ 0/14]  eta: 0:00:34  loss: 161.9902 (161.9902)  time: 2.4801  data: 2.3468  max mem: 15925\n",
      "Valid: [epoch:59]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2623  data: 0.1677  max mem: 15925\n",
      "Valid: [epoch:59] Total time: 0:00:03 (0.2776 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_59_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:60]  [  0/431]  eta: 0:30:23  lr: 0.000200  loss: 147.7014 (147.7014)  time: 4.2309  data: 3.0710  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:60]  [ 10/431]  eta: 0:09:16  lr: 0.000200  loss: 148.1391 (152.9522)  time: 1.3207  data: 0.2794  max mem: 15925\n",
      "Train: [epoch:60]  [ 20/431]  eta: 0:08:11  lr: 0.000200  loss: 147.0554 (144.8666)  time: 1.0441  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [ 30/431]  eta: 0:07:42  lr: 0.000200  loss: 126.3585 (138.2681)  time: 1.0616  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [ 40/431]  eta: 0:07:24  lr: 0.000200  loss: 125.4033 (135.7930)  time: 1.0756  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [ 50/431]  eta: 0:07:10  lr: 0.000200  loss: 128.7677 (134.7315)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 128.5923 (133.5676)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [ 70/431]  eta: 0:06:46  lr: 0.000200  loss: 130.1430 (133.5473)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 138.3605 (134.5308)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 134.8449 (133.8686)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 130.4523 (133.3147)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 124.3053 (132.5465)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 127.0881 (132.2707)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 127.8372 (132.0088)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 132.1552 (132.0508)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 138.3209 (132.6823)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 139.2742 (133.0030)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 133.6225 (132.6179)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 128.4424 (132.3017)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 129.6601 (132.4487)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 137.3469 (132.6429)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 131.9275 (132.5911)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 130.4833 (132.6104)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 137.4863 (132.8853)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 137.5102 (133.1123)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 134.5278 (133.1629)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 139.6040 (133.6811)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 142.6289 (134.0947)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 138.2951 (134.1117)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 135.9564 (134.1724)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 135.9564 (134.3958)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 134.8334 (134.3683)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 129.5983 (134.1847)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 139.7001 (134.6034)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 150.5355 (134.9815)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 150.2574 (135.3226)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 138.3833 (135.2801)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 134.2921 (135.1760)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 133.5956 (135.1679)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 135.4431 (135.2489)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 137.9061 (135.1891)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 138.3078 (135.3506)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:60]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 143.6936 (135.5558)  time: 1.1026  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:60]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 138.5408 (135.5145)  time: 1.0960  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:60] Total time: 0:07:56 (1.1065 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 138.5408 (135.5145)\n",
      "Valid: [epoch:60]  [ 0/14]  eta: 0:00:34  loss: 128.6310 (128.6310)  time: 2.4701  data: 2.2944  max mem: 15925\n",
      "Valid: [epoch:60]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2556  data: 0.1640  max mem: 15925\n",
      "Valid: [epoch:60] Total time: 0:00:03 (0.2703 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_60_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:61]  [  0/431]  eta: 0:35:27  lr: 0.000200  loss: 131.0364 (131.0364)  time: 4.9357  data: 3.8020  max mem: 15925\n",
      "Train: [epoch:61]  [ 10/431]  eta: 0:09:46  lr: 0.000200  loss: 148.9182 (147.2355)  time: 1.3930  data: 0.3458  max mem: 15925\n",
      "Train: [epoch:61]  [ 20/431]  eta: 0:08:31  lr: 0.000200  loss: 144.9197 (141.8715)  time: 1.0600  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [ 30/431]  eta: 0:07:55  lr: 0.000200  loss: 131.3760 (137.0308)  time: 1.0707  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 125.0719 (134.9542)  time: 1.0703  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 125.9394 (132.8368)  time: 1.0792  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 126.3163 (132.0515)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 127.6113 (132.2721)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 138.4639 (133.0458)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 135.5402 (132.7665)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 127.9146 (132.0262)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 127.9939 (131.9321)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 132.6797 (131.8034)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 133.1025 (132.0429)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 132.3101 (132.0608)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 137.3487 (132.4210)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 139.7333 (132.8087)  time: 1.1021  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:61]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 134.8681 (132.7435)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 137.2223 (133.2366)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 138.3113 (133.4469)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 128.5159 (133.1201)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 128.4740 (133.1061)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 130.1453 (133.0641)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 136.2077 (133.4533)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 139.1290 (133.6040)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 140.3621 (133.6730)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 140.9596 (133.9426)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 142.7490 (134.4024)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 137.4886 (134.1983)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 129.3743 (134.1933)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 136.6399 (134.3490)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 134.0843 (134.2214)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 127.2184 (134.0318)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 144.8667 (134.4891)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 148.8120 (134.7971)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 145.0711 (134.9968)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 136.7965 (135.0249)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 139.2132 (135.2010)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 135.1512 (135.1678)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 139.8359 (135.3402)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 138.2072 (135.3470)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 135.4944 (135.3587)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:61]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 134.7009 (135.3324)  time: 1.1008  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:61]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 137.9105 (135.4277)  time: 1.0985  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:61] Total time: 0:07:56 (1.1063 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 137.9105 (135.4277)\n",
      "Valid: [epoch:61]  [ 0/14]  eta: 0:00:35  loss: 159.8465 (159.8465)  time: 2.5256  data: 2.3780  max mem: 15925\n",
      "Valid: [epoch:61]  [13/14]  eta: 0:00:00  loss: 132.8970 (139.7050)  time: 0.2630  data: 0.1699  max mem: 15925\n",
      "Valid: [epoch:61] Total time: 0:00:03 (0.2800 s / it)\n",
      "Averaged stats: loss: 132.8970 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_61_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:62]  [  0/431]  eta: 0:34:06  lr: 0.000200  loss: 151.1757 (151.1757)  time: 4.7477  data: 3.6211  max mem: 15925\n",
      "Train: [epoch:62]  [ 10/431]  eta: 0:09:36  lr: 0.000200  loss: 151.1757 (147.4082)  time: 1.3692  data: 0.3294  max mem: 15925\n",
      "Train: [epoch:62]  [ 20/431]  eta: 0:08:23  lr: 0.000200  loss: 144.6949 (144.7261)  time: 1.0485  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [ 30/431]  eta: 0:07:50  lr: 0.000200  loss: 130.9349 (138.6601)  time: 1.0641  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 128.5066 (136.7713)  time: 1.0777  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 130.6890 (134.8751)  time: 1.0912  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 128.5465 (133.6947)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 129.8725 (134.3732)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 139.2809 (134.8399)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 133.3421 (134.1887)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 130.9676 (133.9198)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 128.5711 (133.2971)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 126.5062 (133.1703)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 133.5964 (133.2697)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 132.9592 (132.9880)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 127.6240 (132.5431)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 134.9258 (132.9877)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 134.9258 (133.0121)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 131.8442 (133.0170)  time: 1.1148  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:62]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 140.2707 (133.5048)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 136.0941 (133.4822)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 132.1891 (133.4348)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 132.0757 (133.3679)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 137.0098 (133.7025)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 144.5899 (134.1866)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 144.5899 (134.4966)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 137.5204 (134.5630)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 143.9863 (135.1946)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 142.6540 (135.0310)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 131.8696 (134.8993)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 132.2267 (135.0175)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 138.0620 (135.1945)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 126.7979 (134.8295)  time: 1.1168  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:62]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 130.6373 (135.0983)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 143.5497 (135.2569)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 141.9479 (135.4772)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 137.5010 (135.3448)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 132.7079 (135.4279)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 138.7253 (135.3932)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 130.1884 (135.3019)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 130.1884 (135.2663)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:62]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 133.7795 (135.3342)  time: 1.1094  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:62]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 134.4082 (135.3467)  time: 1.1029  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:62]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 140.8157 (135.4926)  time: 1.0966  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:62] Total time: 0:08:00 (1.1158 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 140.8157 (135.4926)\n",
      "Valid: [epoch:62]  [ 0/14]  eta: 0:00:34  loss: 128.6310 (128.6310)  time: 2.4829  data: 2.3417  max mem: 15925\n",
      "Valid: [epoch:62]  [13/14]  eta: 0:00:00  loss: 132.8969 (139.7050)  time: 0.2608  data: 0.1674  max mem: 15925\n",
      "Valid: [epoch:62] Total time: 0:00:03 (0.2804 s / it)\n",
      "Averaged stats: loss: 132.8969 (139.7050)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_62_input_n_20.png\n",
      "loss of the network on the 14 valid images: 139.705%\n",
      "Min loss: 139.705\n",
      "Best Epoch: 62.000\n",
      "Train: [epoch:63]  [  0/431]  eta: 0:32:20  lr: 0.000200  loss: 132.8043 (132.8043)  time: 4.5029  data: 3.3021  max mem: 15925\n",
      "Train: [epoch:63]  [ 10/431]  eta: 0:09:23  lr: 0.000200  loss: 145.1083 (148.4573)  time: 1.3395  data: 0.3004  max mem: 15925\n",
      "Train: [epoch:63]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 144.6338 (143.4366)  time: 1.0436  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 131.7091 (137.9738)  time: 1.0676  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [ 40/431]  eta: 0:07:28  lr: 0.000200  loss: 127.1124 (134.6432)  time: 1.0808  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 128.6030 (133.5608)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 128.9569 (132.6135)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 130.3993 (133.0313)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 138.3871 (134.0846)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 133.4178 (133.5764)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 128.1099 (132.9240)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 123.2514 (131.7858)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 127.1329 (131.6937)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 132.2764 (131.5003)  time: 1.1267  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 132.2764 (131.6830)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 139.1309 (132.2119)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 137.0660 (132.5135)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 133.7320 (132.3068)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 129.8032 (132.1236)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 132.2835 (132.3154)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 133.1661 (132.2168)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 133.5332 (132.3304)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 132.8679 (132.2405)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 132.6932 (132.3574)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 146.7732 (133.1393)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 141.9948 (133.2890)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 141.9948 (133.7048)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 146.3367 (134.1819)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 144.3780 (134.4147)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 137.7873 (134.4101)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 137.8813 (134.7497)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 139.3998 (134.6488)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 127.6807 (134.5022)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 135.5081 (134.7482)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 149.5551 (135.1796)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 149.5551 (135.2788)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 127.5867 (133.5569)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 8.0964 (130.1387)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 3.6610 (126.7807)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.6781 (123.5800)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.6155 (120.5395)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.6189 (117.6462)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:63]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.6189 (114.8925)  time: 1.1114  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:63]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.6090 (112.2641)  time: 1.1121  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:63] Total time: 0:08:00 (1.1138 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.6090 (112.2641)\n",
      "Valid: [epoch:63]  [ 0/14]  eta: 0:00:34  loss: 1.6382 (1.6382)  time: 2.4972  data: 2.3700  max mem: 15925\n",
      "Valid: [epoch:63]  [13/14]  eta: 0:00:00  loss: 1.5249 (1.5759)  time: 0.2717  data: 0.1694  max mem: 15925\n",
      "Valid: [epoch:63] Total time: 0:00:04 (0.2893 s / it)\n",
      "Averaged stats: loss: 1.5249 (1.5759)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_63_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.576%\n",
      "Min loss: 1.576\n",
      "Best Epoch: 63.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:64]  [  0/431]  eta: 0:31:55  lr: 0.000200  loss: 1.4982 (1.4982)  time: 4.4440  data: 3.1958  max mem: 15925\n",
      "Train: [epoch:64]  [ 10/431]  eta: 0:09:28  lr: 0.000200  loss: 1.6279 (1.6697)  time: 1.3511  data: 0.2907  max mem: 15925\n",
      "Train: [epoch:64]  [ 20/431]  eta: 0:08:21  lr: 0.000200  loss: 1.5899 (1.6285)  time: 1.0578  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [ 30/431]  eta: 0:07:52  lr: 0.000200  loss: 1.5990 (1.6277)  time: 1.0828  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 1.6056 (1.6320)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 1.6318 (1.6275)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [ 60/431]  eta: 0:07:04  lr: 0.000200  loss: 1.6318 (1.6200)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 1.5751 (1.6242)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 1.6525 (1.6335)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 1.6525 (1.6307)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [100/431]  eta: 0:06:16  lr: 0.000200  loss: 1.5762 (1.6215)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [110/431]  eta: 0:06:04  lr: 0.000200  loss: 1.5160 (1.6146)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 1.6173 (1.6233)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 1.6416 (1.6216)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [140/431]  eta: 0:05:28  lr: 0.000200  loss: 1.6121 (1.6273)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.6313 (1.6297)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 1.5855 (1.6310)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.6221 (1.6297)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.6430 (1.6310)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.6039 (1.6300)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.5728 (1.6282)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.5457 (1.6248)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.5589 (1.6225)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.5863 (1.6248)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.6633 (1.6223)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.6832 (1.6256)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.6851 (1.6251)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.5478 (1.6243)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.5693 (1.6249)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.5965 (1.6256)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.6032 (1.6271)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.5814 (1.6245)  time: 1.1300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.5626 (1.6223)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.5717 (1.6241)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.6463 (1.6254)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.5857 (1.6259)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.5857 (1.6252)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.5916 (1.6246)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.6139 (1.6238)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.6072 (1.6237)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.6072 (1.6245)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.6057 (1.6241)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:64]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.5645 (1.6242)  time: 1.1202  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:64]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.5768 (1.6229)  time: 1.1191  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:64] Total time: 0:08:02 (1.1189 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.5768 (1.6229)\n",
      "Valid: [epoch:64]  [ 0/14]  eta: 0:00:34  loss: 1.4144 (1.4144)  time: 2.4854  data: 2.3330  max mem: 15925\n",
      "Valid: [epoch:64]  [13/14]  eta: 0:00:00  loss: 1.5159 (1.5674)  time: 0.2614  data: 0.1667  max mem: 15925\n",
      "Valid: [epoch:64] Total time: 0:00:03 (0.2776 s / it)\n",
      "Averaged stats: loss: 1.5159 (1.5674)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_64_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.567%\n",
      "Min loss: 1.567\n",
      "Best Epoch: 64.000\n",
      "Train: [epoch:65]  [  0/431]  eta: 0:34:09  lr: 0.000200  loss: 1.6167 (1.6167)  time: 4.7554  data: 3.6133  max mem: 15925\n",
      "Train: [epoch:65]  [ 10/431]  eta: 0:09:35  lr: 0.000200  loss: 1.5786 (1.6113)  time: 1.3679  data: 0.3287  max mem: 15925\n",
      "Train: [epoch:65]  [ 20/431]  eta: 0:08:22  lr: 0.000200  loss: 1.5606 (1.6111)  time: 1.0464  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [ 30/431]  eta: 0:07:54  lr: 0.000200  loss: 1.5655 (1.6153)  time: 1.0798  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [ 40/431]  eta: 0:07:34  lr: 0.000200  loss: 1.5491 (1.5976)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [ 50/431]  eta: 0:07:18  lr: 0.000200  loss: 1.5640 (1.5968)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 1.6073 (1.5930)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 1.6124 (1.5947)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 1.6308 (1.6032)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 1.5607 (1.6036)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 1.4851 (1.5946)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 1.4955 (1.5942)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 1.5687 (1.5984)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 1.6110 (1.6027)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 1.6720 (1.6079)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.6037 (1.6073)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 1.6037 (1.6101)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.6169 (1.6118)  time: 1.1145  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:65]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.5989 (1.6115)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.7013 (1.6181)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.6825 (1.6181)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.5511 (1.6169)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 1.5793 (1.6168)  time: 1.1198  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:65]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.5849 (1.6170)  time: 1.1136  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:65]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.6434 (1.6178)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.5822 (1.6165)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.5654 (1.6165)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.5692 (1.6160)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.5794 (1.6157)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.5725 (1.6135)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.5500 (1.6143)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.6408 (1.6141)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.6074 (1.6130)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.6468 (1.6158)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.6118 (1.6147)  time: 1.1019  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:65]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.6034 (1.6167)  time: 1.1134  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:65]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.6856 (1.6188)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.6188 (1.6175)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.5740 (1.6163)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.6074 (1.6176)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.5724 (1.6174)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:65]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.5399 (1.6172)  time: 1.1093  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:65]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.6305 (1.6186)  time: 1.1166  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:65]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.6334 (1.6193)  time: 1.1182  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:65] Total time: 0:08:01 (1.1174 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.6334 (1.6193)\n",
      "Valid: [epoch:65]  [ 0/14]  eta: 0:00:34  loss: 1.7203 (1.7203)  time: 2.4480  data: 2.2692  max mem: 15925\n",
      "Valid: [epoch:65]  [13/14]  eta: 0:00:00  loss: 1.5148 (1.5661)  time: 0.2639  data: 0.1622  max mem: 15925\n",
      "Valid: [epoch:65] Total time: 0:00:03 (0.2776 s / it)\n",
      "Averaged stats: loss: 1.5148 (1.5661)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_65_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.566%\n",
      "Min loss: 1.566\n",
      "Best Epoch: 65.000\n",
      "Train: [epoch:66]  [  0/431]  eta: 0:31:23  lr: 0.000200  loss: 1.5782 (1.5782)  time: 4.3702  data: 3.2098  max mem: 15925\n",
      "Train: [epoch:66]  [ 10/431]  eta: 0:09:27  lr: 0.000200  loss: 1.6693 (1.7099)  time: 1.3492  data: 0.2920  max mem: 15925\n",
      "Train: [epoch:66]  [ 20/431]  eta: 0:08:20  lr: 0.000200  loss: 1.6423 (1.6637)  time: 1.0612  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [ 30/431]  eta: 0:07:52  lr: 0.000200  loss: 1.5614 (1.6410)  time: 1.0851  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 1.5335 (1.6179)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [ 50/431]  eta: 0:07:16  lr: 0.000200  loss: 1.5335 (1.6149)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 1.5192 (1.6065)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 1.5548 (1.6104)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 1.5734 (1.6201)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 1.5327 (1.6082)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 1.5065 (1.6059)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 1.5259 (1.6041)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.6037 (1.6087)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 1.6464 (1.6085)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.6464 (1.6108)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 1.6124 (1.6134)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.7237 (1.6275)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 1.7082 (1.6274)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 1.5707 (1.6234)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 1.5260 (1.6228)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.6085 (1.6242)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.6213 (1.6228)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 1.6213 (1.6304)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.6135 (1.6271)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.5123 (1.6272)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.5883 (1.6275)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 1.5751 (1.6263)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.5849 (1.6274)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.5849 (1.6265)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.5526 (1.6257)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.5529 (1.6261)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.6155 (1.6278)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.6136 (1.6264)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.5328 (1.6268)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.5989 (1.6281)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.6017 (1.6270)  time: 1.1097  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:66]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.6137 (1.6283)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.6236 (1.6272)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.5830 (1.6267)  time: 1.1103  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:66]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.5663 (1.6251)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.5341 (1.6237)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:66]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.5398 (1.6223)  time: 1.1180  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:66]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.5456 (1.6218)  time: 1.1141  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:66]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.5716 (1.6206)  time: 1.1108  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:66] Total time: 0:08:00 (1.1142 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.5716 (1.6206)\n",
      "Valid: [epoch:66]  [ 0/14]  eta: 0:00:35  loss: 1.4067 (1.4067)  time: 2.5046  data: 2.3828  max mem: 15925\n",
      "Valid: [epoch:66]  [13/14]  eta: 0:00:00  loss: 1.5121 (1.5633)  time: 0.2627  data: 0.1703  max mem: 15925\n",
      "Valid: [epoch:66] Total time: 0:00:03 (0.2810 s / it)\n",
      "Averaged stats: loss: 1.5121 (1.5633)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_66_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.563%\n",
      "Min loss: 1.563\n",
      "Best Epoch: 66.000\n",
      "Train: [epoch:67]  [  0/431]  eta: 0:32:04  lr: 0.000200  loss: 1.7802 (1.7802)  time: 4.4655  data: 3.1692  max mem: 15925\n",
      "Train: [epoch:67]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 1.5990 (1.5965)  time: 1.3413  data: 0.2883  max mem: 15925\n",
      "Train: [epoch:67]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 1.5990 (1.5838)  time: 1.0445  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 1.5763 (1.5795)  time: 1.0840  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 1.5763 (1.5848)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 1.5196 (1.5772)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 1.4874 (1.5840)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 1.5306 (1.5855)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 1.5392 (1.5804)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 1.5200 (1.5704)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 1.4414 (1.5680)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 1.5275 (1.5684)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 1.5382 (1.5650)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 1.5144 (1.5607)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 1.5168 (1.5622)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.5533 (1.5617)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 1.4962 (1.5572)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.5002 (1.5581)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.4916 (1.5541)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 1.4574 (1.5534)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.5609 (1.5552)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.5859 (1.5611)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 1.5156 (1.5569)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.4343 (1.5526)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.4965 (1.5519)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.5273 (1.5496)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.4531 (1.5480)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.4613 (1.5466)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.4456 (1.5445)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.4456 (1.5412)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.4454 (1.5418)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.5119 (1.5408)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.4472 (1.5383)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.4472 (1.5382)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 1.4919 (1.5366)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.4919 (1.5351)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.4826 (1.5339)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.4580 (1.5332)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.4524 (1.5310)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.4640 (1.5303)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.4084 (1.5264)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:67]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.3959 (1.5248)  time: 1.1173  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:67]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.4423 (1.5230)  time: 1.1180  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:67]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.4066 (1.5210)  time: 1.1155  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:67] Total time: 0:08:02 (1.1206 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.4066 (1.5210)\n",
      "Valid: [epoch:67]  [ 0/14]  eta: 0:00:37  loss: 1.3439 (1.3439)  time: 2.6887  data: 2.5583  max mem: 15925\n",
      "Valid: [epoch:67]  [13/14]  eta: 0:00:00  loss: 1.3917 (1.4332)  time: 0.2812  data: 0.1828  max mem: 15925\n",
      "Valid: [epoch:67] Total time: 0:00:04 (0.2964 s / it)\n",
      "Averaged stats: loss: 1.3917 (1.4332)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_67_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.433%\n",
      "Min loss: 1.433\n",
      "Best Epoch: 67.000\n",
      "Train: [epoch:68]  [  0/431]  eta: 0:32:57  lr: 0.000200  loss: 1.6942 (1.6942)  time: 4.5887  data: 3.3201  max mem: 15925\n",
      "Train: [epoch:68]  [ 10/431]  eta: 0:09:27  lr: 0.000200  loss: 1.4501 (1.4698)  time: 1.3480  data: 0.3020  max mem: 15925\n",
      "Train: [epoch:68]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 1.4501 (1.4907)  time: 1.0426  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 1.4313 (1.4758)  time: 1.0801  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 1.3423 (1.4402)  time: 1.0948  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:68]  [ 50/431]  eta: 0:07:16  lr: 0.000200  loss: 1.3423 (1.4395)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 1.3995 (1.4298)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 1.3451 (1.4272)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 1.3637 (1.4220)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 1.3637 (1.4173)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 1.4377 (1.4208)  time: 1.1274  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 1.4297 (1.4188)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 1.3515 (1.4172)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 1.4015 (1.4201)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 1.4252 (1.4194)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.3403 (1.4182)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 1.3350 (1.4191)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.3514 (1.4170)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 1.3584 (1.4202)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.3543 (1.4189)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.3123 (1.4162)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.3174 (1.4159)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.3710 (1.4155)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.3710 (1.4168)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.4091 (1.4176)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.4312 (1.4197)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.4427 (1.4195)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.4427 (1.4207)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.3956 (1.4201)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.3220 (1.4179)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.3357 (1.4157)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.3070 (1.4141)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.3520 (1.4128)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.3820 (1.4137)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.3893 (1.4140)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.3860 (1.4135)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.3438 (1.4121)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.3145 (1.4103)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.2955 (1.4087)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.3011 (1.4081)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.3478 (1.4068)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.3478 (1.4061)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:68]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.4413 (1.4073)  time: 1.1115  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:68]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.4413 (1.4080)  time: 1.1047  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:68] Total time: 0:08:01 (1.1179 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.4413 (1.4080)\n",
      "Valid: [epoch:68]  [ 0/14]  eta: 0:00:35  loss: 1.3088 (1.3088)  time: 2.5458  data: 2.4007  max mem: 15925\n",
      "Valid: [epoch:68]  [13/14]  eta: 0:00:00  loss: 1.3224 (1.3596)  time: 0.2610  data: 0.1716  max mem: 15925\n",
      "Valid: [epoch:68] Total time: 0:00:03 (0.2756 s / it)\n",
      "Averaged stats: loss: 1.3224 (1.3596)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_68_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.360%\n",
      "Min loss: 1.360\n",
      "Best Epoch: 68.000\n",
      "Train: [epoch:69]  [  0/431]  eta: 0:32:07  lr: 0.000200  loss: 1.4323 (1.4323)  time: 4.4714  data: 3.2280  max mem: 15925\n",
      "Train: [epoch:69]  [ 10/431]  eta: 0:09:34  lr: 0.000200  loss: 1.3763 (1.4060)  time: 1.3644  data: 0.2937  max mem: 15925\n",
      "Train: [epoch:69]  [ 20/431]  eta: 0:08:21  lr: 0.000200  loss: 1.3113 (1.3649)  time: 1.0588  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [ 30/431]  eta: 0:07:50  lr: 0.000200  loss: 1.3242 (1.3648)  time: 1.0691  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 1.3621 (1.3741)  time: 1.0849  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 1.3622 (1.3718)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 1.2918 (1.3586)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 1.2814 (1.3608)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [ 80/431]  eta: 0:06:38  lr: 0.000200  loss: 1.4492 (1.3699)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 1.4332 (1.3692)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 1.2860 (1.3690)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 1.3142 (1.3727)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 1.4109 (1.3745)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 1.3668 (1.3734)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 1.3522 (1.3725)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.4002 (1.3730)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 1.4074 (1.3741)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.2606 (1.3684)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 1.3551 (1.3715)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.3551 (1.3696)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.3388 (1.3698)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.3843 (1.3713)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.3770 (1.3725)  time: 1.1171  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:69]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.4062 (1.3744)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.4252 (1.3769)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.2807 (1.3746)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.2807 (1.3721)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.2683 (1.3708)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.3241 (1.3723)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.4127 (1.3734)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.3436 (1.3733)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.2628 (1.3701)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.3166 (1.3710)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.3631 (1.3712)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.3213 (1.3702)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.3247 (1.3693)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.2749 (1.3660)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.2131 (1.3639)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.2580 (1.3615)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.2354 (1.3585)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.2361 (1.3574)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1991 (1.3542)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:69]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1908 (1.3518)  time: 1.1154  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:69]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.2327 (1.3520)  time: 1.1164  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:69] Total time: 0:08:02 (1.1190 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.2327 (1.3520)\n",
      "Valid: [epoch:69]  [ 0/14]  eta: 0:00:36  loss: 1.2477 (1.2477)  time: 2.6012  data: 2.4409  max mem: 15925\n",
      "Valid: [epoch:69]  [13/14]  eta: 0:00:00  loss: 1.2477 (1.2761)  time: 0.2629  data: 0.1744  max mem: 15925\n",
      "Valid: [epoch:69] Total time: 0:00:03 (0.2794 s / it)\n",
      "Averaged stats: loss: 1.2477 (1.2761)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_69_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.276%\n",
      "Min loss: 1.276\n",
      "Best Epoch: 69.000\n",
      "Train: [epoch:70]  [  0/431]  eta: 0:30:40  lr: 0.000200  loss: 1.2270 (1.2270)  time: 4.2698  data: 3.0919  max mem: 15925\n",
      "Train: [epoch:70]  [ 10/431]  eta: 0:09:26  lr: 0.000200  loss: 1.3348 (1.3414)  time: 1.3449  data: 0.2812  max mem: 15925\n",
      "Train: [epoch:70]  [ 20/431]  eta: 0:08:15  lr: 0.000200  loss: 1.3348 (1.3227)  time: 1.0526  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [ 30/431]  eta: 0:07:47  lr: 0.000200  loss: 1.2078 (1.2896)  time: 1.0681  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:70]  [ 40/431]  eta: 0:07:28  lr: 0.000200  loss: 1.1828 (1.2761)  time: 1.0838  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 1.1872 (1.2646)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 1.1536 (1.2531)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 1.2234 (1.2600)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 1.2936 (1.2657)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 1.2597 (1.2627)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 1.2597 (1.2690)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 1.3256 (1.2736)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 1.2772 (1.2736)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.2772 (1.2761)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 1.2938 (1.2813)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 1.2801 (1.2802)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.2299 (1.2795)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 1.2089 (1.2780)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 1.2443 (1.2776)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 1.2559 (1.2790)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.2360 (1.2766)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 1.2360 (1.2749)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 1.2396 (1.2724)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.2396 (1.2711)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.2556 (1.2727)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.2232 (1.2711)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 1.2315 (1.2723)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.2385 (1.2727)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.2502 (1.2734)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.2281 (1.2729)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.2070 (1.2706)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1887 (1.2694)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1773 (1.2680)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1773 (1.2665)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1711 (1.2670)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.2457 (1.2690)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.3198 (1.2696)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.2416 (1.2687)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.2003 (1.2678)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.2329 (1.2668)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.2573 (1.2668)  time: 1.1208  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:70]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.2830 (1.2665)  time: 1.1275  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:70]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.2428 (1.2667)  time: 1.1262  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:70]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.2356 (1.2659)  time: 1.1189  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:70] Total time: 0:08:01 (1.1166 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.2356 (1.2659)\n",
      "Valid: [epoch:70]  [ 0/14]  eta: 0:00:34  loss: 1.2765 (1.2765)  time: 2.4979  data: 2.3373  max mem: 15925\n",
      "Valid: [epoch:70]  [13/14]  eta: 0:00:00  loss: 1.1769 (1.1992)  time: 0.2673  data: 0.1670  max mem: 15925\n",
      "Valid: [epoch:70] Total time: 0:00:03 (0.2841 s / it)\n",
      "Averaged stats: loss: 1.1769 (1.1992)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_70_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.199%\n",
      "Min loss: 1.199\n",
      "Best Epoch: 70.000\n",
      "Train: [epoch:71]  [  0/431]  eta: 0:31:58  lr: 0.000200  loss: 1.1684 (1.1684)  time: 4.4505  data: 3.1782  max mem: 15925\n",
      "Train: [epoch:71]  [ 10/431]  eta: 0:09:31  lr: 0.000200  loss: 1.2269 (1.2790)  time: 1.3587  data: 0.2892  max mem: 15925\n",
      "Train: [epoch:71]  [ 20/431]  eta: 0:08:23  lr: 0.000200  loss: 1.2553 (1.2691)  time: 1.0642  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 1.1933 (1.2563)  time: 1.0841  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [ 40/431]  eta: 0:07:34  lr: 0.000200  loss: 1.1474 (1.2458)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [ 50/431]  eta: 0:07:18  lr: 0.000200  loss: 1.2361 (1.2582)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [ 60/431]  eta: 0:07:04  lr: 0.000200  loss: 1.2361 (1.2525)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 1.2332 (1.2588)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 1.2612 (1.2616)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 1.2483 (1.2575)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 1.2151 (1.2534)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 1.2128 (1.2499)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 1.2065 (1.2459)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 1.2063 (1.2470)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [140/431]  eta: 0:05:29  lr: 0.000200  loss: 1.1899 (1.2448)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [150/431]  eta: 0:05:17  lr: 0.000200  loss: 1.1741 (1.2420)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 1.1586 (1.2389)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [170/431]  eta: 0:04:54  lr: 0.000200  loss: 1.2023 (1.2415)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.2372 (1.2423)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 1.1921 (1.2428)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [200/431]  eta: 0:04:20  lr: 0.000200  loss: 1.1856 (1.2429)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.1868 (1.2406)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 1.1588 (1.2381)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [230/431]  eta: 0:03:46  lr: 0.000200  loss: 1.1738 (1.2390)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.2289 (1.2392)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.1697 (1.2372)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 1.2512 (1.2388)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.2247 (1.2377)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1797 (1.2366)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.1282 (1.2333)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [300/431]  eta: 0:02:27  lr: 0.000200  loss: 1.1451 (1.2339)  time: 1.1321  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [310/431]  eta: 0:02:16  lr: 0.000200  loss: 1.2365 (1.2359)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.2246 (1.2344)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.2042 (1.2347)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 1.2401 (1.2352)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [350/431]  eta: 0:01:31  lr: 0.000200  loss: 1.2754 (1.2361)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1812 (1.2342)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.2084 (1.2353)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.2087 (1.2339)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [390/431]  eta: 0:00:46  lr: 0.000200  loss: 1.1711 (1.2327)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.2154 (1.2334)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:71]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.2364 (1.2334)  time: 1.1213  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:71]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1584 (1.2317)  time: 1.1194  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:71]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1301 (1.2309)  time: 1.1176  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:71] Total time: 0:08:04 (1.1230 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1301 (1.2309)\n",
      "Valid: [epoch:71]  [ 0/14]  eta: 0:00:37  loss: 1.2440 (1.2440)  time: 2.7056  data: 2.5357  max mem: 15925\n",
      "Valid: [epoch:71]  [13/14]  eta: 0:00:00  loss: 1.1507 (1.1694)  time: 0.2814  data: 0.1812  max mem: 15925\n",
      "Valid: [epoch:71] Total time: 0:00:04 (0.2970 s / it)\n",
      "Averaged stats: loss: 1.1507 (1.1694)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_71_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.169%\n",
      "Min loss: 1.169\n",
      "Best Epoch: 71.000\n",
      "Train: [epoch:72]  [  0/431]  eta: 0:37:30  lr: 0.000200  loss: 1.2525 (1.2525)  time: 5.2223  data: 4.0525  max mem: 15925\n",
      "Train: [epoch:72]  [ 10/431]  eta: 0:09:50  lr: 0.000200  loss: 1.1695 (1.2175)  time: 1.4029  data: 0.3686  max mem: 15925\n",
      "Train: [epoch:72]  [ 20/431]  eta: 0:08:31  lr: 0.000200  loss: 1.1907 (1.2256)  time: 1.0458  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [ 30/431]  eta: 0:07:58  lr: 0.000200  loss: 1.1907 (1.2135)  time: 1.0786  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [ 40/431]  eta: 0:07:38  lr: 0.000200  loss: 1.1385 (1.1994)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [ 50/431]  eta: 0:07:22  lr: 0.000200  loss: 1.1525 (1.1992)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [ 60/431]  eta: 0:07:07  lr: 0.000200  loss: 1.1599 (1.1975)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [ 70/431]  eta: 0:06:54  lr: 0.000200  loss: 1.2036 (1.2120)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [ 80/431]  eta: 0:06:41  lr: 0.000200  loss: 1.2036 (1.2105)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [ 90/431]  eta: 0:06:28  lr: 0.000200  loss: 1.1739 (1.2107)  time: 1.1111  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:72]  [100/431]  eta: 0:06:16  lr: 0.000200  loss: 1.1407 (1.2052)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 1.1665 (1.2065)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [120/431]  eta: 0:05:52  lr: 0.000200  loss: 1.2071 (1.2072)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 1.1614 (1.2101)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [140/431]  eta: 0:05:29  lr: 0.000200  loss: 1.2341 (1.2131)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [150/431]  eta: 0:05:17  lr: 0.000200  loss: 1.2341 (1.2145)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 1.2295 (1.2171)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [170/431]  eta: 0:04:54  lr: 0.000200  loss: 1.1992 (1.2138)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.1849 (1.2158)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 1.2245 (1.2220)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [200/431]  eta: 0:04:20  lr: 0.000200  loss: 1.2245 (1.2204)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.1744 (1.2185)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 1.1649 (1.2172)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.1581 (1.2155)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.1402 (1.2128)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.1344 (1.2123)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 1.2429 (1.2123)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.1866 (1.2127)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.2060 (1.2135)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.2316 (1.2150)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [300/431]  eta: 0:02:27  lr: 0.000200  loss: 1.1813 (1.2147)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1692 (1.2125)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1619 (1.2118)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.1865 (1.2136)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 1.1790 (1.2143)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1896 (1.2156)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1985 (1.2147)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.2200 (1.2154)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.1964 (1.2139)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1329 (1.2119)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1548 (1.2128)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.2177 (1.2124)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:72]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1591 (1.2119)  time: 1.1055  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:72]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1471 (1.2122)  time: 1.1120  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:72] Total time: 0:08:02 (1.1198 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1471 (1.2122)\n",
      "Valid: [epoch:72]  [ 0/14]  eta: 0:00:35  loss: 1.0693 (1.0693)  time: 2.5605  data: 2.3984  max mem: 15925\n",
      "Valid: [epoch:72]  [13/14]  eta: 0:00:00  loss: 1.1348 (1.1536)  time: 0.2871  data: 0.1714  max mem: 15925\n",
      "Valid: [epoch:72] Total time: 0:00:04 (0.3028 s / it)\n",
      "Averaged stats: loss: 1.1348 (1.1536)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_72_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.154%\n",
      "Min loss: 1.154\n",
      "Best Epoch: 72.000\n",
      "Train: [epoch:73]  [  0/431]  eta: 0:30:06  lr: 0.000200  loss: 1.3084 (1.3084)  time: 4.1919  data: 2.9168  max mem: 15925\n",
      "Train: [epoch:73]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 1.2003 (1.2400)  time: 1.3408  data: 0.2654  max mem: 15925\n",
      "Train: [epoch:73]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 1.1985 (1.2392)  time: 1.0576  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [ 30/431]  eta: 0:07:48  lr: 0.000200  loss: 1.2107 (1.2316)  time: 1.0750  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 1.2107 (1.2298)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 1.1708 (1.2216)  time: 1.0983  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:73]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 1.1854 (1.2164)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 1.1989 (1.2210)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 1.1959 (1.2160)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.1014 (1.1996)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 1.0925 (1.1973)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 1.1996 (1.2012)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.2280 (1.1996)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.1250 (1.1962)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.1735 (1.1974)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 1.1739 (1.1954)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.1694 (1.1952)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 1.1662 (1.1952)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 1.1806 (1.1985)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 1.2088 (1.2002)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.2105 (1.2040)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.2264 (1.2063)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.1562 (1.2044)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.1562 (1.2035)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.2264 (1.2044)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.1700 (1.2048)  time: 1.1157  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:73]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.1700 (1.2051)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.2178 (1.2066)  time: 1.1021  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:73]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.2220 (1.2082)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1820 (1.2067)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1533 (1.2065)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1533 (1.2047)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1087 (1.2015)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1229 (1.2028)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1709 (1.2017)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1041 (1.1997)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1344 (1.2003)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1840 (1.2014)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1385 (1.2000)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1202 (1.2008)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1885 (1.2009)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:73]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1885 (1.2014)  time: 1.1018  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:73]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1886 (1.2015)  time: 1.1144  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:73]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1635 (1.2009)  time: 1.1183  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:73] Total time: 0:08:01 (1.1161 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1635 (1.2009)\n",
      "Valid: [epoch:73]  [ 0/14]  eta: 0:00:36  loss: 1.1052 (1.1052)  time: 2.5715  data: 2.4488  max mem: 15925\n",
      "Valid: [epoch:73]  [13/14]  eta: 0:00:00  loss: 1.1259 (1.1444)  time: 0.2703  data: 0.1750  max mem: 15925\n",
      "Valid: [epoch:73] Total time: 0:00:03 (0.2851 s / it)\n",
      "Averaged stats: loss: 1.1259 (1.1444)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_73_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.144%\n",
      "Min loss: 1.144\n",
      "Best Epoch: 73.000\n",
      "Train: [epoch:74]  [  0/431]  eta: 0:35:35  lr: 0.000200  loss: 1.0827 (1.0827)  time: 4.9555  data: 3.8693  max mem: 15925\n",
      "Train: [epoch:74]  [ 10/431]  eta: 0:09:50  lr: 0.000200  loss: 1.1111 (1.2050)  time: 1.4019  data: 0.3519  max mem: 15925\n",
      "Train: [epoch:74]  [ 20/431]  eta: 0:08:29  lr: 0.000200  loss: 1.2141 (1.2300)  time: 1.0540  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [ 30/431]  eta: 0:07:57  lr: 0.000200  loss: 1.1802 (1.1990)  time: 1.0735  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [ 40/431]  eta: 0:07:36  lr: 0.000200  loss: 1.1688 (1.1955)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [ 50/431]  eta: 0:07:20  lr: 0.000200  loss: 1.1710 (1.1935)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [ 60/431]  eta: 0:07:06  lr: 0.000200  loss: 1.2111 (1.2003)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [ 70/431]  eta: 0:06:52  lr: 0.000200  loss: 1.2280 (1.2066)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 1.2314 (1.2123)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 1.2423 (1.2163)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 1.1968 (1.2153)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 1.1712 (1.2097)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 1.1438 (1.2036)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 1.1319 (1.2039)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [140/431]  eta: 0:05:28  lr: 0.000200  loss: 1.1697 (1.2030)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [150/431]  eta: 0:05:17  lr: 0.000200  loss: 1.1981 (1.2043)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 1.2209 (1.2083)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [170/431]  eta: 0:04:54  lr: 0.000200  loss: 1.2734 (1.2111)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.2084 (1.2098)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 1.1224 (1.2052)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [200/431]  eta: 0:04:20  lr: 0.000200  loss: 1.1224 (1.2024)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.1199 (1.2004)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 1.1184 (1.1982)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.1184 (1.1962)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.1241 (1.1944)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.1241 (1.1946)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 1.1720 (1.1951)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.1943 (1.1945)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1976 (1.1941)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.1928 (1.1932)  time: 1.1292  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [300/431]  eta: 0:02:27  lr: 0.000200  loss: 1.1729 (1.1936)  time: 1.1260  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1871 (1.1943)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1738 (1.1930)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.1649 (1.1947)  time: 1.1234  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 1.1655 (1.1942)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1689 (1.1952)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1737 (1.1936)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1465 (1.1925)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.1408 (1.1920)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [390/431]  eta: 0:00:46  lr: 0.000200  loss: 1.1408 (1.1920)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1963 (1.1923)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:74]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1785 (1.1918)  time: 1.1230  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:74]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1785 (1.1924)  time: 1.1202  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:74]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1766 (1.1928)  time: 1.1170  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:74] Total time: 0:08:03 (1.1228 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1766 (1.1928)\n",
      "Valid: [epoch:74]  [ 0/14]  eta: 0:00:37  loss: 1.1921 (1.1921)  time: 2.6520  data: 2.5190  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:74]  [13/14]  eta: 0:00:00  loss: 1.1201 (1.1375)  time: 0.2772  data: 0.1800  max mem: 15925\n",
      "Valid: [epoch:74] Total time: 0:00:04 (0.2952 s / it)\n",
      "Averaged stats: loss: 1.1201 (1.1375)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_74_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.138%\n",
      "Min loss: 1.138\n",
      "Best Epoch: 74.000\n",
      "Train: [epoch:75]  [  0/431]  eta: 0:32:39  lr: 0.000200  loss: 1.3665 (1.3665)  time: 4.5454  data: 3.3899  max mem: 15925\n",
      "Train: [epoch:75]  [ 10/431]  eta: 0:09:25  lr: 0.000200  loss: 1.1599 (1.1940)  time: 1.3443  data: 0.3084  max mem: 15925\n",
      "Train: [epoch:75]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 1.1542 (1.2013)  time: 1.0440  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [ 30/431]  eta: 0:07:49  lr: 0.000200  loss: 1.1787 (1.2046)  time: 1.0762  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 1.1866 (1.1922)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [ 50/431]  eta: 0:07:16  lr: 0.000200  loss: 1.1613 (1.1849)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [ 60/431]  eta: 0:07:04  lr: 0.000200  loss: 1.1469 (1.1848)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 1.1496 (1.1832)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 1.1845 (1.1842)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 1.1845 (1.1857)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 1.1392 (1.1810)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [110/431]  eta: 0:06:04  lr: 0.000200  loss: 1.1379 (1.1775)  time: 1.1284  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [120/431]  eta: 0:05:52  lr: 0.000200  loss: 1.1505 (1.1747)  time: 1.1313  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [130/431]  eta: 0:05:41  lr: 0.000200  loss: 1.1529 (1.1755)  time: 1.1286  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [140/431]  eta: 0:05:29  lr: 0.000200  loss: 1.1609 (1.1757)  time: 1.1294  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [150/431]  eta: 0:05:18  lr: 0.000200  loss: 1.1876 (1.1858)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [160/431]  eta: 0:05:06  lr: 0.000200  loss: 1.2121 (1.1875)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [170/431]  eta: 0:04:54  lr: 0.000200  loss: 1.1486 (1.1860)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [180/431]  eta: 0:04:43  lr: 0.000200  loss: 1.1468 (1.1887)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 1.1667 (1.1892)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [200/431]  eta: 0:04:20  lr: 0.000200  loss: 1.1163 (1.1862)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [210/431]  eta: 0:04:09  lr: 0.000200  loss: 1.1792 (1.1878)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 1.1635 (1.1871)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [230/431]  eta: 0:03:46  lr: 0.000200  loss: 1.1353 (1.1872)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.1487 (1.1862)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.1842 (1.1869)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 1.1483 (1.1871)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [270/431]  eta: 0:03:01  lr: 0.000200  loss: 1.1423 (1.1872)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1509 (1.1883)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.2273 (1.1917)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [300/431]  eta: 0:02:27  lr: 0.000200  loss: 1.2073 (1.1921)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1358 (1.1912)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1228 (1.1887)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.1220 (1.1883)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 1.1748 (1.1890)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.2153 (1.1892)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1591 (1.1874)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1349 (1.1872)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.1547 (1.1882)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1467 (1.1871)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1467 (1.1868)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:75]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1723 (1.1856)  time: 1.1069  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:75]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1617 (1.1866)  time: 1.1152  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:75]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1518 (1.1860)  time: 1.1192  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:75] Total time: 0:08:03 (1.1216 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1518 (1.1860)\n",
      "Valid: [epoch:75]  [ 0/14]  eta: 0:00:36  loss: 1.1150 (1.1150)  time: 2.5820  data: 2.4537  max mem: 15925\n",
      "Valid: [epoch:75]  [13/14]  eta: 0:00:00  loss: 1.1150 (1.1310)  time: 0.2667  data: 0.1753  max mem: 15925\n",
      "Valid: [epoch:75] Total time: 0:00:03 (0.2835 s / it)\n",
      "Averaged stats: loss: 1.1150 (1.1310)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_75_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.131%\n",
      "Min loss: 1.131\n",
      "Best Epoch: 75.000\n",
      "Train: [epoch:76]  [  0/431]  eta: 0:31:44  lr: 0.000200  loss: 1.3401 (1.3401)  time: 4.4186  data: 3.2080  max mem: 15925\n",
      "Train: [epoch:76]  [ 10/431]  eta: 0:09:32  lr: 0.000200  loss: 1.2354 (1.2327)  time: 1.3602  data: 0.2918  max mem: 15925\n",
      "Train: [epoch:76]  [ 20/431]  eta: 0:08:21  lr: 0.000200  loss: 1.1892 (1.2177)  time: 1.0610  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [ 30/431]  eta: 0:07:52  lr: 0.000200  loss: 1.1510 (1.1966)  time: 1.0783  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 1.1320 (1.1856)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [ 50/431]  eta: 0:07:18  lr: 0.000200  loss: 1.1049 (1.1717)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [ 60/431]  eta: 0:07:05  lr: 0.000200  loss: 1.1306 (1.1743)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [ 70/431]  eta: 0:06:51  lr: 0.000200  loss: 1.1375 (1.1734)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [ 80/431]  eta: 0:06:39  lr: 0.000200  loss: 1.1281 (1.1703)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 1.1114 (1.1683)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 1.1556 (1.1739)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 1.1766 (1.1720)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 1.1538 (1.1723)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 1.1455 (1.1721)  time: 1.1197  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:76]  [140/431]  eta: 0:05:28  lr: 0.000200  loss: 1.1703 (1.1707)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.1703 (1.1721)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 1.1863 (1.1747)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.1294 (1.1747)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 1.1128 (1.1713)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.1203 (1.1691)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.1203 (1.1687)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.1487 (1.1679)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.1251 (1.1665)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.1674 (1.1702)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.2741 (1.1739)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.2100 (1.1728)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.1758 (1.1734)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.1839 (1.1756)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1631 (1.1750)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1471 (1.1753)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1492 (1.1753)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1626 (1.1753)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1765 (1.1761)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.2037 (1.1776)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1850 (1.1774)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1739 (1.1786)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1754 (1.1788)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1305 (1.1782)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1708 (1.1788)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1418 (1.1778)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.2142 (1.1800)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.2286 (1.1812)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:76]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1695 (1.1807)  time: 1.1065  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:76]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1695 (1.1815)  time: 1.1097  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:76] Total time: 0:08:01 (1.1175 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1695 (1.1815)\n",
      "Valid: [epoch:76]  [ 0/14]  eta: 0:00:35  loss: 1.2005 (1.2005)  time: 2.5616  data: 2.4328  max mem: 15925\n",
      "Valid: [epoch:76]  [13/14]  eta: 0:00:00  loss: 1.1274 (1.1387)  time: 0.2816  data: 0.1739  max mem: 15925\n",
      "Valid: [epoch:76] Total time: 0:00:04 (0.2971 s / it)\n",
      "Averaged stats: loss: 1.1274 (1.1387)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_76_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.139%\n",
      "Min loss: 1.131\n",
      "Best Epoch: 75.000\n",
      "Train: [epoch:77]  [  0/431]  eta: 0:30:57  lr: 0.000200  loss: 1.1668 (1.1668)  time: 4.3106  data: 2.9840  max mem: 15925\n",
      "Train: [epoch:77]  [ 10/431]  eta: 0:09:26  lr: 0.000200  loss: 1.1727 (1.2241)  time: 1.3459  data: 0.2714  max mem: 15925\n",
      "Train: [epoch:77]  [ 20/431]  eta: 0:08:18  lr: 0.000200  loss: 1.1727 (1.1987)  time: 1.0587  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [ 30/431]  eta: 0:07:49  lr: 0.000200  loss: 1.1762 (1.2141)  time: 1.0750  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 1.1899 (1.2036)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 1.1724 (1.1987)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 1.1604 (1.1942)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 1.1604 (1.1878)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 1.1487 (1.1838)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.1402 (1.1815)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 1.1402 (1.1752)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 1.1440 (1.1775)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.1243 (1.1736)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 1.1243 (1.1752)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.1309 (1.1741)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 1.1387 (1.1741)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.1444 (1.1761)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 1.1502 (1.1752)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 1.1324 (1.1750)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.1417 (1.1764)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.1706 (1.1781)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.1580 (1.1758)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.1084 (1.1731)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.1622 (1.1747)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.2128 (1.1763)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.2223 (1.1780)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.2978 (1.1814)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.2540 (1.1824)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.1849 (1.1827)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1070 (1.1802)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1145 (1.1818)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1270 (1.1793)  time: 1.1088  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:77]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1030 (1.1777)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1151 (1.1774)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1391 (1.1768)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1391 (1.1758)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1254 (1.1750)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1254 (1.1745)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1201 (1.1738)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1201 (1.1739)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1629 (1.1741)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:77]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1738 (1.1752)  time: 1.1124  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:77]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1535 (1.1751)  time: 1.1164  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:77]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1388 (1.1751)  time: 1.1222  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:77] Total time: 0:08:01 (1.1168 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1388 (1.1751)\n",
      "Valid: [epoch:77]  [ 0/14]  eta: 0:00:34  loss: 1.0105 (1.0105)  time: 2.4851  data: 2.3115  max mem: 15925\n",
      "Valid: [epoch:77]  [13/14]  eta: 0:00:00  loss: 1.1019 (1.1181)  time: 0.2594  data: 0.1652  max mem: 15925\n",
      "Valid: [epoch:77] Total time: 0:00:03 (0.2732 s / it)\n",
      "Averaged stats: loss: 1.1019 (1.1181)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_77_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.118%\n",
      "Min loss: 1.118\n",
      "Best Epoch: 77.000\n",
      "Train: [epoch:78]  [  0/431]  eta: 0:33:24  lr: 0.000200  loss: 1.1061 (1.1061)  time: 4.6515  data: 3.4673  max mem: 15925\n",
      "Train: [epoch:78]  [ 10/431]  eta: 0:09:38  lr: 0.000200  loss: 1.1229 (1.1554)  time: 1.3735  data: 0.3155  max mem: 15925\n",
      "Train: [epoch:78]  [ 20/431]  eta: 0:08:24  lr: 0.000200  loss: 1.1229 (1.1500)  time: 1.0556  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [ 30/431]  eta: 0:07:52  lr: 0.000200  loss: 1.1167 (1.1480)  time: 1.0722  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [ 40/431]  eta: 0:07:33  lr: 0.000200  loss: 1.1276 (1.1520)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 1.1145 (1.1418)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [ 60/431]  eta: 0:07:03  lr: 0.000200  loss: 1.0856 (1.1360)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 1.0929 (1.1412)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 1.1175 (1.1504)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 1.1500 (1.1517)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 1.1240 (1.1522)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 1.1267 (1.1555)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 1.1335 (1.1540)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 1.1409 (1.1571)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 1.1642 (1.1574)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.1238 (1.1559)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 1.1309 (1.1590)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.1385 (1.1591)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.1773 (1.1614)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 1.1937 (1.1640)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.2138 (1.1646)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.0979 (1.1623)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.1088 (1.1642)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.1180 (1.1622)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.1131 (1.1605)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.1399 (1.1614)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.1399 (1.1617)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.1503 (1.1625)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1400 (1.1624)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1231 (1.1630)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1666 (1.1630)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1784 (1.1648)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1539 (1.1648)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.1481 (1.1668)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.2138 (1.1697)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.2321 (1.1710)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.2064 (1.1717)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1536 (1.1724)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.1536 (1.1729)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1212 (1.1723)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1212 (1.1717)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1429 (1.1710)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:78]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1384 (1.1717)  time: 1.1097  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:78]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1346 (1.1703)  time: 1.1149  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:78] Total time: 0:08:02 (1.1191 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1346 (1.1703)\n",
      "Valid: [epoch:78]  [ 0/14]  eta: 0:00:35  loss: 1.0693 (1.0693)  time: 2.5541  data: 2.3863  max mem: 15925\n",
      "Valid: [epoch:78]  [13/14]  eta: 0:00:00  loss: 1.1005 (1.1151)  time: 0.2703  data: 0.1705  max mem: 15925\n",
      "Valid: [epoch:78] Total time: 0:00:04 (0.2860 s / it)\n",
      "Averaged stats: loss: 1.1005 (1.1151)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_78_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.115%\n",
      "Min loss: 1.115\n",
      "Best Epoch: 78.000\n",
      "Train: [epoch:79]  [  0/431]  eta: 0:31:41  lr: 0.000200  loss: 1.1037 (1.1037)  time: 4.4112  data: 3.1314  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:79]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 1.1581 (1.1634)  time: 1.3418  data: 0.2849  max mem: 15925\n",
      "Train: [epoch:79]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 1.1581 (1.1797)  time: 1.0497  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [ 30/431]  eta: 0:07:48  lr: 0.000200  loss: 1.1491 (1.1796)  time: 1.0716  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 1.1491 (1.1763)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 1.1508 (1.1697)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 1.1217 (1.1675)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 1.1548 (1.1659)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 1.1596 (1.1724)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [ 90/431]  eta: 0:06:26  lr: 0.000200  loss: 1.1506 (1.1734)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [100/431]  eta: 0:06:14  lr: 0.000200  loss: 1.1626 (1.1721)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [110/431]  eta: 0:06:02  lr: 0.000200  loss: 1.1059 (1.1689)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [120/431]  eta: 0:05:50  lr: 0.000200  loss: 1.1059 (1.1703)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [130/431]  eta: 0:05:39  lr: 0.000200  loss: 1.0838 (1.1651)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 1.0936 (1.1654)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.1323 (1.1654)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 1.1323 (1.1670)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.1144 (1.1629)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.1210 (1.1618)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.1242 (1.1623)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.1666 (1.1651)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.1681 (1.1643)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.1427 (1.1654)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.1660 (1.1672)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.1472 (1.1686)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.1317 (1.1695)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.1238 (1.1666)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.1209 (1.1674)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.1513 (1.1682)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1470 (1.1673)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1754 (1.1688)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1754 (1.1678)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1093 (1.1673)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1450 (1.1686)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.2001 (1.1700)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1941 (1.1716)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1355 (1.1724)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1393 (1.1722)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1383 (1.1703)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1286 (1.1695)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1098 (1.1681)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:79]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0687 (1.1669)  time: 1.0988  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:79]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1100 (1.1665)  time: 1.1034  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:79]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1522 (1.1658)  time: 1.1120  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:79] Total time: 0:08:00 (1.1153 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1522 (1.1658)\n",
      "Valid: [epoch:79]  [ 0/14]  eta: 0:00:35  loss: 1.0915 (1.0915)  time: 2.5609  data: 2.4126  max mem: 15925\n",
      "Valid: [epoch:79]  [13/14]  eta: 0:00:00  loss: 1.0915 (1.1075)  time: 0.2776  data: 0.1724  max mem: 15925\n",
      "Valid: [epoch:79] Total time: 0:00:04 (0.2923 s / it)\n",
      "Averaged stats: loss: 1.0915 (1.1075)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_79_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.107%\n",
      "Min loss: 1.107\n",
      "Best Epoch: 79.000\n",
      "Train: [epoch:80]  [  0/431]  eta: 0:34:16  lr: 0.000200  loss: 1.1283 (1.1283)  time: 4.7706  data: 3.5709  max mem: 15925\n",
      "Train: [epoch:80]  [ 10/431]  eta: 0:09:42  lr: 0.000200  loss: 1.1493 (1.1578)  time: 1.3826  data: 0.3248  max mem: 15925\n",
      "Train: [epoch:80]  [ 20/431]  eta: 0:08:26  lr: 0.000200  loss: 1.1555 (1.1742)  time: 1.0566  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [ 30/431]  eta: 0:07:56  lr: 0.000200  loss: 1.1366 (1.1610)  time: 1.0816  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:80]  [ 40/431]  eta: 0:07:34  lr: 0.000200  loss: 1.0859 (1.1460)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [ 50/431]  eta: 0:07:19  lr: 0.000200  loss: 1.1042 (1.1467)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [ 60/431]  eta: 0:07:05  lr: 0.000200  loss: 1.1303 (1.1448)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [ 70/431]  eta: 0:06:52  lr: 0.000200  loss: 1.1310 (1.1500)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [ 80/431]  eta: 0:06:40  lr: 0.000200  loss: 1.1877 (1.1640)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [ 90/431]  eta: 0:06:27  lr: 0.000200  loss: 1.1877 (1.1589)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [100/431]  eta: 0:06:15  lr: 0.000200  loss: 1.1465 (1.1650)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [110/431]  eta: 0:06:03  lr: 0.000200  loss: 1.1497 (1.1615)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [120/431]  eta: 0:05:51  lr: 0.000200  loss: 1.1281 (1.1665)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [130/431]  eta: 0:05:40  lr: 0.000200  loss: 1.1695 (1.1685)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [140/431]  eta: 0:05:28  lr: 0.000200  loss: 1.1695 (1.1698)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.1770 (1.1682)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [160/431]  eta: 0:05:05  lr: 0.000200  loss: 1.1104 (1.1684)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.0675 (1.1650)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.0705 (1.1657)  time: 1.1109  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:80]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 1.1282 (1.1662)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.1282 (1.1667)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.1427 (1.1673)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.1427 (1.1659)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.1549 (1.1687)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.2189 (1.1718)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.1357 (1.1694)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.1038 (1.1687)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.1604 (1.1703)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1688 (1.1703)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1849 (1.1718)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1849 (1.1716)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1423 (1.1707)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1034 (1.1691)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.1009 (1.1680)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1181 (1.1664)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1715 (1.1667)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1230 (1.1651)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.0978 (1.1637)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.1347 (1.1650)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1721 (1.1649)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1017 (1.1645)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:80]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0952 (1.1640)  time: 1.1148  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:80]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1138 (1.1644)  time: 1.1162  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:80]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1561 (1.1648)  time: 1.1148  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:80] Total time: 0:08:02 (1.1186 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1561 (1.1648)\n",
      "Valid: [epoch:80]  [ 0/14]  eta: 0:00:36  loss: 1.1834 (1.1834)  time: 2.5965  data: 2.4481  max mem: 15925\n",
      "Valid: [epoch:80]  [13/14]  eta: 0:00:00  loss: 1.1050 (1.1181)  time: 0.2729  data: 0.1749  max mem: 15925\n",
      "Valid: [epoch:80] Total time: 0:00:04 (0.2921 s / it)\n",
      "Averaged stats: loss: 1.1050 (1.1181)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_80_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.118%\n",
      "Min loss: 1.107\n",
      "Best Epoch: 79.000\n",
      "Train: [epoch:81]  [  0/431]  eta: 0:30:40  lr: 0.000200  loss: 1.0867 (1.0867)  time: 4.2699  data: 3.0742  max mem: 15925\n",
      "Train: [epoch:81]  [ 10/431]  eta: 0:09:19  lr: 0.000200  loss: 1.1150 (1.2224)  time: 1.3285  data: 0.2797  max mem: 15925\n",
      "Train: [epoch:81]  [ 20/431]  eta: 0:08:15  lr: 0.000200  loss: 1.1150 (1.1923)  time: 1.0520  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 1.1179 (1.1859)  time: 1.0743  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 1.1361 (1.1643)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 1.0899 (1.1504)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 1.0899 (1.1532)  time: 1.1054  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:81]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 1.1736 (1.1612)  time: 1.1078  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:81]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 1.1990 (1.1636)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.1287 (1.1611)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 1.0872 (1.1547)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 1.0823 (1.1522)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.1012 (1.1528)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.1012 (1.1504)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.0756 (1.1462)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 1.0922 (1.1458)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.1531 (1.1470)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 1.1531 (1.1500)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 1.1039 (1.1483)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 1.1039 (1.1507)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.2094 (1.1557)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.1540 (1.1544)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 1.1161 (1.1561)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.1948 (1.1581)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.2104 (1.1592)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.1780 (1.1599)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 1.1331 (1.1609)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.1331 (1.1613)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.1041 (1.1604)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1703 (1.1613)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1874 (1.1628)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1247 (1.1620)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.0736 (1.1602)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1142 (1.1598)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1283 (1.1608)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1592 (1.1614)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1862 (1.1628)  time: 1.1171  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:81]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1708 (1.1627)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1363 (1.1623)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1489 (1.1623)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1331 (1.1610)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:81]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0958 (1.1599)  time: 1.1094  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:81]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.0958 (1.1603)  time: 1.1158  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:81]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1167 (1.1601)  time: 1.1147  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:81] Total time: 0:08:01 (1.1166 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1167 (1.1601)\n",
      "Valid: [epoch:81]  [ 0/14]  eta: 0:00:35  loss: 1.1585 (1.1585)  time: 2.5152  data: 2.3438  max mem: 15925\n",
      "Valid: [epoch:81]  [13/14]  eta: 0:00:00  loss: 1.0880 (1.1009)  time: 0.2676  data: 0.1675  max mem: 15925\n",
      "Valid: [epoch:81] Total time: 0:00:03 (0.2825 s / it)\n",
      "Averaged stats: loss: 1.0880 (1.1009)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_81_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.101%\n",
      "Min loss: 1.101\n",
      "Best Epoch: 81.000\n",
      "Train: [epoch:82]  [  0/431]  eta: 0:31:34  lr: 0.000200  loss: 1.1709 (1.1709)  time: 4.3961  data: 3.2285  max mem: 15925\n",
      "Train: [epoch:82]  [ 10/431]  eta: 0:09:21  lr: 0.000200  loss: 1.1242 (1.1551)  time: 1.3329  data: 0.2937  max mem: 15925\n",
      "Train: [epoch:82]  [ 20/431]  eta: 0:08:13  lr: 0.000200  loss: 1.1242 (1.1512)  time: 1.0407  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 1.1303 (1.1430)  time: 1.0682  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:82]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 1.1417 (1.1421)  time: 1.0860  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 1.1579 (1.1509)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 1.1579 (1.1445)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 1.0683 (1.1365)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 1.0683 (1.1371)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.0691 (1.1337)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 1.0637 (1.1355)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 1.0847 (1.1359)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.1197 (1.1344)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.0979 (1.1347)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.1191 (1.1393)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 1.1692 (1.1407)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.1692 (1.1442)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 1.1889 (1.1443)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 1.1889 (1.1491)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.1115 (1.1490)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.1557 (1.1502)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.1557 (1.1481)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.0729 (1.1463)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.1109 (1.1469)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.1133 (1.1460)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.1212 (1.1489)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.1618 (1.1509)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.1906 (1.1534)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.1906 (1.1556)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1598 (1.1549)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1338 (1.1566)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1329 (1.1549)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1155 (1.1545)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1274 (1.1554)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1847 (1.1557)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1437 (1.1573)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1338 (1.1573)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1105 (1.1563)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.0677 (1.1548)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.0738 (1.1538)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1308 (1.1563)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:82]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.2263 (1.1586)  time: 1.1045  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:82]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1681 (1.1575)  time: 1.1102  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:82]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1058 (1.1569)  time: 1.1098  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:82] Total time: 0:08:00 (1.1144 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1058 (1.1569)\n",
      "Valid: [epoch:82]  [ 0/14]  eta: 0:00:36  loss: 1.0264 (1.0264)  time: 2.5765  data: 2.4457  max mem: 15925\n",
      "Valid: [epoch:82]  [13/14]  eta: 0:00:00  loss: 1.0882 (1.1007)  time: 0.2665  data: 0.1748  max mem: 15925\n",
      "Valid: [epoch:82] Total time: 0:00:03 (0.2807 s / it)\n",
      "Averaged stats: loss: 1.0882 (1.1007)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_82_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.101%\n",
      "Min loss: 1.101\n",
      "Best Epoch: 82.000\n",
      "Train: [epoch:83]  [  0/431]  eta: 0:29:40  lr: 0.000200  loss: 1.2538 (1.2538)  time: 4.1309  data: 2.9192  max mem: 15925\n",
      "Train: [epoch:83]  [ 10/431]  eta: 0:09:22  lr: 0.000200  loss: 1.1647 (1.1705)  time: 1.3372  data: 0.2656  max mem: 15925\n",
      "Train: [epoch:83]  [ 20/431]  eta: 0:08:14  lr: 0.000200  loss: 1.1624 (1.1697)  time: 1.0566  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 1.1265 (1.1346)  time: 1.0683  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [ 40/431]  eta: 0:07:28  lr: 0.000200  loss: 1.0836 (1.1417)  time: 1.0862  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 1.1068 (1.1376)  time: 1.0986  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:83]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 1.1063 (1.1333)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 1.1282 (1.1473)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 1.1064 (1.1440)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.1023 (1.1452)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 1.1236 (1.1432)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 1.1644 (1.1450)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.1825 (1.1473)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.1323 (1.1461)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.0797 (1.1454)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 1.1074 (1.1460)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.1563 (1.1507)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 1.1299 (1.1502)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 1.1042 (1.1487)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 1.1356 (1.1488)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.1527 (1.1508)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 1.1452 (1.1527)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 1.1330 (1.1522)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.1056 (1.1517)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.1290 (1.1538)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.1449 (1.1550)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 1.1354 (1.1564)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.1581 (1.1574)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.1320 (1.1575)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1320 (1.1586)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1466 (1.1610)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1308 (1.1593)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1308 (1.1606)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1853 (1.1614)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1714 (1.1620)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1526 (1.1636)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1318 (1.1618)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.0981 (1.1606)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1081 (1.1588)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1174 (1.1590)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1508 (1.1587)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:83]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0950 (1.1575)  time: 1.0996  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:83]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.0935 (1.1568)  time: 1.1098  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:83]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1070 (1.1556)  time: 1.1135  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:83] Total time: 0:08:00 (1.1137 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1070 (1.1556)\n",
      "Valid: [epoch:83]  [ 0/14]  eta: 0:00:36  loss: 1.1400 (1.1400)  time: 2.6256  data: 2.4753  max mem: 15925\n",
      "Valid: [epoch:83]  [13/14]  eta: 0:00:00  loss: 1.0788 (1.0933)  time: 0.2757  data: 0.1769  max mem: 15925\n",
      "Valid: [epoch:83] Total time: 0:00:04 (0.2931 s / it)\n",
      "Averaged stats: loss: 1.0788 (1.0933)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_83_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.093%\n",
      "Min loss: 1.093\n",
      "Best Epoch: 83.000\n",
      "Train: [epoch:84]  [  0/431]  eta: 0:34:09  lr: 0.000200  loss: 1.0811 (1.0811)  time: 4.7558  data: 3.6094  max mem: 15925\n",
      "Train: [epoch:84]  [ 10/431]  eta: 0:09:41  lr: 0.000200  loss: 1.2114 (1.1858)  time: 1.3819  data: 0.3283  max mem: 15925\n",
      "Train: [epoch:84]  [ 20/431]  eta: 0:08:23  lr: 0.000200  loss: 1.1665 (1.1786)  time: 1.0489  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [ 30/431]  eta: 0:07:52  lr: 0.000200  loss: 1.1584 (1.1801)  time: 1.0680  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 1.1437 (1.1668)  time: 1.0840  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 1.1265 (1.1701)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 1.1795 (1.1733)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 1.1266 (1.1682)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [ 80/431]  eta: 0:06:37  lr: 0.000200  loss: 1.1350 (1.1685)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 1.1187 (1.1607)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 1.1065 (1.1565)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 1.0855 (1.1495)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.1133 (1.1526)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 1.1143 (1.1496)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.1329 (1.1502)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [150/431]  eta: 0:05:15  lr: 0.000200  loss: 1.1332 (1.1534)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.0990 (1.1530)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 1.0897 (1.1504)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 1.0725 (1.1468)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 1.0855 (1.1485)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.1406 (1.1527)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.1662 (1.1546)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 1.1015 (1.1533)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.1078 (1.1552)  time: 1.1074  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:84]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.1672 (1.1575)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.1680 (1.1588)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 1.1680 (1.1593)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.1646 (1.1579)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.0657 (1.1564)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.0716 (1.1552)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1098 (1.1548)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1068 (1.1529)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1157 (1.1521)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1669 (1.1534)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1669 (1.1522)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1523 (1.1540)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1590 (1.1535)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.0798 (1.1529)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1451 (1.1545)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1937 (1.1552)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0769 (1.1532)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0822 (1.1534)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:84]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1892 (1.1541)  time: 1.1126  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:84]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1079 (1.1525)  time: 1.1130  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:84] Total time: 0:08:00 (1.1145 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1079 (1.1525)\n",
      "Valid: [epoch:84]  [ 0/14]  eta: 0:00:37  loss: 1.1392 (1.1392)  time: 2.6783  data: 2.5695  max mem: 15925\n",
      "Valid: [epoch:84]  [13/14]  eta: 0:00:00  loss: 1.0835 (1.0950)  time: 0.2743  data: 0.1836  max mem: 15925\n",
      "Valid: [epoch:84] Total time: 0:00:04 (0.2903 s / it)\n",
      "Averaged stats: loss: 1.0835 (1.0950)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_84_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.095%\n",
      "Min loss: 1.093\n",
      "Best Epoch: 83.000\n",
      "Train: [epoch:85]  [  0/431]  eta: 0:29:58  lr: 0.000200  loss: 1.1182 (1.1182)  time: 4.1734  data: 2.9202  max mem: 15925\n",
      "Train: [epoch:85]  [ 10/431]  eta: 0:09:16  lr: 0.000200  loss: 1.1518 (1.1625)  time: 1.3227  data: 0.2657  max mem: 15925\n",
      "Train: [epoch:85]  [ 20/431]  eta: 0:08:15  lr: 0.000200  loss: 1.1168 (1.1470)  time: 1.0562  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [ 30/431]  eta: 0:07:47  lr: 0.000200  loss: 1.1093 (1.1440)  time: 1.0786  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 1.1268 (1.1420)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 1.1598 (1.1520)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 1.1549 (1.1490)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 1.0903 (1.1499)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 1.0863 (1.1453)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.0748 (1.1410)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 1.0930 (1.1399)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 1.0938 (1.1389)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.1340 (1.1415)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 1.1353 (1.1470)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [140/431]  eta: 0:05:27  lr: 0.000200  loss: 1.1724 (1.1509)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [150/431]  eta: 0:05:16  lr: 0.000200  loss: 1.1462 (1.1500)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [160/431]  eta: 0:05:04  lr: 0.000200  loss: 1.1462 (1.1528)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [170/431]  eta: 0:04:53  lr: 0.000200  loss: 1.1575 (1.1534)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [180/431]  eta: 0:04:42  lr: 0.000200  loss: 1.1732 (1.1560)  time: 1.1274  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [190/431]  eta: 0:04:30  lr: 0.000200  loss: 1.1846 (1.1587)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [200/431]  eta: 0:04:19  lr: 0.000200  loss: 1.1475 (1.1573)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.1323 (1.1573)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.1375 (1.1562)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.1375 (1.1569)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.1349 (1.1557)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.1018 (1.1548)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.0636 (1.1538)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.1236 (1.1533)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1425 (1.1535)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1377 (1.1539)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1861 (1.1561)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1430 (1.1541)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1179 (1.1538)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.1370 (1.1545)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1458 (1.1555)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1545 (1.1560)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1295 (1.1553)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.0673 (1.1532)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.0700 (1.1537)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.0850 (1.1531)  time: 1.1269  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1111 (1.1531)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:85]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1549 (1.1542)  time: 1.1175  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:85]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1371 (1.1534)  time: 1.1230  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:85]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.0921 (1.1516)  time: 1.1242  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:85] Total time: 0:08:03 (1.1207 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.0921 (1.1516)\n",
      "Valid: [epoch:85]  [ 0/14]  eta: 0:00:35  loss: 1.0824 (1.0824)  time: 2.5709  data: 2.4342  max mem: 15925\n",
      "Valid: [epoch:85]  [13/14]  eta: 0:00:00  loss: 1.0824 (1.0940)  time: 0.2678  data: 0.1739  max mem: 15925\n",
      "Valid: [epoch:85] Total time: 0:00:03 (0.2844 s / it)\n",
      "Averaged stats: loss: 1.0824 (1.0940)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_85_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.094%\n",
      "Min loss: 1.093\n",
      "Best Epoch: 83.000\n",
      "Train: [epoch:86]  [  0/431]  eta: 0:31:40  lr: 0.000200  loss: 1.2038 (1.2038)  time: 4.4090  data: 3.1721  max mem: 15925\n",
      "Train: [epoch:86]  [ 10/431]  eta: 0:09:26  lr: 0.000200  loss: 1.2736 (1.2561)  time: 1.3461  data: 0.2885  max mem: 15925\n",
      "Train: [epoch:86]  [ 20/431]  eta: 0:08:15  lr: 0.000200  loss: 1.1841 (1.2026)  time: 1.0453  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 1.0784 (1.1722)  time: 1.0617  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 1.0588 (1.1519)  time: 1.0788  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 1.0588 (1.1447)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 1.1082 (1.1425)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 1.1239 (1.1549)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 1.1805 (1.1579)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 1.1412 (1.1550)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 1.0793 (1.1499)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 1.0703 (1.1486)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 1.0703 (1.1440)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.1131 (1.1420)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.1264 (1.1416)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 1.1264 (1.1402)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.1349 (1.1396)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [170/431]  eta: 0:04:52  lr: 0.000200  loss: 1.1349 (1.1389)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [180/431]  eta: 0:04:41  lr: 0.000200  loss: 1.1507 (1.1440)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 1.1546 (1.1475)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.1745 (1.1502)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [210/431]  eta: 0:04:07  lr: 0.000200  loss: 1.1402 (1.1501)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [220/431]  eta: 0:03:56  lr: 0.000200  loss: 1.0943 (1.1462)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.0667 (1.1462)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.1503 (1.1484)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [250/431]  eta: 0:03:22  lr: 0.000200  loss: 1.1218 (1.1472)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [260/431]  eta: 0:03:11  lr: 0.000200  loss: 1.0894 (1.1469)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.0724 (1.1437)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.1262 (1.1456)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.1635 (1.1463)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1952 (1.1477)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1472 (1.1461)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1040 (1.1463)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1592 (1.1477)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1988 (1.1504)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1959 (1.1513)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1352 (1.1521)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1199 (1.1526)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1413 (1.1526)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1413 (1.1526)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1017 (1.1521)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0712 (1.1506)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:86]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.0712 (1.1505)  time: 1.1119  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:86]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1465 (1.1496)  time: 1.1136  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:86] Total time: 0:08:01 (1.1170 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1465 (1.1496)\n",
      "Valid: [epoch:86]  [ 0/14]  eta: 0:00:35  loss: 1.0524 (1.0524)  time: 2.5434  data: 2.3832  max mem: 15925\n",
      "Valid: [epoch:86]  [13/14]  eta: 0:00:00  loss: 1.0753 (1.0884)  time: 0.2835  data: 0.1703  max mem: 15925\n",
      "Valid: [epoch:86] Total time: 0:00:04 (0.2990 s / it)\n",
      "Averaged stats: loss: 1.0753 (1.0884)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_86_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.088%\n",
      "Min loss: 1.088\n",
      "Best Epoch: 86.000\n",
      "Train: [epoch:87]  [  0/431]  eta: 0:35:20  lr: 0.000200  loss: 1.2413 (1.2413)  time: 4.9196  data: 3.7941  max mem: 15925\n",
      "Train: [epoch:87]  [ 10/431]  eta: 0:09:51  lr: 0.000200  loss: 1.1409 (1.1032)  time: 1.4059  data: 0.3451  max mem: 15925\n",
      "Train: [epoch:87]  [ 20/431]  eta: 0:08:32  lr: 0.000200  loss: 1.1217 (1.1185)  time: 1.0625  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [ 30/431]  eta: 0:07:58  lr: 0.000200  loss: 1.1217 (1.1360)  time: 1.0751  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [ 40/431]  eta: 0:07:38  lr: 0.000200  loss: 1.1107 (1.1393)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [ 50/431]  eta: 0:07:21  lr: 0.000200  loss: 1.1302 (1.1499)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [ 60/431]  eta: 0:07:08  lr: 0.000200  loss: 1.1642 (1.1504)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [ 70/431]  eta: 0:06:55  lr: 0.000200  loss: 1.1741 (1.1558)  time: 1.1269  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [ 80/431]  eta: 0:06:42  lr: 0.000200  loss: 1.1572 (1.1585)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [ 90/431]  eta: 0:06:29  lr: 0.000200  loss: 1.1557 (1.1607)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [100/431]  eta: 0:06:17  lr: 0.000200  loss: 1.0844 (1.1615)  time: 1.1130  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:87]  [110/431]  eta: 0:06:05  lr: 0.000200  loss: 1.0844 (1.1566)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [120/431]  eta: 0:05:53  lr: 0.000200  loss: 1.0958 (1.1516)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [130/431]  eta: 0:05:41  lr: 0.000200  loss: 1.1042 (1.1519)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [140/431]  eta: 0:05:29  lr: 0.000200  loss: 1.1869 (1.1547)  time: 1.1191  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:87]  [150/431]  eta: 0:05:18  lr: 0.000200  loss: 1.1449 (1.1494)  time: 1.1215  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:87]  [160/431]  eta: 0:05:06  lr: 0.000200  loss: 1.1145 (1.1524)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [170/431]  eta: 0:04:55  lr: 0.000200  loss: 1.1520 (1.1536)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [180/431]  eta: 0:04:43  lr: 0.000200  loss: 1.1583 (1.1554)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [190/431]  eta: 0:04:32  lr: 0.000200  loss: 1.1749 (1.1605)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [200/431]  eta: 0:04:20  lr: 0.000200  loss: 1.1540 (1.1604)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [210/431]  eta: 0:04:09  lr: 0.000200  loss: 1.1327 (1.1601)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 1.1093 (1.1563)  time: 1.1153  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:87]  [230/431]  eta: 0:03:46  lr: 0.000200  loss: 1.0687 (1.1553)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [240/431]  eta: 0:03:35  lr: 0.000200  loss: 1.1297 (1.1553)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.1404 (1.1541)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 1.1756 (1.1547)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [270/431]  eta: 0:03:01  lr: 0.000200  loss: 1.1526 (1.1542)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1524 (1.1536)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.1514 (1.1530)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [300/431]  eta: 0:02:27  lr: 0.000200  loss: 1.1543 (1.1542)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [310/431]  eta: 0:02:16  lr: 0.000200  loss: 1.1537 (1.1524)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.0882 (1.1525)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.1190 (1.1535)  time: 1.1210  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:87]  [340/431]  eta: 0:01:42  lr: 0.000200  loss: 1.1654 (1.1542)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [350/431]  eta: 0:01:31  lr: 0.000200  loss: 1.1606 (1.1548)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1487 (1.1546)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1184 (1.1548)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.1123 (1.1529)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [390/431]  eta: 0:00:46  lr: 0.000200  loss: 1.1097 (1.1526)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1191 (1.1523)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:87]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1136 (1.1503)  time: 1.1165  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:87]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.0672 (1.1499)  time: 1.1139  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:87]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.0758 (1.1484)  time: 1.1048  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:87] Total time: 0:08:03 (1.1223 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.0758 (1.1484)\n",
      "Valid: [epoch:87]  [ 0/14]  eta: 0:00:36  loss: 1.0733 (1.0733)  time: 2.6237  data: 2.4637  max mem: 15925\n",
      "Valid: [epoch:87]  [13/14]  eta: 0:00:00  loss: 1.0733 (1.0867)  time: 0.2755  data: 0.1761  max mem: 15925\n",
      "Valid: [epoch:87] Total time: 0:00:04 (0.2952 s / it)\n",
      "Averaged stats: loss: 1.0733 (1.0867)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_87_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.087%\n",
      "Min loss: 1.087\n",
      "Best Epoch: 87.000\n",
      "Train: [epoch:88]  [  0/431]  eta: 0:32:06  lr: 0.000200  loss: 1.2072 (1.2072)  time: 4.4696  data: 3.2005  max mem: 15925\n",
      "Train: [epoch:88]  [ 10/431]  eta: 0:09:37  lr: 0.000200  loss: 1.0995 (1.1245)  time: 1.3710  data: 0.2912  max mem: 15925\n",
      "Train: [epoch:88]  [ 20/431]  eta: 0:08:21  lr: 0.000200  loss: 1.1289 (1.1915)  time: 1.0589  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 1.1409 (1.1593)  time: 1.0702  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 1.0757 (1.1574)  time: 1.0886  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 1.1269 (1.1611)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [ 60/431]  eta: 0:07:02  lr: 0.000200  loss: 1.0966 (1.1479)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [ 70/431]  eta: 0:06:50  lr: 0.000200  loss: 1.1203 (1.1512)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [ 80/431]  eta: 0:06:38  lr: 0.000200  loss: 1.1550 (1.1482)  time: 1.1231  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:88]  [ 90/431]  eta: 0:06:25  lr: 0.000200  loss: 1.1267 (1.1495)  time: 1.1134  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:88]  [100/431]  eta: 0:06:13  lr: 0.000200  loss: 1.0817 (1.1458)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [110/431]  eta: 0:06:01  lr: 0.000200  loss: 1.0838 (1.1454)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [120/431]  eta: 0:05:49  lr: 0.000200  loss: 1.1184 (1.1444)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [130/431]  eta: 0:05:38  lr: 0.000200  loss: 1.1116 (1.1471)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.1247 (1.1459)  time: 1.1042  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:88]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 1.0898 (1.1434)  time: 1.0975  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:88]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.1041 (1.1456)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 1.1148 (1.1439)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 1.1228 (1.1467)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [190/431]  eta: 0:04:29  lr: 0.000200  loss: 1.1472 (1.1489)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [200/431]  eta: 0:04:18  lr: 0.000200  loss: 1.1034 (1.1486)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 1.0706 (1.1454)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 1.0534 (1.1430)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [230/431]  eta: 0:03:44  lr: 0.000200  loss: 1.0967 (1.1449)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [240/431]  eta: 0:03:33  lr: 0.000200  loss: 1.1976 (1.1484)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.1625 (1.1473)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:88]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 1.0561 (1.1454)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.0579 (1.1453)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.0976 (1.1444)  time: 1.1114  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:88]  [290/431]  eta: 0:02:37  lr: 0.000200  loss: 1.0896 (1.1426)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.0909 (1.1428)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1120 (1.1432)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.0725 (1.1415)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.0780 (1.1412)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.0984 (1.1412)  time: 1.1079  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:88]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.1428 (1.1437)  time: 1.1130  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:88]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.1708 (1.1454)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1147 (1.1445)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.0912 (1.1430)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1040 (1.1434)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1266 (1.1431)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1141 (1.1431)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:88]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1217 (1.1442)  time: 1.1146  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:88]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1796 (1.1454)  time: 1.1067  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:88] Total time: 0:08:00 (1.1142 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1796 (1.1454)\n",
      "Valid: [epoch:88]  [ 0/14]  eta: 0:00:36  loss: 1.0590 (1.0590)  time: 2.5749  data: 2.4559  max mem: 15925\n",
      "Valid: [epoch:88]  [13/14]  eta: 0:00:00  loss: 1.0827 (1.0922)  time: 0.2770  data: 0.1755  max mem: 15925\n",
      "Valid: [epoch:88] Total time: 0:00:04 (0.2936 s / it)\n",
      "Averaged stats: loss: 1.0827 (1.0922)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_88_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.092%\n",
      "Min loss: 1.087\n",
      "Best Epoch: 87.000\n",
      "Train: [epoch:89]  [  0/431]  eta: 0:35:32  lr: 0.000200  loss: 1.1751 (1.1751)  time: 4.9487  data: 3.7465  max mem: 15925\n",
      "Train: [epoch:89]  [ 10/431]  eta: 0:09:42  lr: 0.000200  loss: 1.1751 (1.1828)  time: 1.3835  data: 0.3408  max mem: 15925\n",
      "Train: [epoch:89]  [ 20/431]  eta: 0:08:28  lr: 0.000200  loss: 1.1664 (1.1691)  time: 1.0524  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [ 30/431]  eta: 0:07:55  lr: 0.000200  loss: 1.1421 (1.1735)  time: 1.0787  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [ 40/431]  eta: 0:07:36  lr: 0.000200  loss: 1.1288 (1.1630)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [ 50/431]  eta: 0:07:19  lr: 0.000200  loss: 1.1276 (1.1535)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [ 60/431]  eta: 0:07:05  lr: 0.000200  loss: 1.0871 (1.1520)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [ 70/431]  eta: 0:06:52  lr: 0.000200  loss: 1.0986 (1.1578)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [ 80/431]  eta: 0:06:40  lr: 0.000200  loss: 1.1175 (1.1594)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [ 90/431]  eta: 0:06:28  lr: 0.000200  loss: 1.1600 (1.1611)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [100/431]  eta: 0:06:16  lr: 0.000200  loss: 1.1323 (1.1562)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [110/431]  eta: 0:06:04  lr: 0.000200  loss: 1.0968 (1.1551)  time: 1.1275  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [120/431]  eta: 0:05:52  lr: 0.000200  loss: 1.0996 (1.1518)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [130/431]  eta: 0:05:41  lr: 0.000200  loss: 1.0796 (1.1472)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [140/431]  eta: 0:05:29  lr: 0.000200  loss: 1.0525 (1.1432)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [150/431]  eta: 0:05:17  lr: 0.000200  loss: 1.1196 (1.1445)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [160/431]  eta: 0:05:06  lr: 0.000200  loss: 1.1295 (1.1446)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [170/431]  eta: 0:04:54  lr: 0.000200  loss: 1.0986 (1.1448)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [180/431]  eta: 0:04:43  lr: 0.000200  loss: 1.0647 (1.1404)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [190/431]  eta: 0:04:31  lr: 0.000200  loss: 1.1311 (1.1456)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [200/431]  eta: 0:04:20  lr: 0.000200  loss: 1.1595 (1.1450)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [210/431]  eta: 0:04:08  lr: 0.000200  loss: 1.1273 (1.1475)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [220/431]  eta: 0:03:57  lr: 0.000200  loss: 1.1662 (1.1469)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [230/431]  eta: 0:03:45  lr: 0.000200  loss: 1.1496 (1.1460)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [240/431]  eta: 0:03:34  lr: 0.000200  loss: 1.0995 (1.1464)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [250/431]  eta: 0:03:23  lr: 0.000200  loss: 1.1013 (1.1462)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [260/431]  eta: 0:03:12  lr: 0.000200  loss: 1.1100 (1.1476)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [270/431]  eta: 0:03:00  lr: 0.000200  loss: 1.1744 (1.1501)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [280/431]  eta: 0:02:49  lr: 0.000200  loss: 1.1389 (1.1491)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [290/431]  eta: 0:02:38  lr: 0.000200  loss: 1.1406 (1.1492)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [300/431]  eta: 0:02:26  lr: 0.000200  loss: 1.1419 (1.1501)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [310/431]  eta: 0:02:15  lr: 0.000200  loss: 1.1000 (1.1496)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [320/431]  eta: 0:02:04  lr: 0.000200  loss: 1.1285 (1.1490)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [330/431]  eta: 0:01:53  lr: 0.000200  loss: 1.1628 (1.1519)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1724 (1.1524)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [350/431]  eta: 0:01:30  lr: 0.000200  loss: 1.0772 (1.1509)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [360/431]  eta: 0:01:19  lr: 0.000200  loss: 1.0772 (1.1498)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [370/431]  eta: 0:01:08  lr: 0.000200  loss: 1.1055 (1.1512)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [380/431]  eta: 0:00:57  lr: 0.000200  loss: 1.1347 (1.1506)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1107 (1.1491)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0641 (1.1478)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:89]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1148 (1.1490)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:89]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1148 (1.1476)  time: 1.1037  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:89]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.0483 (1.1469)  time: 1.1043  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:89] Total time: 0:08:01 (1.1182 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.0483 (1.1469)\n",
      "Valid: [epoch:89]  [ 0/14]  eta: 0:00:36  loss: 1.1413 (1.1413)  time: 2.6016  data: 2.4614  max mem: 15925\n",
      "Valid: [epoch:89]  [13/14]  eta: 0:00:00  loss: 1.0749 (1.0866)  time: 0.2743  data: 0.1759  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:89] Total time: 0:00:04 (0.2908 s / it)\n",
      "Averaged stats: loss: 1.0749 (1.0866)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_89_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.087%\n",
      "Min loss: 1.087\n",
      "Best Epoch: 89.000\n",
      "Train: [epoch:90]  [  0/431]  eta: 0:33:47  lr: 0.000200  loss: 1.1417 (1.1417)  time: 4.7046  data: 3.5621  max mem: 15925\n",
      "Train: [epoch:90]  [ 10/431]  eta: 0:09:42  lr: 0.000200  loss: 1.2094 (1.1994)  time: 1.3834  data: 0.3240  max mem: 15925\n",
      "Train: [epoch:90]  [ 20/431]  eta: 0:08:22  lr: 0.000200  loss: 1.1906 (1.1846)  time: 1.0478  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [ 30/431]  eta: 0:07:50  lr: 0.000200  loss: 1.1756 (1.1794)  time: 1.0582  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [ 40/431]  eta: 0:07:30  lr: 0.000200  loss: 1.1114 (1.1653)  time: 1.0775  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 1.0965 (1.1456)  time: 1.0815  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 1.0726 (1.1394)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 1.1031 (1.1450)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [ 80/431]  eta: 0:06:34  lr: 0.000200  loss: 1.1008 (1.1437)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 1.1400 (1.1487)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 1.1754 (1.1562)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 1.1301 (1.1492)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 1.1542 (1.1519)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 1.1223 (1.1453)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 1.0606 (1.1438)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 1.0537 (1.1400)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 1.1199 (1.1436)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 1.1573 (1.1428)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.1225 (1.1438)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 1.1760 (1.1469)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 1.1760 (1.1461)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 1.1570 (1.1476)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.1530 (1.1462)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.1530 (1.1469)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 1.1638 (1.1469)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.1444 (1.1495)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1355 (1.1489)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.1355 (1.1492)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.1063 (1.1461)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.0728 (1.1464)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.1205 (1.1468)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1056 (1.1456)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.0921 (1.1466)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.2000 (1.1486)  time: 1.1005  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:90]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1659 (1.1497)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1491 (1.1491)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1473 (1.1481)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.0834 (1.1464)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.0954 (1.1459)  time: 1.1036  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:90]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1112 (1.1447)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0862 (1.1436)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:90]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0967 (1.1429)  time: 1.1011  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:90]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1272 (1.1433)  time: 1.0966  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:90]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1390 (1.1439)  time: 1.0918  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:90] Total time: 0:07:57 (1.1069 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1390 (1.1439)\n",
      "Valid: [epoch:90]  [ 0/14]  eta: 0:00:37  loss: 0.9654 (0.9654)  time: 2.6901  data: 2.5313  max mem: 15925\n",
      "Valid: [epoch:90]  [13/14]  eta: 0:00:00  loss: 1.0774 (1.0874)  time: 0.2843  data: 0.1809  max mem: 15925\n",
      "Valid: [epoch:90] Total time: 0:00:04 (0.3015 s / it)\n",
      "Averaged stats: loss: 1.0774 (1.0874)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_90_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.087%\n",
      "Min loss: 1.087\n",
      "Best Epoch: 89.000\n",
      "Train: [epoch:91]  [  0/431]  eta: 0:32:42  lr: 0.000200  loss: 1.2930 (1.2930)  time: 4.5539  data: 3.3855  max mem: 15925\n",
      "Train: [epoch:91]  [ 10/431]  eta: 0:09:31  lr: 0.000200  loss: 1.1343 (1.1786)  time: 1.3565  data: 0.3079  max mem: 15925\n",
      "Train: [epoch:91]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 1.1343 (1.1649)  time: 1.0413  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [ 30/431]  eta: 0:07:45  lr: 0.000200  loss: 1.0725 (1.1362)  time: 1.0540  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 1.0618 (1.1217)  time: 1.0767  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [ 50/431]  eta: 0:07:11  lr: 0.000200  loss: 1.0885 (1.1223)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [ 60/431]  eta: 0:06:58  lr: 0.000200  loss: 1.0928 (1.1261)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [ 70/431]  eta: 0:06:46  lr: 0.000200  loss: 1.1699 (1.1386)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [ 80/431]  eta: 0:06:34  lr: 0.000200  loss: 1.1244 (1.1396)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 1.0895 (1.1362)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 1.0941 (1.1446)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 1.1032 (1.1389)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 1.1032 (1.1368)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 1.0927 (1.1346)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 1.0927 (1.1336)  time: 1.1143  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:91]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 1.1311 (1.1344)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 1.1305 (1.1357)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 1.1167 (1.1369)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.1167 (1.1369)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 1.1539 (1.1400)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 1.1446 (1.1403)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 1.1391 (1.1411)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.1093 (1.1407)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.0792 (1.1377)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 1.0900 (1.1398)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 1.0910 (1.1395)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1350 (1.1404)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.1882 (1.1436)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.1788 (1.1431)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.1290 (1.1424)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.1343 (1.1427)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1361 (1.1423)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 1.1075 (1.1429)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1262 (1.1430)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1313 (1.1442)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.2022 (1.1466)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.2172 (1.1478)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1178 (1.1460)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.0592 (1.1451)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.0950 (1.1445)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0794 (1.1434)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0958 (1.1431)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:91]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1274 (1.1439)  time: 1.0976  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:91]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1135 (1.1439)  time: 1.0976  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:91] Total time: 0:07:56 (1.1064 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1135 (1.1439)\n",
      "Valid: [epoch:91]  [ 0/14]  eta: 0:00:37  loss: 1.1272 (1.1272)  time: 2.6577  data: 2.4775  max mem: 15925\n",
      "Valid: [epoch:91]  [13/14]  eta: 0:00:00  loss: 1.0767 (1.0860)  time: 0.2786  data: 0.1771  max mem: 15925\n",
      "Valid: [epoch:91] Total time: 0:00:04 (0.2949 s / it)\n",
      "Averaged stats: loss: 1.0767 (1.0860)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_91_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.086%\n",
      "Min loss: 1.086\n",
      "Best Epoch: 91.000\n",
      "Train: [epoch:92]  [  0/431]  eta: 0:31:18  lr: 0.000200  loss: 1.4228 (1.4228)  time: 4.3574  data: 3.1146  max mem: 15925\n",
      "Train: [epoch:92]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 1.1377 (1.1637)  time: 1.3405  data: 0.2833  max mem: 15925\n",
      "Train: [epoch:92]  [ 20/431]  eta: 0:08:15  lr: 0.000200  loss: 1.1510 (1.1814)  time: 1.0477  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [ 30/431]  eta: 0:07:45  lr: 0.000200  loss: 1.1242 (1.1462)  time: 1.0634  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [ 40/431]  eta: 0:07:26  lr: 0.000200  loss: 1.0873 (1.1398)  time: 1.0758  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [ 50/431]  eta: 0:07:11  lr: 0.000200  loss: 1.0770 (1.1264)  time: 1.0886  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [ 60/431]  eta: 0:06:58  lr: 0.000200  loss: 1.0493 (1.1202)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [ 70/431]  eta: 0:06:46  lr: 0.000200  loss: 1.1276 (1.1289)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 1.1410 (1.1298)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 1.1498 (1.1330)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 1.0953 (1.1306)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 1.0774 (1.1305)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 1.0926 (1.1292)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 1.1310 (1.1386)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 1.1434 (1.1390)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 1.1303 (1.1424)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [160/431]  eta: 0:05:00  lr: 0.000200  loss: 1.0973 (1.1406)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 1.0973 (1.1401)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 1.0879 (1.1390)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [190/431]  eta: 0:04:26  lr: 0.000200  loss: 1.0584 (1.1372)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [200/431]  eta: 0:04:15  lr: 0.000200  loss: 1.1196 (1.1389)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [210/431]  eta: 0:04:04  lr: 0.000200  loss: 1.1398 (1.1380)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 1.1089 (1.1378)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 1.1094 (1.1391)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 1.1361 (1.1374)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 1.1361 (1.1399)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1562 (1.1406)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.1347 (1.1403)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.1233 (1.1404)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 1.0809 (1.1411)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 1.1616 (1.1427)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 1.1404 (1.1436)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 1.1329 (1.1441)  time: 1.1005  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:92]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1226 (1.1441)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1192 (1.1443)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1613 (1.1461)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1747 (1.1457)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1346 (1.1466)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1342 (1.1458)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.0898 (1.1445)  time: 1.0909  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1038 (1.1439)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:92]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0994 (1.1430)  time: 1.0928  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:92]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1129 (1.1439)  time: 1.1028  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:92]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1506 (1.1440)  time: 1.1054  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:92] Total time: 0:07:55 (1.1044 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1506 (1.1440)\n",
      "Valid: [epoch:92]  [ 0/14]  eta: 0:00:36  loss: 1.0567 (1.0567)  time: 2.6143  data: 2.4737  max mem: 15925\n",
      "Valid: [epoch:92]  [13/14]  eta: 0:00:00  loss: 1.0705 (1.0830)  time: 0.2712  data: 0.1768  max mem: 15925\n",
      "Valid: [epoch:92] Total time: 0:00:04 (0.2878 s / it)\n",
      "Averaged stats: loss: 1.0705 (1.0830)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_92_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.083%\n",
      "Min loss: 1.083\n",
      "Best Epoch: 92.000\n",
      "Train: [epoch:93]  [  0/431]  eta: 0:30:27  lr: 0.000200  loss: 1.3784 (1.3784)  time: 4.2398  data: 2.9907  max mem: 15925\n",
      "Train: [epoch:93]  [ 10/431]  eta: 0:09:19  lr: 0.000200  loss: 1.2349 (1.2017)  time: 1.3290  data: 0.2721  max mem: 15925\n",
      "Train: [epoch:93]  [ 20/431]  eta: 0:08:12  lr: 0.000200  loss: 1.2110 (1.1916)  time: 1.0470  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [ 30/431]  eta: 0:07:44  lr: 0.000200  loss: 1.1726 (1.1757)  time: 1.0655  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [ 40/431]  eta: 0:07:26  lr: 0.000200  loss: 1.0643 (1.1537)  time: 1.0797  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [ 50/431]  eta: 0:07:11  lr: 0.000200  loss: 1.0475 (1.1392)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [ 60/431]  eta: 0:06:57  lr: 0.000200  loss: 1.0924 (1.1331)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [ 70/431]  eta: 0:06:45  lr: 0.000200  loss: 1.1464 (1.1405)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [ 80/431]  eta: 0:06:32  lr: 0.000200  loss: 1.1464 (1.1404)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 1.0840 (1.1339)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [100/431]  eta: 0:06:08  lr: 0.000200  loss: 1.0616 (1.1313)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [110/431]  eta: 0:05:57  lr: 0.000200  loss: 1.0558 (1.1274)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [120/431]  eta: 0:05:45  lr: 0.000200  loss: 1.0901 (1.1283)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [130/431]  eta: 0:05:34  lr: 0.000200  loss: 1.1123 (1.1281)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 1.0883 (1.1269)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [150/431]  eta: 0:05:11  lr: 0.000200  loss: 1.0830 (1.1272)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [160/431]  eta: 0:05:00  lr: 0.000200  loss: 1.0889 (1.1241)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 1.0889 (1.1226)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 1.1224 (1.1276)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 1.1236 (1.1271)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [200/431]  eta: 0:04:15  lr: 0.000200  loss: 1.1171 (1.1273)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [210/431]  eta: 0:04:04  lr: 0.000200  loss: 1.1373 (1.1289)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 1.1287 (1.1295)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 1.1267 (1.1310)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 1.1267 (1.1321)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 1.1558 (1.1342)  time: 1.1063  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:93]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1135 (1.1346)  time: 1.1008  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:93]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.0990 (1.1355)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.0727 (1.1332)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.0810 (1.1336)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 1.1378 (1.1367)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 1.1746 (1.1381)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 1.1360 (1.1377)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1162 (1.1380)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1488 (1.1391)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1568 (1.1397)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1364 (1.1393)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1404 (1.1396)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1054 (1.1385)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.0845 (1.1396)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0809 (1.1382)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1411 (1.1395)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:93]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1976 (1.1409)  time: 1.0992  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:93]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1402 (1.1407)  time: 1.0940  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:93] Total time: 0:07:56 (1.1050 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1402 (1.1407)\n",
      "Valid: [epoch:93]  [ 0/14]  eta: 0:00:37  loss: 1.0537 (1.0537)  time: 2.6805  data: 2.5209  max mem: 15925\n",
      "Valid: [epoch:93]  [13/14]  eta: 0:00:00  loss: 1.0764 (1.0860)  time: 0.2773  data: 0.1801  max mem: 15925\n",
      "Valid: [epoch:93] Total time: 0:00:04 (0.2948 s / it)\n",
      "Averaged stats: loss: 1.0764 (1.0860)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_93_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.086%\n",
      "Min loss: 1.083\n",
      "Best Epoch: 92.000\n",
      "Train: [epoch:94]  [  0/431]  eta: 0:31:41  lr: 0.000200  loss: 1.2475 (1.2475)  time: 4.4118  data: 3.2651  max mem: 15925\n",
      "Train: [epoch:94]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 1.2833 (1.2370)  time: 1.3398  data: 0.2970  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:94]  [ 20/431]  eta: 0:08:13  lr: 0.000200  loss: 1.1243 (1.1756)  time: 1.0410  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 1.0773 (1.1409)  time: 1.0668  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [ 40/431]  eta: 0:07:26  lr: 0.000200  loss: 1.0721 (1.1337)  time: 1.0809  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [ 50/431]  eta: 0:07:11  lr: 0.000200  loss: 1.1106 (1.1405)  time: 1.0831  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [ 60/431]  eta: 0:06:58  lr: 0.000200  loss: 1.1126 (1.1384)  time: 1.0950  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:94]  [ 70/431]  eta: 0:06:45  lr: 0.000200  loss: 1.0739 (1.1337)  time: 1.0990  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:94]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 1.0739 (1.1298)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [ 90/431]  eta: 0:06:21  lr: 0.000200  loss: 1.1149 (1.1329)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [100/431]  eta: 0:06:09  lr: 0.000200  loss: 1.1244 (1.1382)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 1.0961 (1.1299)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [120/431]  eta: 0:05:46  lr: 0.000200  loss: 1.0443 (1.1254)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 1.0743 (1.1223)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [140/431]  eta: 0:05:23  lr: 0.000200  loss: 1.1122 (1.1230)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 1.1254 (1.1247)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [160/431]  eta: 0:05:00  lr: 0.000200  loss: 1.1577 (1.1272)  time: 1.0895  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [170/431]  eta: 0:04:49  lr: 0.000200  loss: 1.0917 (1.1255)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 1.0853 (1.1265)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 1.0853 (1.1250)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [200/431]  eta: 0:04:15  lr: 0.000200  loss: 1.1746 (1.1311)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [210/431]  eta: 0:04:04  lr: 0.000200  loss: 1.1928 (1.1309)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 1.1178 (1.1316)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 1.1178 (1.1318)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 1.1343 (1.1332)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 1.1403 (1.1354)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1791 (1.1353)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [270/431]  eta: 0:02:57  lr: 0.000200  loss: 1.1273 (1.1371)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [280/431]  eta: 0:02:46  lr: 0.000200  loss: 1.0893 (1.1349)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 1.0689 (1.1360)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 1.1193 (1.1369)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 1.1912 (1.1384)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 1.2166 (1.1410)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1761 (1.1404)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1309 (1.1425)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1470 (1.1443)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1337 (1.1448)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.0989 (1.1433)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1172 (1.1439)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1262 (1.1438)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0822 (1.1421)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0524 (1.1411)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:94]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1000 (1.1421)  time: 1.1039  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:94]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.0978 (1.1414)  time: 1.0984  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:94] Total time: 0:07:55 (1.1026 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.0978 (1.1414)\n",
      "Valid: [epoch:94]  [ 0/14]  eta: 0:00:34  loss: 1.1380 (1.1380)  time: 2.4881  data: 2.3245  max mem: 15925\n",
      "Valid: [epoch:94]  [13/14]  eta: 0:00:00  loss: 1.0689 (1.0799)  time: 0.2615  data: 0.1661  max mem: 15925\n",
      "Valid: [epoch:94] Total time: 0:00:03 (0.2766 s / it)\n",
      "Averaged stats: loss: 1.0689 (1.0799)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_94_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.080%\n",
      "Min loss: 1.080\n",
      "Best Epoch: 94.000\n",
      "Train: [epoch:95]  [  0/431]  eta: 0:38:58  lr: 0.000200  loss: 1.2263 (1.2263)  time: 5.4266  data: 4.0381  max mem: 15925\n",
      "Train: [epoch:95]  [ 10/431]  eta: 0:10:00  lr: 0.000200  loss: 1.1525 (1.1521)  time: 1.4254  data: 0.3673  max mem: 15925\n",
      "Train: [epoch:95]  [ 20/431]  eta: 0:08:28  lr: 0.000200  loss: 1.1637 (1.1706)  time: 1.0283  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [ 30/431]  eta: 0:07:56  lr: 0.000200  loss: 1.1637 (1.1537)  time: 1.0587  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [ 40/431]  eta: 0:07:34  lr: 0.000200  loss: 1.0724 (1.1377)  time: 1.0817  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [ 50/431]  eta: 0:07:17  lr: 0.000200  loss: 1.0794 (1.1327)  time: 1.0880  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 1.0740 (1.1251)  time: 1.0866  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 1.0833 (1.1284)  time: 1.0868  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 1.1323 (1.1309)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.1323 (1.1271)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 1.0886 (1.1225)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 1.0671 (1.1251)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 1.0795 (1.1206)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.0851 (1.1204)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 1.1018 (1.1207)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 1.0810 (1.1192)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 1.1185 (1.1198)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 1.1639 (1.1235)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.1890 (1.1254)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 1.1416 (1.1269)  time: 1.0978  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:95]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 1.1111 (1.1264)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 1.1157 (1.1271)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.1101 (1.1284)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.1101 (1.1307)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 1.1124 (1.1307)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 1.1423 (1.1314)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1378 (1.1304)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.1293 (1.1324)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.0893 (1.1316)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.0506 (1.1315)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.1513 (1.1344)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1871 (1.1343)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.0831 (1.1342)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1074 (1.1352)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1454 (1.1374)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1696 (1.1392)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1716 (1.1407)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1716 (1.1413)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1395 (1.1409)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1852 (1.1430)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1372 (1.1423)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0812 (1.1423)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:95]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1015 (1.1415)  time: 1.1013  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:95]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1197 (1.1422)  time: 1.1050  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:95] Total time: 0:07:57 (1.1071 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1197 (1.1422)\n",
      "Valid: [epoch:95]  [ 0/14]  eta: 0:00:37  loss: 1.0597 (1.0597)  time: 2.6609  data: 2.4746  max mem: 15925\n",
      "Valid: [epoch:95]  [13/14]  eta: 0:00:00  loss: 1.0739 (1.0845)  time: 0.2857  data: 0.1770  max mem: 15925\n",
      "Valid: [epoch:95] Total time: 0:00:04 (0.3009 s / it)\n",
      "Averaged stats: loss: 1.0739 (1.0845)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_95_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.085%\n",
      "Min loss: 1.080\n",
      "Best Epoch: 94.000\n",
      "Train: [epoch:96]  [  0/431]  eta: 0:35:59  lr: 0.000200  loss: 0.9977 (0.9977)  time: 5.0107  data: 3.8962  max mem: 15925\n",
      "Train: [epoch:96]  [ 10/431]  eta: 0:09:41  lr: 0.000200  loss: 1.1634 (1.1365)  time: 1.3822  data: 0.3544  max mem: 15925\n",
      "Train: [epoch:96]  [ 20/431]  eta: 0:08:26  lr: 0.000200  loss: 1.1790 (1.1763)  time: 1.0428  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 1.1390 (1.1538)  time: 1.0688  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [ 40/431]  eta: 0:07:32  lr: 0.000200  loss: 1.1135 (1.1666)  time: 1.0793  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [ 50/431]  eta: 0:07:16  lr: 0.000200  loss: 1.0989 (1.1613)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 1.0989 (1.1671)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 1.1506 (1.1610)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 1.1590 (1.1741)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.1553 (1.1649)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 1.1299 (1.1608)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 1.0927 (1.1561)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 1.1495 (1.1561)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.1495 (1.1516)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [140/431]  eta: 0:05:26  lr: 0.000200  loss: 1.0947 (1.1475)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 1.0707 (1.1460)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [160/431]  eta: 0:05:03  lr: 0.000200  loss: 1.1485 (1.1487)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 1.1382 (1.1484)  time: 1.0962  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:96]  [180/431]  eta: 0:04:40  lr: 0.000200  loss: 1.1075 (1.1453)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 1.0813 (1.1420)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 1.0856 (1.1406)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 1.1119 (1.1409)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.1228 (1.1393)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.1307 (1.1419)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 1.1986 (1.1427)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.1317 (1.1423)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1317 (1.1428)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.1683 (1.1437)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.1278 (1.1424)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.0787 (1.1399)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.0405 (1.1383)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.0923 (1.1391)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1546 (1.1395)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1546 (1.1397)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1328 (1.1394)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1328 (1.1398)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1310 (1.1401)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1079 (1.1391)  time: 1.1064  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:96]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1428 (1.1394)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1155 (1.1384)  time: 1.0840  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0976 (1.1387)  time: 1.0874  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:96]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1335 (1.1398)  time: 1.0855  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:96]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1442 (1.1405)  time: 1.0878  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:96]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1133 (1.1403)  time: 1.0932  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:96] Total time: 0:07:56 (1.1049 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1133 (1.1403)\n",
      "Valid: [epoch:96]  [ 0/14]  eta: 0:00:36  loss: 0.9611 (0.9611)  time: 2.6375  data: 2.4756  max mem: 15925\n",
      "Valid: [epoch:96]  [13/14]  eta: 0:00:00  loss: 1.0732 (1.0825)  time: 0.2786  data: 0.1769  max mem: 15925\n",
      "Valid: [epoch:96] Total time: 0:00:04 (0.2976 s / it)\n",
      "Averaged stats: loss: 1.0732 (1.0825)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_96_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.082%\n",
      "Min loss: 1.080\n",
      "Best Epoch: 94.000\n",
      "Train: [epoch:97]  [  0/431]  eta: 0:33:50  lr: 0.000200  loss: 1.3900 (1.3900)  time: 4.7101  data: 3.5198  max mem: 15925\n",
      "Train: [epoch:97]  [ 10/431]  eta: 0:09:31  lr: 0.000200  loss: 1.0880 (1.1509)  time: 1.3577  data: 0.3202  max mem: 15925\n",
      "Train: [epoch:97]  [ 20/431]  eta: 0:08:19  lr: 0.000200  loss: 1.1061 (1.1507)  time: 1.0412  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [ 30/431]  eta: 0:07:49  lr: 0.000200  loss: 1.1068 (1.1357)  time: 1.0669  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 1.1091 (1.1411)  time: 1.0805  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 1.1279 (1.1428)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 1.0824 (1.1300)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [ 70/431]  eta: 0:06:49  lr: 0.000200  loss: 1.0600 (1.1390)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [ 80/431]  eta: 0:06:36  lr: 0.000200  loss: 1.1310 (1.1367)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [ 90/431]  eta: 0:06:24  lr: 0.000200  loss: 1.1034 (1.1349)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [100/431]  eta: 0:06:12  lr: 0.000200  loss: 1.0900 (1.1303)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [110/431]  eta: 0:06:00  lr: 0.000200  loss: 1.0799 (1.1311)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 1.1081 (1.1313)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.0349 (1.1228)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 1.0343 (1.1205)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 1.0847 (1.1268)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 1.1419 (1.1257)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 1.1153 (1.1257)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.1153 (1.1285)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 1.1529 (1.1343)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 1.1252 (1.1331)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 1.0612 (1.1308)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.1196 (1.1327)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.1361 (1.1331)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 1.1287 (1.1357)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.1169 (1.1336)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.0455 (1.1316)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.0674 (1.1304)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.1071 (1.1315)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.1386 (1.1336)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.1725 (1.1357)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1725 (1.1362)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1461 (1.1364)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1359 (1.1375)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1344 (1.1373)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1361 (1.1381)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1372 (1.1378)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1132 (1.1378)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1325 (1.1387)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1130 (1.1382)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0769 (1.1376)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1177 (1.1375)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:97]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1421 (1.1376)  time: 1.1082  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:97]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1421 (1.1379)  time: 1.1075  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:97] Total time: 0:07:57 (1.1079 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1421 (1.1379)\n",
      "Valid: [epoch:97]  [ 0/14]  eta: 0:00:36  loss: 1.0473 (1.0473)  time: 2.6008  data: 2.4341  max mem: 15925\n",
      "Valid: [epoch:97]  [13/14]  eta: 0:00:00  loss: 1.0700 (1.0821)  time: 0.2707  data: 0.1740  max mem: 15925\n",
      "Valid: [epoch:97] Total time: 0:00:04 (0.2867 s / it)\n",
      "Averaged stats: loss: 1.0700 (1.0821)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_97_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.082%\n",
      "Min loss: 1.080\n",
      "Best Epoch: 94.000\n",
      "Train: [epoch:98]  [  0/431]  eta: 0:31:55  lr: 0.000200  loss: 1.1193 (1.1193)  time: 4.4438  data: 3.2128  max mem: 15925\n",
      "Train: [epoch:98]  [ 10/431]  eta: 0:09:26  lr: 0.000200  loss: 1.1376 (1.1697)  time: 1.3465  data: 0.2922  max mem: 15925\n",
      "Train: [epoch:98]  [ 20/431]  eta: 0:08:18  lr: 0.000200  loss: 1.1460 (1.1713)  time: 1.0517  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [ 30/431]  eta: 0:07:51  lr: 0.000200  loss: 1.1400 (1.1522)  time: 1.0825  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 1.0963 (1.1405)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [ 50/431]  eta: 0:07:15  lr: 0.000200  loss: 1.0963 (1.1368)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [ 60/431]  eta: 0:07:00  lr: 0.000200  loss: 1.1452 (1.1389)  time: 1.0932  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:98]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 1.1452 (1.1355)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [ 80/431]  eta: 0:06:34  lr: 0.000200  loss: 1.0913 (1.1325)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 1.0948 (1.1328)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 1.1185 (1.1374)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 1.1421 (1.1374)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 1.1514 (1.1381)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 1.0469 (1.1343)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 1.0813 (1.1354)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 1.0813 (1.1310)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 1.1347 (1.1356)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 1.1687 (1.1370)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [180/431]  eta: 0:04:38  lr: 0.000200  loss: 1.1133 (1.1391)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 1.1450 (1.1409)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 1.1137 (1.1421)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 1.0913 (1.1399)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [220/431]  eta: 0:03:53  lr: 0.000200  loss: 1.0994 (1.1405)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 1.0994 (1.1400)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 1.0790 (1.1385)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 1.0790 (1.1396)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.0695 (1.1380)  time: 1.0895  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.1083 (1.1387)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [280/431]  eta: 0:02:46  lr: 0.000200  loss: 1.1098 (1.1383)  time: 1.0896  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 1.0838 (1.1367)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 1.1006 (1.1380)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 1.1418 (1.1368)  time: 1.0917  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 1.1220 (1.1371)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1589 (1.1382)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1461 (1.1377)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1668 (1.1399)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1768 (1.1393)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.0653 (1.1374)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.0781 (1.1369)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1095 (1.1364)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1189 (1.1364)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:98]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1625 (1.1379)  time: 1.0933  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:98]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1631 (1.1375)  time: 1.0911  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:98]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1503 (1.1389)  time: 1.0980  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:98] Total time: 0:07:55 (1.1026 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1503 (1.1389)\n",
      "Valid: [epoch:98]  [ 0/14]  eta: 0:00:35  loss: 0.9597 (0.9597)  time: 2.5263  data: 2.3832  max mem: 15925\n",
      "Valid: [epoch:98]  [13/14]  eta: 0:00:00  loss: 1.0695 (1.0803)  time: 0.2639  data: 0.1703  max mem: 15925\n",
      "Valid: [epoch:98] Total time: 0:00:03 (0.2809 s / it)\n",
      "Averaged stats: loss: 1.0695 (1.0803)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_98_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.080%\n",
      "Min loss: 1.080\n",
      "Best Epoch: 94.000\n",
      "Train: [epoch:99]  [  0/431]  eta: 0:34:33  lr: 0.000200  loss: 1.1428 (1.1428)  time: 4.8102  data: 3.7099  max mem: 15925\n",
      "Train: [epoch:99]  [ 10/431]  eta: 0:09:42  lr: 0.000200  loss: 1.1428 (1.1749)  time: 1.3841  data: 0.3374  max mem: 15925\n",
      "Train: [epoch:99]  [ 20/431]  eta: 0:08:21  lr: 0.000200  loss: 1.1130 (1.1647)  time: 1.0417  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [ 30/431]  eta: 0:07:50  lr: 0.000200  loss: 1.1064 (1.1487)  time: 1.0580  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 1.0935 (1.1414)  time: 1.0726  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 1.0935 (1.1401)  time: 1.0828  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [ 60/431]  eta: 0:06:58  lr: 0.000200  loss: 1.0885 (1.1306)  time: 1.0876  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [ 70/431]  eta: 0:06:46  lr: 0.000200  loss: 1.0773 (1.1256)  time: 1.0905  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [ 80/431]  eta: 0:06:33  lr: 0.000200  loss: 1.0858 (1.1236)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 1.0810 (1.1214)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [100/431]  eta: 0:06:09  lr: 0.000200  loss: 1.0955 (1.1226)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [110/431]  eta: 0:05:58  lr: 0.000200  loss: 1.1114 (1.1214)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 1.0744 (1.1177)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 1.0995 (1.1234)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 1.1991 (1.1278)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 1.1817 (1.1326)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 1.1664 (1.1376)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 1.1567 (1.1417)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.1136 (1.1408)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 1.0961 (1.1404)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 1.1405 (1.1407)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 1.1642 (1.1407)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.1125 (1.1397)  time: 1.0998  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:99]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.0964 (1.1391)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 1.0831 (1.1370)  time: 1.1046  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:99]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.0873 (1.1367)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1243 (1.1375)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.1329 (1.1397)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.1715 (1.1401)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.1173 (1.1386)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.0836 (1.1374)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1179 (1.1387)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1741 (1.1386)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1741 (1.1402)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1972 (1.1412)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1595 (1.1411)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1655 (1.1423)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1232 (1.1404)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.0798 (1.1391)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.0874 (1.1407)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1504 (1.1405)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:99]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1059 (1.1401)  time: 1.1087  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:99]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.0442 (1.1384)  time: 1.1058  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:99]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.0342 (1.1371)  time: 1.0950  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:99] Total time: 0:07:57 (1.1069 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.0342 (1.1371)\n",
      "Valid: [epoch:99]  [ 0/14]  eta: 0:00:35  loss: 1.1761 (1.1761)  time: 2.5439  data: 2.4029  max mem: 15925\n",
      "Valid: [epoch:99]  [13/14]  eta: 0:00:00  loss: 1.0595 (1.0745)  time: 0.2626  data: 0.1717  max mem: 15925\n",
      "Valid: [epoch:99] Total time: 0:00:03 (0.2803 s / it)\n",
      "Averaged stats: loss: 1.0595 (1.0745)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_99_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:100]  [  0/431]  eta: 0:31:51  lr: 0.000200  loss: 1.1769 (1.1769)  time: 4.4354  data: 3.1649  max mem: 15925\n",
      "Train: [epoch:100]  [ 10/431]  eta: 0:09:28  lr: 0.000200  loss: 1.1655 (1.1920)  time: 1.3501  data: 0.2879  max mem: 15925\n",
      "Train: [epoch:100]  [ 20/431]  eta: 0:08:16  lr: 0.000200  loss: 1.1603 (1.1760)  time: 1.0470  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [ 30/431]  eta: 0:07:47  lr: 0.000200  loss: 1.1587 (1.1815)  time: 1.0647  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 1.0537 (1.1487)  time: 1.0860  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 1.0649 (1.1542)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 1.1705 (1.1539)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [ 70/431]  eta: 0:06:46  lr: 0.000200  loss: 1.1463 (1.1534)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [ 80/431]  eta: 0:06:34  lr: 0.000200  loss: 1.1465 (1.1589)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 1.1017 (1.1513)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 1.1082 (1.1523)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 1.1599 (1.1538)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 1.1599 (1.1529)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [130/431]  eta: 0:05:35  lr: 0.000200  loss: 1.1609 (1.1548)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 1.1640 (1.1537)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 1.1759 (1.1553)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 1.1842 (1.1559)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 1.1433 (1.1575)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.1141 (1.1561)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [190/431]  eta: 0:04:27  lr: 0.000200  loss: 1.1146 (1.1574)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [200/431]  eta: 0:04:16  lr: 0.000200  loss: 1.1345 (1.1549)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 1.1047 (1.1534)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.0974 (1.1512)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [230/431]  eta: 0:03:42  lr: 0.000200  loss: 1.0854 (1.1501)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [240/431]  eta: 0:03:31  lr: 0.000200  loss: 1.0854 (1.1503)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [250/431]  eta: 0:03:20  lr: 0.000200  loss: 1.0954 (1.1488)  time: 1.0889  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.0823 (1.1464)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.0823 (1.1443)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.0529 (1.1425)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [290/431]  eta: 0:02:35  lr: 0.000200  loss: 1.0529 (1.1413)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [300/431]  eta: 0:02:24  lr: 0.000200  loss: 1.0967 (1.1403)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [310/431]  eta: 0:02:13  lr: 0.000200  loss: 1.0716 (1.1384)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [320/431]  eta: 0:02:02  lr: 0.000200  loss: 1.0660 (1.1368)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.0859 (1.1365)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1058 (1.1361)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1210 (1.1369)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1361 (1.1381)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1499 (1.1385)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1347 (1.1383)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1195 (1.1391)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1092 (1.1394)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:100]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0977 (1.1392)  time: 1.0933  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:100]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.0612 (1.1396)  time: 1.0981  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:100]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.0612 (1.1375)  time: 1.1016  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:100] Total time: 0:07:56 (1.1059 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.0612 (1.1375)\n",
      "Valid: [epoch:100]  [ 0/14]  eta: 0:00:37  loss: 1.0415 (1.0415)  time: 2.6479  data: 2.4663  max mem: 15925\n",
      "Valid: [epoch:100]  [13/14]  eta: 0:00:00  loss: 1.0745 (1.0838)  time: 0.2913  data: 0.1763  max mem: 15925\n",
      "Valid: [epoch:100] Total time: 0:00:04 (0.3097 s / it)\n",
      "Averaged stats: loss: 1.0745 (1.0838)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_100_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.084%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:101]  [  0/431]  eta: 0:30:27  lr: 0.000200  loss: 1.2642 (1.2642)  time: 4.2395  data: 2.9902  max mem: 15925\n",
      "Train: [epoch:101]  [ 10/431]  eta: 0:09:24  lr: 0.000200  loss: 1.1969 (1.2013)  time: 1.3414  data: 0.2720  max mem: 15925\n",
      "Train: [epoch:101]  [ 20/431]  eta: 0:08:17  lr: 0.000200  loss: 1.1786 (1.1821)  time: 1.0589  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [ 30/431]  eta: 0:07:49  lr: 0.000200  loss: 1.0989 (1.1412)  time: 1.0763  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [ 40/431]  eta: 0:07:29  lr: 0.000200  loss: 1.0714 (1.1540)  time: 1.0835  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 1.1122 (1.1533)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [ 60/431]  eta: 0:07:01  lr: 0.000200  loss: 1.1122 (1.1425)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [ 70/431]  eta: 0:06:48  lr: 0.000200  loss: 1.0966 (1.1383)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [ 80/431]  eta: 0:06:35  lr: 0.000200  loss: 1.1796 (1.1478)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 1.1119 (1.1405)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [100/431]  eta: 0:06:11  lr: 0.000200  loss: 1.0590 (1.1367)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 1.0886 (1.1367)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [120/431]  eta: 0:05:48  lr: 0.000200  loss: 1.1226 (1.1340)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [130/431]  eta: 0:05:37  lr: 0.000200  loss: 1.0800 (1.1307)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 1.1255 (1.1338)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [150/431]  eta: 0:05:14  lr: 0.000200  loss: 1.1255 (1.1324)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 1.1439 (1.1329)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [170/431]  eta: 0:04:51  lr: 0.000200  loss: 1.1381 (1.1322)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.1118 (1.1315)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 1.1407 (1.1325)  time: 1.0896  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 1.1401 (1.1323)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [210/431]  eta: 0:04:05  lr: 0.000200  loss: 1.1246 (1.1329)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.1246 (1.1323)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.1298 (1.1335)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 1.1019 (1.1340)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.0962 (1.1348)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [260/431]  eta: 0:03:09  lr: 0.000200  loss: 1.1128 (1.1378)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.1411 (1.1390)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.1161 (1.1379)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.1078 (1.1381)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.1158 (1.1381)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1063 (1.1373)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.0975 (1.1368)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [330/431]  eta: 0:01:51  lr: 0.000200  loss: 1.1198 (1.1369)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1195 (1.1385)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.0982 (1.1378)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.0938 (1.1368)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1017 (1.1361)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1054 (1.1356)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.0966 (1.1349)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.0820 (1.1339)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.1135 (1.1346)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:101]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1915 (1.1357)  time: 1.1014  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:101]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1535 (1.1361)  time: 1.1063  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:101] Total time: 0:07:57 (1.1088 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1535 (1.1361)\n",
      "Valid: [epoch:101]  [ 0/14]  eta: 0:00:35  loss: 1.1310 (1.1310)  time: 2.5307  data: 2.3827  max mem: 15925\n",
      "Valid: [epoch:101]  [13/14]  eta: 0:00:00  loss: 1.0641 (1.0758)  time: 0.2658  data: 0.1703  max mem: 15925\n",
      "Valid: [epoch:101] Total time: 0:00:03 (0.2843 s / it)\n",
      "Averaged stats: loss: 1.0641 (1.0758)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_101_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.076%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:102]  [  0/431]  eta: 0:35:51  lr: 0.000200  loss: 1.0879 (1.0879)  time: 4.9928  data: 3.8112  max mem: 15925\n",
      "Train: [epoch:102]  [ 10/431]  eta: 0:09:47  lr: 0.000200  loss: 1.1454 (1.1457)  time: 1.3962  data: 0.3467  max mem: 15925\n",
      "Train: [epoch:102]  [ 20/431]  eta: 0:08:24  lr: 0.000200  loss: 1.1547 (1.1677)  time: 1.0395  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [ 30/431]  eta: 0:07:53  lr: 0.000200  loss: 1.1422 (1.1454)  time: 1.0607  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [ 40/431]  eta: 0:07:31  lr: 0.000200  loss: 1.0605 (1.1272)  time: 1.0760  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [ 50/431]  eta: 0:07:14  lr: 0.000200  loss: 1.0657 (1.1257)  time: 1.0816  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 1.0911 (1.1217)  time: 1.0839  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [ 70/431]  eta: 0:06:46  lr: 0.000200  loss: 1.1039 (1.1249)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [ 80/431]  eta: 0:06:34  lr: 0.000200  loss: 1.1405 (1.1262)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [ 90/431]  eta: 0:06:22  lr: 0.000200  loss: 1.1197 (1.1227)  time: 1.1004  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:102]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 1.0837 (1.1233)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 1.0869 (1.1245)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 1.1211 (1.1269)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 1.0908 (1.1257)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [140/431]  eta: 0:05:24  lr: 0.000200  loss: 1.0908 (1.1253)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [150/431]  eta: 0:05:12  lr: 0.000200  loss: 1.0808 (1.1228)  time: 1.0911  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [160/431]  eta: 0:05:01  lr: 0.000200  loss: 1.0808 (1.1261)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 1.1166 (1.1260)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.0627 (1.1229)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 1.0801 (1.1281)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 1.1166 (1.1281)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 1.1113 (1.1266)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [220/431]  eta: 0:03:54  lr: 0.000200  loss: 1.1157 (1.1258)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.1157 (1.1270)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 1.1431 (1.1306)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.1431 (1.1297)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 1.0685 (1.1291)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [270/431]  eta: 0:02:58  lr: 0.000200  loss: 1.0896 (1.1304)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [280/431]  eta: 0:02:47  lr: 0.000200  loss: 1.0990 (1.1286)  time: 1.1100  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:102]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.1347 (1.1315)  time: 1.1044  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:102]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.1921 (1.1345)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1622 (1.1363)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1335 (1.1368)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1335 (1.1368)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [340/431]  eta: 0:01:40  lr: 0.000200  loss: 1.1232 (1.1361)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1251 (1.1364)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.0526 (1.1341)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.0685 (1.1340)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1234 (1.1351)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1697 (1.1376)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1370 (1.1359)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.0713 (1.1364)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:102]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.1709 (1.1380)  time: 1.0942  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:102]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1516 (1.1373)  time: 1.1002  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:102] Total time: 0:07:57 (1.1076 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1516 (1.1373)\n",
      "Valid: [epoch:102]  [ 0/14]  eta: 0:00:36  loss: 1.1721 (1.1721)  time: 2.6153  data: 2.4827  max mem: 15925\n",
      "Valid: [epoch:102]  [13/14]  eta: 0:00:00  loss: 1.0650 (1.0756)  time: 0.2889  data: 0.1774  max mem: 15925\n",
      "Valid: [epoch:102] Total time: 0:00:04 (0.3048 s / it)\n",
      "Averaged stats: loss: 1.0650 (1.0756)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_102_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.076%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:103]  [  0/431]  eta: 0:29:55  lr: 0.000200  loss: 1.4219 (1.4219)  time: 4.1653  data: 2.9472  max mem: 15925\n",
      "Train: [epoch:103]  [ 10/431]  eta: 0:09:17  lr: 0.000200  loss: 1.1890 (1.1908)  time: 1.3236  data: 0.2681  max mem: 15925\n",
      "Train: [epoch:103]  [ 20/431]  eta: 0:08:13  lr: 0.000200  loss: 1.1436 (1.1774)  time: 1.0520  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [ 30/431]  eta: 0:07:46  lr: 0.000200  loss: 1.1100 (1.1493)  time: 1.0745  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [ 40/431]  eta: 0:07:27  lr: 0.000200  loss: 1.0297 (1.1249)  time: 1.0845  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [ 50/431]  eta: 0:07:13  lr: 0.000200  loss: 1.0417 (1.1263)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [ 60/431]  eta: 0:06:59  lr: 0.000200  loss: 1.0618 (1.1215)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [ 70/431]  eta: 0:06:47  lr: 0.000200  loss: 1.1247 (1.1312)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [ 80/431]  eta: 0:06:34  lr: 0.000200  loss: 1.1507 (1.1356)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [ 90/431]  eta: 0:06:23  lr: 0.000200  loss: 1.1507 (1.1394)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [100/431]  eta: 0:06:10  lr: 0.000200  loss: 1.1321 (1.1328)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [110/431]  eta: 0:05:59  lr: 0.000200  loss: 1.1115 (1.1343)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [120/431]  eta: 0:05:47  lr: 0.000200  loss: 1.1354 (1.1381)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [130/431]  eta: 0:05:36  lr: 0.000200  loss: 1.1135 (1.1314)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [140/431]  eta: 0:05:25  lr: 0.000200  loss: 1.0558 (1.1279)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [150/431]  eta: 0:05:13  lr: 0.000200  loss: 1.0735 (1.1253)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [160/431]  eta: 0:05:02  lr: 0.000200  loss: 1.0824 (1.1259)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [170/431]  eta: 0:04:50  lr: 0.000200  loss: 1.0765 (1.1216)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [180/431]  eta: 0:04:39  lr: 0.000200  loss: 1.0891 (1.1253)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [190/431]  eta: 0:04:28  lr: 0.000200  loss: 1.1634 (1.1285)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [200/431]  eta: 0:04:17  lr: 0.000200  loss: 1.1152 (1.1260)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [210/431]  eta: 0:04:06  lr: 0.000200  loss: 1.0582 (1.1233)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [220/431]  eta: 0:03:55  lr: 0.000200  loss: 1.0666 (1.1223)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [230/431]  eta: 0:03:43  lr: 0.000200  loss: 1.1085 (1.1233)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [240/431]  eta: 0:03:32  lr: 0.000200  loss: 1.1085 (1.1234)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [250/431]  eta: 0:03:21  lr: 0.000200  loss: 1.0741 (1.1217)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [260/431]  eta: 0:03:10  lr: 0.000200  loss: 1.1018 (1.1236)  time: 1.1113  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:103]  [270/431]  eta: 0:02:59  lr: 0.000200  loss: 1.1976 (1.1270)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [280/431]  eta: 0:02:48  lr: 0.000200  loss: 1.1707 (1.1284)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [290/431]  eta: 0:02:36  lr: 0.000200  loss: 1.1290 (1.1284)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [300/431]  eta: 0:02:25  lr: 0.000200  loss: 1.1243 (1.1293)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [310/431]  eta: 0:02:14  lr: 0.000200  loss: 1.1343 (1.1310)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [320/431]  eta: 0:02:03  lr: 0.000200  loss: 1.1150 (1.1306)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [330/431]  eta: 0:01:52  lr: 0.000200  loss: 1.1094 (1.1323)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [340/431]  eta: 0:01:41  lr: 0.000200  loss: 1.1632 (1.1334)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [350/431]  eta: 0:01:29  lr: 0.000200  loss: 1.1274 (1.1334)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [360/431]  eta: 0:01:18  lr: 0.000200  loss: 1.1319 (1.1347)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [370/431]  eta: 0:01:07  lr: 0.000200  loss: 1.1216 (1.1333)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [380/431]  eta: 0:00:56  lr: 0.000200  loss: 1.1102 (1.1340)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [390/431]  eta: 0:00:45  lr: 0.000200  loss: 1.1102 (1.1344)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [400/431]  eta: 0:00:34  lr: 0.000200  loss: 1.1949 (1.1374)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:103]  [410/431]  eta: 0:00:23  lr: 0.000200  loss: 1.2013 (1.1375)  time: 1.0999  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:103]  [420/431]  eta: 0:00:12  lr: 0.000200  loss: 1.0925 (1.1369)  time: 1.1023  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:103]  [430/431]  eta: 0:00:01  lr: 0.000200  loss: 1.1087 (1.1371)  time: 1.1029  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:103] Total time: 0:07:58 (1.1102 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1087 (1.1371)\n",
      "Valid: [epoch:103]  [ 0/14]  eta: 0:00:42  loss: 1.1416 (1.1416)  time: 3.0043  data: 2.8315  max mem: 15925\n",
      "Valid: [epoch:103]  [13/14]  eta: 0:00:00  loss: 1.0711 (1.0818)  time: 0.3052  data: 0.2024  max mem: 15925\n",
      "Valid: [epoch:103] Total time: 0:00:04 (0.3215 s / it)\n",
      "Averaged stats: loss: 1.0711 (1.0818)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_103_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.082%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:104]  [  0/431]  eta: 0:36:32  lr: 0.000199  loss: 1.0301 (1.0301)  time: 5.0880  data: 3.9457  max mem: 15925\n",
      "Train: [epoch:104]  [ 10/431]  eta: 0:09:59  lr: 0.000199  loss: 1.1738 (1.1603)  time: 1.4239  data: 0.3589  max mem: 15925\n",
      "Train: [epoch:104]  [ 20/431]  eta: 0:08:34  lr: 0.000199  loss: 1.1738 (1.1900)  time: 1.0588  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [ 30/431]  eta: 0:07:56  lr: 0.000199  loss: 1.1571 (1.1656)  time: 1.0599  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [ 40/431]  eta: 0:07:34  lr: 0.000199  loss: 1.1067 (1.1463)  time: 1.0687  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [ 50/431]  eta: 0:07:16  lr: 0.000199  loss: 1.0725 (1.1334)  time: 1.0770  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [ 60/431]  eta: 0:07:01  lr: 0.000199  loss: 1.0725 (1.1259)  time: 1.0865  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [ 70/431]  eta: 0:06:48  lr: 0.000199  loss: 1.0993 (1.1286)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [ 80/431]  eta: 0:06:36  lr: 0.000199  loss: 1.1654 (1.1310)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [ 90/431]  eta: 0:06:24  lr: 0.000199  loss: 1.1654 (1.1396)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [100/431]  eta: 0:06:11  lr: 0.000199  loss: 1.1585 (1.1424)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [110/431]  eta: 0:05:59  lr: 0.000199  loss: 1.1585 (1.1438)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [120/431]  eta: 0:05:47  lr: 0.000199  loss: 1.0989 (1.1392)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [130/431]  eta: 0:05:36  lr: 0.000199  loss: 1.0663 (1.1351)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [140/431]  eta: 0:05:24  lr: 0.000199  loss: 1.0913 (1.1348)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [150/431]  eta: 0:05:13  lr: 0.000199  loss: 1.0845 (1.1318)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [160/431]  eta: 0:05:02  lr: 0.000199  loss: 1.0845 (1.1313)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [170/431]  eta: 0:04:50  lr: 0.000199  loss: 1.1075 (1.1316)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [180/431]  eta: 0:04:39  lr: 0.000199  loss: 1.1436 (1.1334)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [190/431]  eta: 0:04:28  lr: 0.000199  loss: 1.1100 (1.1317)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [200/431]  eta: 0:04:17  lr: 0.000199  loss: 1.1100 (1.1336)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [210/431]  eta: 0:04:06  lr: 0.000199  loss: 1.0861 (1.1302)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [220/431]  eta: 0:03:54  lr: 0.000199  loss: 1.0861 (1.1323)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [230/431]  eta: 0:03:43  lr: 0.000199  loss: 1.1961 (1.1355)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [240/431]  eta: 0:03:32  lr: 0.000199  loss: 1.1961 (1.1375)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [250/431]  eta: 0:03:21  lr: 0.000199  loss: 1.0756 (1.1368)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [260/431]  eta: 0:03:09  lr: 0.000199  loss: 1.0899 (1.1365)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [270/431]  eta: 0:02:58  lr: 0.000199  loss: 1.1275 (1.1367)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [280/431]  eta: 0:02:47  lr: 0.000199  loss: 1.0975 (1.1362)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [290/431]  eta: 0:02:36  lr: 0.000199  loss: 1.1083 (1.1357)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [300/431]  eta: 0:02:25  lr: 0.000199  loss: 1.1003 (1.1343)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [310/431]  eta: 0:02:14  lr: 0.000199  loss: 1.1107 (1.1350)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [320/431]  eta: 0:02:03  lr: 0.000199  loss: 1.1206 (1.1357)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [330/431]  eta: 0:01:52  lr: 0.000199  loss: 1.1877 (1.1408)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [340/431]  eta: 0:01:40  lr: 0.000199  loss: 1.1735 (1.1399)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [350/431]  eta: 0:01:29  lr: 0.000199  loss: 1.1151 (1.1384)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [360/431]  eta: 0:01:18  lr: 0.000199  loss: 1.1091 (1.1384)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [370/431]  eta: 0:01:07  lr: 0.000199  loss: 1.1057 (1.1377)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [380/431]  eta: 0:00:56  lr: 0.000199  loss: 1.0653 (1.1372)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [390/431]  eta: 0:00:45  lr: 0.000199  loss: 1.1023 (1.1373)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [400/431]  eta: 0:00:34  lr: 0.000199  loss: 1.1491 (1.1376)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:104]  [410/431]  eta: 0:00:23  lr: 0.000199  loss: 1.1217 (1.1374)  time: 1.1105  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:104]  [420/431]  eta: 0:00:12  lr: 0.000199  loss: 1.1216 (1.1373)  time: 1.1069  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:104]  [430/431]  eta: 0:00:01  lr: 0.000199  loss: 1.0982 (1.1369)  time: 1.0939  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:104] Total time: 0:07:57 (1.1082 s / it)\n",
      "Averaged stats: lr: 0.000199  loss: 1.0982 (1.1369)\n",
      "Valid: [epoch:104]  [ 0/14]  eta: 0:00:37  loss: 1.1360 (1.1360)  time: 2.6792  data: 2.5567  max mem: 15925\n",
      "Valid: [epoch:104]  [13/14]  eta: 0:00:00  loss: 1.0711 (1.0821)  time: 0.2796  data: 0.1827  max mem: 15925\n",
      "Valid: [epoch:104] Total time: 0:00:04 (0.2965 s / it)\n",
      "Averaged stats: loss: 1.0711 (1.0821)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_104_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.082%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:105]  [  0/431]  eta: 0:30:50  lr: 0.000199  loss: 1.0974 (1.0974)  time: 4.2927  data: 3.0414  max mem: 15925\n",
      "Train: [epoch:105]  [ 10/431]  eta: 0:09:23  lr: 0.000199  loss: 1.1800 (1.1992)  time: 1.3391  data: 0.2767  max mem: 15925\n",
      "Train: [epoch:105]  [ 20/431]  eta: 0:08:16  lr: 0.000199  loss: 1.1341 (1.1365)  time: 1.0542  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [ 30/431]  eta: 0:07:48  lr: 0.000199  loss: 1.0801 (1.1160)  time: 1.0727  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [ 40/431]  eta: 0:07:29  lr: 0.000199  loss: 1.0801 (1.1147)  time: 1.0897  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [ 50/431]  eta: 0:07:13  lr: 0.000199  loss: 1.0442 (1.1109)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [ 60/431]  eta: 0:07:00  lr: 0.000199  loss: 1.0442 (1.1092)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [ 70/431]  eta: 0:06:47  lr: 0.000199  loss: 1.1192 (1.1139)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [ 80/431]  eta: 0:06:35  lr: 0.000199  loss: 1.1468 (1.1251)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [ 90/431]  eta: 0:06:23  lr: 0.000199  loss: 1.1395 (1.1209)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [100/431]  eta: 0:06:11  lr: 0.000199  loss: 1.1047 (1.1242)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [110/431]  eta: 0:06:00  lr: 0.000199  loss: 1.1460 (1.1265)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [120/431]  eta: 0:05:48  lr: 0.000199  loss: 1.1460 (1.1282)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [130/431]  eta: 0:05:37  lr: 0.000199  loss: 1.1369 (1.1298)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [140/431]  eta: 0:05:25  lr: 0.000199  loss: 1.1055 (1.1257)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [150/431]  eta: 0:05:14  lr: 0.000199  loss: 1.0854 (1.1255)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [160/431]  eta: 0:05:02  lr: 0.000199  loss: 1.1106 (1.1313)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [170/431]  eta: 0:04:51  lr: 0.000199  loss: 1.1651 (1.1330)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [180/431]  eta: 0:04:40  lr: 0.000199  loss: 1.1638 (1.1331)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [190/431]  eta: 0:04:29  lr: 0.000199  loss: 1.1686 (1.1363)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [200/431]  eta: 0:04:18  lr: 0.000199  loss: 1.1417 (1.1354)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [210/431]  eta: 0:04:06  lr: 0.000199  loss: 1.0545 (1.1338)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [220/431]  eta: 0:03:55  lr: 0.000199  loss: 1.0506 (1.1325)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [230/431]  eta: 0:03:44  lr: 0.000199  loss: 1.0807 (1.1329)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [240/431]  eta: 0:03:32  lr: 0.000199  loss: 1.1503 (1.1361)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [250/431]  eta: 0:03:21  lr: 0.000199  loss: 1.1509 (1.1371)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [260/431]  eta: 0:03:10  lr: 0.000199  loss: 1.1151 (1.1354)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [270/431]  eta: 0:02:59  lr: 0.000199  loss: 1.1542 (1.1384)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [280/431]  eta: 0:02:48  lr: 0.000199  loss: 1.1200 (1.1373)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [290/431]  eta: 0:02:37  lr: 0.000199  loss: 1.0707 (1.1359)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [300/431]  eta: 0:02:26  lr: 0.000199  loss: 1.0830 (1.1355)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [310/431]  eta: 0:02:14  lr: 0.000199  loss: 1.1249 (1.1367)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [320/431]  eta: 0:02:03  lr: 0.000199  loss: 1.0915 (1.1351)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [330/431]  eta: 0:01:52  lr: 0.000199  loss: 1.0915 (1.1347)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [340/431]  eta: 0:01:41  lr: 0.000199  loss: 1.1076 (1.1354)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [350/431]  eta: 0:01:30  lr: 0.000199  loss: 1.1201 (1.1353)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [360/431]  eta: 0:01:19  lr: 0.000199  loss: 1.1625 (1.1363)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [370/431]  eta: 0:01:07  lr: 0.000199  loss: 1.1298 (1.1360)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [380/431]  eta: 0:00:56  lr: 0.000199  loss: 1.1017 (1.1352)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [390/431]  eta: 0:00:45  lr: 0.000199  loss: 1.1017 (1.1349)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [400/431]  eta: 0:00:34  lr: 0.000199  loss: 1.0839 (1.1345)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [410/431]  eta: 0:00:23  lr: 0.000199  loss: 1.1065 (1.1343)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:105]  [420/431]  eta: 0:00:12  lr: 0.000199  loss: 1.1348 (1.1348)  time: 1.0972  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:105]  [430/431]  eta: 0:00:01  lr: 0.000199  loss: 1.1111 (1.1332)  time: 1.1073  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:105] Total time: 0:07:59 (1.1128 s / it)\n",
      "Averaged stats: lr: 0.000199  loss: 1.1111 (1.1332)\n",
      "Valid: [epoch:105]  [ 0/14]  eta: 0:00:36  loss: 1.1159 (1.1159)  time: 2.5731  data: 2.3897  max mem: 15925\n",
      "Valid: [epoch:105]  [13/14]  eta: 0:00:00  loss: 1.0641 (1.0745)  time: 0.2843  data: 0.1708  max mem: 15925\n",
      "Valid: [epoch:105] Total time: 0:00:04 (0.3012 s / it)\n",
      "Averaged stats: loss: 1.0641 (1.0745)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_105_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:106]  [  0/431]  eta: 0:36:11  lr: 0.000199  loss: 1.0866 (1.0866)  time: 5.0383  data: 3.8586  max mem: 15925\n",
      "Train: [epoch:106]  [ 10/431]  eta: 0:09:51  lr: 0.000199  loss: 1.1759 (1.1852)  time: 1.4045  data: 0.3510  max mem: 15925\n",
      "Train: [epoch:106]  [ 20/431]  eta: 0:08:28  lr: 0.000199  loss: 1.1759 (1.2065)  time: 1.0462  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [ 30/431]  eta: 0:07:56  lr: 0.000199  loss: 1.1627 (1.1865)  time: 1.0699  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [ 40/431]  eta: 0:07:34  lr: 0.000199  loss: 1.0755 (1.1654)  time: 1.0874  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [ 50/431]  eta: 0:07:18  lr: 0.000199  loss: 1.1138 (1.1681)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [ 60/431]  eta: 0:07:04  lr: 0.000199  loss: 1.1156 (1.1563)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [ 70/431]  eta: 0:06:50  lr: 0.000199  loss: 1.0980 (1.1567)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [ 80/431]  eta: 0:06:38  lr: 0.000199  loss: 1.1259 (1.1597)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [ 90/431]  eta: 0:06:25  lr: 0.000199  loss: 1.1198 (1.1557)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [100/431]  eta: 0:06:13  lr: 0.000199  loss: 1.0773 (1.1503)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [110/431]  eta: 0:06:01  lr: 0.000199  loss: 1.1148 (1.1526)  time: 1.1098  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:106]  [120/431]  eta: 0:05:49  lr: 0.000199  loss: 1.1186 (1.1508)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [130/431]  eta: 0:05:38  lr: 0.000199  loss: 1.1537 (1.1552)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [140/431]  eta: 0:05:26  lr: 0.000199  loss: 1.1568 (1.1559)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [150/431]  eta: 0:05:15  lr: 0.000199  loss: 1.1464 (1.1520)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [160/431]  eta: 0:05:03  lr: 0.000199  loss: 1.0981 (1.1530)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [170/431]  eta: 0:04:52  lr: 0.000199  loss: 1.1027 (1.1488)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [180/431]  eta: 0:04:40  lr: 0.000199  loss: 1.0544 (1.1469)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [190/431]  eta: 0:04:29  lr: 0.000199  loss: 1.0655 (1.1456)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [200/431]  eta: 0:04:18  lr: 0.000199  loss: 1.0849 (1.1423)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [210/431]  eta: 0:04:06  lr: 0.000199  loss: 1.1283 (1.1429)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [220/431]  eta: 0:03:55  lr: 0.000199  loss: 1.1384 (1.1431)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [230/431]  eta: 0:03:44  lr: 0.000199  loss: 1.1123 (1.1431)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [240/431]  eta: 0:03:32  lr: 0.000199  loss: 1.0800 (1.1407)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [250/431]  eta: 0:03:21  lr: 0.000199  loss: 1.0800 (1.1398)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [260/431]  eta: 0:03:10  lr: 0.000199  loss: 1.1009 (1.1380)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [270/431]  eta: 0:02:59  lr: 0.000199  loss: 1.1009 (1.1366)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [280/431]  eta: 0:02:48  lr: 0.000199  loss: 1.0982 (1.1361)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [290/431]  eta: 0:02:37  lr: 0.000199  loss: 1.1182 (1.1362)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [300/431]  eta: 0:02:25  lr: 0.000199  loss: 1.1816 (1.1380)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [310/431]  eta: 0:02:14  lr: 0.000199  loss: 1.1005 (1.1355)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [320/431]  eta: 0:02:03  lr: 0.000199  loss: 1.0716 (1.1345)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [330/431]  eta: 0:01:52  lr: 0.000199  loss: 1.1509 (1.1374)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [340/431]  eta: 0:01:41  lr: 0.000199  loss: 1.1359 (1.1362)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [350/431]  eta: 0:01:30  lr: 0.000199  loss: 1.1010 (1.1366)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [360/431]  eta: 0:01:18  lr: 0.000199  loss: 1.1379 (1.1371)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [370/431]  eta: 0:01:07  lr: 0.000199  loss: 1.1279 (1.1377)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [380/431]  eta: 0:00:56  lr: 0.000199  loss: 1.1279 (1.1387)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [390/431]  eta: 0:00:45  lr: 0.000199  loss: 1.1261 (1.1383)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [400/431]  eta: 0:00:34  lr: 0.000199  loss: 1.1261 (1.1384)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:106]  [410/431]  eta: 0:00:23  lr: 0.000199  loss: 1.0977 (1.1379)  time: 1.0989  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:106]  [420/431]  eta: 0:00:12  lr: 0.000199  loss: 1.0782 (1.1373)  time: 1.1017  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:106]  [430/431]  eta: 0:00:01  lr: 0.000199  loss: 1.1196 (1.1376)  time: 1.1032  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:106] Total time: 0:07:58 (1.1113 s / it)\n",
      "Averaged stats: lr: 0.000199  loss: 1.1196 (1.1376)\n",
      "Valid: [epoch:106]  [ 0/14]  eta: 0:00:36  loss: 0.9524 (0.9524)  time: 2.6394  data: 2.4960  max mem: 15925\n",
      "Valid: [epoch:106]  [13/14]  eta: 0:00:00  loss: 1.0624 (1.0746)  time: 0.2773  data: 0.1784  max mem: 15925\n",
      "Valid: [epoch:106] Total time: 0:00:04 (0.2934 s / it)\n",
      "Averaged stats: loss: 1.0624 (1.0746)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_106_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.075%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:107]  [  0/431]  eta: 0:35:57  lr: 0.000199  loss: 1.5214 (1.5214)  time: 5.0063  data: 3.9098  max mem: 15925\n",
      "Train: [epoch:107]  [ 10/431]  eta: 0:09:41  lr: 0.000199  loss: 1.1602 (1.1807)  time: 1.3813  data: 0.3556  max mem: 15925\n",
      "Train: [epoch:107]  [ 20/431]  eta: 0:08:25  lr: 0.000199  loss: 1.1850 (1.2029)  time: 1.0409  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [ 30/431]  eta: 0:07:54  lr: 0.000199  loss: 1.1850 (1.1853)  time: 1.0758  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [ 40/431]  eta: 0:07:34  lr: 0.000199  loss: 1.1159 (1.1748)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [ 50/431]  eta: 0:07:18  lr: 0.000199  loss: 1.0616 (1.1469)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [ 60/431]  eta: 0:07:03  lr: 0.000199  loss: 1.0346 (1.1363)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [ 70/431]  eta: 0:06:50  lr: 0.000199  loss: 1.0760 (1.1346)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [ 80/431]  eta: 0:06:37  lr: 0.000199  loss: 1.0819 (1.1274)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [ 90/431]  eta: 0:06:25  lr: 0.000199  loss: 1.0819 (1.1225)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [100/431]  eta: 0:06:13  lr: 0.000199  loss: 1.0947 (1.1238)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [110/431]  eta: 0:06:01  lr: 0.000199  loss: 1.1007 (1.1215)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [120/431]  eta: 0:05:49  lr: 0.000199  loss: 1.1490 (1.1251)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [130/431]  eta: 0:05:37  lr: 0.000199  loss: 1.0853 (1.1252)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [140/431]  eta: 0:05:26  lr: 0.000199  loss: 1.0562 (1.1237)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [150/431]  eta: 0:05:14  lr: 0.000199  loss: 1.1369 (1.1223)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [160/431]  eta: 0:05:02  lr: 0.000199  loss: 1.0472 (1.1207)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [170/431]  eta: 0:04:51  lr: 0.000199  loss: 1.0939 (1.1224)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [180/431]  eta: 0:04:39  lr: 0.000199  loss: 1.1181 (1.1246)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [190/431]  eta: 0:04:28  lr: 0.000199  loss: 1.1658 (1.1291)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [200/431]  eta: 0:04:17  lr: 0.000199  loss: 1.1427 (1.1300)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [210/431]  eta: 0:04:06  lr: 0.000199  loss: 1.1183 (1.1306)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [220/431]  eta: 0:03:55  lr: 0.000199  loss: 1.1183 (1.1305)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [230/431]  eta: 0:03:43  lr: 0.000199  loss: 1.1146 (1.1304)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [240/431]  eta: 0:03:32  lr: 0.000199  loss: 1.1118 (1.1322)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [250/431]  eta: 0:03:21  lr: 0.000199  loss: 1.0996 (1.1296)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [260/431]  eta: 0:03:10  lr: 0.000199  loss: 1.0843 (1.1317)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [270/431]  eta: 0:02:59  lr: 0.000199  loss: 1.1593 (1.1349)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [280/431]  eta: 0:02:48  lr: 0.000199  loss: 1.2019 (1.1377)  time: 1.1117  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:107]  [290/431]  eta: 0:02:36  lr: 0.000199  loss: 1.1302 (1.1369)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [300/431]  eta: 0:02:25  lr: 0.000199  loss: 1.1237 (1.1381)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [310/431]  eta: 0:02:14  lr: 0.000199  loss: 1.1366 (1.1384)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [320/431]  eta: 0:02:03  lr: 0.000199  loss: 1.1206 (1.1377)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [330/431]  eta: 0:01:52  lr: 0.000199  loss: 1.1042 (1.1380)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [340/431]  eta: 0:01:41  lr: 0.000199  loss: 1.0903 (1.1370)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [350/431]  eta: 0:01:30  lr: 0.000199  loss: 1.1015 (1.1366)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [360/431]  eta: 0:01:18  lr: 0.000199  loss: 1.1177 (1.1355)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [370/431]  eta: 0:01:07  lr: 0.000199  loss: 1.1247 (1.1359)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [380/431]  eta: 0:00:56  lr: 0.000199  loss: 1.1247 (1.1359)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [390/431]  eta: 0:00:45  lr: 0.000199  loss: 1.1351 (1.1355)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [400/431]  eta: 0:00:34  lr: 0.000199  loss: 1.1172 (1.1349)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [410/431]  eta: 0:00:23  lr: 0.000199  loss: 1.1238 (1.1352)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:107]  [420/431]  eta: 0:00:12  lr: 0.000199  loss: 1.0898 (1.1351)  time: 1.1014  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:107]  [430/431]  eta: 0:00:01  lr: 0.000199  loss: 1.0583 (1.1354)  time: 1.1035  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:107] Total time: 0:07:58 (1.1111 s / it)\n",
      "Averaged stats: lr: 0.000199  loss: 1.0583 (1.1354)\n",
      "Valid: [epoch:107]  [ 0/14]  eta: 0:00:35  loss: 1.1730 (1.1730)  time: 2.5660  data: 2.4218  max mem: 15925\n",
      "Valid: [epoch:107]  [13/14]  eta: 0:00:00  loss: 1.0675 (1.0793)  time: 0.2684  data: 0.1731  max mem: 15925\n",
      "Valid: [epoch:107] Total time: 0:00:04 (0.2859 s / it)\n",
      "Averaged stats: loss: 1.0675 (1.0793)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_107_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.079%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 99.000\n",
      "Train: [epoch:108]  [  0/431]  eta: 0:32:22  lr: 0.000198  loss: 1.2313 (1.2313)  time: 4.5073  data: 3.3633  max mem: 15925\n",
      "Train: [epoch:108]  [ 10/431]  eta: 0:09:23  lr: 0.000198  loss: 1.1413 (1.1476)  time: 1.3393  data: 0.3059  max mem: 15925\n",
      "Train: [epoch:108]  [ 20/431]  eta: 0:08:15  lr: 0.000198  loss: 1.1355 (1.1598)  time: 1.0394  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [ 30/431]  eta: 0:07:44  lr: 0.000198  loss: 1.0877 (1.1278)  time: 1.0580  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [ 40/431]  eta: 0:07:25  lr: 0.000198  loss: 1.0341 (1.1173)  time: 1.0715  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [ 50/431]  eta: 0:07:10  lr: 0.000198  loss: 1.0530 (1.1203)  time: 1.0836  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [ 60/431]  eta: 0:06:57  lr: 0.000198  loss: 1.1112 (1.1261)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [ 70/431]  eta: 0:06:45  lr: 0.000198  loss: 1.1015 (1.1234)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [ 80/431]  eta: 0:06:32  lr: 0.000198  loss: 1.0776 (1.1273)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [ 90/431]  eta: 0:06:21  lr: 0.000198  loss: 1.0745 (1.1205)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [100/431]  eta: 0:06:09  lr: 0.000198  loss: 1.0580 (1.1152)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [110/431]  eta: 0:05:58  lr: 0.000198  loss: 1.1112 (1.1236)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [120/431]  eta: 0:05:46  lr: 0.000198  loss: 1.1666 (1.1266)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [130/431]  eta: 0:05:35  lr: 0.000198  loss: 1.1335 (1.1261)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [140/431]  eta: 0:05:23  lr: 0.000198  loss: 1.1250 (1.1264)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [150/431]  eta: 0:05:12  lr: 0.000198  loss: 1.0831 (1.1242)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [160/431]  eta: 0:05:01  lr: 0.000198  loss: 1.0836 (1.1248)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [170/431]  eta: 0:04:50  lr: 0.000198  loss: 1.0683 (1.1214)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [180/431]  eta: 0:04:39  lr: 0.000198  loss: 1.0771 (1.1220)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [190/431]  eta: 0:04:28  lr: 0.000198  loss: 1.1140 (1.1227)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [200/431]  eta: 0:04:16  lr: 0.000198  loss: 1.0964 (1.1203)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [210/431]  eta: 0:04:05  lr: 0.000198  loss: 1.0868 (1.1201)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [220/431]  eta: 0:03:54  lr: 0.000198  loss: 1.0868 (1.1204)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [230/431]  eta: 0:03:43  lr: 0.000198  loss: 1.0852 (1.1220)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [240/431]  eta: 0:03:32  lr: 0.000198  loss: 1.0765 (1.1215)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [250/431]  eta: 0:03:20  lr: 0.000198  loss: 1.0965 (1.1242)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [260/431]  eta: 0:03:09  lr: 0.000198  loss: 1.1187 (1.1266)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [270/431]  eta: 0:02:58  lr: 0.000198  loss: 1.1123 (1.1265)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [280/431]  eta: 0:02:47  lr: 0.000198  loss: 1.1096 (1.1280)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [290/431]  eta: 0:02:36  lr: 0.000198  loss: 1.1013 (1.1294)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [300/431]  eta: 0:02:25  lr: 0.000198  loss: 1.1183 (1.1312)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [310/431]  eta: 0:02:14  lr: 0.000198  loss: 1.0963 (1.1303)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [320/431]  eta: 0:02:03  lr: 0.000198  loss: 1.0974 (1.1311)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [330/431]  eta: 0:01:51  lr: 0.000198  loss: 1.1175 (1.1323)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [340/431]  eta: 0:01:40  lr: 0.000198  loss: 1.1511 (1.1335)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [350/431]  eta: 0:01:29  lr: 0.000198  loss: 1.1423 (1.1330)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [360/431]  eta: 0:01:18  lr: 0.000198  loss: 1.1495 (1.1345)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [370/431]  eta: 0:01:07  lr: 0.000198  loss: 1.1625 (1.1353)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [380/431]  eta: 0:00:56  lr: 0.000198  loss: 1.1388 (1.1369)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [390/431]  eta: 0:00:45  lr: 0.000198  loss: 1.1201 (1.1361)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [400/431]  eta: 0:00:34  lr: 0.000198  loss: 1.1201 (1.1365)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [410/431]  eta: 0:00:23  lr: 0.000198  loss: 1.1754 (1.1375)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:108]  [420/431]  eta: 0:00:12  lr: 0.000198  loss: 1.0738 (1.1357)  time: 1.1051  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:108]  [430/431]  eta: 0:00:01  lr: 0.000198  loss: 1.0529 (1.1354)  time: 1.1093  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:108] Total time: 0:07:57 (1.1087 s / it)\n",
      "Averaged stats: lr: 0.000198  loss: 1.0529 (1.1354)\n",
      "Valid: [epoch:108]  [ 0/14]  eta: 0:00:35  loss: 1.1148 (1.1148)  time: 2.5342  data: 2.3713  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:108]  [13/14]  eta: 0:00:00  loss: 1.0635 (1.0743)  time: 0.2901  data: 0.1695  max mem: 15925\n",
      "Valid: [epoch:108] Total time: 0:00:04 (0.3072 s / it)\n",
      "Averaged stats: loss: 1.0635 (1.0743)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_108_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 108.000\n",
      "Train: [epoch:109]  [  0/431]  eta: 0:34:14  lr: 0.000198  loss: 1.1523 (1.1523)  time: 4.7673  data: 3.5713  max mem: 15925\n",
      "Train: [epoch:109]  [ 10/431]  eta: 0:09:50  lr: 0.000198  loss: 1.1706 (1.1954)  time: 1.4029  data: 0.3249  max mem: 15925\n",
      "Train: [epoch:109]  [ 20/431]  eta: 0:08:27  lr: 0.000198  loss: 1.1706 (1.1840)  time: 1.0594  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [ 30/431]  eta: 0:07:56  lr: 0.000198  loss: 1.1573 (1.1781)  time: 1.0710  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [ 40/431]  eta: 0:07:36  lr: 0.000198  loss: 1.0838 (1.1529)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [ 50/431]  eta: 0:07:18  lr: 0.000198  loss: 1.0549 (1.1401)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [ 60/431]  eta: 0:07:04  lr: 0.000198  loss: 1.0529 (1.1314)  time: 1.1006  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:109]  [ 70/431]  eta: 0:06:51  lr: 0.000198  loss: 1.0669 (1.1326)  time: 1.1113  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:109]  [ 80/431]  eta: 0:06:38  lr: 0.000198  loss: 1.1552 (1.1374)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [ 90/431]  eta: 0:06:25  lr: 0.000198  loss: 1.1428 (1.1332)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [100/431]  eta: 0:06:14  lr: 0.000198  loss: 1.0781 (1.1327)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [110/431]  eta: 0:06:01  lr: 0.000198  loss: 1.1251 (1.1304)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [120/431]  eta: 0:05:50  lr: 0.000198  loss: 1.0834 (1.1298)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [130/431]  eta: 0:05:38  lr: 0.000198  loss: 1.0883 (1.1292)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [140/431]  eta: 0:05:27  lr: 0.000198  loss: 1.0934 (1.1295)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [150/431]  eta: 0:05:15  lr: 0.000198  loss: 1.0682 (1.1262)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [160/431]  eta: 0:05:03  lr: 0.000198  loss: 1.1109 (1.1298)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [170/431]  eta: 0:04:52  lr: 0.000198  loss: 1.1241 (1.1268)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [180/431]  eta: 0:04:41  lr: 0.000198  loss: 1.0962 (1.1281)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [190/431]  eta: 0:04:29  lr: 0.000198  loss: 1.1748 (1.1297)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [200/431]  eta: 0:04:18  lr: 0.000198  loss: 1.1748 (1.1327)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [210/431]  eta: 0:04:07  lr: 0.000198  loss: 1.1570 (1.1334)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [220/431]  eta: 0:03:55  lr: 0.000198  loss: 1.1190 (1.1332)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [230/431]  eta: 0:03:44  lr: 0.000198  loss: 1.0965 (1.1321)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [240/431]  eta: 0:03:33  lr: 0.000198  loss: 1.0941 (1.1311)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [250/431]  eta: 0:03:21  lr: 0.000198  loss: 1.1194 (1.1330)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [260/431]  eta: 0:03:10  lr: 0.000198  loss: 1.1311 (1.1344)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [270/431]  eta: 0:02:59  lr: 0.000198  loss: 1.1056 (1.1335)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [280/431]  eta: 0:02:48  lr: 0.000198  loss: 1.0595 (1.1318)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [290/431]  eta: 0:02:37  lr: 0.000198  loss: 1.0813 (1.1316)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [300/431]  eta: 0:02:26  lr: 0.000198  loss: 1.1335 (1.1323)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [310/431]  eta: 0:02:14  lr: 0.000198  loss: 1.1151 (1.1323)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [320/431]  eta: 0:02:03  lr: 0.000198  loss: 1.1151 (1.1334)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [330/431]  eta: 0:01:52  lr: 0.000198  loss: 1.1401 (1.1334)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [340/431]  eta: 0:01:41  lr: 0.000198  loss: 1.1344 (1.1345)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [350/431]  eta: 0:01:30  lr: 0.000198  loss: 1.0937 (1.1335)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [360/431]  eta: 0:01:19  lr: 0.000198  loss: 1.0724 (1.1330)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [370/431]  eta: 0:01:07  lr: 0.000198  loss: 1.0724 (1.1325)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [380/431]  eta: 0:00:56  lr: 0.000198  loss: 1.1336 (1.1325)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [390/431]  eta: 0:00:45  lr: 0.000198  loss: 1.1453 (1.1327)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [400/431]  eta: 0:00:34  lr: 0.000198  loss: 1.0824 (1.1313)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [410/431]  eta: 0:00:23  lr: 0.000198  loss: 1.0920 (1.1313)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:109]  [420/431]  eta: 0:00:12  lr: 0.000198  loss: 1.1695 (1.1332)  time: 1.1053  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:109]  [430/431]  eta: 0:00:01  lr: 0.000198  loss: 1.1578 (1.1327)  time: 1.1005  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:109] Total time: 0:08:00 (1.1139 s / it)\n",
      "Averaged stats: lr: 0.000198  loss: 1.1578 (1.1327)\n",
      "Valid: [epoch:109]  [ 0/14]  eta: 0:00:36  loss: 1.1407 (1.1407)  time: 2.5805  data: 2.4366  max mem: 15925\n",
      "Valid: [epoch:109]  [13/14]  eta: 0:00:00  loss: 1.0853 (1.0912)  time: 0.2851  data: 0.1741  max mem: 15925\n",
      "Valid: [epoch:109] Total time: 0:00:04 (0.3022 s / it)\n",
      "Averaged stats: loss: 1.0853 (1.0912)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_109_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.091%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 108.000\n",
      "Train: [epoch:110]  [  0/431]  eta: 0:32:06  lr: 0.000198  loss: 1.1011 (1.1011)  time: 4.4688  data: 3.1894  max mem: 15925\n",
      "Train: [epoch:110]  [ 10/431]  eta: 0:09:31  lr: 0.000198  loss: 1.1305 (1.1634)  time: 1.3584  data: 0.2901  max mem: 15925\n",
      "Train: [epoch:110]  [ 20/431]  eta: 0:08:16  lr: 0.000198  loss: 1.1274 (1.1434)  time: 1.0449  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [ 30/431]  eta: 0:07:46  lr: 0.000198  loss: 1.1122 (1.1376)  time: 1.0569  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [ 40/431]  eta: 0:07:26  lr: 0.000198  loss: 1.1281 (1.1359)  time: 1.0731  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [ 50/431]  eta: 0:07:11  lr: 0.000198  loss: 1.1409 (1.1423)  time: 1.0835  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [ 60/431]  eta: 0:06:58  lr: 0.000198  loss: 1.0695 (1.1202)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [ 70/431]  eta: 0:06:47  lr: 0.000198  loss: 1.0525 (1.1219)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [ 80/431]  eta: 0:06:35  lr: 0.000198  loss: 1.1164 (1.1295)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [ 90/431]  eta: 0:06:22  lr: 0.000198  loss: 1.1123 (1.1276)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [100/431]  eta: 0:06:10  lr: 0.000198  loss: 1.0701 (1.1248)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [110/431]  eta: 0:05:59  lr: 0.000198  loss: 1.0723 (1.1232)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [120/431]  eta: 0:05:48  lr: 0.000198  loss: 1.0870 (1.1219)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [130/431]  eta: 0:05:37  lr: 0.000198  loss: 1.1208 (1.1256)  time: 1.1191  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:110]  [140/431]  eta: 0:05:25  lr: 0.000198  loss: 1.1576 (1.1308)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [150/431]  eta: 0:05:14  lr: 0.000198  loss: 1.1562 (1.1297)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [160/431]  eta: 0:05:02  lr: 0.000198  loss: 1.1152 (1.1322)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [170/431]  eta: 0:04:51  lr: 0.000198  loss: 1.1150 (1.1301)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [180/431]  eta: 0:04:40  lr: 0.000198  loss: 1.1150 (1.1306)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [190/431]  eta: 0:04:28  lr: 0.000198  loss: 1.1014 (1.1284)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [200/431]  eta: 0:04:17  lr: 0.000198  loss: 1.0739 (1.1283)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [210/431]  eta: 0:04:06  lr: 0.000198  loss: 1.0916 (1.1296)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [220/431]  eta: 0:03:55  lr: 0.000198  loss: 1.0474 (1.1271)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [230/431]  eta: 0:03:43  lr: 0.000198  loss: 1.1197 (1.1283)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [240/431]  eta: 0:03:32  lr: 0.000198  loss: 1.1569 (1.1292)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [250/431]  eta: 0:03:21  lr: 0.000198  loss: 1.1107 (1.1303)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [260/431]  eta: 0:03:10  lr: 0.000198  loss: 1.1118 (1.1298)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [270/431]  eta: 0:02:59  lr: 0.000198  loss: 1.1136 (1.1312)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [280/431]  eta: 0:02:48  lr: 0.000198  loss: 1.1188 (1.1314)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [290/431]  eta: 0:02:36  lr: 0.000198  loss: 1.1248 (1.1329)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [300/431]  eta: 0:02:25  lr: 0.000198  loss: 1.1645 (1.1343)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [310/431]  eta: 0:02:14  lr: 0.000198  loss: 1.1350 (1.1352)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [320/431]  eta: 0:02:03  lr: 0.000198  loss: 1.0572 (1.1336)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [330/431]  eta: 0:01:52  lr: 0.000198  loss: 1.0853 (1.1345)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [340/431]  eta: 0:01:41  lr: 0.000198  loss: 1.1634 (1.1356)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [350/431]  eta: 0:01:30  lr: 0.000198  loss: 1.1634 (1.1360)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [360/431]  eta: 0:01:18  lr: 0.000198  loss: 1.1094 (1.1354)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [370/431]  eta: 0:01:07  lr: 0.000198  loss: 1.1064 (1.1353)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [380/431]  eta: 0:00:56  lr: 0.000198  loss: 1.1052 (1.1342)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [390/431]  eta: 0:00:45  lr: 0.000198  loss: 1.0867 (1.1345)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [400/431]  eta: 0:00:34  lr: 0.000198  loss: 1.1336 (1.1347)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:110]  [410/431]  eta: 0:00:23  lr: 0.000198  loss: 1.1078 (1.1334)  time: 1.1047  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:110]  [420/431]  eta: 0:00:12  lr: 0.000198  loss: 1.0854 (1.1334)  time: 1.1031  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:110]  [430/431]  eta: 0:00:01  lr: 0.000198  loss: 1.1215 (1.1360)  time: 1.1030  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:110] Total time: 0:07:59 (1.1114 s / it)\n",
      "Averaged stats: lr: 0.000198  loss: 1.1215 (1.1360)\n",
      "Valid: [epoch:110]  [ 0/14]  eta: 0:00:35  loss: 1.1408 (1.1408)  time: 2.5613  data: 2.3879  max mem: 15925\n",
      "Valid: [epoch:110]  [13/14]  eta: 0:00:00  loss: 1.0829 (1.0884)  time: 0.2721  data: 0.1707  max mem: 15925\n",
      "Valid: [epoch:110] Total time: 0:00:04 (0.2882 s / it)\n",
      "Averaged stats: loss: 1.0829 (1.0884)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_110_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.088%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 108.000\n",
      "Train: [epoch:111]  [  0/431]  eta: 0:34:13  lr: 0.000198  loss: 1.1538 (1.1538)  time: 4.7652  data: 3.5891  max mem: 15925\n",
      "Train: [epoch:111]  [ 10/431]  eta: 0:09:34  lr: 0.000198  loss: 1.1538 (1.1951)  time: 1.3638  data: 0.3264  max mem: 15925\n",
      "Train: [epoch:111]  [ 20/431]  eta: 0:08:20  lr: 0.000198  loss: 1.1307 (1.1718)  time: 1.0416  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [ 30/431]  eta: 0:07:51  lr: 0.000198  loss: 1.1021 (1.1506)  time: 1.0727  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [ 40/431]  eta: 0:07:31  lr: 0.000198  loss: 1.1021 (1.1468)  time: 1.0877  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [ 50/431]  eta: 0:07:16  lr: 0.000198  loss: 1.0885 (1.1460)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [ 60/431]  eta: 0:07:02  lr: 0.000198  loss: 1.0794 (1.1372)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [ 70/431]  eta: 0:06:49  lr: 0.000198  loss: 1.0515 (1.1381)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [ 80/431]  eta: 0:06:36  lr: 0.000198  loss: 1.0855 (1.1374)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [ 90/431]  eta: 0:06:25  lr: 0.000198  loss: 1.0874 (1.1388)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [100/431]  eta: 0:06:12  lr: 0.000198  loss: 1.1230 (1.1399)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [110/431]  eta: 0:06:00  lr: 0.000198  loss: 1.1396 (1.1390)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [120/431]  eta: 0:05:48  lr: 0.000198  loss: 1.1396 (1.1420)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [130/431]  eta: 0:05:37  lr: 0.000198  loss: 1.1695 (1.1438)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [140/431]  eta: 0:05:25  lr: 0.000198  loss: 1.1652 (1.1437)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [150/431]  eta: 0:05:14  lr: 0.000198  loss: 1.1429 (1.1423)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [160/431]  eta: 0:05:02  lr: 0.000198  loss: 1.1103 (1.1400)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [170/431]  eta: 0:04:51  lr: 0.000198  loss: 1.1111 (1.1397)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [180/431]  eta: 0:04:40  lr: 0.000198  loss: 1.1127 (1.1394)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [190/431]  eta: 0:04:28  lr: 0.000198  loss: 1.1321 (1.1405)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [200/431]  eta: 0:04:17  lr: 0.000198  loss: 1.1006 (1.1398)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [210/431]  eta: 0:04:06  lr: 0.000198  loss: 1.0683 (1.1370)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [220/431]  eta: 0:03:55  lr: 0.000198  loss: 1.0367 (1.1354)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [230/431]  eta: 0:03:43  lr: 0.000198  loss: 1.0945 (1.1367)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [240/431]  eta: 0:03:32  lr: 0.000198  loss: 1.1538 (1.1398)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [250/431]  eta: 0:03:21  lr: 0.000198  loss: 1.1406 (1.1390)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [260/431]  eta: 0:03:10  lr: 0.000198  loss: 1.1153 (1.1371)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [270/431]  eta: 0:02:59  lr: 0.000198  loss: 1.1229 (1.1386)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [280/431]  eta: 0:02:47  lr: 0.000198  loss: 1.1530 (1.1381)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [290/431]  eta: 0:02:36  lr: 0.000198  loss: 1.1081 (1.1372)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [300/431]  eta: 0:02:25  lr: 0.000198  loss: 1.0864 (1.1373)  time: 1.1050  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:111]  [310/431]  eta: 0:02:14  lr: 0.000198  loss: 1.1112 (1.1365)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [320/431]  eta: 0:02:03  lr: 0.000198  loss: 1.1045 (1.1351)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [330/431]  eta: 0:01:52  lr: 0.000198  loss: 1.1078 (1.1360)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [340/431]  eta: 0:01:41  lr: 0.000198  loss: 1.1948 (1.1391)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [350/431]  eta: 0:01:29  lr: 0.000198  loss: 1.1373 (1.1373)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [360/431]  eta: 0:01:18  lr: 0.000198  loss: 1.1038 (1.1372)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [370/431]  eta: 0:01:07  lr: 0.000198  loss: 1.1372 (1.1363)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [380/431]  eta: 0:00:56  lr: 0.000198  loss: 1.0866 (1.1351)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [390/431]  eta: 0:00:45  lr: 0.000198  loss: 1.0597 (1.1348)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [400/431]  eta: 0:00:34  lr: 0.000198  loss: 1.1105 (1.1339)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:111]  [410/431]  eta: 0:00:23  lr: 0.000198  loss: 1.0865 (1.1340)  time: 1.1042  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:111]  [420/431]  eta: 0:00:12  lr: 0.000198  loss: 1.0767 (1.1344)  time: 1.1102  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:111]  [430/431]  eta: 0:00:01  lr: 0.000198  loss: 1.0716 (1.1336)  time: 1.1137  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:111] Total time: 0:07:58 (1.1106 s / it)\n",
      "Averaged stats: lr: 0.000198  loss: 1.0716 (1.1336)\n",
      "Valid: [epoch:111]  [ 0/14]  eta: 0:00:37  loss: 1.0557 (1.0557)  time: 2.7064  data: 2.5256  max mem: 15925\n",
      "Valid: [epoch:111]  [13/14]  eta: 0:00:00  loss: 1.0718 (1.0790)  time: 0.2871  data: 0.1805  max mem: 15925\n",
      "Valid: [epoch:111] Total time: 0:00:04 (0.3046 s / it)\n",
      "Averaged stats: loss: 1.0718 (1.0790)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_111_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.079%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 108.000\n",
      "Train: [epoch:112]  [  0/431]  eta: 0:32:41  lr: 0.000198  loss: 1.0975 (1.0975)  time: 4.5517  data: 3.0000  max mem: 15925\n",
      "Train: [epoch:112]  [ 10/431]  eta: 0:09:34  lr: 0.000198  loss: 1.1555 (1.1776)  time: 1.3639  data: 0.2729  max mem: 15925\n",
      "Train: [epoch:112]  [ 20/431]  eta: 0:08:23  lr: 0.000198  loss: 1.1555 (1.1869)  time: 1.0584  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [ 30/431]  eta: 0:07:52  lr: 0.000198  loss: 1.1315 (1.1611)  time: 1.0778  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [ 40/431]  eta: 0:07:30  lr: 0.000198  loss: 1.0766 (1.1497)  time: 1.0771  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [ 50/431]  eta: 0:07:14  lr: 0.000198  loss: 1.0863 (1.1473)  time: 1.0834  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [ 60/431]  eta: 0:07:00  lr: 0.000198  loss: 1.0746 (1.1361)  time: 1.0918  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [ 70/431]  eta: 0:06:47  lr: 0.000198  loss: 1.0881 (1.1390)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [ 80/431]  eta: 0:06:35  lr: 0.000198  loss: 1.1264 (1.1375)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [ 90/431]  eta: 0:06:23  lr: 0.000198  loss: 1.1123 (1.1361)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [100/431]  eta: 0:06:11  lr: 0.000198  loss: 1.0738 (1.1321)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [110/431]  eta: 0:05:59  lr: 0.000198  loss: 1.0644 (1.1271)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [120/431]  eta: 0:05:47  lr: 0.000198  loss: 1.1020 (1.1276)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [130/431]  eta: 0:05:36  lr: 0.000198  loss: 1.1125 (1.1273)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [140/431]  eta: 0:05:25  lr: 0.000198  loss: 1.1128 (1.1269)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [150/431]  eta: 0:05:13  lr: 0.000198  loss: 1.1153 (1.1280)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [160/431]  eta: 0:05:02  lr: 0.000198  loss: 1.1389 (1.1299)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [170/431]  eta: 0:04:50  lr: 0.000198  loss: 1.0538 (1.1271)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [180/431]  eta: 0:04:39  lr: 0.000198  loss: 1.1216 (1.1288)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [190/431]  eta: 0:04:28  lr: 0.000198  loss: 1.1649 (1.1313)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [200/431]  eta: 0:04:16  lr: 0.000198  loss: 1.1397 (1.1313)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [210/431]  eta: 0:04:05  lr: 0.000198  loss: 1.0728 (1.1290)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [220/431]  eta: 0:03:54  lr: 0.000198  loss: 1.0728 (1.1296)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [230/431]  eta: 0:03:43  lr: 0.000198  loss: 1.0886 (1.1307)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [240/431]  eta: 0:03:32  lr: 0.000198  loss: 1.0886 (1.1303)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [250/431]  eta: 0:03:21  lr: 0.000198  loss: 1.1307 (1.1311)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [260/431]  eta: 0:03:09  lr: 0.000198  loss: 1.0982 (1.1311)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [270/431]  eta: 0:02:58  lr: 0.000198  loss: 1.0675 (1.1324)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [280/431]  eta: 0:02:47  lr: 0.000198  loss: 1.0774 (1.1323)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [290/431]  eta: 0:02:36  lr: 0.000198  loss: 1.0774 (1.1309)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [300/431]  eta: 0:02:25  lr: 0.000198  loss: 1.0953 (1.1311)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [310/431]  eta: 0:02:14  lr: 0.000198  loss: 1.1390 (1.1310)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [320/431]  eta: 0:02:03  lr: 0.000198  loss: 1.1592 (1.1338)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [330/431]  eta: 0:01:51  lr: 0.000198  loss: 1.1473 (1.1340)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [340/431]  eta: 0:01:40  lr: 0.000198  loss: 1.1059 (1.1332)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [350/431]  eta: 0:01:29  lr: 0.000198  loss: 1.1150 (1.1342)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [360/431]  eta: 0:01:18  lr: 0.000198  loss: 1.1398 (1.1340)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [370/431]  eta: 0:01:07  lr: 0.000198  loss: 1.1370 (1.1342)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [380/431]  eta: 0:00:56  lr: 0.000198  loss: 1.1370 (1.1352)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [390/431]  eta: 0:00:45  lr: 0.000198  loss: 1.1318 (1.1350)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [400/431]  eta: 0:00:34  lr: 0.000198  loss: 1.1990 (1.1382)  time: 1.1133  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:112]  [410/431]  eta: 0:00:23  lr: 0.000198  loss: 1.1283 (1.1367)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:112]  [420/431]  eta: 0:00:12  lr: 0.000198  loss: 1.0720 (1.1359)  time: 1.1239  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:112]  [430/431]  eta: 0:00:01  lr: 0.000198  loss: 1.0712 (1.1349)  time: 1.1138  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:112] Total time: 0:07:58 (1.1103 s / it)\n",
      "Averaged stats: lr: 0.000198  loss: 1.0712 (1.1349)\n",
      "Valid: [epoch:112]  [ 0/14]  eta: 0:00:36  loss: 1.1199 (1.1199)  time: 2.6046  data: 2.4448  max mem: 15925\n",
      "Valid: [epoch:112]  [13/14]  eta: 0:00:00  loss: 1.0643 (1.0760)  time: 0.2695  data: 0.1747  max mem: 15925\n",
      "Valid: [epoch:112] Total time: 0:00:03 (0.2852 s / it)\n",
      "Averaged stats: loss: 1.0643 (1.0760)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_112_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 1.076%\n",
      "Min loss: 1.074\n",
      "Best Epoch: 108.000\n",
      "Train: [epoch:113]  [  0/431]  eta: 0:35:20  lr: 0.000197  loss: 1.1952 (1.1952)  time: 4.9194  data: 3.8214  max mem: 15925\n",
      "Train: [epoch:113]  [ 10/431]  eta: 0:09:47  lr: 0.000197  loss: 1.1952 (1.1676)  time: 1.3956  data: 0.3476  max mem: 15925\n",
      "Train: [epoch:113]  [ 20/431]  eta: 0:08:24  lr: 0.000197  loss: 1.1141 (1.1447)  time: 1.0439  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [ 30/431]  eta: 0:07:52  lr: 0.000197  loss: 1.0876 (1.1444)  time: 1.0604  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:113]  [ 40/431]  eta: 0:07:33  lr: 0.000197  loss: 1.1079 (1.1533)  time: 1.0864  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [ 50/431]  eta: 0:07:16  lr: 0.000197  loss: 1.1079 (1.1420)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [ 60/431]  eta: 0:07:02  lr: 0.000197  loss: 1.0979 (1.1398)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [ 70/431]  eta: 0:06:50  lr: 0.000197  loss: 1.1351 (1.1427)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [ 80/431]  eta: 0:06:37  lr: 0.000197  loss: 1.1541 (1.1487)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [ 90/431]  eta: 0:06:25  lr: 0.000197  loss: 1.1422 (1.1511)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [100/431]  eta: 0:06:13  lr: 0.000197  loss: 1.1219 (1.1449)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [110/431]  eta: 0:06:01  lr: 0.000197  loss: 1.0718 (1.1436)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [120/431]  eta: 0:05:49  lr: 0.000197  loss: 1.0864 (1.1431)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [130/431]  eta: 0:05:38  lr: 0.000197  loss: 1.0864 (1.1417)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [140/431]  eta: 0:05:26  lr: 0.000197  loss: 1.0880 (1.1413)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [150/431]  eta: 0:05:14  lr: 0.000197  loss: 1.1285 (1.1412)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [160/431]  eta: 0:05:03  lr: 0.000197  loss: 1.1138 (1.1388)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [170/431]  eta: 0:04:51  lr: 0.000197  loss: 1.0828 (1.1363)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [180/431]  eta: 0:04:40  lr: 0.000197  loss: 1.0839 (1.1360)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [190/431]  eta: 0:04:28  lr: 0.000197  loss: 1.1382 (1.1374)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [200/431]  eta: 0:04:17  lr: 0.000197  loss: 1.1388 (1.1384)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [210/431]  eta: 0:04:06  lr: 0.000197  loss: 1.1518 (1.1425)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [220/431]  eta: 0:03:55  lr: 0.000197  loss: 1.1809 (1.1440)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [230/431]  eta: 0:03:43  lr: 0.000197  loss: 1.0958 (1.1419)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [240/431]  eta: 0:03:32  lr: 0.000197  loss: 1.0839 (1.1398)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [250/431]  eta: 0:03:21  lr: 0.000197  loss: 1.0939 (1.1387)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [260/431]  eta: 0:03:10  lr: 0.000197  loss: 1.0939 (1.1389)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [270/431]  eta: 0:02:58  lr: 0.000197  loss: 1.1293 (1.1384)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [280/431]  eta: 0:02:47  lr: 0.000197  loss: 1.1224 (1.1383)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [290/431]  eta: 0:02:36  lr: 0.000197  loss: 1.1082 (1.1364)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [300/431]  eta: 0:02:25  lr: 0.000197  loss: 1.1661 (1.1383)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [310/431]  eta: 0:02:14  lr: 0.000197  loss: 1.1384 (1.1368)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [320/431]  eta: 0:02:03  lr: 0.000197  loss: 1.1024 (1.1367)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [330/431]  eta: 0:01:52  lr: 0.000197  loss: 1.1153 (1.1366)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [340/431]  eta: 0:01:41  lr: 0.000197  loss: 1.0827 (1.1345)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [350/431]  eta: 0:01:29  lr: 0.000197  loss: 1.0683 (1.1331)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [360/431]  eta: 0:01:18  lr: 0.000197  loss: 1.1131 (1.1346)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [370/431]  eta: 0:01:07  lr: 0.000197  loss: 1.1404 (1.1351)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [380/431]  eta: 0:00:56  lr: 0.000197  loss: 1.1141 (1.1355)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [390/431]  eta: 0:00:45  lr: 0.000197  loss: 1.1516 (1.1369)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [400/431]  eta: 0:00:34  lr: 0.000197  loss: 1.0771 (1.1353)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:113]  [410/431]  eta: 0:00:23  lr: 0.000197  loss: 1.0782 (1.1355)  time: 1.0883  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:113]  [420/431]  eta: 0:00:12  lr: 0.000197  loss: 1.0963 (1.1351)  time: 1.0932  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:113]  [430/431]  eta: 0:00:01  lr: 0.000197  loss: 1.1163 (1.1356)  time: 1.1047  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:113] Total time: 0:07:58 (1.1093 s / it)\n",
      "Averaged stats: lr: 0.000197  loss: 1.1163 (1.1356)\n",
      "Valid: [epoch:113]  [ 0/14]  eta: 0:00:35  loss: 0.9762 (0.9762)  time: 2.5245  data: 2.3835  max mem: 15925\n",
      "Valid: [epoch:113]  [13/14]  eta: 0:00:00  loss: 1.0615 (1.0722)  time: 0.2600  data: 0.1703  max mem: 15925\n",
      "Valid: [epoch:113] Total time: 0:00:03 (0.2761 s / it)\n",
      "Averaged stats: loss: 1.0615 (1.0722)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_113_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.072\n",
      "Best Epoch: 113.000\n",
      "Train: [epoch:114]  [  0/431]  eta: 0:33:39  lr: 0.000197  loss: 1.0246 (1.0246)  time: 4.6858  data: 3.2797  max mem: 15925\n",
      "Train: [epoch:114]  [ 10/431]  eta: 0:09:35  lr: 0.000197  loss: 1.1519 (1.1301)  time: 1.3670  data: 0.2984  max mem: 15925\n",
      "Train: [epoch:114]  [ 20/431]  eta: 0:08:22  lr: 0.000197  loss: 1.0766 (1.1172)  time: 1.0501  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [ 30/431]  eta: 0:07:51  lr: 0.000197  loss: 1.0723 (1.1189)  time: 1.0720  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [ 40/431]  eta: 0:07:31  lr: 0.000197  loss: 1.1756 (1.1281)  time: 1.0840  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [ 50/431]  eta: 0:07:15  lr: 0.000197  loss: 1.1419 (1.1241)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [ 60/431]  eta: 0:07:02  lr: 0.000197  loss: 1.1200 (1.1275)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [ 70/431]  eta: 0:06:49  lr: 0.000197  loss: 1.1209 (1.1326)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [ 80/431]  eta: 0:06:36  lr: 0.000197  loss: 1.1341 (1.1401)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [ 90/431]  eta: 0:06:25  lr: 0.000197  loss: 1.1361 (1.1371)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [100/431]  eta: 0:06:13  lr: 0.000197  loss: 1.0978 (1.1370)  time: 1.1256  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:114]  [110/431]  eta: 0:06:02  lr: 0.000197  loss: 1.0978 (1.1324)  time: 1.1387  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:114]  [120/431]  eta: 0:05:51  lr: 0.000197  loss: 1.0857 (1.1315)  time: 1.1320  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:114]  [130/431]  eta: 0:05:40  lr: 0.000197  loss: 1.1100 (1.1322)  time: 1.1234  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:114]  [140/431]  eta: 0:05:28  lr: 0.000197  loss: 1.1162 (1.1328)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [150/431]  eta: 0:05:16  lr: 0.000197  loss: 1.1592 (1.1342)  time: 1.1092  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:114]  [160/431]  eta: 0:05:04  lr: 0.000197  loss: 1.1051 (1.1345)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [170/431]  eta: 0:04:53  lr: 0.000197  loss: 1.1051 (1.1368)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [180/431]  eta: 0:04:41  lr: 0.000197  loss: 1.1857 (1.1385)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [190/431]  eta: 0:04:30  lr: 0.000197  loss: 1.0836 (1.1377)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [200/431]  eta: 0:04:19  lr: 0.000197  loss: 1.0949 (1.1387)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [210/431]  eta: 0:04:07  lr: 0.000197  loss: 1.1060 (1.1367)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [220/431]  eta: 0:03:56  lr: 0.000197  loss: 1.0338 (1.1323)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [230/431]  eta: 0:03:44  lr: 0.000197  loss: 1.0321 (1.1310)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [240/431]  eta: 0:03:33  lr: 0.000197  loss: 1.0716 (1.1289)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [250/431]  eta: 0:03:22  lr: 0.000197  loss: 1.0715 (1.1283)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [260/431]  eta: 0:03:10  lr: 0.000197  loss: 1.0994 (1.1280)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [270/431]  eta: 0:02:59  lr: 0.000197  loss: 1.1202 (1.1279)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [280/431]  eta: 0:02:48  lr: 0.000197  loss: 1.1202 (1.1280)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [290/431]  eta: 0:02:37  lr: 0.000197  loss: 1.1037 (1.1273)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [300/431]  eta: 0:02:26  lr: 0.000197  loss: 1.1094 (1.1284)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [310/431]  eta: 0:02:15  lr: 0.000197  loss: 1.1290 (1.1290)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [320/431]  eta: 0:02:03  lr: 0.000197  loss: 1.0613 (1.1271)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [330/431]  eta: 0:01:52  lr: 0.000197  loss: 1.0706 (1.1274)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [340/431]  eta: 0:01:41  lr: 0.000197  loss: 1.1865 (1.1302)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [350/431]  eta: 0:01:30  lr: 0.000197  loss: 1.2103 (1.1320)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [360/431]  eta: 0:01:19  lr: 0.000197  loss: 1.1367 (1.1313)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [370/431]  eta: 0:01:07  lr: 0.000197  loss: 1.0914 (1.1310)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [380/431]  eta: 0:00:56  lr: 0.000197  loss: 1.1870 (1.1336)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [390/431]  eta: 0:00:45  lr: 0.000197  loss: 1.1778 (1.1337)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [400/431]  eta: 0:00:34  lr: 0.000197  loss: 1.1323 (1.1348)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [410/431]  eta: 0:00:23  lr: 0.000197  loss: 1.1573 (1.1357)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:114]  [420/431]  eta: 0:00:12  lr: 0.000197  loss: 1.1534 (1.1357)  time: 1.0982  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:114]  [430/431]  eta: 0:00:01  lr: 0.000197  loss: 1.1366 (1.1352)  time: 1.1023  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:114] Total time: 0:07:59 (1.1132 s / it)\n",
      "Averaged stats: lr: 0.000197  loss: 1.1366 (1.1352)\n",
      "Valid: [epoch:114]  [ 0/14]  eta: 0:00:36  loss: 1.0485 (1.0485)  time: 2.6253  data: 2.4715  max mem: 15925\n",
      "Valid: [epoch:114]  [13/14]  eta: 0:00:00  loss: 1.0640 (1.0738)  time: 0.2873  data: 0.1766  max mem: 15925\n",
      "Valid: [epoch:114] Total time: 0:00:04 (0.3053 s / it)\n",
      "Averaged stats: loss: 1.0640 (1.0738)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_114_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.072\n",
      "Best Epoch: 113.000\n",
      "Train: [epoch:115]  [  0/431]  eta: 0:32:53  lr: 0.000197  loss: 1.1054 (1.1054)  time: 4.5785  data: 3.4680  max mem: 15925\n",
      "Train: [epoch:115]  [ 10/431]  eta: 0:09:36  lr: 0.000197  loss: 1.2223 (1.2240)  time: 1.3684  data: 0.3155  max mem: 15925\n",
      "Train: [epoch:115]  [ 20/431]  eta: 0:08:24  lr: 0.000197  loss: 1.1507 (1.1697)  time: 1.0588  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [ 30/431]  eta: 0:07:52  lr: 0.000197  loss: 1.0845 (1.1390)  time: 1.0720  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [ 40/431]  eta: 0:07:31  lr: 0.000197  loss: 1.0723 (1.1329)  time: 1.0814  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [ 50/431]  eta: 0:07:15  lr: 0.000197  loss: 1.0684 (1.1314)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [ 60/431]  eta: 0:07:02  lr: 0.000197  loss: 1.1246 (1.1262)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [ 70/431]  eta: 0:06:50  lr: 0.000197  loss: 1.1408 (1.1286)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [ 80/431]  eta: 0:06:37  lr: 0.000197  loss: 1.1516 (1.1331)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [ 90/431]  eta: 0:06:24  lr: 0.000197  loss: 1.1217 (1.1304)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [100/431]  eta: 0:06:12  lr: 0.000197  loss: 1.1009 (1.1319)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [110/431]  eta: 0:06:00  lr: 0.000197  loss: 1.0687 (1.1263)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [120/431]  eta: 0:05:49  lr: 0.000197  loss: 1.0687 (1.1281)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [130/431]  eta: 0:05:37  lr: 0.000197  loss: 1.0983 (1.1273)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [140/431]  eta: 0:05:26  lr: 0.000197  loss: 1.0956 (1.1291)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [150/431]  eta: 0:05:14  lr: 0.000197  loss: 1.1124 (1.1283)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [160/431]  eta: 0:05:02  lr: 0.000197  loss: 1.1034 (1.1307)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [170/431]  eta: 0:04:51  lr: 0.000197  loss: 1.0876 (1.1268)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [180/431]  eta: 0:04:40  lr: 0.000197  loss: 1.0681 (1.1277)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [190/431]  eta: 0:04:28  lr: 0.000197  loss: 1.0878 (1.1276)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [200/431]  eta: 0:04:17  lr: 0.000197  loss: 1.0517 (1.1253)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [210/431]  eta: 0:04:06  lr: 0.000197  loss: 1.0842 (1.1253)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [220/431]  eta: 0:03:54  lr: 0.000197  loss: 1.0963 (1.1256)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [230/431]  eta: 0:03:43  lr: 0.000197  loss: 1.1045 (1.1275)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [240/431]  eta: 0:03:32  lr: 0.000197  loss: 1.1587 (1.1300)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [250/431]  eta: 0:03:21  lr: 0.000197  loss: 1.1513 (1.1320)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [260/431]  eta: 0:03:10  lr: 0.000197  loss: 1.1275 (1.1316)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [270/431]  eta: 0:02:59  lr: 0.000197  loss: 1.0834 (1.1306)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [280/431]  eta: 0:02:48  lr: 0.000197  loss: 1.1051 (1.1305)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [290/431]  eta: 0:02:36  lr: 0.000197  loss: 1.1156 (1.1304)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [300/431]  eta: 0:02:25  lr: 0.000197  loss: 1.0799 (1.1306)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [310/431]  eta: 0:02:14  lr: 0.000197  loss: 1.1301 (1.1328)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [320/431]  eta: 0:02:03  lr: 0.000197  loss: 1.1081 (1.1316)  time: 1.1039  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:115]  [330/431]  eta: 0:01:52  lr: 0.000197  loss: 1.1081 (1.1323)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [340/431]  eta: 0:01:41  lr: 0.000197  loss: 1.1813 (1.1345)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [350/431]  eta: 0:01:30  lr: 0.000197  loss: 1.1489 (1.1350)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [360/431]  eta: 0:01:18  lr: 0.000197  loss: 1.0884 (1.1335)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [370/431]  eta: 0:01:07  lr: 0.000197  loss: 1.0844 (1.1341)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [380/431]  eta: 0:00:56  lr: 0.000197  loss: 1.1371 (1.1347)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [390/431]  eta: 0:00:45  lr: 0.000197  loss: 1.0874 (1.1331)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [400/431]  eta: 0:00:34  lr: 0.000197  loss: 1.0874 (1.1344)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:115]  [410/431]  eta: 0:00:23  lr: 0.000197  loss: 1.1337 (1.1346)  time: 1.0967  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:115]  [420/431]  eta: 0:00:12  lr: 0.000197  loss: 1.1497 (1.1349)  time: 1.0955  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:115]  [430/431]  eta: 0:00:01  lr: 0.000197  loss: 1.1291 (1.1345)  time: 1.1060  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:115] Total time: 0:07:58 (1.1107 s / it)\n",
      "Averaged stats: lr: 0.000197  loss: 1.1291 (1.1345)\n",
      "Valid: [epoch:115]  [ 0/14]  eta: 0:00:35  loss: 1.1806 (1.1806)  time: 2.5572  data: 2.3777  max mem: 15925\n",
      "Valid: [epoch:115]  [13/14]  eta: 0:00:00  loss: 1.0727 (1.0814)  time: 0.2709  data: 0.1699  max mem: 15925\n",
      "Valid: [epoch:115] Total time: 0:00:04 (0.2899 s / it)\n",
      "Averaged stats: loss: 1.0727 (1.0814)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_115_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.081%\n",
      "Min loss: 1.072\n",
      "Best Epoch: 113.000\n",
      "Train: [epoch:116]  [  0/431]  eta: 0:33:06  lr: 0.000197  loss: 1.1722 (1.1722)  time: 4.6098  data: 3.4301  max mem: 15925\n",
      "Train: [epoch:116]  [ 10/431]  eta: 0:09:37  lr: 0.000197  loss: 1.1518 (1.1325)  time: 1.3717  data: 0.3120  max mem: 15925\n",
      "Train: [epoch:116]  [ 20/431]  eta: 0:08:25  lr: 0.000197  loss: 1.1051 (1.1323)  time: 1.0609  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [ 30/431]  eta: 0:07:55  lr: 0.000197  loss: 1.1051 (1.1186)  time: 1.0839  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [ 40/431]  eta: 0:07:35  lr: 0.000197  loss: 1.0732 (1.1199)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [ 50/431]  eta: 0:07:18  lr: 0.000197  loss: 1.0642 (1.1099)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [ 60/431]  eta: 0:07:03  lr: 0.000197  loss: 1.0465 (1.1048)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [ 70/431]  eta: 0:06:50  lr: 0.000197  loss: 1.0928 (1.1055)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [ 80/431]  eta: 0:06:37  lr: 0.000197  loss: 1.0564 (1.1074)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [ 90/431]  eta: 0:06:25  lr: 0.000197  loss: 1.0564 (1.1032)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [100/431]  eta: 0:06:13  lr: 0.000197  loss: 1.0648 (1.1067)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [110/431]  eta: 0:06:01  lr: 0.000197  loss: 1.0920 (1.1063)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [120/431]  eta: 0:05:49  lr: 0.000197  loss: 1.0871 (1.1078)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [130/431]  eta: 0:05:37  lr: 0.000197  loss: 1.1003 (1.1097)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [140/431]  eta: 0:05:26  lr: 0.000197  loss: 1.1357 (1.1126)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [150/431]  eta: 0:05:14  lr: 0.000197  loss: 1.1301 (1.1136)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [160/431]  eta: 0:05:03  lr: 0.000197  loss: 1.1205 (1.1147)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [170/431]  eta: 0:04:51  lr: 0.000197  loss: 1.1205 (1.1154)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [180/431]  eta: 0:04:40  lr: 0.000197  loss: 1.1354 (1.1198)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [190/431]  eta: 0:04:29  lr: 0.000197  loss: 1.1940 (1.1240)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [200/431]  eta: 0:04:17  lr: 0.000197  loss: 1.1534 (1.1239)  time: 1.1015  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:116]  [210/431]  eta: 0:04:06  lr: 0.000197  loss: 1.1534 (1.1275)  time: 1.1030  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:116]  [220/431]  eta: 0:03:55  lr: 0.000197  loss: 1.1744 (1.1285)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [230/431]  eta: 0:03:44  lr: 0.000197  loss: 1.1016 (1.1276)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [240/431]  eta: 0:03:32  lr: 0.000197  loss: 1.0963 (1.1256)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [250/431]  eta: 0:03:21  lr: 0.000197  loss: 1.1137 (1.1269)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [260/431]  eta: 0:03:10  lr: 0.000197  loss: 1.1528 (1.1292)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [270/431]  eta: 0:02:59  lr: 0.000197  loss: 1.1868 (1.1306)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [280/431]  eta: 0:02:48  lr: 0.000197  loss: 1.0838 (1.1292)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [290/431]  eta: 0:02:36  lr: 0.000197  loss: 1.0806 (1.1296)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [300/431]  eta: 0:02:25  lr: 0.000197  loss: 1.1040 (1.1299)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [310/431]  eta: 0:02:14  lr: 0.000197  loss: 1.0961 (1.1296)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [320/431]  eta: 0:02:03  lr: 0.000197  loss: 1.1238 (1.1305)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [330/431]  eta: 0:01:52  lr: 0.000197  loss: 1.1366 (1.1314)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [340/431]  eta: 0:01:41  lr: 0.000197  loss: 1.1890 (1.1338)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [350/431]  eta: 0:01:30  lr: 0.000197  loss: 1.1157 (1.1325)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [360/431]  eta: 0:01:18  lr: 0.000197  loss: 1.0633 (1.1314)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [370/431]  eta: 0:01:07  lr: 0.000197  loss: 1.1095 (1.1330)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [380/431]  eta: 0:00:56  lr: 0.000197  loss: 1.1682 (1.1325)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [390/431]  eta: 0:00:45  lr: 0.000197  loss: 1.0998 (1.1324)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [400/431]  eta: 0:00:34  lr: 0.000197  loss: 1.0987 (1.1323)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:116]  [410/431]  eta: 0:00:23  lr: 0.000197  loss: 1.1834 (1.1334)  time: 1.0986  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:116]  [420/431]  eta: 0:00:12  lr: 0.000197  loss: 1.2098 (1.1340)  time: 1.1028  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:116]  [430/431]  eta: 0:00:01  lr: 0.000197  loss: 1.1050 (1.1330)  time: 1.1057  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:116] Total time: 0:07:58 (1.1108 s / it)\n",
      "Averaged stats: lr: 0.000197  loss: 1.1050 (1.1330)\n",
      "Valid: [epoch:116]  [ 0/14]  eta: 0:00:30  loss: 1.1271 (1.1271)  time: 2.2098  data: 2.0446  max mem: 15925\n",
      "Valid: [epoch:116]  [13/14]  eta: 0:00:00  loss: 1.0588 (1.0723)  time: 0.2411  data: 0.1461  max mem: 15925\n",
      "Valid: [epoch:116] Total time: 0:00:03 (0.2561 s / it)\n",
      "Averaged stats: loss: 1.0588 (1.0723)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_116_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.072\n",
      "Best Epoch: 113.000\n",
      "Train: [epoch:117]  [  0/431]  eta: 0:36:23  lr: 0.000196  loss: 1.1117 (1.1117)  time: 5.0672  data: 3.8984  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:117]  [ 10/431]  eta: 0:09:59  lr: 0.000196  loss: 1.1332 (1.1394)  time: 1.4236  data: 0.3546  max mem: 15925\n",
      "Train: [epoch:117]  [ 20/431]  eta: 0:08:31  lr: 0.000196  loss: 1.1150 (1.1074)  time: 1.0536  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [ 30/431]  eta: 0:07:59  lr: 0.000196  loss: 1.0910 (1.1113)  time: 1.0691  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [ 40/431]  eta: 0:07:37  lr: 0.000196  loss: 1.1195 (1.1278)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [ 50/431]  eta: 0:07:20  lr: 0.000196  loss: 1.0635 (1.1158)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [ 60/431]  eta: 0:07:05  lr: 0.000196  loss: 1.0572 (1.1103)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [ 70/431]  eta: 0:06:52  lr: 0.000196  loss: 1.0732 (1.1079)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [ 80/431]  eta: 0:06:39  lr: 0.000196  loss: 1.0967 (1.1130)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [ 90/431]  eta: 0:06:26  lr: 0.000196  loss: 1.1378 (1.1209)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [100/431]  eta: 0:06:14  lr: 0.000196  loss: 1.1104 (1.1178)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [110/431]  eta: 0:06:02  lr: 0.000196  loss: 1.0631 (1.1153)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [120/431]  eta: 0:05:50  lr: 0.000196  loss: 1.0790 (1.1192)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [130/431]  eta: 0:05:38  lr: 0.000196  loss: 1.1415 (1.1218)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [140/431]  eta: 0:05:27  lr: 0.000196  loss: 1.1415 (1.1261)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [150/431]  eta: 0:05:15  lr: 0.000196  loss: 1.1345 (1.1285)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [160/431]  eta: 0:05:04  lr: 0.000196  loss: 1.0780 (1.1267)  time: 1.1260  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [170/431]  eta: 0:04:53  lr: 0.000196  loss: 1.0632 (1.1271)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [180/431]  eta: 0:04:41  lr: 0.000196  loss: 1.0727 (1.1253)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [190/431]  eta: 0:04:30  lr: 0.000196  loss: 1.1135 (1.1268)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [200/431]  eta: 0:04:18  lr: 0.000196  loss: 1.1195 (1.1268)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [210/431]  eta: 0:04:07  lr: 0.000196  loss: 1.1113 (1.1247)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [220/431]  eta: 0:03:56  lr: 0.000196  loss: 1.1124 (1.1274)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [230/431]  eta: 0:03:44  lr: 0.000196  loss: 1.1226 (1.1270)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [240/431]  eta: 0:03:33  lr: 0.000196  loss: 1.0926 (1.1264)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [250/431]  eta: 0:03:22  lr: 0.000196  loss: 1.1136 (1.1265)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [260/431]  eta: 0:03:11  lr: 0.000196  loss: 1.0894 (1.1253)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [270/431]  eta: 0:02:59  lr: 0.000196  loss: 1.0972 (1.1278)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [280/431]  eta: 0:02:48  lr: 0.000196  loss: 1.1843 (1.1297)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [290/431]  eta: 0:02:37  lr: 0.000196  loss: 1.1643 (1.1318)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [300/431]  eta: 0:02:26  lr: 0.000196  loss: 1.1934 (1.1357)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [310/431]  eta: 0:02:14  lr: 0.000196  loss: 1.2438 (1.1372)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [320/431]  eta: 0:02:03  lr: 0.000196  loss: 1.1199 (1.1367)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [330/431]  eta: 0:01:52  lr: 0.000196  loss: 1.1668 (1.1390)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [340/431]  eta: 0:01:41  lr: 0.000196  loss: 1.1277 (1.1374)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [350/431]  eta: 0:01:30  lr: 0.000196  loss: 1.0748 (1.1363)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [360/431]  eta: 0:01:19  lr: 0.000196  loss: 1.0744 (1.1350)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [370/431]  eta: 0:01:08  lr: 0.000196  loss: 1.0744 (1.1344)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [380/431]  eta: 0:00:56  lr: 0.000196  loss: 1.0945 (1.1328)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [390/431]  eta: 0:00:45  lr: 0.000196  loss: 1.1070 (1.1339)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [400/431]  eta: 0:00:34  lr: 0.000196  loss: 1.1195 (1.1333)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [410/431]  eta: 0:00:23  lr: 0.000196  loss: 1.0914 (1.1330)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:117]  [420/431]  eta: 0:00:12  lr: 0.000196  loss: 1.1081 (1.1338)  time: 1.1251  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:117]  [430/431]  eta: 0:00:01  lr: 0.000196  loss: 1.1081 (1.1333)  time: 1.1115  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:117] Total time: 0:08:01 (1.1163 s / it)\n",
      "Averaged stats: lr: 0.000196  loss: 1.1081 (1.1333)\n",
      "Valid: [epoch:117]  [ 0/14]  eta: 0:00:36  loss: 1.1263 (1.1263)  time: 2.5791  data: 2.4179  max mem: 15925\n",
      "Valid: [epoch:117]  [13/14]  eta: 0:00:00  loss: 1.0696 (1.0775)  time: 0.2709  data: 0.1728  max mem: 15925\n",
      "Valid: [epoch:117] Total time: 0:00:04 (0.2883 s / it)\n",
      "Averaged stats: loss: 1.0696 (1.0775)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_117_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.078%\n",
      "Min loss: 1.072\n",
      "Best Epoch: 113.000\n",
      "Train: [epoch:118]  [  0/431]  eta: 0:36:26  lr: 0.000196  loss: 1.0536 (1.0536)  time: 5.0732  data: 3.7804  max mem: 15925\n",
      "Train: [epoch:118]  [ 10/431]  eta: 0:09:51  lr: 0.000196  loss: 1.1440 (1.1578)  time: 1.4056  data: 0.3438  max mem: 15925\n",
      "Train: [epoch:118]  [ 20/431]  eta: 0:08:30  lr: 0.000196  loss: 1.1233 (1.1425)  time: 1.0512  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [ 30/431]  eta: 0:07:58  lr: 0.000196  loss: 1.1144 (1.1465)  time: 1.0780  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [ 40/431]  eta: 0:07:37  lr: 0.000196  loss: 1.1096 (1.1374)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [ 50/431]  eta: 0:07:21  lr: 0.000196  loss: 1.0570 (1.1306)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [ 60/431]  eta: 0:07:05  lr: 0.000196  loss: 1.0257 (1.1185)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [ 70/431]  eta: 0:06:52  lr: 0.000196  loss: 1.0952 (1.1210)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [ 80/431]  eta: 0:06:39  lr: 0.000196  loss: 1.1320 (1.1246)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [ 90/431]  eta: 0:06:27  lr: 0.000196  loss: 1.0861 (1.1175)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [100/431]  eta: 0:06:15  lr: 0.000196  loss: 1.0558 (1.1210)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [110/431]  eta: 0:06:03  lr: 0.000196  loss: 1.1081 (1.1232)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [120/431]  eta: 0:05:51  lr: 0.000196  loss: 1.0757 (1.1193)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [130/431]  eta: 0:05:39  lr: 0.000196  loss: 1.0695 (1.1215)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [140/431]  eta: 0:05:28  lr: 0.000196  loss: 1.1151 (1.1224)  time: 1.1231  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:118]  [150/431]  eta: 0:05:16  lr: 0.000196  loss: 1.0990 (1.1210)  time: 1.1126  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:118]  [160/431]  eta: 0:05:05  lr: 0.000196  loss: 1.0521 (1.1201)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [170/431]  eta: 0:04:53  lr: 0.000196  loss: 1.0634 (1.1216)  time: 1.1055  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:118]  [180/431]  eta: 0:04:42  lr: 0.000196  loss: 1.0522 (1.1182)  time: 1.1040  data: 0.0009  max mem: 15925\n",
      "Train: [epoch:118]  [190/431]  eta: 0:04:30  lr: 0.000196  loss: 1.0633 (1.1195)  time: 1.1066  data: 0.0009  max mem: 15925\n",
      "Train: [epoch:118]  [200/431]  eta: 0:04:19  lr: 0.000196  loss: 1.0913 (1.1191)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [210/431]  eta: 0:04:07  lr: 0.000196  loss: 1.0825 (1.1198)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [220/431]  eta: 0:03:56  lr: 0.000196  loss: 1.0825 (1.1201)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [230/431]  eta: 0:03:45  lr: 0.000196  loss: 1.1274 (1.1247)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [240/431]  eta: 0:03:33  lr: 0.000196  loss: 1.1274 (1.1239)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [250/431]  eta: 0:03:22  lr: 0.000196  loss: 1.0857 (1.1237)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [260/431]  eta: 0:03:11  lr: 0.000196  loss: 1.1252 (1.1248)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [270/431]  eta: 0:03:00  lr: 0.000196  loss: 1.1252 (1.1256)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [280/431]  eta: 0:02:48  lr: 0.000196  loss: 1.0950 (1.1264)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [290/431]  eta: 0:02:37  lr: 0.000196  loss: 1.0950 (1.1261)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [300/431]  eta: 0:02:26  lr: 0.000196  loss: 1.1469 (1.1273)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [310/431]  eta: 0:02:15  lr: 0.000196  loss: 1.1690 (1.1288)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [320/431]  eta: 0:02:04  lr: 0.000196  loss: 1.1873 (1.1293)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [330/431]  eta: 0:01:52  lr: 0.000196  loss: 1.1838 (1.1324)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [340/431]  eta: 0:01:41  lr: 0.000196  loss: 1.1433 (1.1330)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [350/431]  eta: 0:01:30  lr: 0.000196  loss: 1.1689 (1.1351)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [360/431]  eta: 0:01:19  lr: 0.000196  loss: 1.1677 (1.1350)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [370/431]  eta: 0:01:08  lr: 0.000196  loss: 1.1220 (1.1350)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [380/431]  eta: 0:00:56  lr: 0.000196  loss: 1.0968 (1.1338)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [390/431]  eta: 0:00:45  lr: 0.000196  loss: 1.0869 (1.1333)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [400/431]  eta: 0:00:34  lr: 0.000196  loss: 1.0897 (1.1337)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [410/431]  eta: 0:00:23  lr: 0.000196  loss: 1.1406 (1.1336)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:118]  [420/431]  eta: 0:00:12  lr: 0.000196  loss: 1.1245 (1.1340)  time: 1.1044  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:118]  [430/431]  eta: 0:00:01  lr: 0.000196  loss: 1.1443 (1.1343)  time: 1.1112  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:118] Total time: 0:08:01 (1.1161 s / it)\n",
      "Averaged stats: lr: 0.000196  loss: 1.1443 (1.1343)\n",
      "Valid: [epoch:118]  [ 0/14]  eta: 0:00:36  loss: 0.9766 (0.9766)  time: 2.6398  data: 2.5108  max mem: 15925\n",
      "Valid: [epoch:118]  [13/14]  eta: 0:00:00  loss: 1.0643 (1.0738)  time: 0.2781  data: 0.1794  max mem: 15925\n",
      "Valid: [epoch:118] Total time: 0:00:04 (0.2946 s / it)\n",
      "Averaged stats: loss: 1.0643 (1.0738)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_118_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.072\n",
      "Best Epoch: 113.000\n",
      "Train: [epoch:119]  [  0/431]  eta: 0:33:27  lr: 0.000196  loss: 1.2530 (1.2530)  time: 4.6568  data: 3.5062  max mem: 15925\n",
      "Train: [epoch:119]  [ 10/431]  eta: 0:09:30  lr: 0.000196  loss: 1.2230 (1.2000)  time: 1.3559  data: 0.3189  max mem: 15925\n",
      "Train: [epoch:119]  [ 20/431]  eta: 0:08:21  lr: 0.000196  loss: 1.1231 (1.1742)  time: 1.0483  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [ 30/431]  eta: 0:07:51  lr: 0.000196  loss: 1.0839 (1.1457)  time: 1.0780  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [ 40/431]  eta: 0:07:33  lr: 0.000196  loss: 1.0839 (1.1550)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [ 50/431]  eta: 0:07:17  lr: 0.000196  loss: 1.0939 (1.1413)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [ 60/431]  eta: 0:07:02  lr: 0.000196  loss: 1.1177 (1.1428)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [ 70/431]  eta: 0:06:49  lr: 0.000196  loss: 1.1148 (1.1380)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [ 80/431]  eta: 0:06:37  lr: 0.000196  loss: 1.1148 (1.1424)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [ 90/431]  eta: 0:06:25  lr: 0.000196  loss: 1.1557 (1.1361)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [100/431]  eta: 0:06:13  lr: 0.000196  loss: 1.0310 (1.1263)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [110/431]  eta: 0:06:01  lr: 0.000196  loss: 1.0333 (1.1210)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [120/431]  eta: 0:05:49  lr: 0.000196  loss: 1.0895 (1.1219)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [130/431]  eta: 0:05:38  lr: 0.000196  loss: 1.1383 (1.1274)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [140/431]  eta: 0:05:26  lr: 0.000196  loss: 1.1388 (1.1261)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [150/431]  eta: 0:05:15  lr: 0.000196  loss: 1.1126 (1.1252)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [160/431]  eta: 0:05:03  lr: 0.000196  loss: 1.1133 (1.1285)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [170/431]  eta: 0:04:52  lr: 0.000196  loss: 1.1151 (1.1318)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [180/431]  eta: 0:04:40  lr: 0.000196  loss: 1.1874 (1.1322)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [190/431]  eta: 0:04:29  lr: 0.000196  loss: 1.1128 (1.1317)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [200/431]  eta: 0:04:18  lr: 0.000196  loss: 1.0879 (1.1320)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [210/431]  eta: 0:04:07  lr: 0.000196  loss: 1.0814 (1.1290)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [220/431]  eta: 0:03:55  lr: 0.000196  loss: 1.0688 (1.1293)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [230/431]  eta: 0:03:44  lr: 0.000196  loss: 1.1032 (1.1306)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [240/431]  eta: 0:03:33  lr: 0.000196  loss: 1.1013 (1.1284)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [250/431]  eta: 0:03:22  lr: 0.000196  loss: 1.0804 (1.1286)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [260/431]  eta: 0:03:10  lr: 0.000196  loss: 1.0960 (1.1289)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [270/431]  eta: 0:02:59  lr: 0.000196  loss: 1.0978 (1.1308)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [280/431]  eta: 0:02:48  lr: 0.000196  loss: 1.1253 (1.1306)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [290/431]  eta: 0:02:37  lr: 0.000196  loss: 1.1386 (1.1307)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [300/431]  eta: 0:02:26  lr: 0.000196  loss: 1.1528 (1.1319)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [310/431]  eta: 0:02:15  lr: 0.000196  loss: 1.1796 (1.1322)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [320/431]  eta: 0:02:03  lr: 0.000196  loss: 1.1397 (1.1328)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [330/431]  eta: 0:01:52  lr: 0.000196  loss: 1.1319 (1.1324)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [340/431]  eta: 0:01:41  lr: 0.000196  loss: 1.1264 (1.1330)  time: 1.1030  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:119]  [350/431]  eta: 0:01:30  lr: 0.000196  loss: 1.1242 (1.1328)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [360/431]  eta: 0:01:19  lr: 0.000196  loss: 1.1098 (1.1323)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [370/431]  eta: 0:01:08  lr: 0.000196  loss: 1.1077 (1.1348)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [380/431]  eta: 0:00:56  lr: 0.000196  loss: 1.1071 (1.1345)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [390/431]  eta: 0:00:45  lr: 0.000196  loss: 1.1033 (1.1344)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [400/431]  eta: 0:00:34  lr: 0.000196  loss: 1.1159 (1.1343)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [410/431]  eta: 0:00:23  lr: 0.000196  loss: 1.1311 (1.1330)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:119]  [420/431]  eta: 0:00:12  lr: 0.000196  loss: 1.0747 (1.1332)  time: 1.1022  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:119]  [430/431]  eta: 0:00:01  lr: 0.000196  loss: 1.1063 (1.1330)  time: 1.1130  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:119] Total time: 0:08:00 (1.1145 s / it)\n",
      "Averaged stats: lr: 0.000196  loss: 1.1063 (1.1330)\n",
      "Valid: [epoch:119]  [ 0/14]  eta: 0:00:36  loss: 1.1118 (1.1118)  time: 2.5745  data: 2.4019  max mem: 15925\n",
      "Valid: [epoch:119]  [13/14]  eta: 0:00:00  loss: 1.0580 (1.0696)  time: 0.2841  data: 0.1717  max mem: 15925\n",
      "Valid: [epoch:119] Total time: 0:00:04 (0.3008 s / it)\n",
      "Averaged stats: loss: 1.0580 (1.0696)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_119_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.070%\n",
      "Min loss: 1.070\n",
      "Best Epoch: 119.000\n",
      "Train: [epoch:120]  [  0/431]  eta: 0:29:44  lr: 0.000196  loss: 1.0237 (1.0237)  time: 4.1409  data: 2.8959  max mem: 15925\n",
      "Train: [epoch:120]  [ 10/431]  eta: 0:09:21  lr: 0.000196  loss: 1.1524 (1.1693)  time: 1.3337  data: 0.2635  max mem: 15925\n",
      "Train: [epoch:120]  [ 20/431]  eta: 0:08:17  lr: 0.000196  loss: 1.1524 (1.1584)  time: 1.0635  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [ 30/431]  eta: 0:07:49  lr: 0.000196  loss: 1.0958 (1.1322)  time: 1.0808  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [ 40/431]  eta: 0:07:31  lr: 0.000196  loss: 1.0757 (1.1234)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [ 50/431]  eta: 0:07:17  lr: 0.000196  loss: 1.1189 (1.1349)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [ 60/431]  eta: 0:07:02  lr: 0.000196  loss: 1.0939 (1.1265)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [ 70/431]  eta: 0:06:49  lr: 0.000196  loss: 1.0817 (1.1281)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [ 80/431]  eta: 0:06:37  lr: 0.000196  loss: 1.1395 (1.1275)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [ 90/431]  eta: 0:06:25  lr: 0.000196  loss: 1.0975 (1.1221)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [100/431]  eta: 0:06:13  lr: 0.000196  loss: 1.1005 (1.1245)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [110/431]  eta: 0:06:01  lr: 0.000196  loss: 1.0950 (1.1251)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [120/431]  eta: 0:05:50  lr: 0.000196  loss: 1.0546 (1.1239)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [130/431]  eta: 0:05:38  lr: 0.000196  loss: 1.0552 (1.1245)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [140/431]  eta: 0:05:26  lr: 0.000196  loss: 1.1123 (1.1263)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [150/431]  eta: 0:05:14  lr: 0.000196  loss: 1.1116 (1.1256)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [160/431]  eta: 0:05:03  lr: 0.000196  loss: 1.1116 (1.1258)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [170/431]  eta: 0:04:52  lr: 0.000196  loss: 1.1277 (1.1273)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [180/431]  eta: 0:04:41  lr: 0.000196  loss: 1.0956 (1.1279)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [190/431]  eta: 0:04:29  lr: 0.000196  loss: 1.0799 (1.1266)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [200/431]  eta: 0:04:18  lr: 0.000196  loss: 1.0737 (1.1287)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [210/431]  eta: 0:04:07  lr: 0.000196  loss: 1.1040 (1.1285)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [220/431]  eta: 0:03:55  lr: 0.000196  loss: 1.1171 (1.1292)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [230/431]  eta: 0:03:44  lr: 0.000196  loss: 1.1160 (1.1298)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [240/431]  eta: 0:03:33  lr: 0.000196  loss: 1.1126 (1.1323)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [250/431]  eta: 0:03:22  lr: 0.000196  loss: 1.1155 (1.1327)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [260/431]  eta: 0:03:10  lr: 0.000196  loss: 1.1155 (1.1324)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [270/431]  eta: 0:02:59  lr: 0.000196  loss: 1.1495 (1.1344)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [280/431]  eta: 0:02:48  lr: 0.000196  loss: 1.0998 (1.1318)  time: 1.1125  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:120]  [290/431]  eta: 0:02:37  lr: 0.000196  loss: 1.0473 (1.1314)  time: 1.1129  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:120]  [300/431]  eta: 0:02:26  lr: 0.000196  loss: 1.1237 (1.1344)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [310/431]  eta: 0:02:14  lr: 0.000196  loss: 1.1252 (1.1353)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [320/431]  eta: 0:02:03  lr: 0.000196  loss: 1.0952 (1.1360)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [330/431]  eta: 0:01:52  lr: 0.000196  loss: 1.1324 (1.1377)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [340/431]  eta: 0:01:41  lr: 0.000196  loss: 1.1166 (1.1375)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [350/431]  eta: 0:01:30  lr: 0.000196  loss: 1.1383 (1.1400)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [360/431]  eta: 0:01:19  lr: 0.000196  loss: 1.0953 (1.1380)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [370/431]  eta: 0:01:08  lr: 0.000196  loss: 1.0417 (1.1372)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [380/431]  eta: 0:00:56  lr: 0.000196  loss: 1.0868 (1.1359)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [390/431]  eta: 0:00:45  lr: 0.000196  loss: 1.0924 (1.1348)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [400/431]  eta: 0:00:34  lr: 0.000196  loss: 1.0924 (1.1343)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:120]  [410/431]  eta: 0:00:23  lr: 0.000196  loss: 1.0699 (1.1332)  time: 1.1158  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:120]  [420/431]  eta: 0:00:12  lr: 0.000196  loss: 1.0699 (1.1330)  time: 1.1052  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:120]  [430/431]  eta: 0:00:01  lr: 0.000196  loss: 1.0820 (1.1324)  time: 1.1062  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:120] Total time: 0:08:00 (1.1151 s / it)\n",
      "Averaged stats: lr: 0.000196  loss: 1.0820 (1.1324)\n",
      "Valid: [epoch:120]  [ 0/14]  eta: 0:00:34  loss: 1.1354 (1.1354)  time: 2.4924  data: 2.3583  max mem: 15925\n",
      "Valid: [epoch:120]  [13/14]  eta: 0:00:00  loss: 1.0814 (1.0866)  time: 0.2714  data: 0.1699  max mem: 15925\n",
      "Valid: [epoch:120] Total time: 0:00:04 (0.2873 s / it)\n",
      "Averaged stats: loss: 1.0814 (1.0866)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_120_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.087%\n",
      "Min loss: 1.070\n",
      "Best Epoch: 119.000\n",
      "Train: [epoch:121]  [  0/431]  eta: 0:30:58  lr: 0.000196  loss: 1.0823 (1.0823)  time: 4.3127  data: 3.1051  max mem: 15925\n",
      "Train: [epoch:121]  [ 10/431]  eta: 0:09:31  lr: 0.000196  loss: 1.1782 (1.1794)  time: 1.3566  data: 0.2825  max mem: 15925\n",
      "Train: [epoch:121]  [ 20/431]  eta: 0:08:21  lr: 0.000196  loss: 1.1733 (1.1472)  time: 1.0668  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:121]  [ 30/431]  eta: 0:07:53  lr: 0.000196  loss: 1.0825 (1.1339)  time: 1.0827  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [ 40/431]  eta: 0:07:32  lr: 0.000196  loss: 1.0825 (1.1284)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [ 50/431]  eta: 0:07:16  lr: 0.000196  loss: 1.0809 (1.1268)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [ 60/431]  eta: 0:07:03  lr: 0.000196  loss: 1.0750 (1.1192)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [ 70/431]  eta: 0:06:50  lr: 0.000196  loss: 1.0423 (1.1116)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [ 80/431]  eta: 0:06:37  lr: 0.000196  loss: 1.0802 (1.1153)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [ 90/431]  eta: 0:06:25  lr: 0.000196  loss: 1.1113 (1.1230)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [100/431]  eta: 0:06:12  lr: 0.000196  loss: 1.0950 (1.1208)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [110/431]  eta: 0:06:01  lr: 0.000196  loss: 1.0924 (1.1194)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [120/431]  eta: 0:05:49  lr: 0.000196  loss: 1.1172 (1.1191)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [130/431]  eta: 0:05:38  lr: 0.000196  loss: 1.0817 (1.1189)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [140/431]  eta: 0:05:26  lr: 0.000196  loss: 1.1040 (1.1214)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [150/431]  eta: 0:05:15  lr: 0.000196  loss: 1.1206 (1.1238)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [160/431]  eta: 0:05:03  lr: 0.000196  loss: 1.1622 (1.1292)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [170/431]  eta: 0:04:52  lr: 0.000196  loss: 1.1249 (1.1282)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [180/431]  eta: 0:04:40  lr: 0.000196  loss: 1.0683 (1.1263)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [190/431]  eta: 0:04:29  lr: 0.000196  loss: 1.0633 (1.1256)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [200/431]  eta: 0:04:18  lr: 0.000196  loss: 1.0669 (1.1254)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [210/431]  eta: 0:04:07  lr: 0.000196  loss: 1.1081 (1.1253)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [220/431]  eta: 0:03:55  lr: 0.000196  loss: 1.2086 (1.1310)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [230/431]  eta: 0:03:44  lr: 0.000196  loss: 1.2145 (1.1322)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [240/431]  eta: 0:03:33  lr: 0.000196  loss: 1.1286 (1.1324)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [250/431]  eta: 0:03:22  lr: 0.000196  loss: 1.0794 (1.1298)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [260/431]  eta: 0:03:11  lr: 0.000196  loss: 1.1084 (1.1321)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [270/431]  eta: 0:02:59  lr: 0.000196  loss: 1.1159 (1.1326)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [280/431]  eta: 0:02:48  lr: 0.000196  loss: 1.0717 (1.1302)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [290/431]  eta: 0:02:37  lr: 0.000196  loss: 1.0616 (1.1300)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [300/431]  eta: 0:02:26  lr: 0.000196  loss: 1.1316 (1.1294)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [310/431]  eta: 0:02:14  lr: 0.000196  loss: 1.1316 (1.1313)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [320/431]  eta: 0:02:03  lr: 0.000196  loss: 1.0959 (1.1297)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [330/431]  eta: 0:01:52  lr: 0.000196  loss: 1.1011 (1.1336)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [340/431]  eta: 0:01:41  lr: 0.000196  loss: 1.1596 (1.1339)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [350/431]  eta: 0:01:30  lr: 0.000196  loss: 1.1132 (1.1344)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [360/431]  eta: 0:01:19  lr: 0.000196  loss: 1.1061 (1.1319)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [370/431]  eta: 0:01:07  lr: 0.000196  loss: 1.0827 (1.1319)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [380/431]  eta: 0:00:56  lr: 0.000196  loss: 1.0951 (1.1318)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [390/431]  eta: 0:00:45  lr: 0.000196  loss: 1.0889 (1.1314)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [400/431]  eta: 0:00:34  lr: 0.000196  loss: 1.1380 (1.1329)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [410/431]  eta: 0:00:23  lr: 0.000196  loss: 1.1380 (1.1325)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:121]  [420/431]  eta: 0:00:12  lr: 0.000196  loss: 1.1292 (1.1335)  time: 1.1091  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:121]  [430/431]  eta: 0:00:01  lr: 0.000196  loss: 1.1079 (1.1329)  time: 1.1104  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:121] Total time: 0:08:00 (1.1150 s / it)\n",
      "Averaged stats: lr: 0.000196  loss: 1.1079 (1.1329)\n",
      "Valid: [epoch:121]  [ 0/14]  eta: 0:00:34  loss: 1.1678 (1.1678)  time: 2.4856  data: 2.3247  max mem: 15925\n",
      "Valid: [epoch:121]  [13/14]  eta: 0:00:00  loss: 1.0622 (1.0720)  time: 0.2587  data: 0.1661  max mem: 15925\n",
      "Valid: [epoch:121] Total time: 0:00:03 (0.2757 s / it)\n",
      "Averaged stats: loss: 1.0622 (1.0720)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_121_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.070\n",
      "Best Epoch: 119.000\n",
      "Train: [epoch:122]  [  0/431]  eta: 0:33:23  lr: 0.000195  loss: 0.9746 (0.9746)  time: 4.6491  data: 3.4756  max mem: 15925\n",
      "Train: [epoch:122]  [ 10/431]  eta: 0:09:37  lr: 0.000195  loss: 1.1289 (1.1231)  time: 1.3715  data: 0.3161  max mem: 15925\n",
      "Train: [epoch:122]  [ 20/431]  eta: 0:08:24  lr: 0.000195  loss: 1.1043 (1.1196)  time: 1.0559  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [ 30/431]  eta: 0:07:52  lr: 0.000195  loss: 1.0853 (1.1169)  time: 1.0709  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [ 40/431]  eta: 0:07:32  lr: 0.000195  loss: 1.0781 (1.1145)  time: 1.0825  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [ 50/431]  eta: 0:07:15  lr: 0.000195  loss: 1.0673 (1.1032)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [ 60/431]  eta: 0:07:01  lr: 0.000195  loss: 1.0864 (1.1093)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [ 70/431]  eta: 0:06:48  lr: 0.000195  loss: 1.1175 (1.1179)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [ 80/431]  eta: 0:06:36  lr: 0.000195  loss: 1.1274 (1.1208)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [ 90/431]  eta: 0:06:23  lr: 0.000195  loss: 1.0938 (1.1185)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [100/431]  eta: 0:06:12  lr: 0.000195  loss: 1.0926 (1.1206)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [110/431]  eta: 0:06:00  lr: 0.000195  loss: 1.0710 (1.1129)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [120/431]  eta: 0:05:48  lr: 0.000195  loss: 1.0526 (1.1106)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [130/431]  eta: 0:05:37  lr: 0.000195  loss: 1.1163 (1.1135)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [140/431]  eta: 0:05:26  lr: 0.000195  loss: 1.0911 (1.1107)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [150/431]  eta: 0:05:14  lr: 0.000195  loss: 1.0770 (1.1136)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [160/431]  eta: 0:05:03  lr: 0.000195  loss: 1.1352 (1.1181)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [170/431]  eta: 0:04:52  lr: 0.000195  loss: 1.1406 (1.1218)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [180/431]  eta: 0:04:41  lr: 0.000195  loss: 1.0926 (1.1203)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [190/431]  eta: 0:04:29  lr: 0.000195  loss: 1.0591 (1.1228)  time: 1.1144  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:122]  [200/431]  eta: 0:04:18  lr: 0.000195  loss: 1.1160 (1.1228)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [210/431]  eta: 0:04:07  lr: 0.000195  loss: 1.1283 (1.1222)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [220/431]  eta: 0:03:56  lr: 0.000195  loss: 1.1151 (1.1220)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [230/431]  eta: 0:03:44  lr: 0.000195  loss: 1.0738 (1.1205)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [240/431]  eta: 0:03:33  lr: 0.000195  loss: 1.1020 (1.1227)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [250/431]  eta: 0:03:22  lr: 0.000195  loss: 1.1769 (1.1257)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [260/431]  eta: 0:03:11  lr: 0.000195  loss: 1.1440 (1.1254)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [270/431]  eta: 0:02:59  lr: 0.000195  loss: 1.1240 (1.1264)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [280/431]  eta: 0:02:48  lr: 0.000195  loss: 1.1418 (1.1283)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [290/431]  eta: 0:02:37  lr: 0.000195  loss: 1.1373 (1.1279)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [300/431]  eta: 0:02:26  lr: 0.000195  loss: 1.1364 (1.1283)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [310/431]  eta: 0:02:15  lr: 0.000195  loss: 1.1173 (1.1271)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [320/431]  eta: 0:02:03  lr: 0.000195  loss: 1.1059 (1.1282)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [330/431]  eta: 0:01:52  lr: 0.000195  loss: 1.1273 (1.1304)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [340/431]  eta: 0:01:41  lr: 0.000195  loss: 1.1147 (1.1306)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [350/431]  eta: 0:01:30  lr: 0.000195  loss: 1.0792 (1.1311)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [360/431]  eta: 0:01:19  lr: 0.000195  loss: 1.1988 (1.1342)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [370/431]  eta: 0:01:08  lr: 0.000195  loss: 1.1812 (1.1336)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [380/431]  eta: 0:00:56  lr: 0.000195  loss: 1.0538 (1.1315)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [390/431]  eta: 0:00:45  lr: 0.000195  loss: 1.0705 (1.1316)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [400/431]  eta: 0:00:34  lr: 0.000195  loss: 1.1106 (1.1314)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [410/431]  eta: 0:00:23  lr: 0.000195  loss: 1.1141 (1.1311)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:122]  [420/431]  eta: 0:00:12  lr: 0.000195  loss: 1.1280 (1.1318)  time: 1.1043  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:122]  [430/431]  eta: 0:00:01  lr: 0.000195  loss: 1.0857 (1.1307)  time: 1.1158  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:122] Total time: 0:08:00 (1.1160 s / it)\n",
      "Averaged stats: lr: 0.000195  loss: 1.0857 (1.1307)\n",
      "Valid: [epoch:122]  [ 0/14]  eta: 0:00:37  loss: 1.0019 (1.0019)  time: 2.7064  data: 2.5622  max mem: 15925\n",
      "Valid: [epoch:122]  [13/14]  eta: 0:00:00  loss: 1.0625 (1.0719)  time: 0.2948  data: 0.1831  max mem: 15925\n",
      "Valid: [epoch:122] Total time: 0:00:04 (0.3124 s / it)\n",
      "Averaged stats: loss: 1.0625 (1.0719)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_122_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.070\n",
      "Best Epoch: 119.000\n",
      "Train: [epoch:123]  [  0/431]  eta: 0:32:36  lr: 0.000195  loss: 1.0349 (1.0349)  time: 4.5401  data: 3.2932  max mem: 15925\n",
      "Train: [epoch:123]  [ 10/431]  eta: 0:09:36  lr: 0.000195  loss: 1.0915 (1.1156)  time: 1.3697  data: 0.2996  max mem: 15925\n",
      "Train: [epoch:123]  [ 20/431]  eta: 0:08:24  lr: 0.000195  loss: 1.1635 (1.1932)  time: 1.0620  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [ 30/431]  eta: 0:07:51  lr: 0.000195  loss: 1.1627 (1.1738)  time: 1.0700  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [ 40/431]  eta: 0:07:32  lr: 0.000195  loss: 1.0863 (1.1653)  time: 1.0824  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [ 50/431]  eta: 0:07:15  lr: 0.000195  loss: 1.1311 (1.1615)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [ 60/431]  eta: 0:07:02  lr: 0.000195  loss: 1.1107 (1.1486)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [ 70/431]  eta: 0:06:49  lr: 0.000195  loss: 1.1088 (1.1446)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [ 80/431]  eta: 0:06:36  lr: 0.000195  loss: 1.1069 (1.1419)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [ 90/431]  eta: 0:06:25  lr: 0.000195  loss: 1.0972 (1.1411)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [100/431]  eta: 0:06:13  lr: 0.000195  loss: 1.0445 (1.1331)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [110/431]  eta: 0:06:02  lr: 0.000195  loss: 1.0352 (1.1282)  time: 1.1245  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [120/431]  eta: 0:05:51  lr: 0.000195  loss: 1.0436 (1.1287)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [130/431]  eta: 0:05:39  lr: 0.000195  loss: 1.0436 (1.1242)  time: 1.1329  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [140/431]  eta: 0:05:28  lr: 0.000195  loss: 1.0642 (1.1267)  time: 1.1316  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [150/431]  eta: 0:05:17  lr: 0.000195  loss: 1.0959 (1.1274)  time: 1.1276  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [160/431]  eta: 0:05:05  lr: 0.000195  loss: 1.1256 (1.1304)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [170/431]  eta: 0:04:54  lr: 0.000195  loss: 1.1256 (1.1294)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [180/431]  eta: 0:04:42  lr: 0.000195  loss: 1.0953 (1.1290)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [190/431]  eta: 0:04:31  lr: 0.000195  loss: 1.0953 (1.1286)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [200/431]  eta: 0:04:19  lr: 0.000195  loss: 1.1221 (1.1300)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [210/431]  eta: 0:04:08  lr: 0.000195  loss: 1.0979 (1.1277)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [220/431]  eta: 0:03:56  lr: 0.000195  loss: 1.1225 (1.1309)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [230/431]  eta: 0:03:45  lr: 0.000195  loss: 1.1871 (1.1343)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [240/431]  eta: 0:03:34  lr: 0.000195  loss: 1.1221 (1.1341)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [250/431]  eta: 0:03:23  lr: 0.000195  loss: 1.1445 (1.1355)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [260/431]  eta: 0:03:11  lr: 0.000195  loss: 1.1070 (1.1341)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [270/431]  eta: 0:03:00  lr: 0.000195  loss: 1.1070 (1.1351)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [280/431]  eta: 0:02:49  lr: 0.000195  loss: 1.1563 (1.1340)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [290/431]  eta: 0:02:38  lr: 0.000195  loss: 1.0865 (1.1327)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [300/431]  eta: 0:02:26  lr: 0.000195  loss: 1.1223 (1.1337)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [310/431]  eta: 0:02:15  lr: 0.000195  loss: 1.1335 (1.1326)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [320/431]  eta: 0:02:04  lr: 0.000195  loss: 1.2057 (1.1356)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [330/431]  eta: 0:01:53  lr: 0.000195  loss: 1.1853 (1.1345)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [340/431]  eta: 0:01:41  lr: 0.000195  loss: 1.0715 (1.1349)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [350/431]  eta: 0:01:30  lr: 0.000195  loss: 1.1145 (1.1363)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [360/431]  eta: 0:01:19  lr: 0.000195  loss: 1.1690 (1.1369)  time: 1.1118  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:123]  [370/431]  eta: 0:01:08  lr: 0.000195  loss: 1.1238 (1.1366)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [380/431]  eta: 0:00:57  lr: 0.000195  loss: 1.1166 (1.1357)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [390/431]  eta: 0:00:45  lr: 0.000195  loss: 1.0633 (1.1338)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [400/431]  eta: 0:00:34  lr: 0.000195  loss: 1.0633 (1.1335)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:123]  [410/431]  eta: 0:00:23  lr: 0.000195  loss: 1.0994 (1.1337)  time: 1.1059  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:123]  [420/431]  eta: 0:00:12  lr: 0.000195  loss: 1.1080 (1.1334)  time: 1.1127  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:123]  [430/431]  eta: 0:00:01  lr: 0.000195  loss: 1.1249 (1.1339)  time: 1.1096  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:123] Total time: 0:08:02 (1.1187 s / it)\n",
      "Averaged stats: lr: 0.000195  loss: 1.1249 (1.1339)\n",
      "Valid: [epoch:123]  [ 0/14]  eta: 0:00:35  loss: 1.0346 (1.0346)  time: 2.5527  data: 2.3866  max mem: 15925\n",
      "Valid: [epoch:123]  [13/14]  eta: 0:00:00  loss: 1.0568 (1.0693)  time: 0.2764  data: 0.1707  max mem: 15925\n",
      "Valid: [epoch:123] Total time: 0:00:04 (0.2920 s / it)\n",
      "Averaged stats: loss: 1.0568 (1.0693)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_123_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.069%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:124]  [  0/431]  eta: 0:30:34  lr: 0.000195  loss: 1.2366 (1.2366)  time: 4.2555  data: 2.9909  max mem: 15925\n",
      "Train: [epoch:124]  [ 10/431]  eta: 0:09:25  lr: 0.000195  loss: 1.1722 (1.1544)  time: 1.3421  data: 0.2721  max mem: 15925\n",
      "Train: [epoch:124]  [ 20/431]  eta: 0:08:19  lr: 0.000195  loss: 1.1722 (1.1827)  time: 1.0637  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [ 30/431]  eta: 0:07:51  lr: 0.000195  loss: 1.1834 (1.1884)  time: 1.0833  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [ 40/431]  eta: 0:07:30  lr: 0.000195  loss: 1.0967 (1.1595)  time: 1.0862  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [ 50/431]  eta: 0:07:16  lr: 0.000195  loss: 1.0548 (1.1542)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [ 60/431]  eta: 0:07:02  lr: 0.000195  loss: 1.0543 (1.1401)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [ 70/431]  eta: 0:06:51  lr: 0.000195  loss: 1.0543 (1.1363)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [ 80/431]  eta: 0:06:39  lr: 0.000195  loss: 1.0930 (1.1373)  time: 1.1333  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [ 90/431]  eta: 0:06:26  lr: 0.000195  loss: 1.1079 (1.1342)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [100/431]  eta: 0:06:14  lr: 0.000195  loss: 1.0938 (1.1334)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [110/431]  eta: 0:06:02  lr: 0.000195  loss: 1.1141 (1.1311)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [120/431]  eta: 0:05:51  lr: 0.000195  loss: 1.0791 (1.1265)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [130/431]  eta: 0:05:39  lr: 0.000195  loss: 1.0791 (1.1267)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [140/431]  eta: 0:05:27  lr: 0.000195  loss: 1.0996 (1.1256)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [150/431]  eta: 0:05:16  lr: 0.000195  loss: 1.1644 (1.1282)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [160/431]  eta: 0:05:04  lr: 0.000195  loss: 1.1479 (1.1277)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [170/431]  eta: 0:04:53  lr: 0.000195  loss: 1.1472 (1.1306)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [180/431]  eta: 0:04:41  lr: 0.000195  loss: 1.1464 (1.1291)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [190/431]  eta: 0:04:30  lr: 0.000195  loss: 1.0949 (1.1278)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [200/431]  eta: 0:04:19  lr: 0.000195  loss: 1.1300 (1.1293)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [210/431]  eta: 0:04:07  lr: 0.000195  loss: 1.1311 (1.1305)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [220/431]  eta: 0:03:56  lr: 0.000195  loss: 1.1016 (1.1285)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [230/431]  eta: 0:03:45  lr: 0.000195  loss: 1.0853 (1.1298)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [240/431]  eta: 0:03:34  lr: 0.000195  loss: 1.1115 (1.1313)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [250/431]  eta: 0:03:22  lr: 0.000195  loss: 1.1016 (1.1296)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [260/431]  eta: 0:03:11  lr: 0.000195  loss: 1.1016 (1.1299)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [270/431]  eta: 0:03:00  lr: 0.000195  loss: 1.1704 (1.1330)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [280/431]  eta: 0:02:49  lr: 0.000195  loss: 1.2018 (1.1349)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [290/431]  eta: 0:02:37  lr: 0.000195  loss: 1.1844 (1.1350)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [300/431]  eta: 0:02:26  lr: 0.000195  loss: 1.1781 (1.1363)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [310/431]  eta: 0:02:15  lr: 0.000195  loss: 1.1038 (1.1351)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [320/431]  eta: 0:02:04  lr: 0.000195  loss: 1.0775 (1.1349)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [330/431]  eta: 0:01:53  lr: 0.000195  loss: 1.0973 (1.1354)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [340/431]  eta: 0:01:41  lr: 0.000195  loss: 1.0973 (1.1347)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [350/431]  eta: 0:01:30  lr: 0.000195  loss: 1.1336 (1.1358)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [360/431]  eta: 0:01:19  lr: 0.000195  loss: 1.1098 (1.1336)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [370/431]  eta: 0:01:08  lr: 0.000195  loss: 1.0495 (1.1315)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [380/431]  eta: 0:00:57  lr: 0.000195  loss: 1.0737 (1.1303)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [390/431]  eta: 0:00:45  lr: 0.000195  loss: 1.0760 (1.1305)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [400/431]  eta: 0:00:34  lr: 0.000195  loss: 1.1309 (1.1313)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [410/431]  eta: 0:00:23  lr: 0.000195  loss: 1.1192 (1.1316)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:124]  [420/431]  eta: 0:00:12  lr: 0.000195  loss: 1.1131 (1.1324)  time: 1.1085  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:124]  [430/431]  eta: 0:00:01  lr: 0.000195  loss: 1.0946 (1.1323)  time: 1.1148  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:124] Total time: 0:08:02 (1.1189 s / it)\n",
      "Averaged stats: lr: 0.000195  loss: 1.0946 (1.1323)\n",
      "Valid: [epoch:124]  [ 0/14]  eta: 0:00:35  loss: 0.9482 (0.9482)  time: 2.5654  data: 2.4083  max mem: 15925\n",
      "Valid: [epoch:124]  [13/14]  eta: 0:00:00  loss: 1.0579 (1.0701)  time: 0.2718  data: 0.1721  max mem: 15925\n",
      "Valid: [epoch:124] Total time: 0:00:04 (0.2877 s / it)\n",
      "Averaged stats: loss: 1.0579 (1.0701)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_124_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.070%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:125]  [  0/431]  eta: 0:31:36  lr: 0.000195  loss: 1.2096 (1.2096)  time: 4.3994  data: 3.1951  max mem: 15925\n",
      "Train: [epoch:125]  [ 10/431]  eta: 0:09:30  lr: 0.000195  loss: 1.2258 (1.2273)  time: 1.3560  data: 0.2906  max mem: 15925\n",
      "Train: [epoch:125]  [ 20/431]  eta: 0:08:21  lr: 0.000195  loss: 1.1726 (1.1787)  time: 1.0616  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [ 30/431]  eta: 0:07:51  lr: 0.000195  loss: 1.0875 (1.1518)  time: 1.0763  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [ 40/431]  eta: 0:07:32  lr: 0.000195  loss: 1.0527 (1.1348)  time: 1.0932  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:125]  [ 50/431]  eta: 0:07:17  lr: 0.000195  loss: 1.0737 (1.1296)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [ 60/431]  eta: 0:07:03  lr: 0.000195  loss: 1.0877 (1.1257)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [ 70/431]  eta: 0:06:51  lr: 0.000195  loss: 1.1152 (1.1272)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [ 80/431]  eta: 0:06:38  lr: 0.000195  loss: 1.1422 (1.1334)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [ 90/431]  eta: 0:06:26  lr: 0.000195  loss: 1.1015 (1.1295)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [100/431]  eta: 0:06:14  lr: 0.000195  loss: 1.0718 (1.1213)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [110/431]  eta: 0:06:02  lr: 0.000195  loss: 1.0423 (1.1163)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [120/431]  eta: 0:05:51  lr: 0.000195  loss: 1.0768 (1.1176)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [130/431]  eta: 0:05:39  lr: 0.000195  loss: 1.1360 (1.1217)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [140/431]  eta: 0:05:28  lr: 0.000195  loss: 1.1162 (1.1199)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [150/431]  eta: 0:05:16  lr: 0.000195  loss: 1.1393 (1.1260)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [160/431]  eta: 0:05:05  lr: 0.000195  loss: 1.1308 (1.1212)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [170/431]  eta: 0:04:53  lr: 0.000195  loss: 1.0126 (1.1184)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [180/431]  eta: 0:04:42  lr: 0.000195  loss: 1.0600 (1.1220)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [190/431]  eta: 0:04:30  lr: 0.000195  loss: 1.1337 (1.1243)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [200/431]  eta: 0:04:19  lr: 0.000195  loss: 1.1337 (1.1285)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [210/431]  eta: 0:04:08  lr: 0.000195  loss: 1.1847 (1.1299)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [220/431]  eta: 0:03:56  lr: 0.000195  loss: 1.1159 (1.1291)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [230/431]  eta: 0:03:45  lr: 0.000195  loss: 1.1159 (1.1294)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [240/431]  eta: 0:03:34  lr: 0.000195  loss: 1.1342 (1.1318)  time: 1.1312  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [250/431]  eta: 0:03:23  lr: 0.000195  loss: 1.0961 (1.1302)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [260/431]  eta: 0:03:12  lr: 0.000195  loss: 1.0694 (1.1299)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [270/431]  eta: 0:03:00  lr: 0.000195  loss: 1.1535 (1.1319)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [280/431]  eta: 0:02:49  lr: 0.000195  loss: 1.1484 (1.1311)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [290/431]  eta: 0:02:38  lr: 0.000195  loss: 1.1185 (1.1320)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [300/431]  eta: 0:02:26  lr: 0.000195  loss: 1.0942 (1.1318)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [310/431]  eta: 0:02:15  lr: 0.000195  loss: 1.1116 (1.1340)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [320/431]  eta: 0:02:04  lr: 0.000195  loss: 1.1323 (1.1340)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [330/431]  eta: 0:01:53  lr: 0.000195  loss: 1.1398 (1.1366)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [340/431]  eta: 0:01:41  lr: 0.000195  loss: 1.1587 (1.1360)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [350/431]  eta: 0:01:30  lr: 0.000195  loss: 1.0960 (1.1362)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [360/431]  eta: 0:01:19  lr: 0.000195  loss: 1.0954 (1.1346)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [370/431]  eta: 0:01:08  lr: 0.000195  loss: 1.0871 (1.1346)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [380/431]  eta: 0:00:57  lr: 0.000195  loss: 1.0789 (1.1330)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [390/431]  eta: 0:00:45  lr: 0.000195  loss: 1.1150 (1.1347)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [400/431]  eta: 0:00:34  lr: 0.000195  loss: 1.1448 (1.1340)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:125]  [410/431]  eta: 0:00:23  lr: 0.000195  loss: 1.1100 (1.1337)  time: 1.1068  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:125]  [420/431]  eta: 0:00:12  lr: 0.000195  loss: 1.1380 (1.1350)  time: 1.1083  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:125]  [430/431]  eta: 0:00:01  lr: 0.000195  loss: 1.0684 (1.1335)  time: 1.1208  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:125] Total time: 0:08:02 (1.1197 s / it)\n",
      "Averaged stats: lr: 0.000195  loss: 1.0684 (1.1335)\n",
      "Valid: [epoch:125]  [ 0/14]  eta: 0:00:37  loss: 0.9768 (0.9768)  time: 2.6942  data: 2.5128  max mem: 15925\n",
      "Valid: [epoch:125]  [13/14]  eta: 0:00:00  loss: 1.0620 (1.0725)  time: 0.2883  data: 0.1796  max mem: 15925\n",
      "Valid: [epoch:125] Total time: 0:00:04 (0.3053 s / it)\n",
      "Averaged stats: loss: 1.0620 (1.0725)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_125_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:126]  [  0/431]  eta: 0:34:57  lr: 0.000194  loss: 1.3221 (1.3221)  time: 4.8671  data: 3.7215  max mem: 15925\n",
      "Train: [epoch:126]  [ 10/431]  eta: 0:09:51  lr: 0.000194  loss: 1.1053 (1.1481)  time: 1.4049  data: 0.3385  max mem: 15925\n",
      "Train: [epoch:126]  [ 20/431]  eta: 0:08:30  lr: 0.000194  loss: 1.1285 (1.1926)  time: 1.0613  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [ 30/431]  eta: 0:07:56  lr: 0.000194  loss: 1.1164 (1.1451)  time: 1.0705  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [ 40/431]  eta: 0:07:36  lr: 0.000194  loss: 1.0279 (1.1452)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [ 50/431]  eta: 0:07:19  lr: 0.000194  loss: 1.0630 (1.1355)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [ 60/431]  eta: 0:07:05  lr: 0.000194  loss: 1.0557 (1.1267)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [ 70/431]  eta: 0:06:51  lr: 0.000194  loss: 1.0660 (1.1213)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [ 80/431]  eta: 0:06:38  lr: 0.000194  loss: 1.0787 (1.1177)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [ 90/431]  eta: 0:06:26  lr: 0.000194  loss: 1.1231 (1.1241)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [100/431]  eta: 0:06:14  lr: 0.000194  loss: 1.1347 (1.1267)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [110/431]  eta: 0:06:02  lr: 0.000194  loss: 1.0784 (1.1231)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [120/431]  eta: 0:05:50  lr: 0.000194  loss: 1.1121 (1.1254)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [130/431]  eta: 0:05:39  lr: 0.000194  loss: 1.1320 (1.1246)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [140/431]  eta: 0:05:27  lr: 0.000194  loss: 1.0468 (1.1216)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [150/431]  eta: 0:05:16  lr: 0.000194  loss: 1.1087 (1.1238)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [160/431]  eta: 0:05:05  lr: 0.000194  loss: 1.1332 (1.1258)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [170/431]  eta: 0:04:53  lr: 0.000194  loss: 1.0624 (1.1236)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [180/431]  eta: 0:04:42  lr: 0.000194  loss: 1.0992 (1.1273)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [190/431]  eta: 0:04:31  lr: 0.000194  loss: 1.1524 (1.1270)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [200/431]  eta: 0:04:20  lr: 0.000194  loss: 1.1337 (1.1270)  time: 1.1339  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [210/431]  eta: 0:04:08  lr: 0.000194  loss: 1.1386 (1.1290)  time: 1.1270  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:126]  [220/431]  eta: 0:03:57  lr: 0.000194  loss: 1.1098 (1.1296)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [230/431]  eta: 0:03:45  lr: 0.000194  loss: 1.0934 (1.1274)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [240/431]  eta: 0:03:34  lr: 0.000194  loss: 1.0693 (1.1248)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [250/431]  eta: 0:03:23  lr: 0.000194  loss: 1.0856 (1.1260)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [260/431]  eta: 0:03:12  lr: 0.000194  loss: 1.1425 (1.1271)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [270/431]  eta: 0:03:00  lr: 0.000194  loss: 1.1053 (1.1266)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [280/431]  eta: 0:02:49  lr: 0.000194  loss: 1.0763 (1.1271)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [290/431]  eta: 0:02:38  lr: 0.000194  loss: 1.0592 (1.1258)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [300/431]  eta: 0:02:26  lr: 0.000194  loss: 1.0684 (1.1260)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [310/431]  eta: 0:02:15  lr: 0.000194  loss: 1.0841 (1.1251)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [320/431]  eta: 0:02:04  lr: 0.000194  loss: 1.0977 (1.1254)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [330/431]  eta: 0:01:53  lr: 0.000194  loss: 1.1460 (1.1268)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [340/431]  eta: 0:01:41  lr: 0.000194  loss: 1.1460 (1.1280)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [350/431]  eta: 0:01:30  lr: 0.000194  loss: 1.1414 (1.1283)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [360/431]  eta: 0:01:19  lr: 0.000194  loss: 1.1386 (1.1295)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [370/431]  eta: 0:01:08  lr: 0.000194  loss: 1.1386 (1.1296)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [380/431]  eta: 0:00:57  lr: 0.000194  loss: 1.1016 (1.1296)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [390/431]  eta: 0:00:45  lr: 0.000194  loss: 1.0927 (1.1301)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:126]  [400/431]  eta: 0:00:34  lr: 0.000194  loss: 1.1514 (1.1308)  time: 1.1164  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:126]  [410/431]  eta: 0:00:23  lr: 0.000194  loss: 1.1357 (1.1309)  time: 1.1147  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:126]  [420/431]  eta: 0:00:12  lr: 0.000194  loss: 1.1095 (1.1299)  time: 1.1127  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:126]  [430/431]  eta: 0:00:01  lr: 0.000194  loss: 1.1003 (1.1307)  time: 1.1075  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:126] Total time: 0:08:02 (1.1206 s / it)\n",
      "Averaged stats: lr: 0.000194  loss: 1.1003 (1.1307)\n",
      "Valid: [epoch:126]  [ 0/14]  eta: 0:00:35  loss: 1.0325 (1.0325)  time: 2.5124  data: 2.3700  max mem: 15925\n",
      "Valid: [epoch:126]  [13/14]  eta: 0:00:00  loss: 1.0637 (1.0728)  time: 0.2661  data: 0.1697  max mem: 15925\n",
      "Valid: [epoch:126] Total time: 0:00:03 (0.2842 s / it)\n",
      "Averaged stats: loss: 1.0637 (1.0728)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_126_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.073%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:127]  [  0/431]  eta: 0:29:57  lr: 0.000194  loss: 1.1759 (1.1759)  time: 4.1700  data: 2.9676  max mem: 15925\n",
      "Train: [epoch:127]  [ 10/431]  eta: 0:09:21  lr: 0.000194  loss: 1.2088 (1.1929)  time: 1.3332  data: 0.2699  max mem: 15925\n",
      "Train: [epoch:127]  [ 20/431]  eta: 0:08:15  lr: 0.000194  loss: 1.1936 (1.1738)  time: 1.0579  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [ 30/431]  eta: 0:07:47  lr: 0.000194  loss: 1.0989 (1.1471)  time: 1.0743  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [ 40/431]  eta: 0:07:29  lr: 0.000194  loss: 1.0834 (1.1409)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [ 50/431]  eta: 0:07:14  lr: 0.000194  loss: 1.0756 (1.1370)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [ 60/431]  eta: 0:07:00  lr: 0.000194  loss: 1.1182 (1.1333)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [ 70/431]  eta: 0:06:48  lr: 0.000194  loss: 1.1378 (1.1347)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [ 80/431]  eta: 0:06:36  lr: 0.000194  loss: 1.1264 (1.1346)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [ 90/431]  eta: 0:06:24  lr: 0.000194  loss: 1.1264 (1.1336)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [100/431]  eta: 0:06:13  lr: 0.000194  loss: 1.0881 (1.1300)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [110/431]  eta: 0:06:01  lr: 0.000194  loss: 1.0916 (1.1275)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [120/431]  eta: 0:05:49  lr: 0.000194  loss: 1.1022 (1.1251)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [130/431]  eta: 0:05:38  lr: 0.000194  loss: 1.1182 (1.1257)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [140/431]  eta: 0:05:26  lr: 0.000194  loss: 1.0848 (1.1232)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [150/431]  eta: 0:05:15  lr: 0.000194  loss: 1.0605 (1.1234)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [160/431]  eta: 0:05:03  lr: 0.000194  loss: 1.0813 (1.1234)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [170/431]  eta: 0:04:52  lr: 0.000194  loss: 1.1100 (1.1256)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [180/431]  eta: 0:04:41  lr: 0.000194  loss: 1.1064 (1.1241)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [190/431]  eta: 0:04:29  lr: 0.000194  loss: 1.0649 (1.1206)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [200/431]  eta: 0:04:18  lr: 0.000194  loss: 1.0815 (1.1211)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [210/431]  eta: 0:04:07  lr: 0.000194  loss: 1.1041 (1.1230)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [220/431]  eta: 0:03:55  lr: 0.000194  loss: 1.1436 (1.1230)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [230/431]  eta: 0:03:44  lr: 0.000194  loss: 1.1636 (1.1269)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [240/431]  eta: 0:03:33  lr: 0.000194  loss: 1.2087 (1.1278)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [250/431]  eta: 0:03:22  lr: 0.000194  loss: 1.1023 (1.1274)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [260/431]  eta: 0:03:11  lr: 0.000194  loss: 1.1352 (1.1293)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [270/431]  eta: 0:02:59  lr: 0.000194  loss: 1.1451 (1.1314)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [280/431]  eta: 0:02:48  lr: 0.000194  loss: 1.1110 (1.1302)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [290/431]  eta: 0:02:37  lr: 0.000194  loss: 1.1229 (1.1305)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [300/431]  eta: 0:02:26  lr: 0.000194  loss: 1.1425 (1.1319)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [310/431]  eta: 0:02:15  lr: 0.000194  loss: 1.1108 (1.1305)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [320/431]  eta: 0:02:03  lr: 0.000194  loss: 1.1086 (1.1306)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [330/431]  eta: 0:01:52  lr: 0.000194  loss: 1.1193 (1.1321)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [340/431]  eta: 0:01:41  lr: 0.000194  loss: 1.1327 (1.1334)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [350/431]  eta: 0:01:30  lr: 0.000194  loss: 1.1327 (1.1339)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [360/431]  eta: 0:01:19  lr: 0.000194  loss: 1.1113 (1.1352)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [370/431]  eta: 0:01:08  lr: 0.000194  loss: 1.1072 (1.1350)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [380/431]  eta: 0:00:56  lr: 0.000194  loss: 1.1046 (1.1339)  time: 1.1052  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:127]  [390/431]  eta: 0:00:45  lr: 0.000194  loss: 1.1311 (1.1334)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [400/431]  eta: 0:00:34  lr: 0.000194  loss: 1.1140 (1.1323)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:127]  [410/431]  eta: 0:00:23  lr: 0.000194  loss: 1.1055 (1.1330)  time: 1.1002  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:127]  [420/431]  eta: 0:00:12  lr: 0.000194  loss: 1.1055 (1.1326)  time: 1.1011  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:127]  [430/431]  eta: 0:00:01  lr: 0.000194  loss: 1.1050 (1.1323)  time: 1.1044  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:127] Total time: 0:08:00 (1.1157 s / it)\n",
      "Averaged stats: lr: 0.000194  loss: 1.1050 (1.1323)\n",
      "Valid: [epoch:127]  [ 0/14]  eta: 0:00:37  loss: 1.0234 (1.0234)  time: 2.6748  data: 2.5207  max mem: 15925\n",
      "Valid: [epoch:127]  [13/14]  eta: 0:00:00  loss: 1.0843 (1.0913)  time: 0.2729  data: 0.1801  max mem: 15925\n",
      "Valid: [epoch:127] Total time: 0:00:04 (0.2919 s / it)\n",
      "Averaged stats: loss: 1.0843 (1.0913)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_127_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.091%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:128]  [  0/431]  eta: 0:32:25  lr: 0.000194  loss: 1.2788 (1.2788)  time: 4.5147  data: 3.2575  max mem: 15925\n",
      "Train: [epoch:128]  [ 10/431]  eta: 0:09:35  lr: 0.000194  loss: 1.1044 (1.1242)  time: 1.3666  data: 0.2964  max mem: 15925\n",
      "Train: [epoch:128]  [ 20/431]  eta: 0:08:23  lr: 0.000194  loss: 1.0575 (1.1154)  time: 1.0605  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [ 30/431]  eta: 0:07:53  lr: 0.000194  loss: 1.0470 (1.0975)  time: 1.0800  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [ 40/431]  eta: 0:07:34  lr: 0.000194  loss: 1.0453 (1.0952)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [ 50/431]  eta: 0:07:19  lr: 0.000194  loss: 1.0820 (1.1002)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [ 60/431]  eta: 0:07:04  lr: 0.000194  loss: 1.0619 (1.0932)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [ 70/431]  eta: 0:06:51  lr: 0.000194  loss: 1.0619 (1.1026)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [ 80/431]  eta: 0:06:39  lr: 0.000194  loss: 1.0832 (1.1060)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [ 90/431]  eta: 0:06:27  lr: 0.000194  loss: 1.1347 (1.1075)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [100/431]  eta: 0:06:14  lr: 0.000194  loss: 1.1101 (1.1095)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [110/431]  eta: 0:06:03  lr: 0.000194  loss: 1.1027 (1.1072)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [120/431]  eta: 0:05:50  lr: 0.000194  loss: 1.0658 (1.1081)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [130/431]  eta: 0:05:39  lr: 0.000194  loss: 1.0844 (1.1086)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [140/431]  eta: 0:05:27  lr: 0.000194  loss: 1.1314 (1.1141)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [150/431]  eta: 0:05:16  lr: 0.000194  loss: 1.1314 (1.1134)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [160/431]  eta: 0:05:04  lr: 0.000194  loss: 1.0929 (1.1148)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [170/431]  eta: 0:04:53  lr: 0.000194  loss: 1.1262 (1.1179)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [180/431]  eta: 0:04:42  lr: 0.000194  loss: 1.1086 (1.1169)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [190/431]  eta: 0:04:30  lr: 0.000194  loss: 1.0937 (1.1175)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [200/431]  eta: 0:04:19  lr: 0.000194  loss: 1.0923 (1.1175)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [210/431]  eta: 0:04:08  lr: 0.000194  loss: 1.1015 (1.1169)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [220/431]  eta: 0:03:56  lr: 0.000194  loss: 1.1015 (1.1195)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [230/431]  eta: 0:03:45  lr: 0.000194  loss: 1.1384 (1.1221)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [240/431]  eta: 0:03:34  lr: 0.000194  loss: 1.1718 (1.1251)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [250/431]  eta: 0:03:22  lr: 0.000194  loss: 1.1599 (1.1244)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [260/431]  eta: 0:03:11  lr: 0.000194  loss: 1.0869 (1.1235)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [270/431]  eta: 0:03:00  lr: 0.000194  loss: 1.0930 (1.1231)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [280/431]  eta: 0:02:49  lr: 0.000194  loss: 1.0930 (1.1224)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [290/431]  eta: 0:02:37  lr: 0.000194  loss: 1.0687 (1.1198)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [300/431]  eta: 0:02:26  lr: 0.000194  loss: 1.0824 (1.1210)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [310/431]  eta: 0:02:15  lr: 0.000194  loss: 1.1427 (1.1216)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [320/431]  eta: 0:02:04  lr: 0.000194  loss: 1.1849 (1.1232)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [330/431]  eta: 0:01:53  lr: 0.000194  loss: 1.1084 (1.1222)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [340/431]  eta: 0:01:41  lr: 0.000194  loss: 1.1064 (1.1242)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [350/431]  eta: 0:01:30  lr: 0.000194  loss: 1.1632 (1.1241)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [360/431]  eta: 0:01:19  lr: 0.000194  loss: 1.1144 (1.1252)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [370/431]  eta: 0:01:08  lr: 0.000194  loss: 1.1144 (1.1255)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [380/431]  eta: 0:00:57  lr: 0.000194  loss: 1.1350 (1.1271)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [390/431]  eta: 0:00:45  lr: 0.000194  loss: 1.1350 (1.1271)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [400/431]  eta: 0:00:34  lr: 0.000194  loss: 1.1290 (1.1291)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [410/431]  eta: 0:00:23  lr: 0.000194  loss: 1.1469 (1.1303)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:128]  [420/431]  eta: 0:00:12  lr: 0.000194  loss: 1.0979 (1.1293)  time: 1.1176  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:128]  [430/431]  eta: 0:00:01  lr: 0.000194  loss: 1.0793 (1.1313)  time: 1.1188  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:128] Total time: 0:08:02 (1.1188 s / it)\n",
      "Averaged stats: lr: 0.000194  loss: 1.0793 (1.1313)\n",
      "Valid: [epoch:128]  [ 0/14]  eta: 0:00:36  loss: 1.1395 (1.1395)  time: 2.6337  data: 2.4982  max mem: 15925\n",
      "Valid: [epoch:128]  [13/14]  eta: 0:00:00  loss: 1.0760 (1.0828)  time: 0.2911  data: 0.1785  max mem: 15925\n",
      "Valid: [epoch:128] Total time: 0:00:04 (0.3097 s / it)\n",
      "Averaged stats: loss: 1.0760 (1.0828)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_128_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.083%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:129]  [  0/431]  eta: 0:36:08  lr: 0.000194  loss: 1.2078 (1.2078)  time: 5.0310  data: 3.8487  max mem: 15925\n",
      "Train: [epoch:129]  [ 10/431]  eta: 0:09:49  lr: 0.000194  loss: 1.1537 (1.1626)  time: 1.4011  data: 0.3501  max mem: 15925\n",
      "Train: [epoch:129]  [ 20/431]  eta: 0:08:31  lr: 0.000194  loss: 1.1491 (1.1758)  time: 1.0554  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [ 30/431]  eta: 0:07:57  lr: 0.000194  loss: 1.1620 (1.1665)  time: 1.0758  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [ 40/431]  eta: 0:07:36  lr: 0.000194  loss: 1.0362 (1.1391)  time: 1.0886  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [ 50/431]  eta: 0:07:19  lr: 0.000194  loss: 1.0354 (1.1234)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [ 60/431]  eta: 0:07:06  lr: 0.000194  loss: 1.0319 (1.1127)  time: 1.1078  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:129]  [ 70/431]  eta: 0:06:52  lr: 0.000194  loss: 1.1094 (1.1182)  time: 1.1098  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:129]  [ 80/431]  eta: 0:06:39  lr: 0.000194  loss: 1.1233 (1.1197)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [ 90/431]  eta: 0:06:27  lr: 0.000194  loss: 1.1266 (1.1219)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [100/431]  eta: 0:06:15  lr: 0.000194  loss: 1.1080 (1.1188)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [110/431]  eta: 0:06:02  lr: 0.000194  loss: 1.0833 (1.1188)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [120/431]  eta: 0:05:50  lr: 0.000194  loss: 1.0789 (1.1202)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [130/431]  eta: 0:05:39  lr: 0.000194  loss: 1.0910 (1.1184)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [140/431]  eta: 0:05:27  lr: 0.000194  loss: 1.1094 (1.1212)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [150/431]  eta: 0:05:16  lr: 0.000194  loss: 1.0845 (1.1195)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [160/431]  eta: 0:05:04  lr: 0.000194  loss: 1.0798 (1.1188)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [170/431]  eta: 0:04:52  lr: 0.000194  loss: 1.0820 (1.1215)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [180/431]  eta: 0:04:41  lr: 0.000194  loss: 1.0867 (1.1217)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [190/431]  eta: 0:04:30  lr: 0.000194  loss: 1.0974 (1.1225)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [200/431]  eta: 0:04:18  lr: 0.000194  loss: 1.0872 (1.1208)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [210/431]  eta: 0:04:07  lr: 0.000194  loss: 1.1063 (1.1213)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [220/431]  eta: 0:03:56  lr: 0.000194  loss: 1.1194 (1.1218)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [230/431]  eta: 0:03:44  lr: 0.000194  loss: 1.1194 (1.1217)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [240/431]  eta: 0:03:33  lr: 0.000194  loss: 1.1126 (1.1236)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [250/431]  eta: 0:03:22  lr: 0.000194  loss: 1.1253 (1.1254)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [260/431]  eta: 0:03:11  lr: 0.000194  loss: 1.1588 (1.1252)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [270/431]  eta: 0:03:00  lr: 0.000194  loss: 1.0709 (1.1229)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [280/431]  eta: 0:02:48  lr: 0.000194  loss: 1.0678 (1.1223)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [290/431]  eta: 0:02:37  lr: 0.000194  loss: 1.0795 (1.1225)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [300/431]  eta: 0:02:26  lr: 0.000194  loss: 1.0960 (1.1254)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [310/431]  eta: 0:02:15  lr: 0.000194  loss: 1.1216 (1.1263)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [320/431]  eta: 0:02:03  lr: 0.000194  loss: 1.1216 (1.1270)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [330/431]  eta: 0:01:52  lr: 0.000194  loss: 1.1660 (1.1297)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [340/431]  eta: 0:01:41  lr: 0.000194  loss: 1.1103 (1.1294)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [350/431]  eta: 0:01:30  lr: 0.000194  loss: 1.0654 (1.1281)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [360/431]  eta: 0:01:19  lr: 0.000194  loss: 1.1277 (1.1307)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [370/431]  eta: 0:01:08  lr: 0.000194  loss: 1.1497 (1.1311)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [380/431]  eta: 0:00:56  lr: 0.000194  loss: 1.1252 (1.1314)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [390/431]  eta: 0:00:45  lr: 0.000194  loss: 1.0933 (1.1305)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [400/431]  eta: 0:00:34  lr: 0.000194  loss: 1.0703 (1.1293)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:129]  [410/431]  eta: 0:00:23  lr: 0.000194  loss: 1.1203 (1.1309)  time: 1.1082  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:129]  [420/431]  eta: 0:00:12  lr: 0.000194  loss: 1.1815 (1.1317)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:129]  [430/431]  eta: 0:00:01  lr: 0.000194  loss: 1.1224 (1.1314)  time: 1.1105  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:129] Total time: 0:08:00 (1.1152 s / it)\n",
      "Averaged stats: lr: 0.000194  loss: 1.1224 (1.1314)\n",
      "Valid: [epoch:129]  [ 0/14]  eta: 0:00:36  loss: 1.1158 (1.1158)  time: 2.5956  data: 2.4457  max mem: 15925\n",
      "Valid: [epoch:129]  [13/14]  eta: 0:00:00  loss: 1.0611 (1.0736)  time: 0.2719  data: 0.1748  max mem: 15925\n",
      "Valid: [epoch:129] Total time: 0:00:04 (0.2881 s / it)\n",
      "Averaged stats: loss: 1.0611 (1.0736)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_129_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:130]  [  0/431]  eta: 0:32:54  lr: 0.000194  loss: 1.1558 (1.1558)  time: 4.5822  data: 3.3909  max mem: 15925\n",
      "Train: [epoch:130]  [ 10/431]  eta: 0:09:37  lr: 0.000194  loss: 1.2112 (1.2303)  time: 1.3721  data: 0.3085  max mem: 15925\n",
      "Train: [epoch:130]  [ 20/431]  eta: 0:08:24  lr: 0.000194  loss: 1.1350 (1.1847)  time: 1.0608  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [ 30/431]  eta: 0:07:53  lr: 0.000194  loss: 1.0830 (1.1751)  time: 1.0739  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [ 40/431]  eta: 0:07:32  lr: 0.000194  loss: 1.0640 (1.1502)  time: 1.0842  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [ 50/431]  eta: 0:07:17  lr: 0.000194  loss: 1.0522 (1.1325)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [ 60/431]  eta: 0:07:03  lr: 0.000194  loss: 1.1017 (1.1462)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [ 70/431]  eta: 0:06:50  lr: 0.000194  loss: 1.1312 (1.1452)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [ 80/431]  eta: 0:06:37  lr: 0.000194  loss: 1.1459 (1.1459)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [ 90/431]  eta: 0:06:25  lr: 0.000194  loss: 1.1459 (1.1421)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [100/431]  eta: 0:06:13  lr: 0.000194  loss: 1.0696 (1.1400)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [110/431]  eta: 0:06:01  lr: 0.000194  loss: 1.1398 (1.1454)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [120/431]  eta: 0:05:49  lr: 0.000194  loss: 1.1398 (1.1432)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [130/431]  eta: 0:05:37  lr: 0.000194  loss: 1.0856 (1.1421)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [140/431]  eta: 0:05:26  lr: 0.000194  loss: 1.1045 (1.1405)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [150/431]  eta: 0:05:14  lr: 0.000194  loss: 1.0791 (1.1365)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [160/431]  eta: 0:05:03  lr: 0.000194  loss: 1.1217 (1.1408)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [170/431]  eta: 0:04:52  lr: 0.000194  loss: 1.1741 (1.1390)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [180/431]  eta: 0:04:40  lr: 0.000194  loss: 1.0975 (1.1382)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [190/431]  eta: 0:04:29  lr: 0.000194  loss: 1.0808 (1.1367)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [200/431]  eta: 0:04:18  lr: 0.000194  loss: 1.0845 (1.1386)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [210/431]  eta: 0:04:07  lr: 0.000194  loss: 1.1060 (1.1348)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [220/431]  eta: 0:03:55  lr: 0.000194  loss: 1.0743 (1.1348)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [230/431]  eta: 0:03:44  lr: 0.000194  loss: 1.1395 (1.1367)  time: 1.1116  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:130]  [240/431]  eta: 0:03:33  lr: 0.000194  loss: 1.1081 (1.1349)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [250/431]  eta: 0:03:22  lr: 0.000194  loss: 1.0998 (1.1356)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [260/431]  eta: 0:03:10  lr: 0.000194  loss: 1.1500 (1.1377)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [270/431]  eta: 0:02:59  lr: 0.000194  loss: 1.1248 (1.1385)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [280/431]  eta: 0:02:48  lr: 0.000194  loss: 1.0869 (1.1361)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [290/431]  eta: 0:02:37  lr: 0.000194  loss: 1.0724 (1.1344)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [300/431]  eta: 0:02:26  lr: 0.000194  loss: 1.1332 (1.1364)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [310/431]  eta: 0:02:14  lr: 0.000194  loss: 1.1659 (1.1367)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [320/431]  eta: 0:02:03  lr: 0.000194  loss: 1.0802 (1.1347)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [330/431]  eta: 0:01:52  lr: 0.000194  loss: 1.0802 (1.1358)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [340/431]  eta: 0:01:41  lr: 0.000194  loss: 1.1413 (1.1363)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [350/431]  eta: 0:01:30  lr: 0.000194  loss: 1.1413 (1.1361)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [360/431]  eta: 0:01:19  lr: 0.000194  loss: 1.0753 (1.1352)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [370/431]  eta: 0:01:07  lr: 0.000194  loss: 1.0613 (1.1332)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [380/431]  eta: 0:00:56  lr: 0.000194  loss: 1.0613 (1.1331)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [390/431]  eta: 0:00:45  lr: 0.000194  loss: 1.0720 (1.1321)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [400/431]  eta: 0:00:34  lr: 0.000194  loss: 1.0720 (1.1314)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [410/431]  eta: 0:00:23  lr: 0.000194  loss: 1.1103 (1.1318)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:130]  [420/431]  eta: 0:00:12  lr: 0.000194  loss: 1.0985 (1.1320)  time: 1.1114  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:130]  [430/431]  eta: 0:00:01  lr: 0.000194  loss: 1.0669 (1.1310)  time: 1.1063  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:130] Total time: 0:07:59 (1.1136 s / it)\n",
      "Averaged stats: lr: 0.000194  loss: 1.0669 (1.1310)\n",
      "Valid: [epoch:130]  [ 0/14]  eta: 0:00:36  loss: 1.1697 (1.1697)  time: 2.6032  data: 2.4337  max mem: 15925\n",
      "Valid: [epoch:130]  [13/14]  eta: 0:00:00  loss: 1.0576 (1.0714)  time: 0.2593  data: 0.1739  max mem: 15925\n",
      "Valid: [epoch:130] Total time: 0:00:03 (0.2740 s / it)\n",
      "Averaged stats: loss: 1.0576 (1.0714)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_130_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.071%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:131]  [  0/431]  eta: 0:34:42  lr: 0.000193  loss: 1.0893 (1.0893)  time: 4.8323  data: 3.7470  max mem: 15925\n",
      "Train: [epoch:131]  [ 10/431]  eta: 0:09:42  lr: 0.000193  loss: 1.1525 (1.1486)  time: 1.3839  data: 0.3408  max mem: 15925\n",
      "Train: [epoch:131]  [ 20/431]  eta: 0:08:24  lr: 0.000193  loss: 1.1205 (1.1604)  time: 1.0473  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [ 30/431]  eta: 0:07:51  lr: 0.000193  loss: 1.0790 (1.1411)  time: 1.0613  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [ 40/431]  eta: 0:07:31  lr: 0.000193  loss: 1.0765 (1.1259)  time: 1.0764  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:131]  [ 50/431]  eta: 0:07:14  lr: 0.000193  loss: 1.1267 (1.1433)  time: 1.0878  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [ 60/431]  eta: 0:07:01  lr: 0.000193  loss: 1.1130 (1.1379)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [ 70/431]  eta: 0:06:48  lr: 0.000193  loss: 1.0998 (1.1337)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [ 80/431]  eta: 0:06:36  lr: 0.000193  loss: 1.0771 (1.1272)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [ 90/431]  eta: 0:06:24  lr: 0.000193  loss: 1.0490 (1.1216)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [100/431]  eta: 0:06:12  lr: 0.000193  loss: 1.0644 (1.1228)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [110/431]  eta: 0:06:01  lr: 0.000193  loss: 1.1443 (1.1273)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [120/431]  eta: 0:05:49  lr: 0.000193  loss: 1.1778 (1.1299)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [130/431]  eta: 0:05:38  lr: 0.000193  loss: 1.1469 (1.1297)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [140/431]  eta: 0:05:26  lr: 0.000193  loss: 1.0952 (1.1279)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [150/431]  eta: 0:05:14  lr: 0.000193  loss: 1.0908 (1.1266)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [160/431]  eta: 0:05:03  lr: 0.000193  loss: 1.1071 (1.1254)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [170/431]  eta: 0:04:52  lr: 0.000193  loss: 1.0798 (1.1243)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [180/431]  eta: 0:04:40  lr: 0.000193  loss: 1.1625 (1.1267)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [190/431]  eta: 0:04:29  lr: 0.000193  loss: 1.1324 (1.1272)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [200/431]  eta: 0:04:18  lr: 0.000193  loss: 1.0994 (1.1245)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [210/431]  eta: 0:04:07  lr: 0.000193  loss: 1.0299 (1.1233)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [220/431]  eta: 0:03:55  lr: 0.000193  loss: 1.0794 (1.1235)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [230/431]  eta: 0:03:44  lr: 0.000193  loss: 1.1321 (1.1271)  time: 1.1245  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:131]  [240/431]  eta: 0:03:33  lr: 0.000193  loss: 1.1754 (1.1271)  time: 1.1357  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:131]  [250/431]  eta: 0:03:22  lr: 0.000193  loss: 1.0642 (1.1257)  time: 1.1305  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:131]  [260/431]  eta: 0:03:11  lr: 0.000193  loss: 1.0642 (1.1239)  time: 1.1265  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [270/431]  eta: 0:03:00  lr: 0.000193  loss: 1.1037 (1.1259)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [280/431]  eta: 0:02:49  lr: 0.000193  loss: 1.2164 (1.1290)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [290/431]  eta: 0:02:37  lr: 0.000193  loss: 1.2052 (1.1285)  time: 1.1273  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:131]  [300/431]  eta: 0:02:26  lr: 0.000193  loss: 1.1517 (1.1302)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [310/431]  eta: 0:02:15  lr: 0.000193  loss: 1.1517 (1.1306)  time: 1.1269  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [320/431]  eta: 0:02:04  lr: 0.000193  loss: 1.0751 (1.1304)  time: 1.1302  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [330/431]  eta: 0:01:53  lr: 0.000193  loss: 1.1838 (1.1335)  time: 1.1367  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:131]  [340/431]  eta: 0:01:42  lr: 0.000193  loss: 1.1636 (1.1340)  time: 1.1259  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:131]  [350/431]  eta: 0:01:30  lr: 0.000193  loss: 1.1314 (1.1338)  time: 1.1188  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:131]  [360/431]  eta: 0:01:19  lr: 0.000193  loss: 1.1105 (1.1336)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [370/431]  eta: 0:01:08  lr: 0.000193  loss: 1.0714 (1.1325)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [380/431]  eta: 0:00:57  lr: 0.000193  loss: 1.0674 (1.1314)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [390/431]  eta: 0:00:45  lr: 0.000193  loss: 1.1170 (1.1322)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [400/431]  eta: 0:00:34  lr: 0.000193  loss: 1.0986 (1.1311)  time: 1.1129  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:131]  [410/431]  eta: 0:00:23  lr: 0.000193  loss: 1.0747 (1.1311)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [420/431]  eta: 0:00:12  lr: 0.000193  loss: 1.1117 (1.1315)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131]  [430/431]  eta: 0:00:01  lr: 0.000193  loss: 1.1301 (1.1319)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:131] Total time: 0:08:03 (1.1216 s / it)\n",
      "Averaged stats: lr: 0.000193  loss: 1.1301 (1.1319)\n",
      "Valid: [epoch:131]  [ 0/14]  eta: 0:00:38  loss: 1.1712 (1.1712)  time: 2.7382  data: 2.5776  max mem: 15925\n",
      "Valid: [epoch:131]  [13/14]  eta: 0:00:00  loss: 1.0597 (1.0712)  time: 0.2970  data: 0.1842  max mem: 15925\n",
      "Valid: [epoch:131] Total time: 0:00:04 (0.3157 s / it)\n",
      "Averaged stats: loss: 1.0597 (1.0712)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_131_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.071%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:132]  [  0/431]  eta: 0:29:56  lr: 0.000193  loss: 1.1458 (1.1458)  time: 4.1678  data: 2.9809  max mem: 15925\n",
      "Train: [epoch:132]  [ 10/431]  eta: 0:09:30  lr: 0.000193  loss: 1.1458 (1.1469)  time: 1.3551  data: 0.2712  max mem: 15925\n",
      "Train: [epoch:132]  [ 20/431]  eta: 0:08:27  lr: 0.000193  loss: 1.1326 (1.1449)  time: 1.0887  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [ 30/431]  eta: 0:07:58  lr: 0.000193  loss: 1.1326 (1.1505)  time: 1.1058  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [ 40/431]  eta: 0:07:40  lr: 0.000193  loss: 1.1086 (1.1399)  time: 1.1198  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [ 50/431]  eta: 0:07:25  lr: 0.000193  loss: 1.0896 (1.1404)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [ 60/431]  eta: 0:07:11  lr: 0.000193  loss: 1.0841 (1.1259)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [ 70/431]  eta: 0:06:58  lr: 0.000193  loss: 1.0823 (1.1284)  time: 1.1291  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [ 80/431]  eta: 0:06:45  lr: 0.000193  loss: 1.1193 (1.1285)  time: 1.1313  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [ 90/431]  eta: 0:06:33  lr: 0.000193  loss: 1.0934 (1.1237)  time: 1.1383  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [100/431]  eta: 0:06:20  lr: 0.000193  loss: 1.0276 (1.1158)  time: 1.1329  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [110/431]  eta: 0:06:08  lr: 0.000193  loss: 1.0560 (1.1202)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [120/431]  eta: 0:05:56  lr: 0.000193  loss: 1.1152 (1.1194)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [130/431]  eta: 0:05:44  lr: 0.000193  loss: 1.0917 (1.1201)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [140/431]  eta: 0:05:32  lr: 0.000193  loss: 1.0630 (1.1162)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [150/431]  eta: 0:05:20  lr: 0.000193  loss: 1.1260 (1.1212)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [160/431]  eta: 0:05:08  lr: 0.000193  loss: 1.1314 (1.1205)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [170/431]  eta: 0:04:57  lr: 0.000193  loss: 1.1370 (1.1248)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [180/431]  eta: 0:04:45  lr: 0.000193  loss: 1.1465 (1.1238)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [190/431]  eta: 0:04:34  lr: 0.000193  loss: 1.0636 (1.1203)  time: 1.1247  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [200/431]  eta: 0:04:22  lr: 0.000193  loss: 1.0865 (1.1217)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [210/431]  eta: 0:04:10  lr: 0.000193  loss: 1.0944 (1.1228)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [220/431]  eta: 0:03:59  lr: 0.000193  loss: 1.1041 (1.1228)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [230/431]  eta: 0:03:48  lr: 0.000193  loss: 1.0631 (1.1208)  time: 1.1262  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [240/431]  eta: 0:03:36  lr: 0.000193  loss: 1.0745 (1.1222)  time: 1.1369  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [250/431]  eta: 0:03:25  lr: 0.000193  loss: 1.0843 (1.1204)  time: 1.1342  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [260/431]  eta: 0:03:13  lr: 0.000193  loss: 1.1272 (1.1233)  time: 1.1231  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [270/431]  eta: 0:03:02  lr: 0.000193  loss: 1.1739 (1.1257)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [280/431]  eta: 0:02:51  lr: 0.000193  loss: 1.1999 (1.1274)  time: 1.1450  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [290/431]  eta: 0:02:40  lr: 0.000193  loss: 1.1549 (1.1270)  time: 1.1460  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [300/431]  eta: 0:02:28  lr: 0.000193  loss: 1.1176 (1.1277)  time: 1.1294  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [310/431]  eta: 0:02:17  lr: 0.000193  loss: 1.1185 (1.1282)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [320/431]  eta: 0:02:05  lr: 0.000193  loss: 1.1185 (1.1265)  time: 1.1272  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [330/431]  eta: 0:01:54  lr: 0.000193  loss: 1.1270 (1.1279)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [340/431]  eta: 0:01:43  lr: 0.000193  loss: 1.1336 (1.1293)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [350/431]  eta: 0:01:31  lr: 0.000193  loss: 1.1391 (1.1301)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [360/431]  eta: 0:01:20  lr: 0.000193  loss: 1.1391 (1.1305)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [370/431]  eta: 0:01:09  lr: 0.000193  loss: 1.1471 (1.1308)  time: 1.1358  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [380/431]  eta: 0:00:57  lr: 0.000193  loss: 1.1092 (1.1308)  time: 1.1351  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [390/431]  eta: 0:00:46  lr: 0.000193  loss: 1.0894 (1.1301)  time: 1.1399  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:132]  [400/431]  eta: 0:00:35  lr: 0.000193  loss: 1.1037 (1.1310)  time: 1.1420  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [410/431]  eta: 0:00:23  lr: 0.000193  loss: 1.1246 (1.1314)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [420/431]  eta: 0:00:12  lr: 0.000193  loss: 1.0768 (1.1308)  time: 1.1285  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132]  [430/431]  eta: 0:00:01  lr: 0.000193  loss: 1.0831 (1.1314)  time: 1.1381  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:132] Total time: 0:08:08 (1.1341 s / it)\n",
      "Averaged stats: lr: 0.000193  loss: 1.0831 (1.1314)\n",
      "Valid: [epoch:132]  [ 0/14]  eta: 0:00:37  loss: 1.1222 (1.1222)  time: 2.6456  data: 2.4825  max mem: 15925\n",
      "Valid: [epoch:132]  [13/14]  eta: 0:00:00  loss: 1.0683 (1.0824)  time: 0.2973  data: 0.1774  max mem: 15925\n",
      "Valid: [epoch:132] Total time: 0:00:04 (0.3146 s / it)\n",
      "Averaged stats: loss: 1.0683 (1.0824)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_132_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.082%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:133]  [  0/431]  eta: 0:32:43  lr: 0.000193  loss: 1.1190 (1.1190)  time: 4.5567  data: 3.3993  max mem: 15925\n",
      "Train: [epoch:133]  [ 10/431]  eta: 0:09:41  lr: 0.000193  loss: 1.1621 (1.1882)  time: 1.3819  data: 0.3092  max mem: 15925\n",
      "Train: [epoch:133]  [ 20/431]  eta: 0:08:32  lr: 0.000193  loss: 1.1621 (1.1799)  time: 1.0821  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [ 30/431]  eta: 0:07:58  lr: 0.000193  loss: 1.1375 (1.1646)  time: 1.0881  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [ 40/431]  eta: 0:07:40  lr: 0.000193  loss: 1.0885 (1.1609)  time: 1.1049  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [ 50/431]  eta: 0:07:26  lr: 0.000193  loss: 1.1669 (1.1549)  time: 1.1376  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [ 60/431]  eta: 0:07:13  lr: 0.000193  loss: 1.0628 (1.1378)  time: 1.1462  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [ 70/431]  eta: 0:06:58  lr: 0.000193  loss: 1.0737 (1.1378)  time: 1.1340  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [ 80/431]  eta: 0:06:45  lr: 0.000193  loss: 1.1104 (1.1371)  time: 1.1178  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:133]  [ 90/431]  eta: 0:06:32  lr: 0.000193  loss: 1.1104 (1.1393)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [100/431]  eta: 0:06:19  lr: 0.000193  loss: 1.1501 (1.1400)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [110/431]  eta: 0:06:06  lr: 0.000193  loss: 1.1378 (1.1379)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [120/431]  eta: 0:05:55  lr: 0.000193  loss: 1.1378 (1.1414)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [130/431]  eta: 0:05:43  lr: 0.000193  loss: 1.1706 (1.1397)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [140/431]  eta: 0:05:31  lr: 0.000193  loss: 1.1740 (1.1459)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [150/431]  eta: 0:05:20  lr: 0.000193  loss: 1.1869 (1.1446)  time: 1.1330  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [160/431]  eta: 0:05:08  lr: 0.000193  loss: 1.1327 (1.1455)  time: 1.1289  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [170/431]  eta: 0:04:56  lr: 0.000193  loss: 1.1329 (1.1468)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [180/431]  eta: 0:04:45  lr: 0.000193  loss: 1.1329 (1.1467)  time: 1.1273  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [190/431]  eta: 0:04:33  lr: 0.000193  loss: 1.0868 (1.1442)  time: 1.1361  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [200/431]  eta: 0:04:22  lr: 0.000193  loss: 1.0941 (1.1466)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [210/431]  eta: 0:04:11  lr: 0.000193  loss: 1.1383 (1.1467)  time: 1.1433  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [220/431]  eta: 0:03:59  lr: 0.000193  loss: 1.0736 (1.1461)  time: 1.1377  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [230/431]  eta: 0:03:48  lr: 0.000193  loss: 1.0736 (1.1456)  time: 1.1270  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [240/431]  eta: 0:03:37  lr: 0.000193  loss: 1.0810 (1.1446)  time: 1.1389  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [250/431]  eta: 0:03:25  lr: 0.000193  loss: 1.0810 (1.1437)  time: 1.1456  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [260/431]  eta: 0:03:14  lr: 0.000193  loss: 1.1156 (1.1435)  time: 1.1433  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [270/431]  eta: 0:03:03  lr: 0.000193  loss: 1.0988 (1.1433)  time: 1.1426  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [280/431]  eta: 0:02:51  lr: 0.000193  loss: 1.0981 (1.1410)  time: 1.1274  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [290/431]  eta: 0:02:40  lr: 0.000193  loss: 1.1302 (1.1414)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [300/431]  eta: 0:02:28  lr: 0.000193  loss: 1.1113 (1.1403)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [310/431]  eta: 0:02:17  lr: 0.000193  loss: 1.0789 (1.1398)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [320/431]  eta: 0:02:06  lr: 0.000193  loss: 1.1077 (1.1386)  time: 1.1380  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [330/431]  eta: 0:01:54  lr: 0.000193  loss: 1.1047 (1.1379)  time: 1.1422  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [340/431]  eta: 0:01:43  lr: 0.000193  loss: 1.0909 (1.1376)  time: 1.1248  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [350/431]  eta: 0:01:31  lr: 0.000193  loss: 1.0861 (1.1383)  time: 1.1235  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [360/431]  eta: 0:01:20  lr: 0.000193  loss: 1.1074 (1.1378)  time: 1.1309  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [370/431]  eta: 0:01:09  lr: 0.000193  loss: 1.1105 (1.1382)  time: 1.1337  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [380/431]  eta: 0:00:57  lr: 0.000193  loss: 1.1105 (1.1376)  time: 1.1359  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [390/431]  eta: 0:00:46  lr: 0.000193  loss: 1.0975 (1.1365)  time: 1.1401  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [400/431]  eta: 0:00:35  lr: 0.000193  loss: 1.0997 (1.1369)  time: 1.1420  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:133]  [410/431]  eta: 0:00:23  lr: 0.000193  loss: 1.0914 (1.1354)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133]  [420/431]  eta: 0:00:12  lr: 0.000193  loss: 1.0498 (1.1336)  time: 1.1157  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:133]  [430/431]  eta: 0:00:01  lr: 0.000193  loss: 1.0498 (1.1331)  time: 1.1394  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:133] Total time: 0:08:09 (1.1355 s / it)\n",
      "Averaged stats: lr: 0.000193  loss: 1.0498 (1.1331)\n",
      "Valid: [epoch:133]  [ 0/14]  eta: 0:00:38  loss: 0.9529 (0.9529)  time: 2.7544  data: 2.5951  max mem: 15925\n",
      "Valid: [epoch:133]  [13/14]  eta: 0:00:00  loss: 1.0626 (1.0727)  time: 0.3163  data: 0.1855  max mem: 15925\n",
      "Valid: [epoch:133] Total time: 0:00:04 (0.3358 s / it)\n",
      "Averaged stats: loss: 1.0626 (1.0727)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_133_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.073%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:134]  [  0/431]  eta: 0:31:17  lr: 0.000193  loss: 1.0606 (1.0606)  time: 4.3557  data: 3.0692  max mem: 15925\n",
      "Train: [epoch:134]  [ 10/431]  eta: 0:09:33  lr: 0.000193  loss: 1.0606 (1.1585)  time: 1.3622  data: 0.2792  max mem: 15925\n",
      "Train: [epoch:134]  [ 20/431]  eta: 0:08:24  lr: 0.000193  loss: 1.1297 (1.1377)  time: 1.0707  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [ 30/431]  eta: 0:07:55  lr: 0.000193  loss: 1.1297 (1.1494)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [ 40/431]  eta: 0:07:36  lr: 0.000193  loss: 1.1069 (1.1419)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [ 50/431]  eta: 0:07:20  lr: 0.000193  loss: 1.0653 (1.1306)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [ 60/431]  eta: 0:07:05  lr: 0.000193  loss: 1.0706 (1.1253)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [ 70/431]  eta: 0:06:53  lr: 0.000193  loss: 1.1001 (1.1263)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [ 80/431]  eta: 0:06:41  lr: 0.000193  loss: 1.1474 (1.1243)  time: 1.1379  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [ 90/431]  eta: 0:06:29  lr: 0.000193  loss: 1.1163 (1.1224)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [100/431]  eta: 0:06:17  lr: 0.000193  loss: 1.1163 (1.1249)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [110/431]  eta: 0:06:05  lr: 0.000193  loss: 1.1161 (1.1251)  time: 1.1181  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [120/431]  eta: 0:05:54  lr: 0.000193  loss: 1.0680 (1.1213)  time: 1.1332  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [130/431]  eta: 0:05:42  lr: 0.000193  loss: 1.0680 (1.1233)  time: 1.1469  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [140/431]  eta: 0:05:31  lr: 0.000193  loss: 1.1178 (1.1272)  time: 1.1338  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [150/431]  eta: 0:05:19  lr: 0.000193  loss: 1.0767 (1.1239)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [160/431]  eta: 0:05:08  lr: 0.000193  loss: 1.0767 (1.1229)  time: 1.1389  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [170/431]  eta: 0:04:57  lr: 0.000193  loss: 1.0989 (1.1233)  time: 1.1427  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [180/431]  eta: 0:04:46  lr: 0.000193  loss: 1.0989 (1.1225)  time: 1.1571  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [190/431]  eta: 0:04:34  lr: 0.000193  loss: 1.1709 (1.1285)  time: 1.1572  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [200/431]  eta: 0:04:23  lr: 0.000193  loss: 1.2255 (1.1300)  time: 1.1384  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [210/431]  eta: 0:04:11  lr: 0.000193  loss: 1.1222 (1.1282)  time: 1.1292  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [220/431]  eta: 0:04:00  lr: 0.000193  loss: 1.0963 (1.1284)  time: 1.1310  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [230/431]  eta: 0:03:48  lr: 0.000193  loss: 1.0926 (1.1278)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [240/431]  eta: 0:03:37  lr: 0.000193  loss: 1.0819 (1.1290)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [250/431]  eta: 0:03:25  lr: 0.000193  loss: 1.0664 (1.1274)  time: 1.1210  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:134]  [260/431]  eta: 0:03:14  lr: 0.000193  loss: 1.1128 (1.1294)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [270/431]  eta: 0:03:02  lr: 0.000193  loss: 1.1774 (1.1322)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [280/431]  eta: 0:02:51  lr: 0.000193  loss: 1.1170 (1.1329)  time: 1.1415  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [290/431]  eta: 0:02:40  lr: 0.000193  loss: 1.1137 (1.1310)  time: 1.1382  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [300/431]  eta: 0:02:28  lr: 0.000193  loss: 1.1308 (1.1323)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [310/431]  eta: 0:02:17  lr: 0.000193  loss: 1.1308 (1.1320)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [320/431]  eta: 0:02:05  lr: 0.000193  loss: 1.1085 (1.1293)  time: 1.1310  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [330/431]  eta: 0:01:54  lr: 0.000193  loss: 1.1027 (1.1292)  time: 1.1353  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [340/431]  eta: 0:01:43  lr: 0.000193  loss: 1.1621 (1.1321)  time: 1.1317  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [350/431]  eta: 0:01:31  lr: 0.000193  loss: 1.1931 (1.1321)  time: 1.1251  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [360/431]  eta: 0:01:20  lr: 0.000193  loss: 1.0909 (1.1321)  time: 1.1291  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [370/431]  eta: 0:01:09  lr: 0.000193  loss: 1.1124 (1.1318)  time: 1.1412  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [380/431]  eta: 0:00:57  lr: 0.000193  loss: 1.0833 (1.1305)  time: 1.1332  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [390/431]  eta: 0:00:46  lr: 0.000193  loss: 1.0943 (1.1319)  time: 1.1326  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [400/431]  eta: 0:00:35  lr: 0.000193  loss: 1.1097 (1.1309)  time: 1.1347  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:134]  [410/431]  eta: 0:00:23  lr: 0.000193  loss: 1.0412 (1.1303)  time: 1.1327  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:134]  [420/431]  eta: 0:00:12  lr: 0.000193  loss: 1.0651 (1.1306)  time: 1.1385  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:134]  [430/431]  eta: 0:00:01  lr: 0.000193  loss: 1.1518 (1.1313)  time: 1.1194  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:134] Total time: 0:08:08 (1.1345 s / it)\n",
      "Averaged stats: lr: 0.000193  loss: 1.1518 (1.1313)\n",
      "Valid: [epoch:134]  [ 0/14]  eta: 0:00:33  loss: 1.1272 (1.1272)  time: 2.4032  data: 2.2345  max mem: 15925\n",
      "Valid: [epoch:134]  [13/14]  eta: 0:00:00  loss: 1.0607 (1.0716)  time: 0.2569  data: 0.1597  max mem: 15925\n",
      "Valid: [epoch:134] Total time: 0:00:03 (0.2717 s / it)\n",
      "Averaged stats: loss: 1.0607 (1.0716)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_134_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:135]  [  0/431]  eta: 0:30:15  lr: 0.000192  loss: 1.0298 (1.0298)  time: 4.2132  data: 2.9707  max mem: 15925\n",
      "Train: [epoch:135]  [ 10/431]  eta: 0:09:31  lr: 0.000192  loss: 1.2331 (1.1649)  time: 1.3571  data: 0.2703  max mem: 15925\n",
      "Train: [epoch:135]  [ 20/431]  eta: 0:08:17  lr: 0.000192  loss: 1.1350 (1.1439)  time: 1.0611  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [ 30/431]  eta: 0:07:51  lr: 0.000192  loss: 1.0982 (1.1351)  time: 1.0745  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [ 40/431]  eta: 0:07:32  lr: 0.000192  loss: 1.0982 (1.1343)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [ 50/431]  eta: 0:07:17  lr: 0.000192  loss: 1.0602 (1.1342)  time: 1.1041  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [ 60/431]  eta: 0:07:03  lr: 0.000192  loss: 1.0730 (1.1324)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [ 70/431]  eta: 0:06:50  lr: 0.000192  loss: 1.0995 (1.1338)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [ 80/431]  eta: 0:06:39  lr: 0.000192  loss: 1.1097 (1.1310)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [ 90/431]  eta: 0:06:27  lr: 0.000192  loss: 1.1120 (1.1302)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [100/431]  eta: 0:06:15  lr: 0.000192  loss: 1.0712 (1.1269)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [110/431]  eta: 0:06:04  lr: 0.000192  loss: 1.1107 (1.1321)  time: 1.1355  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [120/431]  eta: 0:05:52  lr: 0.000192  loss: 1.1313 (1.1335)  time: 1.1357  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [130/431]  eta: 0:05:41  lr: 0.000192  loss: 1.1190 (1.1324)  time: 1.1291  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [140/431]  eta: 0:05:30  lr: 0.000192  loss: 1.1255 (1.1389)  time: 1.1345  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [150/431]  eta: 0:05:19  lr: 0.000192  loss: 1.0688 (1.1308)  time: 1.1517  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [160/431]  eta: 0:05:08  lr: 0.000192  loss: 1.0527 (1.1308)  time: 1.1650  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [170/431]  eta: 0:04:56  lr: 0.000192  loss: 1.0626 (1.1276)  time: 1.1360  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [180/431]  eta: 0:04:45  lr: 0.000192  loss: 1.0626 (1.1270)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [190/431]  eta: 0:04:33  lr: 0.000192  loss: 1.1188 (1.1301)  time: 1.1294  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [200/431]  eta: 0:04:22  lr: 0.000192  loss: 1.0889 (1.1276)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [210/431]  eta: 0:04:10  lr: 0.000192  loss: 1.1005 (1.1294)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [220/431]  eta: 0:03:59  lr: 0.000192  loss: 1.1404 (1.1303)  time: 1.1304  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [230/431]  eta: 0:03:47  lr: 0.000192  loss: 1.1346 (1.1331)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [240/431]  eta: 0:03:36  lr: 0.000192  loss: 1.1026 (1.1328)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [250/431]  eta: 0:03:24  lr: 0.000192  loss: 1.1349 (1.1355)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [260/431]  eta: 0:03:13  lr: 0.000192  loss: 1.1680 (1.1359)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [270/431]  eta: 0:03:02  lr: 0.000192  loss: 1.1056 (1.1355)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [280/431]  eta: 0:02:50  lr: 0.000192  loss: 1.0873 (1.1367)  time: 1.1277  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [290/431]  eta: 0:02:39  lr: 0.000192  loss: 1.1350 (1.1366)  time: 1.1459  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [300/431]  eta: 0:02:28  lr: 0.000192  loss: 1.0874 (1.1359)  time: 1.1480  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [310/431]  eta: 0:02:17  lr: 0.000192  loss: 1.1300 (1.1373)  time: 1.1418  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [320/431]  eta: 0:02:05  lr: 0.000192  loss: 1.1475 (1.1375)  time: 1.1332  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [330/431]  eta: 0:01:54  lr: 0.000192  loss: 1.1383 (1.1373)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [340/431]  eta: 0:01:43  lr: 0.000192  loss: 1.1354 (1.1380)  time: 1.1225  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [350/431]  eta: 0:01:31  lr: 0.000192  loss: 1.1354 (1.1375)  time: 1.1316  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [360/431]  eta: 0:01:20  lr: 0.000192  loss: 1.1061 (1.1370)  time: 1.1341  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:135]  [370/431]  eta: 0:01:09  lr: 0.000192  loss: 1.0478 (1.1367)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [380/431]  eta: 0:00:57  lr: 0.000192  loss: 1.0584 (1.1355)  time: 1.1302  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [390/431]  eta: 0:00:46  lr: 0.000192  loss: 1.0866 (1.1350)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [400/431]  eta: 0:00:35  lr: 0.000192  loss: 1.1146 (1.1351)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [410/431]  eta: 0:00:23  lr: 0.000192  loss: 1.1120 (1.1340)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135]  [420/431]  eta: 0:00:12  lr: 0.000192  loss: 1.0930 (1.1331)  time: 1.1228  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:135]  [430/431]  eta: 0:00:01  lr: 0.000192  loss: 1.1089 (1.1331)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:135] Total time: 0:08:07 (1.1318 s / it)\n",
      "Averaged stats: lr: 0.000192  loss: 1.1089 (1.1331)\n",
      "Valid: [epoch:135]  [ 0/14]  eta: 0:00:37  loss: 1.1698 (1.1698)  time: 2.6454  data: 2.5121  max mem: 15925\n",
      "Valid: [epoch:135]  [13/14]  eta: 0:00:00  loss: 1.0655 (1.0750)  time: 0.2753  data: 0.1795  max mem: 15925\n",
      "Valid: [epoch:135] Total time: 0:00:04 (0.2950 s / it)\n",
      "Averaged stats: loss: 1.0655 (1.0750)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_135_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.075%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:136]  [  0/431]  eta: 0:34:01  lr: 0.000192  loss: 1.1078 (1.1078)  time: 4.7358  data: 3.5837  max mem: 15925\n",
      "Train: [epoch:136]  [ 10/431]  eta: 0:09:40  lr: 0.000192  loss: 1.1655 (1.1443)  time: 1.3798  data: 0.3260  max mem: 15925\n",
      "Train: [epoch:136]  [ 20/431]  eta: 0:08:26  lr: 0.000192  loss: 1.1540 (1.1456)  time: 1.0575  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [ 30/431]  eta: 0:07:55  lr: 0.000192  loss: 1.1141 (1.1408)  time: 1.0779  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [ 40/431]  eta: 0:07:35  lr: 0.000192  loss: 1.0832 (1.1338)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [ 50/431]  eta: 0:07:20  lr: 0.000192  loss: 1.0644 (1.1213)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [ 60/431]  eta: 0:07:07  lr: 0.000192  loss: 1.1089 (1.1258)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [ 70/431]  eta: 0:06:54  lr: 0.000192  loss: 1.1461 (1.1365)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [ 80/431]  eta: 0:06:42  lr: 0.000192  loss: 1.1363 (1.1382)  time: 1.1361  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [ 90/431]  eta: 0:06:30  lr: 0.000192  loss: 1.0820 (1.1331)  time: 1.1409  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [100/431]  eta: 0:06:19  lr: 0.000192  loss: 1.1053 (1.1291)  time: 1.1386  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [110/431]  eta: 0:06:07  lr: 0.000192  loss: 1.1083 (1.1285)  time: 1.1367  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [120/431]  eta: 0:05:55  lr: 0.000192  loss: 1.1295 (1.1293)  time: 1.1324  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [130/431]  eta: 0:05:43  lr: 0.000192  loss: 1.1104 (1.1267)  time: 1.1283  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [140/431]  eta: 0:05:31  lr: 0.000192  loss: 1.1205 (1.1287)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [150/431]  eta: 0:05:20  lr: 0.000192  loss: 1.1465 (1.1293)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [160/431]  eta: 0:05:08  lr: 0.000192  loss: 1.1331 (1.1301)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [170/431]  eta: 0:04:56  lr: 0.000192  loss: 1.1331 (1.1315)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [180/431]  eta: 0:04:44  lr: 0.000192  loss: 1.1003 (1.1306)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [190/431]  eta: 0:04:33  lr: 0.000192  loss: 1.1003 (1.1329)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [200/431]  eta: 0:04:21  lr: 0.000192  loss: 1.1466 (1.1360)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [210/431]  eta: 0:04:10  lr: 0.000192  loss: 1.0860 (1.1336)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [220/431]  eta: 0:03:58  lr: 0.000192  loss: 1.1023 (1.1349)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [230/431]  eta: 0:03:47  lr: 0.000192  loss: 1.1579 (1.1346)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [240/431]  eta: 0:03:35  lr: 0.000192  loss: 1.0781 (1.1344)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [250/431]  eta: 0:03:24  lr: 0.000192  loss: 1.0688 (1.1341)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [260/431]  eta: 0:03:12  lr: 0.000192  loss: 1.0853 (1.1329)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [270/431]  eta: 0:03:01  lr: 0.000192  loss: 1.1143 (1.1319)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [280/431]  eta: 0:02:50  lr: 0.000192  loss: 1.0480 (1.1281)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [290/431]  eta: 0:02:38  lr: 0.000192  loss: 1.0480 (1.1276)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [300/431]  eta: 0:02:27  lr: 0.000192  loss: 1.0843 (1.1278)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [310/431]  eta: 0:02:16  lr: 0.000192  loss: 1.0657 (1.1267)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [320/431]  eta: 0:02:04  lr: 0.000192  loss: 1.0631 (1.1257)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [330/431]  eta: 0:01:53  lr: 0.000192  loss: 1.1219 (1.1271)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [340/431]  eta: 0:01:42  lr: 0.000192  loss: 1.1373 (1.1277)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [350/431]  eta: 0:01:31  lr: 0.000192  loss: 1.1073 (1.1277)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [360/431]  eta: 0:01:19  lr: 0.000192  loss: 1.0739 (1.1276)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [370/431]  eta: 0:01:08  lr: 0.000192  loss: 1.0964 (1.1280)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [380/431]  eta: 0:00:57  lr: 0.000192  loss: 1.0853 (1.1275)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [390/431]  eta: 0:00:46  lr: 0.000192  loss: 1.1082 (1.1282)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [400/431]  eta: 0:00:34  lr: 0.000192  loss: 1.1343 (1.1298)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:136]  [410/431]  eta: 0:00:23  lr: 0.000192  loss: 1.1343 (1.1297)  time: 1.1042  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:136]  [420/431]  eta: 0:00:12  lr: 0.000192  loss: 1.1249 (1.1293)  time: 1.0992  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:136]  [430/431]  eta: 0:00:01  lr: 0.000192  loss: 1.1045 (1.1295)  time: 1.0997  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:136] Total time: 0:08:03 (1.1218 s / it)\n",
      "Averaged stats: lr: 0.000192  loss: 1.1045 (1.1295)\n",
      "Valid: [epoch:136]  [ 0/14]  eta: 0:00:36  loss: 1.1703 (1.1703)  time: 2.5751  data: 2.4559  max mem: 15925\n",
      "Valid: [epoch:136]  [13/14]  eta: 0:00:00  loss: 1.0587 (1.0699)  time: 0.2718  data: 0.1755  max mem: 15925\n",
      "Valid: [epoch:136] Total time: 0:00:04 (0.2898 s / it)\n",
      "Averaged stats: loss: 1.0587 (1.0699)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_136_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.070%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:137]  [  0/431]  eta: 0:31:28  lr: 0.000192  loss: 1.3134 (1.3134)  time: 4.3823  data: 3.1462  max mem: 15925\n",
      "Train: [epoch:137]  [ 10/431]  eta: 0:09:24  lr: 0.000192  loss: 1.1650 (1.1937)  time: 1.3408  data: 0.2862  max mem: 15925\n",
      "Train: [epoch:137]  [ 20/431]  eta: 0:08:16  lr: 0.000192  loss: 1.1188 (1.1704)  time: 1.0503  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [ 30/431]  eta: 0:07:46  lr: 0.000192  loss: 1.0668 (1.1402)  time: 1.0657  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [ 40/431]  eta: 0:07:27  lr: 0.000192  loss: 1.0625 (1.1276)  time: 1.0754  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [ 50/431]  eta: 0:07:12  lr: 0.000192  loss: 1.0651 (1.1236)  time: 1.0896  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [ 60/431]  eta: 0:06:59  lr: 0.000192  loss: 1.0837 (1.1215)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [ 70/431]  eta: 0:06:46  lr: 0.000192  loss: 1.1265 (1.1259)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [ 80/431]  eta: 0:06:34  lr: 0.000192  loss: 1.1382 (1.1297)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [ 90/431]  eta: 0:06:23  lr: 0.000192  loss: 1.1066 (1.1223)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [100/431]  eta: 0:06:11  lr: 0.000192  loss: 1.0568 (1.1198)  time: 1.1085  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:137]  [110/431]  eta: 0:05:59  lr: 0.000192  loss: 1.0491 (1.1141)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [120/431]  eta: 0:05:48  lr: 0.000192  loss: 1.0502 (1.1142)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [130/431]  eta: 0:05:36  lr: 0.000192  loss: 1.0751 (1.1141)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [140/431]  eta: 0:05:25  lr: 0.000192  loss: 1.0756 (1.1139)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [150/431]  eta: 0:05:14  lr: 0.000192  loss: 1.0807 (1.1152)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [160/431]  eta: 0:05:02  lr: 0.000192  loss: 1.1221 (1.1195)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [170/431]  eta: 0:04:51  lr: 0.000192  loss: 1.1346 (1.1213)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [180/431]  eta: 0:04:40  lr: 0.000192  loss: 1.1225 (1.1223)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [190/431]  eta: 0:04:28  lr: 0.000192  loss: 1.1164 (1.1214)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [200/431]  eta: 0:04:17  lr: 0.000192  loss: 1.1046 (1.1213)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [210/431]  eta: 0:04:06  lr: 0.000192  loss: 1.1088 (1.1216)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [220/431]  eta: 0:03:55  lr: 0.000192  loss: 1.0644 (1.1206)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [230/431]  eta: 0:03:43  lr: 0.000192  loss: 1.1311 (1.1243)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [240/431]  eta: 0:03:32  lr: 0.000192  loss: 1.1508 (1.1258)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [250/431]  eta: 0:03:21  lr: 0.000192  loss: 1.1590 (1.1275)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [260/431]  eta: 0:03:10  lr: 0.000192  loss: 1.1466 (1.1275)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [270/431]  eta: 0:02:59  lr: 0.000192  loss: 1.1176 (1.1307)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [280/431]  eta: 0:02:48  lr: 0.000192  loss: 1.1036 (1.1292)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [290/431]  eta: 0:02:36  lr: 0.000192  loss: 1.1062 (1.1296)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [300/431]  eta: 0:02:25  lr: 0.000192  loss: 1.1454 (1.1303)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [310/431]  eta: 0:02:14  lr: 0.000192  loss: 1.0948 (1.1297)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [320/431]  eta: 0:02:03  lr: 0.000192  loss: 1.0851 (1.1285)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [330/431]  eta: 0:01:52  lr: 0.000192  loss: 1.1372 (1.1294)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [340/431]  eta: 0:01:41  lr: 0.000192  loss: 1.1486 (1.1305)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [350/431]  eta: 0:01:30  lr: 0.000192  loss: 1.0902 (1.1312)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [360/431]  eta: 0:01:18  lr: 0.000192  loss: 1.0733 (1.1313)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [370/431]  eta: 0:01:07  lr: 0.000192  loss: 1.1336 (1.1320)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [380/431]  eta: 0:00:56  lr: 0.000192  loss: 1.1336 (1.1317)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [390/431]  eta: 0:00:45  lr: 0.000192  loss: 1.0909 (1.1313)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [400/431]  eta: 0:00:34  lr: 0.000192  loss: 1.0710 (1.1306)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:137]  [410/431]  eta: 0:00:23  lr: 0.000192  loss: 1.1300 (1.1303)  time: 1.1087  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:137]  [420/431]  eta: 0:00:12  lr: 0.000192  loss: 1.1339 (1.1315)  time: 1.1114  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:137]  [430/431]  eta: 0:00:01  lr: 0.000192  loss: 1.1335 (1.1313)  time: 1.1076  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:137] Total time: 0:07:59 (1.1121 s / it)\n",
      "Averaged stats: lr: 0.000192  loss: 1.1335 (1.1313)\n",
      "Valid: [epoch:137]  [ 0/14]  eta: 0:00:37  loss: 0.9779 (0.9779)  time: 2.6591  data: 2.4956  max mem: 15925\n",
      "Valid: [epoch:137]  [13/14]  eta: 0:00:00  loss: 1.0638 (1.0732)  time: 0.2756  data: 0.1783  max mem: 15925\n",
      "Valid: [epoch:137] Total time: 0:00:04 (0.2923 s / it)\n",
      "Averaged stats: loss: 1.0638 (1.0732)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_137_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.073%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:138]  [  0/431]  eta: 0:30:41  lr: 0.000192  loss: 1.0229 (1.0229)  time: 4.2732  data: 2.9839  max mem: 15925\n",
      "Train: [epoch:138]  [ 10/431]  eta: 0:09:31  lr: 0.000192  loss: 1.1167 (1.1407)  time: 1.3569  data: 0.2715  max mem: 15925\n",
      "Train: [epoch:138]  [ 20/431]  eta: 0:08:23  lr: 0.000192  loss: 1.1296 (1.1406)  time: 1.0719  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [ 30/431]  eta: 0:07:52  lr: 0.000192  loss: 1.1022 (1.1326)  time: 1.0794  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [ 40/431]  eta: 0:07:33  lr: 0.000192  loss: 1.1104 (1.1389)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [ 50/431]  eta: 0:07:17  lr: 0.000192  loss: 1.1178 (1.1239)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [ 60/431]  eta: 0:07:03  lr: 0.000192  loss: 1.0408 (1.1215)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [ 70/431]  eta: 0:06:50  lr: 0.000192  loss: 1.0976 (1.1189)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [ 80/431]  eta: 0:06:37  lr: 0.000192  loss: 1.1070 (1.1191)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [ 90/431]  eta: 0:06:25  lr: 0.000192  loss: 1.0888 (1.1183)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [100/431]  eta: 0:06:12  lr: 0.000192  loss: 1.0944 (1.1164)  time: 1.0992  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:138]  [110/431]  eta: 0:06:00  lr: 0.000192  loss: 1.1000 (1.1123)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [120/431]  eta: 0:05:49  lr: 0.000192  loss: 1.1051 (1.1192)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [130/431]  eta: 0:05:37  lr: 0.000192  loss: 1.1154 (1.1202)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [140/431]  eta: 0:05:26  lr: 0.000192  loss: 1.1042 (1.1179)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [150/431]  eta: 0:05:14  lr: 0.000192  loss: 1.1030 (1.1193)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [160/431]  eta: 0:05:03  lr: 0.000192  loss: 1.1030 (1.1171)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [170/431]  eta: 0:04:51  lr: 0.000192  loss: 1.0734 (1.1160)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [180/431]  eta: 0:04:40  lr: 0.000192  loss: 1.0447 (1.1148)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [190/431]  eta: 0:04:29  lr: 0.000192  loss: 1.0930 (1.1149)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [200/431]  eta: 0:04:17  lr: 0.000192  loss: 1.1206 (1.1162)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [210/431]  eta: 0:04:06  lr: 0.000192  loss: 1.1473 (1.1179)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [220/431]  eta: 0:03:55  lr: 0.000192  loss: 1.1102 (1.1190)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [230/431]  eta: 0:03:44  lr: 0.000192  loss: 1.1291 (1.1190)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [240/431]  eta: 0:03:33  lr: 0.000192  loss: 1.1606 (1.1223)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [250/431]  eta: 0:03:21  lr: 0.000192  loss: 1.0836 (1.1213)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [260/431]  eta: 0:03:10  lr: 0.000192  loss: 1.0836 (1.1208)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [270/431]  eta: 0:02:59  lr: 0.000192  loss: 1.1168 (1.1224)  time: 1.1113  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:138]  [280/431]  eta: 0:02:48  lr: 0.000192  loss: 1.1168 (1.1222)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [290/431]  eta: 0:02:37  lr: 0.000192  loss: 1.0845 (1.1244)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [300/431]  eta: 0:02:25  lr: 0.000192  loss: 1.1398 (1.1261)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [310/431]  eta: 0:02:14  lr: 0.000192  loss: 1.1231 (1.1246)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [320/431]  eta: 0:02:03  lr: 0.000192  loss: 1.0984 (1.1251)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [330/431]  eta: 0:01:52  lr: 0.000192  loss: 1.1424 (1.1271)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [340/431]  eta: 0:01:41  lr: 0.000192  loss: 1.1523 (1.1279)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [350/431]  eta: 0:01:30  lr: 0.000192  loss: 1.1033 (1.1266)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [360/431]  eta: 0:01:18  lr: 0.000192  loss: 1.1020 (1.1272)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [370/431]  eta: 0:01:07  lr: 0.000192  loss: 1.1244 (1.1280)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [380/431]  eta: 0:00:56  lr: 0.000192  loss: 1.1251 (1.1276)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [390/431]  eta: 0:00:45  lr: 0.000192  loss: 1.1126 (1.1282)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [400/431]  eta: 0:00:34  lr: 0.000192  loss: 1.0850 (1.1284)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:138]  [410/431]  eta: 0:00:23  lr: 0.000192  loss: 1.1132 (1.1293)  time: 1.1071  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:138]  [420/431]  eta: 0:00:12  lr: 0.000192  loss: 1.1182 (1.1300)  time: 1.1093  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:138]  [430/431]  eta: 0:00:01  lr: 0.000192  loss: 1.1406 (1.1317)  time: 1.1118  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:138] Total time: 0:07:59 (1.1122 s / it)\n",
      "Averaged stats: lr: 0.000192  loss: 1.1406 (1.1317)\n",
      "Valid: [epoch:138]  [ 0/14]  eta: 0:00:36  loss: 1.0517 (1.0517)  time: 2.5799  data: 2.4310  max mem: 15925\n",
      "Valid: [epoch:138]  [13/14]  eta: 0:00:00  loss: 1.0712 (1.0835)  time: 0.2680  data: 0.1737  max mem: 15925\n",
      "Valid: [epoch:138] Total time: 0:00:04 (0.2870 s / it)\n",
      "Averaged stats: loss: 1.0712 (1.0835)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_138_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.083%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:139]  [  0/431]  eta: 0:34:45  lr: 0.000192  loss: 1.2267 (1.2267)  time: 4.8377  data: 3.7507  max mem: 15925\n",
      "Train: [epoch:139]  [ 10/431]  eta: 0:09:44  lr: 0.000192  loss: 1.1630 (1.1615)  time: 1.3894  data: 0.3412  max mem: 15925\n",
      "Train: [epoch:139]  [ 20/431]  eta: 0:08:26  lr: 0.000192  loss: 1.0969 (1.1373)  time: 1.0528  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [ 30/431]  eta: 0:07:55  lr: 0.000192  loss: 1.0962 (1.1275)  time: 1.0750  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [ 40/431]  eta: 0:07:34  lr: 0.000192  loss: 1.0962 (1.1271)  time: 1.0867  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [ 50/431]  eta: 0:07:17  lr: 0.000192  loss: 1.0894 (1.1219)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [ 60/431]  eta: 0:07:02  lr: 0.000192  loss: 1.1226 (1.1265)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [ 70/431]  eta: 0:06:49  lr: 0.000192  loss: 1.1578 (1.1330)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [ 80/431]  eta: 0:06:37  lr: 0.000192  loss: 1.1578 (1.1419)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [ 90/431]  eta: 0:06:24  lr: 0.000192  loss: 1.1374 (1.1387)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [100/431]  eta: 0:06:12  lr: 0.000192  loss: 1.0966 (1.1411)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [110/431]  eta: 0:06:00  lr: 0.000192  loss: 1.0687 (1.1351)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [120/431]  eta: 0:05:49  lr: 0.000192  loss: 1.0472 (1.1315)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [130/431]  eta: 0:05:37  lr: 0.000192  loss: 1.0786 (1.1307)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [140/431]  eta: 0:05:26  lr: 0.000192  loss: 1.0732 (1.1275)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [150/431]  eta: 0:05:14  lr: 0.000192  loss: 1.0732 (1.1268)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [160/431]  eta: 0:05:03  lr: 0.000192  loss: 1.0870 (1.1253)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [170/431]  eta: 0:04:52  lr: 0.000192  loss: 1.1106 (1.1280)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [180/431]  eta: 0:04:40  lr: 0.000192  loss: 1.1763 (1.1305)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [190/431]  eta: 0:04:29  lr: 0.000192  loss: 1.1520 (1.1308)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [200/431]  eta: 0:04:18  lr: 0.000192  loss: 1.1477 (1.1301)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [210/431]  eta: 0:04:06  lr: 0.000192  loss: 1.1037 (1.1290)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [220/431]  eta: 0:03:55  lr: 0.000192  loss: 1.0925 (1.1277)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [230/431]  eta: 0:03:44  lr: 0.000192  loss: 1.1047 (1.1284)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [240/431]  eta: 0:03:33  lr: 0.000192  loss: 1.1193 (1.1300)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [250/431]  eta: 0:03:21  lr: 0.000192  loss: 1.1305 (1.1306)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [260/431]  eta: 0:03:10  lr: 0.000192  loss: 1.0705 (1.1281)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [270/431]  eta: 0:02:59  lr: 0.000192  loss: 1.0680 (1.1290)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [280/431]  eta: 0:02:48  lr: 0.000192  loss: 1.1033 (1.1278)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [290/431]  eta: 0:02:37  lr: 0.000192  loss: 1.1033 (1.1281)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [300/431]  eta: 0:02:26  lr: 0.000192  loss: 1.1157 (1.1302)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [310/431]  eta: 0:02:14  lr: 0.000192  loss: 1.0918 (1.1293)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [320/431]  eta: 0:02:03  lr: 0.000192  loss: 1.0918 (1.1310)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [330/431]  eta: 0:01:52  lr: 0.000192  loss: 1.1308 (1.1308)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [340/431]  eta: 0:01:41  lr: 0.000192  loss: 1.1281 (1.1309)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [350/431]  eta: 0:01:30  lr: 0.000192  loss: 1.0830 (1.1302)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [360/431]  eta: 0:01:19  lr: 0.000192  loss: 1.1041 (1.1298)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [370/431]  eta: 0:01:07  lr: 0.000192  loss: 1.1205 (1.1298)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [380/431]  eta: 0:00:56  lr: 0.000192  loss: 1.1700 (1.1313)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [390/431]  eta: 0:00:45  lr: 0.000192  loss: 1.1413 (1.1310)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [400/431]  eta: 0:00:34  lr: 0.000192  loss: 1.0947 (1.1304)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:139]  [410/431]  eta: 0:00:23  lr: 0.000192  loss: 1.0982 (1.1307)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:139]  [420/431]  eta: 0:00:12  lr: 0.000192  loss: 1.0968 (1.1307)  time: 1.1119  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:139]  [430/431]  eta: 0:00:01  lr: 0.000192  loss: 1.0942 (1.1322)  time: 1.1060  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:139] Total time: 0:07:59 (1.1131 s / it)\n",
      "Averaged stats: lr: 0.000192  loss: 1.0942 (1.1322)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:139]  [ 0/14]  eta: 0:00:34  loss: 1.1865 (1.1865)  time: 2.4763  data: 2.3162  max mem: 15925\n",
      "Valid: [epoch:139]  [13/14]  eta: 0:00:00  loss: 1.0638 (1.0761)  time: 0.2624  data: 0.1663  max mem: 15925\n",
      "Valid: [epoch:139] Total time: 0:00:03 (0.2783 s / it)\n",
      "Averaged stats: loss: 1.0638 (1.0761)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_139_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.076%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 123.000\n",
      "Train: [epoch:140]  [  0/431]  eta: 0:31:20  lr: 0.000191  loss: 1.1676 (1.1676)  time: 4.3621  data: 3.2126  max mem: 15925\n",
      "Train: [epoch:140]  [ 10/431]  eta: 0:09:23  lr: 0.000191  loss: 1.2718 (1.2265)  time: 1.3393  data: 0.2922  max mem: 15925\n",
      "Train: [epoch:140]  [ 20/431]  eta: 0:08:15  lr: 0.000191  loss: 1.1336 (1.1631)  time: 1.0470  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [ 30/431]  eta: 0:07:47  lr: 0.000191  loss: 1.0522 (1.1367)  time: 1.0688  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [ 40/431]  eta: 0:07:26  lr: 0.000191  loss: 1.0787 (1.1352)  time: 1.0776  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [ 50/431]  eta: 0:07:12  lr: 0.000191  loss: 1.1690 (1.1455)  time: 1.0872  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [ 60/431]  eta: 0:06:59  lr: 0.000191  loss: 1.1009 (1.1337)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [ 70/431]  eta: 0:06:46  lr: 0.000191  loss: 1.1374 (1.1352)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [ 80/431]  eta: 0:06:34  lr: 0.000191  loss: 1.1571 (1.1405)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [ 90/431]  eta: 0:06:22  lr: 0.000191  loss: 1.1142 (1.1383)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [100/431]  eta: 0:06:10  lr: 0.000191  loss: 1.0880 (1.1378)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [110/431]  eta: 0:05:59  lr: 0.000191  loss: 1.1020 (1.1398)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [120/431]  eta: 0:05:47  lr: 0.000191  loss: 1.0953 (1.1343)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [130/431]  eta: 0:05:36  lr: 0.000191  loss: 1.0988 (1.1345)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [140/431]  eta: 0:05:25  lr: 0.000191  loss: 1.1032 (1.1333)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [150/431]  eta: 0:05:14  lr: 0.000191  loss: 1.1054 (1.1338)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [160/431]  eta: 0:05:02  lr: 0.000191  loss: 1.1292 (1.1343)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [170/431]  eta: 0:04:51  lr: 0.000191  loss: 1.1448 (1.1352)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [180/431]  eta: 0:04:40  lr: 0.000191  loss: 1.0882 (1.1332)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [190/431]  eta: 0:04:29  lr: 0.000191  loss: 1.1323 (1.1364)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [200/431]  eta: 0:04:17  lr: 0.000191  loss: 1.1432 (1.1339)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [210/431]  eta: 0:04:06  lr: 0.000191  loss: 1.0746 (1.1332)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [220/431]  eta: 0:03:55  lr: 0.000191  loss: 1.0766 (1.1314)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [230/431]  eta: 0:03:44  lr: 0.000191  loss: 1.0631 (1.1289)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [240/431]  eta: 0:03:32  lr: 0.000191  loss: 1.0631 (1.1279)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [250/431]  eta: 0:03:21  lr: 0.000191  loss: 1.1053 (1.1286)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [260/431]  eta: 0:03:10  lr: 0.000191  loss: 1.1429 (1.1304)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [270/431]  eta: 0:02:59  lr: 0.000191  loss: 1.1489 (1.1290)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [280/431]  eta: 0:02:48  lr: 0.000191  loss: 1.0884 (1.1282)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [290/431]  eta: 0:02:36  lr: 0.000191  loss: 1.1236 (1.1293)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [300/431]  eta: 0:02:25  lr: 0.000191  loss: 1.1986 (1.1337)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [310/431]  eta: 0:02:14  lr: 0.000191  loss: 1.1618 (1.1332)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [320/431]  eta: 0:02:03  lr: 0.000191  loss: 1.1036 (1.1342)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [330/431]  eta: 0:01:52  lr: 0.000191  loss: 1.1036 (1.1340)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [340/431]  eta: 0:01:41  lr: 0.000191  loss: 1.0852 (1.1327)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [350/431]  eta: 0:01:30  lr: 0.000191  loss: 1.1174 (1.1329)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [360/431]  eta: 0:01:18  lr: 0.000191  loss: 1.1474 (1.1332)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [370/431]  eta: 0:01:07  lr: 0.000191  loss: 1.0994 (1.1314)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [380/431]  eta: 0:00:56  lr: 0.000191  loss: 1.1269 (1.1327)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [390/431]  eta: 0:00:45  lr: 0.000191  loss: 1.1445 (1.1319)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [400/431]  eta: 0:00:34  lr: 0.000191  loss: 1.0825 (1.1319)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:140]  [410/431]  eta: 0:00:23  lr: 0.000191  loss: 1.0946 (1.1306)  time: 1.1037  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:140]  [420/431]  eta: 0:00:12  lr: 0.000191  loss: 1.0946 (1.1305)  time: 1.1124  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:140]  [430/431]  eta: 0:00:01  lr: 0.000191  loss: 1.1482 (1.1312)  time: 1.1138  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:140] Total time: 0:07:59 (1.1131 s / it)\n",
      "Averaged stats: lr: 0.000191  loss: 1.1482 (1.1312)\n",
      "Valid: [epoch:140]  [ 0/14]  eta: 0:00:37  loss: 0.9994 (0.9994)  time: 2.6769  data: 2.4987  max mem: 15925\n",
      "Valid: [epoch:140]  [13/14]  eta: 0:00:00  loss: 1.0568 (1.0690)  time: 0.2802  data: 0.1786  max mem: 15925\n",
      "Valid: [epoch:140] Total time: 0:00:04 (0.2983 s / it)\n",
      "Averaged stats: loss: 1.0568 (1.0690)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_140_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.069%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n",
      "Train: [epoch:141]  [  0/431]  eta: 0:34:46  lr: 0.000191  loss: 1.1100 (1.1100)  time: 4.8400  data: 3.6651  max mem: 15925\n",
      "Train: [epoch:141]  [ 10/431]  eta: 0:09:48  lr: 0.000191  loss: 1.1100 (1.1453)  time: 1.3983  data: 0.3334  max mem: 15925\n",
      "Train: [epoch:141]  [ 20/431]  eta: 0:08:27  lr: 0.000191  loss: 1.1578 (1.1503)  time: 1.0535  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [ 30/431]  eta: 0:07:55  lr: 0.000191  loss: 1.1529 (1.1482)  time: 1.0681  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [ 40/431]  eta: 0:07:34  lr: 0.000191  loss: 1.1236 (1.1419)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [ 50/431]  eta: 0:07:17  lr: 0.000191  loss: 1.0964 (1.1292)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [ 60/431]  eta: 0:07:03  lr: 0.000191  loss: 1.0964 (1.1322)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [ 70/431]  eta: 0:06:50  lr: 0.000191  loss: 1.1170 (1.1383)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [ 80/431]  eta: 0:06:37  lr: 0.000191  loss: 1.1377 (1.1421)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [ 90/431]  eta: 0:06:25  lr: 0.000191  loss: 1.1018 (1.1370)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [100/431]  eta: 0:06:13  lr: 0.000191  loss: 1.0654 (1.1339)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [110/431]  eta: 0:06:01  lr: 0.000191  loss: 1.0669 (1.1295)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [120/431]  eta: 0:05:49  lr: 0.000191  loss: 1.1021 (1.1321)  time: 1.1099  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:141]  [130/431]  eta: 0:05:38  lr: 0.000191  loss: 1.1021 (1.1303)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [140/431]  eta: 0:05:26  lr: 0.000191  loss: 1.0989 (1.1319)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [150/431]  eta: 0:05:15  lr: 0.000191  loss: 1.1537 (1.1345)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [160/431]  eta: 0:05:03  lr: 0.000191  loss: 1.1315 (1.1356)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [170/431]  eta: 0:04:52  lr: 0.000191  loss: 1.0964 (1.1360)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [180/431]  eta: 0:04:40  lr: 0.000191  loss: 1.1065 (1.1336)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [190/431]  eta: 0:04:29  lr: 0.000191  loss: 1.0801 (1.1314)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [200/431]  eta: 0:04:18  lr: 0.000191  loss: 1.1296 (1.1338)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [210/431]  eta: 0:04:06  lr: 0.000191  loss: 1.1003 (1.1316)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [220/431]  eta: 0:03:55  lr: 0.000191  loss: 1.0874 (1.1310)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [230/431]  eta: 0:03:44  lr: 0.000191  loss: 1.1178 (1.1323)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [240/431]  eta: 0:03:33  lr: 0.000191  loss: 1.1480 (1.1342)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [250/431]  eta: 0:03:21  lr: 0.000191  loss: 1.1363 (1.1331)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [260/431]  eta: 0:03:10  lr: 0.000191  loss: 1.0820 (1.1312)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [270/431]  eta: 0:02:59  lr: 0.000191  loss: 1.0947 (1.1307)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [280/431]  eta: 0:02:48  lr: 0.000191  loss: 1.1027 (1.1312)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [290/431]  eta: 0:02:37  lr: 0.000191  loss: 1.1019 (1.1291)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [300/431]  eta: 0:02:26  lr: 0.000191  loss: 1.1295 (1.1316)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [310/431]  eta: 0:02:14  lr: 0.000191  loss: 1.0928 (1.1292)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [320/431]  eta: 0:02:03  lr: 0.000191  loss: 1.0807 (1.1297)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [330/431]  eta: 0:01:52  lr: 0.000191  loss: 1.1256 (1.1310)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [340/431]  eta: 0:01:41  lr: 0.000191  loss: 1.0990 (1.1298)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [350/431]  eta: 0:01:30  lr: 0.000191  loss: 1.0990 (1.1312)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [360/431]  eta: 0:01:19  lr: 0.000191  loss: 1.1085 (1.1314)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [370/431]  eta: 0:01:07  lr: 0.000191  loss: 1.0775 (1.1303)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [380/431]  eta: 0:00:56  lr: 0.000191  loss: 1.0852 (1.1288)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [390/431]  eta: 0:00:45  lr: 0.000191  loss: 1.0981 (1.1290)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [400/431]  eta: 0:00:34  lr: 0.000191  loss: 1.0950 (1.1277)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:141]  [410/431]  eta: 0:00:23  lr: 0.000191  loss: 1.0925 (1.1279)  time: 1.1088  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:141]  [420/431]  eta: 0:00:12  lr: 0.000191  loss: 1.1450 (1.1296)  time: 1.1052  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:141]  [430/431]  eta: 0:00:01  lr: 0.000191  loss: 1.1402 (1.1304)  time: 1.0984  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:141] Total time: 0:07:59 (1.1131 s / it)\n",
      "Averaged stats: lr: 0.000191  loss: 1.1402 (1.1304)\n",
      "Valid: [epoch:141]  [ 0/14]  eta: 0:00:35  loss: 1.0062 (1.0062)  time: 2.5052  data: 2.3455  max mem: 15925\n",
      "Valid: [epoch:141]  [13/14]  eta: 0:00:00  loss: 1.0655 (1.0748)  time: 0.2633  data: 0.1676  max mem: 15925\n",
      "Valid: [epoch:141] Total time: 0:00:03 (0.2804 s / it)\n",
      "Averaged stats: loss: 1.0655 (1.0748)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_141_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.075%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n",
      "Train: [epoch:142]  [  0/431]  eta: 0:34:27  lr: 0.000191  loss: 1.2801 (1.2801)  time: 4.7977  data: 3.5455  max mem: 15925\n",
      "Train: [epoch:142]  [ 10/431]  eta: 0:09:43  lr: 0.000191  loss: 1.1218 (1.1157)  time: 1.3850  data: 0.3225  max mem: 15925\n",
      "Train: [epoch:142]  [ 20/431]  eta: 0:08:29  lr: 0.000191  loss: 1.1052 (1.1056)  time: 1.0614  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [ 30/431]  eta: 0:07:57  lr: 0.000191  loss: 1.1052 (1.1307)  time: 1.0830  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [ 40/431]  eta: 0:07:37  lr: 0.000191  loss: 1.1109 (1.1254)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [ 50/431]  eta: 0:07:22  lr: 0.000191  loss: 1.1278 (1.1222)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [ 60/431]  eta: 0:07:07  lr: 0.000191  loss: 1.0816 (1.1152)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [ 70/431]  eta: 0:06:53  lr: 0.000191  loss: 1.0816 (1.1171)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [ 80/431]  eta: 0:06:40  lr: 0.000191  loss: 1.1308 (1.1217)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [ 90/431]  eta: 0:06:28  lr: 0.000191  loss: 1.1425 (1.1224)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [100/431]  eta: 0:06:15  lr: 0.000191  loss: 1.1138 (1.1281)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [110/431]  eta: 0:06:03  lr: 0.000191  loss: 1.0960 (1.1246)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [120/431]  eta: 0:05:51  lr: 0.000191  loss: 1.0479 (1.1221)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [130/431]  eta: 0:05:39  lr: 0.000191  loss: 1.1130 (1.1239)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [140/431]  eta: 0:05:28  lr: 0.000191  loss: 1.1314 (1.1268)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [150/431]  eta: 0:05:16  lr: 0.000191  loss: 1.1251 (1.1271)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [160/431]  eta: 0:05:04  lr: 0.000191  loss: 1.0855 (1.1272)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [170/431]  eta: 0:04:53  lr: 0.000191  loss: 1.1003 (1.1298)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [180/431]  eta: 0:04:41  lr: 0.000191  loss: 1.1157 (1.1290)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [190/431]  eta: 0:04:30  lr: 0.000191  loss: 1.1643 (1.1331)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [200/431]  eta: 0:04:19  lr: 0.000191  loss: 1.1118 (1.1306)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [210/431]  eta: 0:04:07  lr: 0.000191  loss: 1.1118 (1.1303)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [220/431]  eta: 0:03:56  lr: 0.000191  loss: 1.1586 (1.1345)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [230/431]  eta: 0:03:45  lr: 0.000191  loss: 1.1085 (1.1318)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [240/431]  eta: 0:03:33  lr: 0.000191  loss: 1.0646 (1.1293)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [250/431]  eta: 0:03:22  lr: 0.000191  loss: 1.1021 (1.1300)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [260/431]  eta: 0:03:11  lr: 0.000191  loss: 1.1339 (1.1309)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [270/431]  eta: 0:03:00  lr: 0.000191  loss: 1.0949 (1.1308)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [280/431]  eta: 0:02:48  lr: 0.000191  loss: 1.1383 (1.1323)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [290/431]  eta: 0:02:37  lr: 0.000191  loss: 1.1086 (1.1311)  time: 1.1201  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:142]  [300/431]  eta: 0:02:26  lr: 0.000191  loss: 1.0971 (1.1328)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [310/431]  eta: 0:02:15  lr: 0.000191  loss: 1.1350 (1.1332)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [320/431]  eta: 0:02:04  lr: 0.000191  loss: 1.1114 (1.1330)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [330/431]  eta: 0:01:52  lr: 0.000191  loss: 1.1035 (1.1327)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [340/431]  eta: 0:01:41  lr: 0.000191  loss: 1.1024 (1.1329)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [350/431]  eta: 0:01:30  lr: 0.000191  loss: 1.0986 (1.1322)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [360/431]  eta: 0:01:19  lr: 0.000191  loss: 1.0988 (1.1317)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [370/431]  eta: 0:01:08  lr: 0.000191  loss: 1.1454 (1.1331)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [380/431]  eta: 0:00:56  lr: 0.000191  loss: 1.1313 (1.1321)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [390/431]  eta: 0:00:45  lr: 0.000191  loss: 1.0804 (1.1318)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [400/431]  eta: 0:00:34  lr: 0.000191  loss: 1.1167 (1.1321)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [410/431]  eta: 0:00:23  lr: 0.000191  loss: 1.1358 (1.1318)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:142]  [420/431]  eta: 0:00:12  lr: 0.000191  loss: 1.1027 (1.1315)  time: 1.1165  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:142]  [430/431]  eta: 0:00:01  lr: 0.000191  loss: 1.1452 (1.1327)  time: 1.1032  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:142] Total time: 0:08:01 (1.1171 s / it)\n",
      "Averaged stats: lr: 0.000191  loss: 1.1452 (1.1327)\n",
      "Valid: [epoch:142]  [ 0/14]  eta: 0:00:35  loss: 1.0454 (1.0454)  time: 2.5457  data: 2.3737  max mem: 15925\n",
      "Valid: [epoch:142]  [13/14]  eta: 0:00:00  loss: 1.0761 (1.0845)  time: 0.2687  data: 0.1696  max mem: 15925\n",
      "Valid: [epoch:142] Total time: 0:00:03 (0.2851 s / it)\n",
      "Averaged stats: loss: 1.0761 (1.0845)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_142_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.084%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n",
      "Train: [epoch:143]  [  0/431]  eta: 0:34:57  lr: 0.000191  loss: 1.2637 (1.2637)  time: 4.8675  data: 3.6388  max mem: 15925\n",
      "Train: [epoch:143]  [ 10/431]  eta: 0:09:47  lr: 0.000191  loss: 1.2041 (1.2022)  time: 1.3955  data: 0.3310  max mem: 15925\n",
      "Train: [epoch:143]  [ 20/431]  eta: 0:08:26  lr: 0.000191  loss: 1.1849 (1.1979)  time: 1.0506  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [ 30/431]  eta: 0:07:55  lr: 0.000191  loss: 1.1308 (1.1678)  time: 1.0720  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [ 40/431]  eta: 0:07:34  lr: 0.000191  loss: 1.0753 (1.1521)  time: 1.0909  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [ 50/431]  eta: 0:07:19  lr: 0.000191  loss: 1.0497 (1.1463)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [ 60/431]  eta: 0:07:04  lr: 0.000191  loss: 1.0299 (1.1268)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [ 70/431]  eta: 0:06:52  lr: 0.000191  loss: 1.1101 (1.1419)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [ 80/431]  eta: 0:06:38  lr: 0.000191  loss: 1.1804 (1.1416)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [ 90/431]  eta: 0:06:26  lr: 0.000191  loss: 1.0816 (1.1393)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [100/431]  eta: 0:06:14  lr: 0.000191  loss: 1.0816 (1.1402)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [110/431]  eta: 0:06:02  lr: 0.000191  loss: 1.0868 (1.1349)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [120/431]  eta: 0:05:51  lr: 0.000191  loss: 1.1055 (1.1360)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [130/431]  eta: 0:05:39  lr: 0.000191  loss: 1.1101 (1.1372)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [140/431]  eta: 0:05:28  lr: 0.000191  loss: 1.0713 (1.1343)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [150/431]  eta: 0:05:16  lr: 0.000191  loss: 1.0731 (1.1316)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [160/431]  eta: 0:05:04  lr: 0.000191  loss: 1.1011 (1.1330)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [170/431]  eta: 0:04:53  lr: 0.000191  loss: 1.0801 (1.1313)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [180/431]  eta: 0:04:42  lr: 0.000191  loss: 1.0801 (1.1317)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [190/431]  eta: 0:04:30  lr: 0.000191  loss: 1.0751 (1.1304)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [200/431]  eta: 0:04:19  lr: 0.000191  loss: 1.0787 (1.1304)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [210/431]  eta: 0:04:07  lr: 0.000191  loss: 1.1043 (1.1301)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [220/431]  eta: 0:03:56  lr: 0.000191  loss: 1.1056 (1.1303)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [230/431]  eta: 0:03:45  lr: 0.000191  loss: 1.0617 (1.1292)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [240/431]  eta: 0:03:34  lr: 0.000191  loss: 1.0617 (1.1284)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [250/431]  eta: 0:03:22  lr: 0.000191  loss: 1.0688 (1.1259)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [260/431]  eta: 0:03:11  lr: 0.000191  loss: 1.0688 (1.1252)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [270/431]  eta: 0:03:00  lr: 0.000191  loss: 1.0950 (1.1252)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [280/431]  eta: 0:02:49  lr: 0.000191  loss: 1.0853 (1.1249)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [290/431]  eta: 0:02:37  lr: 0.000191  loss: 1.1109 (1.1269)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [300/431]  eta: 0:02:26  lr: 0.000191  loss: 1.1709 (1.1291)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [310/431]  eta: 0:02:15  lr: 0.000191  loss: 1.1056 (1.1281)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [320/431]  eta: 0:02:04  lr: 0.000191  loss: 1.0844 (1.1273)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [330/431]  eta: 0:01:53  lr: 0.000191  loss: 1.1133 (1.1286)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [340/431]  eta: 0:01:41  lr: 0.000191  loss: 1.2142 (1.1309)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [350/431]  eta: 0:01:30  lr: 0.000191  loss: 1.1745 (1.1306)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [360/431]  eta: 0:01:19  lr: 0.000191  loss: 1.0902 (1.1301)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [370/431]  eta: 0:01:08  lr: 0.000191  loss: 1.0918 (1.1295)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [380/431]  eta: 0:00:57  lr: 0.000191  loss: 1.1171 (1.1293)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [390/431]  eta: 0:00:45  lr: 0.000191  loss: 1.1391 (1.1302)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [400/431]  eta: 0:00:34  lr: 0.000191  loss: 1.1391 (1.1305)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [410/431]  eta: 0:00:23  lr: 0.000191  loss: 1.1229 (1.1308)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:143]  [420/431]  eta: 0:00:12  lr: 0.000191  loss: 1.1102 (1.1312)  time: 1.1106  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:143]  [430/431]  eta: 0:00:01  lr: 0.000191  loss: 1.0990 (1.1315)  time: 1.1171  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:143] Total time: 0:08:01 (1.1182 s / it)\n",
      "Averaged stats: lr: 0.000191  loss: 1.0990 (1.1315)\n",
      "Valid: [epoch:143]  [ 0/14]  eta: 0:00:36  loss: 0.9570 (0.9570)  time: 2.6366  data: 2.5050  max mem: 15925\n",
      "Valid: [epoch:143]  [13/14]  eta: 0:00:00  loss: 1.0658 (1.0760)  time: 0.2736  data: 0.1790  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:143] Total time: 0:00:04 (0.2887 s / it)\n",
      "Averaged stats: loss: 1.0658 (1.0760)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_143_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.076%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n",
      "Train: [epoch:144]  [  0/431]  eta: 0:32:34  lr: 0.000190  loss: 1.1503 (1.1503)  time: 4.5346  data: 3.3722  max mem: 15925\n",
      "Train: [epoch:144]  [ 10/431]  eta: 0:09:44  lr: 0.000190  loss: 1.1686 (1.1825)  time: 1.3875  data: 0.3067  max mem: 15925\n",
      "Train: [epoch:144]  [ 20/431]  eta: 0:08:28  lr: 0.000190  loss: 1.1555 (1.1543)  time: 1.0728  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [ 30/431]  eta: 0:07:56  lr: 0.000190  loss: 1.1084 (1.1506)  time: 1.0794  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [ 40/431]  eta: 0:07:37  lr: 0.000190  loss: 1.1227 (1.1477)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [ 50/431]  eta: 0:07:20  lr: 0.000190  loss: 1.1340 (1.1556)  time: 1.1083  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:144]  [ 60/431]  eta: 0:07:06  lr: 0.000190  loss: 1.0992 (1.1499)  time: 1.1121  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:144]  [ 70/431]  eta: 0:06:54  lr: 0.000190  loss: 1.1000 (1.1512)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [ 80/431]  eta: 0:06:41  lr: 0.000190  loss: 1.1545 (1.1579)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [ 90/431]  eta: 0:06:28  lr: 0.000190  loss: 1.1451 (1.1583)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [100/431]  eta: 0:06:16  lr: 0.000190  loss: 1.1061 (1.1530)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [110/431]  eta: 0:06:03  lr: 0.000190  loss: 1.0846 (1.1456)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [120/431]  eta: 0:05:52  lr: 0.000190  loss: 1.0835 (1.1452)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [130/431]  eta: 0:05:40  lr: 0.000190  loss: 1.1184 (1.1450)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [140/431]  eta: 0:05:28  lr: 0.000190  loss: 1.1121 (1.1422)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [150/431]  eta: 0:05:16  lr: 0.000190  loss: 1.1033 (1.1418)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [160/431]  eta: 0:05:05  lr: 0.000190  loss: 1.1582 (1.1435)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [170/431]  eta: 0:04:54  lr: 0.000190  loss: 1.1506 (1.1421)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [180/431]  eta: 0:04:42  lr: 0.000190  loss: 1.0986 (1.1406)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [190/431]  eta: 0:04:31  lr: 0.000190  loss: 1.1028 (1.1399)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [200/431]  eta: 0:04:20  lr: 0.000190  loss: 1.1450 (1.1417)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [210/431]  eta: 0:04:08  lr: 0.000190  loss: 1.1480 (1.1424)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [220/431]  eta: 0:03:57  lr: 0.000190  loss: 1.1392 (1.1416)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [230/431]  eta: 0:03:46  lr: 0.000190  loss: 1.0909 (1.1405)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [240/431]  eta: 0:03:34  lr: 0.000190  loss: 1.1162 (1.1399)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [250/431]  eta: 0:03:23  lr: 0.000190  loss: 1.0606 (1.1368)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [260/431]  eta: 0:03:12  lr: 0.000190  loss: 1.0831 (1.1369)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [270/431]  eta: 0:03:00  lr: 0.000190  loss: 1.1487 (1.1369)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [280/431]  eta: 0:02:49  lr: 0.000190  loss: 1.1012 (1.1352)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [290/431]  eta: 0:02:38  lr: 0.000190  loss: 1.0918 (1.1345)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [300/431]  eta: 0:02:27  lr: 0.000190  loss: 1.0986 (1.1336)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [310/431]  eta: 0:02:15  lr: 0.000190  loss: 1.0708 (1.1306)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [320/431]  eta: 0:02:04  lr: 0.000190  loss: 1.0361 (1.1309)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [330/431]  eta: 0:01:53  lr: 0.000190  loss: 1.1477 (1.1316)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [340/431]  eta: 0:01:42  lr: 0.000190  loss: 1.1254 (1.1319)  time: 1.1185  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:144]  [350/431]  eta: 0:01:30  lr: 0.000190  loss: 1.0698 (1.1307)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [360/431]  eta: 0:01:19  lr: 0.000190  loss: 1.0779 (1.1296)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [370/431]  eta: 0:01:08  lr: 0.000190  loss: 1.0545 (1.1285)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [380/431]  eta: 0:00:57  lr: 0.000190  loss: 1.1243 (1.1295)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [390/431]  eta: 0:00:45  lr: 0.000190  loss: 1.1243 (1.1280)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [400/431]  eta: 0:00:34  lr: 0.000190  loss: 1.0732 (1.1280)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [410/431]  eta: 0:00:23  lr: 0.000190  loss: 1.1055 (1.1274)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:144]  [420/431]  eta: 0:00:12  lr: 0.000190  loss: 1.1155 (1.1280)  time: 1.1048  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:144]  [430/431]  eta: 0:00:01  lr: 0.000190  loss: 1.1246 (1.1288)  time: 1.1155  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:144] Total time: 0:08:03 (1.1207 s / it)\n",
      "Averaged stats: lr: 0.000190  loss: 1.1246 (1.1288)\n",
      "Valid: [epoch:144]  [ 0/14]  eta: 0:00:37  loss: 1.0537 (1.0537)  time: 2.6585  data: 2.4851  max mem: 15925\n",
      "Valid: [epoch:144]  [13/14]  eta: 0:00:00  loss: 1.0661 (1.0768)  time: 0.2955  data: 0.1776  max mem: 15925\n",
      "Valid: [epoch:144] Total time: 0:00:04 (0.3142 s / it)\n",
      "Averaged stats: loss: 1.0661 (1.0768)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_144_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.077%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n",
      "Train: [epoch:145]  [  0/431]  eta: 0:31:11  lr: 0.000190  loss: 1.2266 (1.2266)  time: 4.3418  data: 3.1558  max mem: 15925\n",
      "Train: [epoch:145]  [ 10/431]  eta: 0:09:25  lr: 0.000190  loss: 1.0947 (1.1497)  time: 1.3428  data: 0.2871  max mem: 15925\n",
      "Train: [epoch:145]  [ 20/431]  eta: 0:08:14  lr: 0.000190  loss: 1.0737 (1.1356)  time: 1.0467  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [ 30/431]  eta: 0:07:47  lr: 0.000190  loss: 1.0560 (1.1134)  time: 1.0671  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [ 40/431]  eta: 0:07:28  lr: 0.000190  loss: 1.0315 (1.0957)  time: 1.0884  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [ 50/431]  eta: 0:07:13  lr: 0.000190  loss: 1.0396 (1.0880)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [ 60/431]  eta: 0:07:00  lr: 0.000190  loss: 1.0990 (1.0962)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [ 70/431]  eta: 0:06:49  lr: 0.000190  loss: 1.1296 (1.1074)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [ 80/431]  eta: 0:06:37  lr: 0.000190  loss: 1.1441 (1.1118)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [ 90/431]  eta: 0:06:25  lr: 0.000190  loss: 1.1339 (1.1134)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [100/431]  eta: 0:06:13  lr: 0.000190  loss: 1.0905 (1.1132)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [110/431]  eta: 0:06:02  lr: 0.000190  loss: 1.0675 (1.1131)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [120/431]  eta: 0:05:51  lr: 0.000190  loss: 1.0319 (1.1063)  time: 1.1259  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [130/431]  eta: 0:05:39  lr: 0.000190  loss: 1.0201 (1.1043)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [140/431]  eta: 0:05:28  lr: 0.000190  loss: 1.0870 (1.1042)  time: 1.1333  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:145]  [150/431]  eta: 0:05:16  lr: 0.000190  loss: 1.0916 (1.1065)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [160/431]  eta: 0:05:05  lr: 0.000190  loss: 1.1186 (1.1092)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [170/431]  eta: 0:04:54  lr: 0.000190  loss: 1.0922 (1.1090)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [180/431]  eta: 0:04:42  lr: 0.000190  loss: 1.1217 (1.1155)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [190/431]  eta: 0:04:31  lr: 0.000190  loss: 1.1517 (1.1178)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [200/431]  eta: 0:04:19  lr: 0.000190  loss: 1.0904 (1.1178)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [210/431]  eta: 0:04:08  lr: 0.000190  loss: 1.0791 (1.1175)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [220/431]  eta: 0:03:57  lr: 0.000190  loss: 1.0839 (1.1151)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [230/431]  eta: 0:03:45  lr: 0.000190  loss: 1.1294 (1.1187)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [240/431]  eta: 0:03:34  lr: 0.000190  loss: 1.1853 (1.1211)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [250/431]  eta: 0:03:23  lr: 0.000190  loss: 1.1215 (1.1226)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [260/431]  eta: 0:03:11  lr: 0.000190  loss: 1.0791 (1.1213)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [270/431]  eta: 0:03:00  lr: 0.000190  loss: 1.0953 (1.1239)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [280/431]  eta: 0:02:49  lr: 0.000190  loss: 1.1103 (1.1234)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [290/431]  eta: 0:02:37  lr: 0.000190  loss: 1.1259 (1.1255)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [300/431]  eta: 0:02:26  lr: 0.000190  loss: 1.1843 (1.1271)  time: 1.1127  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:145]  [310/431]  eta: 0:02:15  lr: 0.000190  loss: 1.1775 (1.1290)  time: 1.1114  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:145]  [320/431]  eta: 0:02:04  lr: 0.000190  loss: 1.0962 (1.1280)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [330/431]  eta: 0:01:53  lr: 0.000190  loss: 1.0962 (1.1275)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [340/431]  eta: 0:01:41  lr: 0.000190  loss: 1.1266 (1.1304)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [350/431]  eta: 0:01:30  lr: 0.000190  loss: 1.1368 (1.1298)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [360/431]  eta: 0:01:19  lr: 0.000190  loss: 1.1130 (1.1301)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [370/431]  eta: 0:01:08  lr: 0.000190  loss: 1.0973 (1.1300)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [380/431]  eta: 0:00:57  lr: 0.000190  loss: 1.1473 (1.1313)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [390/431]  eta: 0:00:45  lr: 0.000190  loss: 1.1509 (1.1315)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [400/431]  eta: 0:00:34  lr: 0.000190  loss: 1.0994 (1.1306)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [410/431]  eta: 0:00:23  lr: 0.000190  loss: 1.0963 (1.1306)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:145]  [420/431]  eta: 0:00:12  lr: 0.000190  loss: 1.1063 (1.1307)  time: 1.1161  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:145]  [430/431]  eta: 0:00:01  lr: 0.000190  loss: 1.0963 (1.1302)  time: 1.1148  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:145] Total time: 0:08:02 (1.1188 s / it)\n",
      "Averaged stats: lr: 0.000190  loss: 1.0963 (1.1302)\n",
      "Valid: [epoch:145]  [ 0/14]  eta: 0:00:36  loss: 1.1726 (1.1726)  time: 2.5865  data: 2.4278  max mem: 15925\n",
      "Valid: [epoch:145]  [13/14]  eta: 0:00:00  loss: 1.0647 (1.0737)  time: 0.2776  data: 0.1735  max mem: 15925\n",
      "Valid: [epoch:145] Total time: 0:00:04 (0.2953 s / it)\n",
      "Averaged stats: loss: 1.0647 (1.0737)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_145_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n",
      "Train: [epoch:146]  [  0/431]  eta: 0:35:01  lr: 0.000190  loss: 1.1344 (1.1344)  time: 4.8758  data: 3.5184  max mem: 15925\n",
      "Train: [epoch:146]  [ 10/431]  eta: 0:09:45  lr: 0.000190  loss: 1.1344 (1.1488)  time: 1.3907  data: 0.3200  max mem: 15925\n",
      "Train: [epoch:146]  [ 20/431]  eta: 0:08:27  lr: 0.000190  loss: 1.1022 (1.1321)  time: 1.0515  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [ 30/431]  eta: 0:07:56  lr: 0.000190  loss: 1.1385 (1.1458)  time: 1.0772  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [ 40/431]  eta: 0:07:35  lr: 0.000190  loss: 1.1410 (1.1464)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [ 50/431]  eta: 0:07:19  lr: 0.000190  loss: 1.1345 (1.1434)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [ 60/431]  eta: 0:07:04  lr: 0.000190  loss: 1.1314 (1.1361)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [ 70/431]  eta: 0:06:51  lr: 0.000190  loss: 1.0717 (1.1326)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [ 80/431]  eta: 0:06:38  lr: 0.000190  loss: 1.0782 (1.1350)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [ 90/431]  eta: 0:06:26  lr: 0.000190  loss: 1.0719 (1.1272)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [100/431]  eta: 0:06:14  lr: 0.000190  loss: 1.0652 (1.1250)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [110/431]  eta: 0:06:02  lr: 0.000190  loss: 1.0881 (1.1214)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [120/431]  eta: 0:05:50  lr: 0.000190  loss: 1.0775 (1.1184)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [130/431]  eta: 0:05:38  lr: 0.000190  loss: 1.1036 (1.1177)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [140/431]  eta: 0:05:27  lr: 0.000190  loss: 1.1172 (1.1187)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [150/431]  eta: 0:05:15  lr: 0.000190  loss: 1.0437 (1.1192)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [160/431]  eta: 0:05:04  lr: 0.000190  loss: 1.0583 (1.1160)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [170/431]  eta: 0:04:52  lr: 0.000190  loss: 1.0952 (1.1172)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [180/431]  eta: 0:04:41  lr: 0.000190  loss: 1.1338 (1.1191)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [190/431]  eta: 0:04:30  lr: 0.000190  loss: 1.1338 (1.1230)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [200/431]  eta: 0:04:18  lr: 0.000190  loss: 1.1411 (1.1230)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [210/431]  eta: 0:04:07  lr: 0.000190  loss: 1.0844 (1.1210)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [220/431]  eta: 0:03:56  lr: 0.000190  loss: 1.1218 (1.1235)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [230/431]  eta: 0:03:44  lr: 0.000190  loss: 1.1498 (1.1253)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [240/431]  eta: 0:03:33  lr: 0.000190  loss: 1.1979 (1.1285)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [250/431]  eta: 0:03:22  lr: 0.000190  loss: 1.1783 (1.1303)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [260/431]  eta: 0:03:11  lr: 0.000190  loss: 1.1006 (1.1288)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [270/431]  eta: 0:02:59  lr: 0.000190  loss: 1.0920 (1.1297)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [280/431]  eta: 0:02:48  lr: 0.000190  loss: 1.0850 (1.1288)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [290/431]  eta: 0:02:37  lr: 0.000190  loss: 1.0895 (1.1292)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [300/431]  eta: 0:02:26  lr: 0.000190  loss: 1.1521 (1.1317)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [310/431]  eta: 0:02:15  lr: 0.000190  loss: 1.1061 (1.1307)  time: 1.1239  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:146]  [320/431]  eta: 0:02:03  lr: 0.000190  loss: 1.0844 (1.1304)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [330/431]  eta: 0:01:52  lr: 0.000190  loss: 1.1064 (1.1297)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [340/431]  eta: 0:01:41  lr: 0.000190  loss: 1.1160 (1.1300)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [350/431]  eta: 0:01:30  lr: 0.000190  loss: 1.1268 (1.1299)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [360/431]  eta: 0:01:19  lr: 0.000190  loss: 1.1309 (1.1311)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [370/431]  eta: 0:01:08  lr: 0.000190  loss: 1.1086 (1.1303)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [380/431]  eta: 0:00:56  lr: 0.000190  loss: 1.0901 (1.1298)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [390/431]  eta: 0:00:45  lr: 0.000190  loss: 1.1348 (1.1304)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [400/431]  eta: 0:00:34  lr: 0.000190  loss: 1.1479 (1.1306)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [410/431]  eta: 0:00:23  lr: 0.000190  loss: 1.1078 (1.1301)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:146]  [420/431]  eta: 0:00:12  lr: 0.000190  loss: 1.0991 (1.1304)  time: 1.1257  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:146]  [430/431]  eta: 0:00:01  lr: 0.000190  loss: 1.1437 (1.1308)  time: 1.1286  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:146] Total time: 0:08:01 (1.1182 s / it)\n",
      "Averaged stats: lr: 0.000190  loss: 1.1437 (1.1308)\n",
      "Valid: [epoch:146]  [ 0/14]  eta: 0:00:36  loss: 1.0467 (1.0467)  time: 2.5851  data: 2.3890  max mem: 15925\n",
      "Valid: [epoch:146]  [13/14]  eta: 0:00:00  loss: 1.0687 (1.0774)  time: 0.2925  data: 0.1715  max mem: 15925\n",
      "Valid: [epoch:146] Total time: 0:00:04 (0.3089 s / it)\n",
      "Averaged stats: loss: 1.0687 (1.0774)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_146_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.077%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n",
      "Train: [epoch:147]  [  0/431]  eta: 0:36:01  lr: 0.000190  loss: 1.1119 (1.1119)  time: 5.0159  data: 3.8421  max mem: 15925\n",
      "Train: [epoch:147]  [ 10/431]  eta: 0:09:58  lr: 0.000190  loss: 1.1682 (1.1968)  time: 1.4218  data: 0.3495  max mem: 15925\n",
      "Train: [epoch:147]  [ 20/431]  eta: 0:08:37  lr: 0.000190  loss: 1.1249 (1.1661)  time: 1.0710  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [ 30/431]  eta: 0:08:02  lr: 0.000190  loss: 1.0761 (1.1445)  time: 1.0843  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [ 40/431]  eta: 0:07:41  lr: 0.000190  loss: 1.0580 (1.1272)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [ 50/431]  eta: 0:07:24  lr: 0.000190  loss: 1.0773 (1.1227)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [ 60/431]  eta: 0:07:10  lr: 0.000190  loss: 1.0783 (1.1246)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [ 70/431]  eta: 0:06:55  lr: 0.000190  loss: 1.1320 (1.1198)  time: 1.1088  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:147]  [ 80/431]  eta: 0:06:42  lr: 0.000190  loss: 1.1023 (1.1222)  time: 1.1063  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:147]  [ 90/431]  eta: 0:06:29  lr: 0.000190  loss: 1.1023 (1.1198)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [100/431]  eta: 0:06:17  lr: 0.000190  loss: 1.0616 (1.1204)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [110/431]  eta: 0:06:05  lr: 0.000190  loss: 1.0907 (1.1207)  time: 1.1259  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [120/431]  eta: 0:05:53  lr: 0.000190  loss: 1.0968 (1.1185)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [130/431]  eta: 0:05:41  lr: 0.000190  loss: 1.0897 (1.1180)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [140/431]  eta: 0:05:30  lr: 0.000190  loss: 1.0739 (1.1190)  time: 1.1186  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:147]  [150/431]  eta: 0:05:18  lr: 0.000190  loss: 1.0900 (1.1194)  time: 1.1212  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:147]  [160/431]  eta: 0:05:07  lr: 0.000190  loss: 1.1251 (1.1222)  time: 1.1286  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [170/431]  eta: 0:04:55  lr: 0.000190  loss: 1.1075 (1.1227)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [180/431]  eta: 0:04:44  lr: 0.000190  loss: 1.0850 (1.1257)  time: 1.1284  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:147]  [190/431]  eta: 0:04:32  lr: 0.000190  loss: 1.1705 (1.1267)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [200/431]  eta: 0:04:21  lr: 0.000190  loss: 1.1364 (1.1262)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [210/431]  eta: 0:04:09  lr: 0.000190  loss: 1.1281 (1.1265)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [220/431]  eta: 0:03:58  lr: 0.000190  loss: 1.0998 (1.1266)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [230/431]  eta: 0:03:47  lr: 0.000190  loss: 1.0708 (1.1255)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [240/431]  eta: 0:03:35  lr: 0.000190  loss: 1.0708 (1.1245)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [250/431]  eta: 0:03:24  lr: 0.000190  loss: 1.1174 (1.1255)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [260/431]  eta: 0:03:12  lr: 0.000190  loss: 1.1242 (1.1258)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [270/431]  eta: 0:03:01  lr: 0.000190  loss: 1.0964 (1.1272)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [280/431]  eta: 0:02:50  lr: 0.000190  loss: 1.0932 (1.1266)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [290/431]  eta: 0:02:38  lr: 0.000190  loss: 1.0802 (1.1279)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [300/431]  eta: 0:02:27  lr: 0.000190  loss: 1.1117 (1.1300)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [310/431]  eta: 0:02:16  lr: 0.000190  loss: 1.1117 (1.1298)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [320/431]  eta: 0:02:04  lr: 0.000190  loss: 1.0879 (1.1293)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [330/431]  eta: 0:01:53  lr: 0.000190  loss: 1.1244 (1.1307)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [340/431]  eta: 0:01:42  lr: 0.000190  loss: 1.1556 (1.1313)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [350/431]  eta: 0:01:31  lr: 0.000190  loss: 1.1298 (1.1317)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [360/431]  eta: 0:01:19  lr: 0.000190  loss: 1.1511 (1.1326)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [370/431]  eta: 0:01:08  lr: 0.000190  loss: 1.0964 (1.1328)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [380/431]  eta: 0:00:57  lr: 0.000190  loss: 1.0792 (1.1309)  time: 1.1306  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [390/431]  eta: 0:00:46  lr: 0.000190  loss: 1.0900 (1.1314)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [400/431]  eta: 0:00:34  lr: 0.000190  loss: 1.1039 (1.1311)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [410/431]  eta: 0:00:23  lr: 0.000190  loss: 1.0995 (1.1303)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:147]  [420/431]  eta: 0:00:12  lr: 0.000190  loss: 1.1282 (1.1311)  time: 1.1182  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:147]  [430/431]  eta: 0:00:01  lr: 0.000190  loss: 1.1366 (1.1308)  time: 1.1192  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:147] Total time: 0:08:04 (1.1241 s / it)\n",
      "Averaged stats: lr: 0.000190  loss: 1.1366 (1.1308)\n",
      "Valid: [epoch:147]  [ 0/14]  eta: 0:00:36  loss: 1.0493 (1.0493)  time: 2.5741  data: 2.4076  max mem: 15925\n",
      "Valid: [epoch:147]  [13/14]  eta: 0:00:00  loss: 1.0625 (1.0738)  time: 0.2859  data: 0.1738  max mem: 15925\n",
      "Valid: [epoch:147] Total time: 0:00:04 (0.3030 s / it)\n",
      "Averaged stats: loss: 1.0625 (1.0738)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_147_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:148]  [  0/431]  eta: 0:37:08  lr: 0.000190  loss: 1.1463 (1.1463)  time: 5.1701  data: 4.0466  max mem: 15925\n",
      "Train: [epoch:148]  [ 10/431]  eta: 0:09:56  lr: 0.000190  loss: 1.1008 (1.1188)  time: 1.4175  data: 0.3681  max mem: 15925\n",
      "Train: [epoch:148]  [ 20/431]  eta: 0:08:37  lr: 0.000190  loss: 1.0836 (1.1143)  time: 1.0648  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [ 30/431]  eta: 0:08:03  lr: 0.000190  loss: 1.1260 (1.1200)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [ 40/431]  eta: 0:07:42  lr: 0.000190  loss: 1.0364 (1.1032)  time: 1.1004  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:148]  [ 50/431]  eta: 0:07:23  lr: 0.000190  loss: 1.0364 (1.1059)  time: 1.0971  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:148]  [ 60/431]  eta: 0:07:09  lr: 0.000190  loss: 1.0984 (1.1064)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [ 70/431]  eta: 0:06:54  lr: 0.000190  loss: 1.0984 (1.1134)  time: 1.1109  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:148]  [ 80/431]  eta: 0:06:41  lr: 0.000190  loss: 1.0807 (1.1109)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [ 90/431]  eta: 0:06:28  lr: 0.000190  loss: 1.0833 (1.1151)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [100/431]  eta: 0:06:16  lr: 0.000190  loss: 1.0912 (1.1145)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [110/431]  eta: 0:06:04  lr: 0.000190  loss: 1.0606 (1.1139)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [120/431]  eta: 0:05:53  lr: 0.000190  loss: 1.0842 (1.1125)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [130/431]  eta: 0:05:41  lr: 0.000190  loss: 1.0776 (1.1102)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [140/431]  eta: 0:05:29  lr: 0.000190  loss: 1.0535 (1.1091)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [150/431]  eta: 0:05:17  lr: 0.000190  loss: 1.0552 (1.1069)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [160/431]  eta: 0:05:05  lr: 0.000190  loss: 1.0586 (1.1088)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [170/431]  eta: 0:04:54  lr: 0.000190  loss: 1.1038 (1.1106)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [180/431]  eta: 0:04:43  lr: 0.000190  loss: 1.1548 (1.1141)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [190/431]  eta: 0:04:31  lr: 0.000190  loss: 1.1438 (1.1147)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [200/431]  eta: 0:04:20  lr: 0.000190  loss: 1.0856 (1.1150)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [210/431]  eta: 0:04:08  lr: 0.000190  loss: 1.0923 (1.1174)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [220/431]  eta: 0:03:57  lr: 0.000190  loss: 1.1310 (1.1186)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [230/431]  eta: 0:03:46  lr: 0.000190  loss: 1.0557 (1.1176)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [240/431]  eta: 0:03:34  lr: 0.000190  loss: 1.0904 (1.1188)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [250/431]  eta: 0:03:23  lr: 0.000190  loss: 1.1482 (1.1204)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [260/431]  eta: 0:03:12  lr: 0.000190  loss: 1.1342 (1.1217)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [270/431]  eta: 0:03:01  lr: 0.000190  loss: 1.1644 (1.1245)  time: 1.1285  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [280/431]  eta: 0:02:49  lr: 0.000190  loss: 1.1648 (1.1245)  time: 1.1194  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:148]  [290/431]  eta: 0:02:38  lr: 0.000190  loss: 1.1160 (1.1239)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [300/431]  eta: 0:02:27  lr: 0.000190  loss: 1.1118 (1.1245)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [310/431]  eta: 0:02:16  lr: 0.000190  loss: 1.1250 (1.1257)  time: 1.1324  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [320/431]  eta: 0:02:04  lr: 0.000190  loss: 1.1250 (1.1267)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [330/431]  eta: 0:01:53  lr: 0.000190  loss: 1.1551 (1.1296)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [340/431]  eta: 0:01:42  lr: 0.000190  loss: 1.1636 (1.1305)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [350/431]  eta: 0:01:31  lr: 0.000190  loss: 1.0910 (1.1290)  time: 1.1284  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:148]  [360/431]  eta: 0:01:19  lr: 0.000190  loss: 1.0388 (1.1273)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [370/431]  eta: 0:01:08  lr: 0.000190  loss: 1.0672 (1.1263)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [380/431]  eta: 0:00:57  lr: 0.000190  loss: 1.1391 (1.1271)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [390/431]  eta: 0:00:46  lr: 0.000190  loss: 1.1106 (1.1267)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:148]  [400/431]  eta: 0:00:34  lr: 0.000190  loss: 1.1090 (1.1277)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [410/431]  eta: 0:00:23  lr: 0.000190  loss: 1.1633 (1.1290)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:148]  [420/431]  eta: 0:00:12  lr: 0.000190  loss: 1.1539 (1.1300)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:148]  [430/431]  eta: 0:00:01  lr: 0.000190  loss: 1.1196 (1.1295)  time: 1.1138  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:148] Total time: 0:08:04 (1.1235 s / it)\n",
      "Averaged stats: lr: 0.000190  loss: 1.1196 (1.1295)\n",
      "Valid: [epoch:148]  [ 0/14]  eta: 0:00:35  loss: 1.1110 (1.1110)  time: 2.5452  data: 2.3490  max mem: 15925\n",
      "Valid: [epoch:148]  [13/14]  eta: 0:00:00  loss: 1.0600 (1.0709)  time: 0.2845  data: 0.1679  max mem: 15925\n",
      "Valid: [epoch:148] Total time: 0:00:04 (0.3000 s / it)\n",
      "Averaged stats: loss: 1.0600 (1.0709)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_148_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.071%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 140.000\n",
      "Train: [epoch:149]  [  0/431]  eta: 0:32:13  lr: 0.000189  loss: 1.1802 (1.1802)  time: 4.4859  data: 3.3136  max mem: 15925\n",
      "Train: [epoch:149]  [ 10/431]  eta: 0:09:49  lr: 0.000189  loss: 1.2444 (1.2435)  time: 1.4002  data: 0.3017  max mem: 15925\n",
      "Train: [epoch:149]  [ 20/431]  eta: 0:08:36  lr: 0.000189  loss: 1.1675 (1.1788)  time: 1.0946  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:149]  [ 30/431]  eta: 0:08:03  lr: 0.000189  loss: 1.0519 (1.1714)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [ 40/431]  eta: 0:07:43  lr: 0.000189  loss: 1.0682 (1.1521)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [ 50/431]  eta: 0:07:26  lr: 0.000189  loss: 1.0696 (1.1387)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [ 60/431]  eta: 0:07:12  lr: 0.000189  loss: 1.0504 (1.1326)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [ 70/431]  eta: 0:06:59  lr: 0.000189  loss: 1.0677 (1.1252)  time: 1.1371  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [ 80/431]  eta: 0:06:46  lr: 0.000189  loss: 1.0983 (1.1257)  time: 1.1335  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [ 90/431]  eta: 0:06:34  lr: 0.000189  loss: 1.1365 (1.1308)  time: 1.1325  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [100/431]  eta: 0:06:21  lr: 0.000189  loss: 1.1254 (1.1281)  time: 1.1291  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [110/431]  eta: 0:06:09  lr: 0.000189  loss: 1.1022 (1.1280)  time: 1.1380  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [120/431]  eta: 0:05:57  lr: 0.000189  loss: 1.1022 (1.1281)  time: 1.1406  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [130/431]  eta: 0:05:46  lr: 0.000189  loss: 1.1306 (1.1321)  time: 1.1409  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [140/431]  eta: 0:05:34  lr: 0.000189  loss: 1.1254 (1.1292)  time: 1.1471  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [150/431]  eta: 0:05:22  lr: 0.000189  loss: 1.1296 (1.1307)  time: 1.1333  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [160/431]  eta: 0:05:10  lr: 0.000189  loss: 1.1184 (1.1309)  time: 1.1171  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:149]  [170/431]  eta: 0:04:58  lr: 0.000189  loss: 1.1059 (1.1340)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [180/431]  eta: 0:04:46  lr: 0.000189  loss: 1.0965 (1.1335)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [190/431]  eta: 0:04:34  lr: 0.000189  loss: 1.1019 (1.1350)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [200/431]  eta: 0:04:23  lr: 0.000189  loss: 1.0815 (1.1323)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [210/431]  eta: 0:04:11  lr: 0.000189  loss: 1.0752 (1.1312)  time: 1.1341  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [220/431]  eta: 0:04:00  lr: 0.000189  loss: 1.1144 (1.1320)  time: 1.1296  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [230/431]  eta: 0:03:48  lr: 0.000189  loss: 1.0997 (1.1330)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [240/431]  eta: 0:03:37  lr: 0.000189  loss: 1.0997 (1.1324)  time: 1.1322  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [250/431]  eta: 0:03:26  lr: 0.000189  loss: 1.1175 (1.1320)  time: 1.1379  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [260/431]  eta: 0:03:14  lr: 0.000189  loss: 1.0830 (1.1307)  time: 1.1351  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [270/431]  eta: 0:03:03  lr: 0.000189  loss: 1.0952 (1.1317)  time: 1.1437  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [280/431]  eta: 0:02:51  lr: 0.000189  loss: 1.0952 (1.1291)  time: 1.1380  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [290/431]  eta: 0:02:40  lr: 0.000189  loss: 1.1004 (1.1301)  time: 1.1315  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [300/431]  eta: 0:02:29  lr: 0.000189  loss: 1.1236 (1.1308)  time: 1.1268  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [310/431]  eta: 0:02:17  lr: 0.000189  loss: 1.1475 (1.1316)  time: 1.1317  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [320/431]  eta: 0:02:06  lr: 0.000189  loss: 1.1540 (1.1310)  time: 1.1396  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [330/431]  eta: 0:01:54  lr: 0.000189  loss: 1.1540 (1.1320)  time: 1.1365  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [340/431]  eta: 0:01:43  lr: 0.000189  loss: 1.1609 (1.1330)  time: 1.1301  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [350/431]  eta: 0:01:32  lr: 0.000189  loss: 1.1609 (1.1328)  time: 1.1255  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:149]  [360/431]  eta: 0:01:20  lr: 0.000189  loss: 1.1600 (1.1340)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [370/431]  eta: 0:01:09  lr: 0.000189  loss: 1.1460 (1.1335)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [380/431]  eta: 0:00:57  lr: 0.000189  loss: 1.0888 (1.1330)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [390/431]  eta: 0:00:46  lr: 0.000189  loss: 1.0888 (1.1318)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [400/431]  eta: 0:00:35  lr: 0.000189  loss: 1.0743 (1.1315)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [410/431]  eta: 0:00:23  lr: 0.000189  loss: 1.1201 (1.1332)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [420/431]  eta: 0:00:12  lr: 0.000189  loss: 1.1201 (1.1322)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149]  [430/431]  eta: 0:00:01  lr: 0.000189  loss: 1.0533 (1.1308)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:149] Total time: 0:08:09 (1.1353 s / it)\n",
      "Averaged stats: lr: 0.000189  loss: 1.0533 (1.1308)\n",
      "Valid: [epoch:149]  [ 0/14]  eta: 0:00:35  loss: 1.1279 (1.1279)  time: 2.5390  data: 2.3702  max mem: 15925\n",
      "Valid: [epoch:149]  [13/14]  eta: 0:00:00  loss: 1.0574 (1.0689)  time: 0.2790  data: 0.1694  max mem: 15925\n",
      "Valid: [epoch:149] Total time: 0:00:04 (0.2956 s / it)\n",
      "Averaged stats: loss: 1.0574 (1.0689)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_149_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.069%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 149.000\n",
      "Train: [epoch:150]  [  0/431]  eta: 0:32:38  lr: 0.000189  loss: 0.9439 (0.9439)  time: 4.5443  data: 3.4107  max mem: 15925\n",
      "Train: [epoch:150]  [ 10/431]  eta: 0:09:45  lr: 0.000189  loss: 1.1315 (1.1984)  time: 1.3905  data: 0.3104  max mem: 15925\n",
      "Train: [epoch:150]  [ 20/431]  eta: 0:08:33  lr: 0.000189  loss: 1.1192 (1.1602)  time: 1.0854  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [ 30/431]  eta: 0:08:00  lr: 0.000189  loss: 1.0749 (1.1504)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [ 40/431]  eta: 0:07:39  lr: 0.000189  loss: 1.0503 (1.1259)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [ 50/431]  eta: 0:07:24  lr: 0.000189  loss: 1.0365 (1.1203)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [ 60/431]  eta: 0:07:12  lr: 0.000189  loss: 1.0703 (1.1272)  time: 1.1424  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [ 70/431]  eta: 0:06:59  lr: 0.000189  loss: 1.1074 (1.1215)  time: 1.1466  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [ 80/431]  eta: 0:06:46  lr: 0.000189  loss: 1.0909 (1.1309)  time: 1.1364  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [ 90/431]  eta: 0:06:34  lr: 0.000189  loss: 1.1586 (1.1316)  time: 1.1402  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [100/431]  eta: 0:06:21  lr: 0.000189  loss: 1.1632 (1.1356)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [110/431]  eta: 0:06:09  lr: 0.000189  loss: 1.0955 (1.1275)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [120/431]  eta: 0:05:56  lr: 0.000189  loss: 1.0592 (1.1284)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [130/431]  eta: 0:05:44  lr: 0.000189  loss: 1.1043 (1.1328)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [140/431]  eta: 0:05:33  lr: 0.000189  loss: 1.1043 (1.1316)  time: 1.1299  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [150/431]  eta: 0:05:21  lr: 0.000189  loss: 1.0902 (1.1302)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [160/431]  eta: 0:05:09  lr: 0.000189  loss: 1.1175 (1.1306)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [170/431]  eta: 0:04:57  lr: 0.000189  loss: 1.1083 (1.1284)  time: 1.1292  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [180/431]  eta: 0:04:46  lr: 0.000189  loss: 1.0983 (1.1294)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [190/431]  eta: 0:04:34  lr: 0.000189  loss: 1.0874 (1.1277)  time: 1.1323  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [200/431]  eta: 0:04:23  lr: 0.000189  loss: 1.0808 (1.1279)  time: 1.1389  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [210/431]  eta: 0:04:11  lr: 0.000189  loss: 1.1494 (1.1283)  time: 1.1311  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [220/431]  eta: 0:04:00  lr: 0.000189  loss: 1.1223 (1.1313)  time: 1.1278  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [230/431]  eta: 0:03:48  lr: 0.000189  loss: 1.1223 (1.1301)  time: 1.1378  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [240/431]  eta: 0:03:37  lr: 0.000189  loss: 1.1179 (1.1290)  time: 1.1359  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [250/431]  eta: 0:03:26  lr: 0.000189  loss: 1.0846 (1.1290)  time: 1.1325  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [260/431]  eta: 0:03:14  lr: 0.000189  loss: 1.0676 (1.1275)  time: 1.1302  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [270/431]  eta: 0:03:03  lr: 0.000189  loss: 1.1125 (1.1287)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [280/431]  eta: 0:02:51  lr: 0.000189  loss: 1.1125 (1.1276)  time: 1.1328  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [290/431]  eta: 0:02:40  lr: 0.000189  loss: 1.0819 (1.1266)  time: 1.1324  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [300/431]  eta: 0:02:28  lr: 0.000189  loss: 1.1246 (1.1282)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [310/431]  eta: 0:02:17  lr: 0.000189  loss: 1.1133 (1.1259)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [320/431]  eta: 0:02:06  lr: 0.000189  loss: 1.1005 (1.1264)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [330/431]  eta: 0:01:54  lr: 0.000189  loss: 1.1073 (1.1275)  time: 1.1125  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:150]  [340/431]  eta: 0:01:43  lr: 0.000189  loss: 1.0929 (1.1282)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [350/431]  eta: 0:01:31  lr: 0.000189  loss: 1.0929 (1.1283)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [360/431]  eta: 0:01:20  lr: 0.000189  loss: 1.0784 (1.1278)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [370/431]  eta: 0:01:09  lr: 0.000189  loss: 1.0784 (1.1273)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [380/431]  eta: 0:00:57  lr: 0.000189  loss: 1.1569 (1.1296)  time: 1.1304  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [390/431]  eta: 0:00:46  lr: 0.000189  loss: 1.1913 (1.1308)  time: 1.1339  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:150]  [400/431]  eta: 0:00:35  lr: 0.000189  loss: 1.1252 (1.1321)  time: 1.1239  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:150]  [410/431]  eta: 0:00:23  lr: 0.000189  loss: 1.0820 (1.1310)  time: 1.1302  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [420/431]  eta: 0:00:12  lr: 0.000189  loss: 1.0991 (1.1320)  time: 1.1343  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150]  [430/431]  eta: 0:00:01  lr: 0.000189  loss: 1.1259 (1.1322)  time: 1.1357  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:150] Total time: 0:08:08 (1.1338 s / it)\n",
      "Averaged stats: lr: 0.000189  loss: 1.1259 (1.1322)\n",
      "Valid: [epoch:150]  [ 0/14]  eta: 0:00:46  loss: 1.1282 (1.1282)  time: 3.3262  data: 3.1594  max mem: 15925\n",
      "Valid: [epoch:150]  [13/14]  eta: 0:00:00  loss: 1.0605 (1.0719)  time: 0.3208  data: 0.2258  max mem: 15925\n",
      "Valid: [epoch:150] Total time: 0:00:04 (0.3361 s / it)\n",
      "Averaged stats: loss: 1.0605 (1.0719)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_150_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 149.000\n",
      "Train: [epoch:151]  [  0/431]  eta: 0:36:47  lr: 0.000189  loss: 1.4006 (1.4006)  time: 5.1227  data: 3.9333  max mem: 15925\n",
      "Train: [epoch:151]  [ 10/431]  eta: 0:09:57  lr: 0.000189  loss: 1.1579 (1.1930)  time: 1.4184  data: 0.3578  max mem: 15925\n",
      "Train: [epoch:151]  [ 20/431]  eta: 0:08:36  lr: 0.000189  loss: 1.1072 (1.1671)  time: 1.0626  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [ 30/431]  eta: 0:08:05  lr: 0.000189  loss: 1.1038 (1.1572)  time: 1.0960  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [ 40/431]  eta: 0:07:43  lr: 0.000189  loss: 1.0988 (1.1461)  time: 1.1136  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [ 50/431]  eta: 0:07:27  lr: 0.000189  loss: 1.1033 (1.1439)  time: 1.1158  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [ 60/431]  eta: 0:07:12  lr: 0.000189  loss: 1.1011 (1.1377)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [ 70/431]  eta: 0:06:58  lr: 0.000189  loss: 1.0807 (1.1377)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [ 80/431]  eta: 0:06:45  lr: 0.000189  loss: 1.1112 (1.1428)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [ 90/431]  eta: 0:06:32  lr: 0.000189  loss: 1.1112 (1.1385)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [100/431]  eta: 0:06:20  lr: 0.000189  loss: 1.0851 (1.1344)  time: 1.1297  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [110/431]  eta: 0:06:08  lr: 0.000189  loss: 1.0735 (1.1351)  time: 1.1351  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [120/431]  eta: 0:05:57  lr: 0.000189  loss: 1.1103 (1.1355)  time: 1.1459  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [130/431]  eta: 0:05:44  lr: 0.000189  loss: 1.0628 (1.1280)  time: 1.1318  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [140/431]  eta: 0:05:33  lr: 0.000189  loss: 1.0657 (1.1283)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [150/431]  eta: 0:05:21  lr: 0.000189  loss: 1.0992 (1.1284)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [160/431]  eta: 0:05:09  lr: 0.000189  loss: 1.0644 (1.1286)  time: 1.1328  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [170/431]  eta: 0:04:57  lr: 0.000189  loss: 1.0795 (1.1276)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [180/431]  eta: 0:04:46  lr: 0.000189  loss: 1.1093 (1.1283)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [190/431]  eta: 0:04:34  lr: 0.000189  loss: 1.0977 (1.1299)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [200/431]  eta: 0:04:23  lr: 0.000189  loss: 1.1145 (1.1307)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [210/431]  eta: 0:04:11  lr: 0.000189  loss: 1.1252 (1.1305)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [220/431]  eta: 0:04:00  lr: 0.000189  loss: 1.0723 (1.1276)  time: 1.1350  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [230/431]  eta: 0:03:48  lr: 0.000189  loss: 1.0479 (1.1259)  time: 1.1453  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [240/431]  eta: 0:03:37  lr: 0.000189  loss: 1.0979 (1.1266)  time: 1.1369  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [250/431]  eta: 0:03:26  lr: 0.000189  loss: 1.1459 (1.1273)  time: 1.1352  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [260/431]  eta: 0:03:14  lr: 0.000189  loss: 1.0981 (1.1285)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [270/431]  eta: 0:03:02  lr: 0.000189  loss: 1.0858 (1.1280)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [280/431]  eta: 0:02:51  lr: 0.000189  loss: 1.0811 (1.1258)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [290/431]  eta: 0:02:40  lr: 0.000189  loss: 1.0951 (1.1250)  time: 1.1286  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [300/431]  eta: 0:02:28  lr: 0.000189  loss: 1.1178 (1.1250)  time: 1.1349  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [310/431]  eta: 0:02:17  lr: 0.000189  loss: 1.0943 (1.1245)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [320/431]  eta: 0:02:05  lr: 0.000189  loss: 1.0983 (1.1247)  time: 1.1229  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [330/431]  eta: 0:01:54  lr: 0.000189  loss: 1.1011 (1.1247)  time: 1.1306  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [340/431]  eta: 0:01:43  lr: 0.000189  loss: 1.1069 (1.1242)  time: 1.1342  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [350/431]  eta: 0:01:31  lr: 0.000189  loss: 1.1301 (1.1246)  time: 1.1368  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [360/431]  eta: 0:01:20  lr: 0.000189  loss: 1.1295 (1.1247)  time: 1.1350  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [370/431]  eta: 0:01:09  lr: 0.000189  loss: 1.1499 (1.1248)  time: 1.1344  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [380/431]  eta: 0:00:57  lr: 0.000189  loss: 1.1499 (1.1259)  time: 1.1316  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [390/431]  eta: 0:00:46  lr: 0.000189  loss: 1.1503 (1.1267)  time: 1.1343  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:151]  [400/431]  eta: 0:00:35  lr: 0.000189  loss: 1.1186 (1.1271)  time: 1.1360  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:151]  [410/431]  eta: 0:00:23  lr: 0.000189  loss: 1.0893 (1.1280)  time: 1.1294  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:151]  [420/431]  eta: 0:00:12  lr: 0.000189  loss: 1.0967 (1.1295)  time: 1.1414  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151]  [430/431]  eta: 0:00:01  lr: 0.000189  loss: 1.1066 (1.1300)  time: 1.1478  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:151] Total time: 0:08:09 (1.1363 s / it)\n",
      "Averaged stats: lr: 0.000189  loss: 1.1066 (1.1300)\n",
      "Valid: [epoch:151]  [ 0/14]  eta: 0:00:36  loss: 1.1461 (1.1461)  time: 2.5851  data: 2.4114  max mem: 15925\n",
      "Valid: [epoch:151]  [13/14]  eta: 0:00:00  loss: 1.0953 (1.1007)  time: 0.2769  data: 0.1723  max mem: 15925\n",
      "Valid: [epoch:151] Total time: 0:00:04 (0.2918 s / it)\n",
      "Averaged stats: loss: 1.0953 (1.1007)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_151_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.101%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 149.000\n",
      "Train: [epoch:152]  [  0/431]  eta: 0:30:26  lr: 0.000189  loss: 1.2555 (1.2555)  time: 4.2370  data: 3.0926  max mem: 15925\n",
      "Train: [epoch:152]  [ 10/431]  eta: 0:09:25  lr: 0.000189  loss: 1.1298 (1.1740)  time: 1.3441  data: 0.2814  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:152]  [ 20/431]  eta: 0:08:19  lr: 0.000189  loss: 1.1765 (1.1840)  time: 1.0645  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [ 30/431]  eta: 0:07:52  lr: 0.000189  loss: 1.1554 (1.1609)  time: 1.0852  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [ 40/431]  eta: 0:07:33  lr: 0.000189  loss: 1.1037 (1.1449)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [ 50/431]  eta: 0:07:18  lr: 0.000189  loss: 1.1037 (1.1453)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [ 60/431]  eta: 0:07:06  lr: 0.000189  loss: 1.1343 (1.1502)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [ 70/431]  eta: 0:06:53  lr: 0.000189  loss: 1.1088 (1.1426)  time: 1.1313  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [ 80/431]  eta: 0:06:41  lr: 0.000189  loss: 1.0978 (1.1370)  time: 1.1288  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [ 90/431]  eta: 0:06:29  lr: 0.000189  loss: 1.0375 (1.1286)  time: 1.1349  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [100/431]  eta: 0:06:18  lr: 0.000189  loss: 1.0258 (1.1257)  time: 1.1333  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [110/431]  eta: 0:06:06  lr: 0.000189  loss: 1.1243 (1.1286)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [120/431]  eta: 0:05:54  lr: 0.000189  loss: 1.1314 (1.1323)  time: 1.1298  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [130/431]  eta: 0:05:43  lr: 0.000189  loss: 1.1230 (1.1309)  time: 1.1410  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [140/431]  eta: 0:05:31  lr: 0.000189  loss: 1.0887 (1.1301)  time: 1.1411  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [150/431]  eta: 0:05:20  lr: 0.000189  loss: 1.1000 (1.1319)  time: 1.1342  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [160/431]  eta: 0:05:08  lr: 0.000189  loss: 1.1390 (1.1327)  time: 1.1306  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [170/431]  eta: 0:04:57  lr: 0.000189  loss: 1.1095 (1.1311)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [180/431]  eta: 0:04:45  lr: 0.000189  loss: 1.0423 (1.1327)  time: 1.1312  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [190/431]  eta: 0:04:34  lr: 0.000189  loss: 1.0939 (1.1316)  time: 1.1341  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [200/431]  eta: 0:04:22  lr: 0.000189  loss: 1.0939 (1.1316)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [210/431]  eta: 0:04:10  lr: 0.000189  loss: 1.1138 (1.1321)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [220/431]  eta: 0:03:59  lr: 0.000189  loss: 1.0832 (1.1316)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [230/431]  eta: 0:03:47  lr: 0.000189  loss: 1.0408 (1.1289)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [240/431]  eta: 0:03:36  lr: 0.000189  loss: 1.0505 (1.1268)  time: 1.1293  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [250/431]  eta: 0:03:25  lr: 0.000189  loss: 1.0693 (1.1276)  time: 1.1325  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [260/431]  eta: 0:03:13  lr: 0.000189  loss: 1.0881 (1.1257)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [270/431]  eta: 0:03:02  lr: 0.000189  loss: 1.0881 (1.1261)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [280/431]  eta: 0:02:51  lr: 0.000189  loss: 1.1158 (1.1266)  time: 1.1326  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [290/431]  eta: 0:02:39  lr: 0.000189  loss: 1.1118 (1.1262)  time: 1.1332  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [300/431]  eta: 0:02:28  lr: 0.000189  loss: 1.0979 (1.1258)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [310/431]  eta: 0:02:17  lr: 0.000189  loss: 1.0979 (1.1273)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [320/431]  eta: 0:02:05  lr: 0.000189  loss: 1.1019 (1.1264)  time: 1.1341  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [330/431]  eta: 0:01:54  lr: 0.000189  loss: 1.1259 (1.1293)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [340/431]  eta: 0:01:43  lr: 0.000189  loss: 1.1466 (1.1290)  time: 1.1227  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [350/431]  eta: 0:01:31  lr: 0.000189  loss: 1.1107 (1.1294)  time: 1.1425  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [360/431]  eta: 0:01:20  lr: 0.000189  loss: 1.1388 (1.1306)  time: 1.1540  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [370/431]  eta: 0:01:09  lr: 0.000189  loss: 1.1388 (1.1309)  time: 1.1419  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [380/431]  eta: 0:00:57  lr: 0.000189  loss: 1.1153 (1.1312)  time: 1.1336  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [390/431]  eta: 0:00:46  lr: 0.000189  loss: 1.1062 (1.1315)  time: 1.1293  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:152]  [400/431]  eta: 0:00:35  lr: 0.000189  loss: 1.0693 (1.1311)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [410/431]  eta: 0:00:23  lr: 0.000189  loss: 1.1260 (1.1320)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [420/431]  eta: 0:00:12  lr: 0.000189  loss: 1.1013 (1.1306)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:152]  [430/431]  eta: 0:00:01  lr: 0.000189  loss: 1.1013 (1.1313)  time: 1.1213  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:152] Total time: 0:08:08 (1.1324 s / it)\n",
      "Averaged stats: lr: 0.000189  loss: 1.1013 (1.1313)\n",
      "Valid: [epoch:152]  [ 0/14]  eta: 0:00:35  loss: 1.1293 (1.1293)  time: 2.5526  data: 2.3701  max mem: 15925\n",
      "Valid: [epoch:152]  [13/14]  eta: 0:00:00  loss: 1.0606 (1.0723)  time: 0.2872  data: 0.1694  max mem: 15925\n",
      "Valid: [epoch:152] Total time: 0:00:04 (0.3050 s / it)\n",
      "Averaged stats: loss: 1.0606 (1.0723)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_152_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 149.000\n",
      "Train: [epoch:153]  [  0/431]  eta: 0:32:33  lr: 0.000188  loss: 1.1343 (1.1343)  time: 4.5322  data: 3.3098  max mem: 15925\n",
      "Train: [epoch:153]  [ 10/431]  eta: 0:09:37  lr: 0.000188  loss: 1.0689 (1.1023)  time: 1.3722  data: 0.3011  max mem: 15925\n",
      "Train: [epoch:153]  [ 20/431]  eta: 0:08:30  lr: 0.000188  loss: 1.0750 (1.1109)  time: 1.0765  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [ 30/431]  eta: 0:08:00  lr: 0.000188  loss: 1.0750 (1.1013)  time: 1.1015  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [ 40/431]  eta: 0:07:40  lr: 0.000188  loss: 1.1126 (1.1065)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [ 50/431]  eta: 0:07:25  lr: 0.000188  loss: 1.1184 (1.1129)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [ 60/431]  eta: 0:07:13  lr: 0.000188  loss: 1.0510 (1.0966)  time: 1.1439  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [ 70/431]  eta: 0:06:59  lr: 0.000188  loss: 1.0333 (1.0990)  time: 1.1429  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [ 80/431]  eta: 0:06:47  lr: 0.000188  loss: 1.0791 (1.1041)  time: 1.1389  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [ 90/431]  eta: 0:06:34  lr: 0.000188  loss: 1.0600 (1.1036)  time: 1.1343  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [100/431]  eta: 0:06:22  lr: 0.000188  loss: 1.1186 (1.1114)  time: 1.1399  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [110/431]  eta: 0:06:09  lr: 0.000188  loss: 1.1092 (1.1081)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [120/431]  eta: 0:05:57  lr: 0.000188  loss: 1.0974 (1.1098)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [130/431]  eta: 0:05:45  lr: 0.000188  loss: 1.1061 (1.1085)  time: 1.1359  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [140/431]  eta: 0:05:33  lr: 0.000188  loss: 1.1215 (1.1104)  time: 1.1323  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [150/431]  eta: 0:05:21  lr: 0.000188  loss: 1.1091 (1.1112)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [160/431]  eta: 0:05:09  lr: 0.000188  loss: 1.1242 (1.1153)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [170/431]  eta: 0:04:58  lr: 0.000188  loss: 1.1294 (1.1157)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [180/431]  eta: 0:04:46  lr: 0.000188  loss: 1.1220 (1.1151)  time: 1.1190  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:153]  [190/431]  eta: 0:04:34  lr: 0.000188  loss: 1.0999 (1.1148)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [200/431]  eta: 0:04:23  lr: 0.000188  loss: 1.1263 (1.1169)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [210/431]  eta: 0:04:11  lr: 0.000188  loss: 1.1332 (1.1180)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [220/431]  eta: 0:03:59  lr: 0.000188  loss: 1.1622 (1.1217)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [230/431]  eta: 0:03:48  lr: 0.000188  loss: 1.1321 (1.1218)  time: 1.1190  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [240/431]  eta: 0:03:37  lr: 0.000188  loss: 1.1040 (1.1219)  time: 1.1325  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [250/431]  eta: 0:03:25  lr: 0.000188  loss: 1.1040 (1.1211)  time: 1.1433  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [260/431]  eta: 0:03:14  lr: 0.000188  loss: 1.1535 (1.1236)  time: 1.1377  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [270/431]  eta: 0:03:02  lr: 0.000188  loss: 1.1308 (1.1226)  time: 1.1297  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [280/431]  eta: 0:02:51  lr: 0.000188  loss: 1.0809 (1.1237)  time: 1.1352  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [290/431]  eta: 0:02:40  lr: 0.000188  loss: 1.1066 (1.1226)  time: 1.1322  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [300/431]  eta: 0:02:28  lr: 0.000188  loss: 1.0920 (1.1228)  time: 1.1304  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:153]  [310/431]  eta: 0:02:17  lr: 0.000188  loss: 1.1329 (1.1243)  time: 1.1317  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [320/431]  eta: 0:02:06  lr: 0.000188  loss: 1.1299 (1.1254)  time: 1.1357  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [330/431]  eta: 0:01:54  lr: 0.000188  loss: 1.1194 (1.1260)  time: 1.1401  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [340/431]  eta: 0:01:43  lr: 0.000188  loss: 1.1282 (1.1275)  time: 1.1295  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [350/431]  eta: 0:01:31  lr: 0.000188  loss: 1.1057 (1.1272)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [360/431]  eta: 0:01:20  lr: 0.000188  loss: 1.0871 (1.1269)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [370/431]  eta: 0:01:09  lr: 0.000188  loss: 1.1446 (1.1293)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [380/431]  eta: 0:00:57  lr: 0.000188  loss: 1.1648 (1.1302)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [390/431]  eta: 0:00:46  lr: 0.000188  loss: 1.1908 (1.1315)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [400/431]  eta: 0:00:35  lr: 0.000188  loss: 1.1369 (1.1310)  time: 1.1349  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [410/431]  eta: 0:00:23  lr: 0.000188  loss: 1.0917 (1.1302)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [420/431]  eta: 0:00:12  lr: 0.000188  loss: 1.0917 (1.1304)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153]  [430/431]  eta: 0:00:01  lr: 0.000188  loss: 1.0893 (1.1302)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:153] Total time: 0:08:08 (1.1335 s / it)\n",
      "Averaged stats: lr: 0.000188  loss: 1.0893 (1.1302)\n",
      "Valid: [epoch:153]  [ 0/14]  eta: 0:00:32  loss: 1.1643 (1.1643)  time: 2.3059  data: 2.1386  max mem: 15925\n",
      "Valid: [epoch:153]  [13/14]  eta: 0:00:00  loss: 1.0588 (1.0694)  time: 0.2510  data: 0.1528  max mem: 15925\n",
      "Valid: [epoch:153] Total time: 0:00:03 (0.2684 s / it)\n",
      "Averaged stats: loss: 1.0588 (1.0694)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_153_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.069%\n",
      "Min loss: 1.069\n",
      "Best Epoch: 149.000\n",
      "Train: [epoch:154]  [  0/431]  eta: 0:31:28  lr: 0.000188  loss: 1.1608 (1.1608)  time: 4.3819  data: 3.2047  max mem: 15925\n",
      "Train: [epoch:154]  [ 10/431]  eta: 0:09:31  lr: 0.000188  loss: 1.1321 (1.1576)  time: 1.3576  data: 0.2915  max mem: 15925\n",
      "Train: [epoch:154]  [ 20/431]  eta: 0:08:27  lr: 0.000188  loss: 1.1321 (1.1631)  time: 1.0763  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [ 30/431]  eta: 0:07:59  lr: 0.000188  loss: 1.1363 (1.1454)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [ 40/431]  eta: 0:07:39  lr: 0.000188  loss: 1.0393 (1.1197)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [ 50/431]  eta: 0:07:24  lr: 0.000188  loss: 1.0332 (1.1074)  time: 1.1189  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [ 60/431]  eta: 0:07:10  lr: 0.000188  loss: 1.0568 (1.1063)  time: 1.1307  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [ 70/431]  eta: 0:06:58  lr: 0.000188  loss: 1.1011 (1.1177)  time: 1.1420  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [ 80/431]  eta: 0:06:45  lr: 0.000188  loss: 1.1957 (1.1322)  time: 1.1337  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [ 90/431]  eta: 0:06:32  lr: 0.000188  loss: 1.1498 (1.1314)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [100/431]  eta: 0:06:19  lr: 0.000188  loss: 1.0828 (1.1292)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [110/431]  eta: 0:06:06  lr: 0.000188  loss: 1.0772 (1.1286)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [120/431]  eta: 0:05:54  lr: 0.000188  loss: 1.0690 (1.1292)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [130/431]  eta: 0:05:42  lr: 0.000188  loss: 1.1249 (1.1313)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [140/431]  eta: 0:05:31  lr: 0.000188  loss: 1.1011 (1.1283)  time: 1.1336  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [150/431]  eta: 0:05:19  lr: 0.000188  loss: 1.1011 (1.1295)  time: 1.1335  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [160/431]  eta: 0:05:08  lr: 0.000188  loss: 1.1340 (1.1318)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [170/431]  eta: 0:04:56  lr: 0.000188  loss: 1.1268 (1.1338)  time: 1.1295  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [180/431]  eta: 0:04:45  lr: 0.000188  loss: 1.1052 (1.1355)  time: 1.1380  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [190/431]  eta: 0:04:33  lr: 0.000188  loss: 1.0731 (1.1333)  time: 1.1324  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [200/431]  eta: 0:04:22  lr: 0.000188  loss: 1.0746 (1.1319)  time: 1.1288  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [210/431]  eta: 0:04:10  lr: 0.000188  loss: 1.0923 (1.1313)  time: 1.1127  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [220/431]  eta: 0:03:59  lr: 0.000188  loss: 1.0995 (1.1320)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [230/431]  eta: 0:03:47  lr: 0.000188  loss: 1.1282 (1.1328)  time: 1.1232  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [240/431]  eta: 0:03:36  lr: 0.000188  loss: 1.1123 (1.1321)  time: 1.1312  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [250/431]  eta: 0:03:24  lr: 0.000188  loss: 1.0803 (1.1312)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [260/431]  eta: 0:03:13  lr: 0.000188  loss: 1.1190 (1.1302)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [270/431]  eta: 0:03:02  lr: 0.000188  loss: 1.1141 (1.1303)  time: 1.1277  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [280/431]  eta: 0:02:51  lr: 0.000188  loss: 1.1048 (1.1287)  time: 1.1375  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [290/431]  eta: 0:02:39  lr: 0.000188  loss: 1.1048 (1.1289)  time: 1.1310  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [300/431]  eta: 0:02:28  lr: 0.000188  loss: 1.1280 (1.1298)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [310/431]  eta: 0:02:16  lr: 0.000188  loss: 1.0834 (1.1275)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [320/431]  eta: 0:02:05  lr: 0.000188  loss: 1.0772 (1.1274)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [330/431]  eta: 0:01:54  lr: 0.000188  loss: 1.1038 (1.1292)  time: 1.1292  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [340/431]  eta: 0:01:42  lr: 0.000188  loss: 1.1047 (1.1296)  time: 1.1387  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [350/431]  eta: 0:01:31  lr: 0.000188  loss: 1.1047 (1.1289)  time: 1.1274  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:154]  [360/431]  eta: 0:01:20  lr: 0.000188  loss: 1.1215 (1.1298)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [370/431]  eta: 0:01:09  lr: 0.000188  loss: 1.0700 (1.1278)  time: 1.1444  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [380/431]  eta: 0:00:57  lr: 0.000188  loss: 1.0880 (1.1295)  time: 1.1406  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [390/431]  eta: 0:00:46  lr: 0.000188  loss: 1.1350 (1.1300)  time: 1.1232  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:154]  [400/431]  eta: 0:00:35  lr: 0.000188  loss: 1.0937 (1.1297)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [410/431]  eta: 0:00:23  lr: 0.000188  loss: 1.0784 (1.1303)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [420/431]  eta: 0:00:12  lr: 0.000188  loss: 1.1177 (1.1305)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154]  [430/431]  eta: 0:00:01  lr: 0.000188  loss: 1.0892 (1.1301)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:154] Total time: 0:08:07 (1.1313 s / it)\n",
      "Averaged stats: lr: 0.000188  loss: 1.0892 (1.1301)\n",
      "Valid: [epoch:154]  [ 0/14]  eta: 0:00:35  loss: 1.0429 (1.0429)  time: 2.5470  data: 2.3542  max mem: 15925\n",
      "Valid: [epoch:154]  [13/14]  eta: 0:00:00  loss: 1.0555 (1.0668)  time: 0.2874  data: 0.1683  max mem: 15925\n",
      "Valid: [epoch:154] Total time: 0:00:04 (0.3037 s / it)\n",
      "Averaged stats: loss: 1.0555 (1.0668)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_154_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.067%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:155]  [  0/431]  eta: 0:31:42  lr: 0.000188  loss: 1.0130 (1.0130)  time: 4.4145  data: 3.2125  max mem: 15925\n",
      "Train: [epoch:155]  [ 10/431]  eta: 0:09:44  lr: 0.000188  loss: 1.1223 (1.1604)  time: 1.3884  data: 0.2923  max mem: 15925\n",
      "Train: [epoch:155]  [ 20/431]  eta: 0:08:35  lr: 0.000188  loss: 1.1496 (1.1628)  time: 1.0975  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [ 30/431]  eta: 0:08:00  lr: 0.000188  loss: 1.1352 (1.1544)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [ 40/431]  eta: 0:07:39  lr: 0.000188  loss: 1.1289 (1.1512)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [ 50/431]  eta: 0:07:22  lr: 0.000188  loss: 1.1190 (1.1422)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [ 60/431]  eta: 0:07:07  lr: 0.000188  loss: 1.1159 (1.1444)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [ 70/431]  eta: 0:06:54  lr: 0.000188  loss: 1.0966 (1.1350)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [ 80/431]  eta: 0:06:42  lr: 0.000188  loss: 1.0605 (1.1320)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [ 90/431]  eta: 0:06:30  lr: 0.000188  loss: 1.0824 (1.1297)  time: 1.1407  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [100/431]  eta: 0:06:19  lr: 0.000188  loss: 1.0979 (1.1266)  time: 1.1489  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [110/431]  eta: 0:06:06  lr: 0.000188  loss: 1.1119 (1.1312)  time: 1.1306  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [120/431]  eta: 0:05:55  lr: 0.000188  loss: 1.1299 (1.1307)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [130/431]  eta: 0:05:43  lr: 0.000188  loss: 1.0774 (1.1264)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [140/431]  eta: 0:05:31  lr: 0.000188  loss: 1.0718 (1.1226)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [150/431]  eta: 0:05:20  lr: 0.000188  loss: 1.0813 (1.1261)  time: 1.1325  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [160/431]  eta: 0:05:08  lr: 0.000188  loss: 1.0735 (1.1244)  time: 1.1370  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [170/431]  eta: 0:04:56  lr: 0.000188  loss: 1.1067 (1.1250)  time: 1.1302  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [180/431]  eta: 0:04:45  lr: 0.000188  loss: 1.1447 (1.1271)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [190/431]  eta: 0:04:33  lr: 0.000188  loss: 1.1837 (1.1304)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [200/431]  eta: 0:04:22  lr: 0.000188  loss: 1.0790 (1.1290)  time: 1.1378  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [210/431]  eta: 0:04:11  lr: 0.000188  loss: 1.0790 (1.1294)  time: 1.1531  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [220/431]  eta: 0:04:00  lr: 0.000188  loss: 1.0913 (1.1288)  time: 1.1478  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [230/431]  eta: 0:03:48  lr: 0.000188  loss: 1.0913 (1.1287)  time: 1.1335  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [240/431]  eta: 0:03:37  lr: 0.000188  loss: 1.1235 (1.1303)  time: 1.1193  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [250/431]  eta: 0:03:25  lr: 0.000188  loss: 1.1524 (1.1322)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [260/431]  eta: 0:03:14  lr: 0.000188  loss: 1.1458 (1.1323)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [270/431]  eta: 0:03:02  lr: 0.000188  loss: 1.0990 (1.1331)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [280/431]  eta: 0:02:51  lr: 0.000188  loss: 1.0876 (1.1318)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [290/431]  eta: 0:02:39  lr: 0.000188  loss: 1.0753 (1.1316)  time: 1.1182  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [300/431]  eta: 0:02:28  lr: 0.000188  loss: 1.1034 (1.1306)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [310/431]  eta: 0:02:17  lr: 0.000188  loss: 1.1162 (1.1314)  time: 1.1302  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [320/431]  eta: 0:02:05  lr: 0.000188  loss: 1.0778 (1.1300)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [330/431]  eta: 0:01:54  lr: 0.000188  loss: 1.1185 (1.1322)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [340/431]  eta: 0:01:43  lr: 0.000188  loss: 1.1409 (1.1313)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [350/431]  eta: 0:01:31  lr: 0.000188  loss: 1.0866 (1.1314)  time: 1.1385  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [360/431]  eta: 0:01:20  lr: 0.000188  loss: 1.0923 (1.1305)  time: 1.1437  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [370/431]  eta: 0:01:09  lr: 0.000188  loss: 1.0724 (1.1296)  time: 1.1419  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [380/431]  eta: 0:00:57  lr: 0.000188  loss: 1.0491 (1.1277)  time: 1.1396  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [390/431]  eta: 0:00:46  lr: 0.000188  loss: 1.0892 (1.1290)  time: 1.1352  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [400/431]  eta: 0:00:35  lr: 0.000188  loss: 1.1745 (1.1313)  time: 1.1358  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:155]  [410/431]  eta: 0:00:23  lr: 0.000188  loss: 1.1508 (1.1307)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [420/431]  eta: 0:00:12  lr: 0.000188  loss: 1.0968 (1.1311)  time: 1.1267  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155]  [430/431]  eta: 0:00:01  lr: 0.000188  loss: 1.1055 (1.1313)  time: 1.1372  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:155] Total time: 0:08:08 (1.1338 s / it)\n",
      "Averaged stats: lr: 0.000188  loss: 1.1055 (1.1313)\n",
      "Valid: [epoch:155]  [ 0/14]  eta: 0:00:36  loss: 1.0529 (1.0529)  time: 2.6196  data: 2.4281  max mem: 15925\n",
      "Valid: [epoch:155]  [13/14]  eta: 0:00:00  loss: 1.0840 (1.0886)  time: 0.2803  data: 0.1735  max mem: 15925\n",
      "Valid: [epoch:155] Total time: 0:00:04 (0.2968 s / it)\n",
      "Averaged stats: loss: 1.0840 (1.0886)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_155_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.089%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:156]  [  0/431]  eta: 0:34:46  lr: 0.000188  loss: 1.1600 (1.1600)  time: 4.8408  data: 3.6553  max mem: 15925\n",
      "Train: [epoch:156]  [ 10/431]  eta: 0:09:46  lr: 0.000188  loss: 1.1606 (1.2125)  time: 1.3930  data: 0.3325  max mem: 15925\n",
      "Train: [epoch:156]  [ 20/431]  eta: 0:08:28  lr: 0.000188  loss: 1.1429 (1.1567)  time: 1.0561  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [ 30/431]  eta: 0:07:59  lr: 0.000188  loss: 1.1033 (1.1415)  time: 1.0891  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:156]  [ 40/431]  eta: 0:07:41  lr: 0.000188  loss: 1.1075 (1.1360)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [ 50/431]  eta: 0:07:24  lr: 0.000188  loss: 1.0856 (1.1294)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [ 60/431]  eta: 0:07:12  lr: 0.000188  loss: 1.0910 (1.1247)  time: 1.1361  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:156]  [ 70/431]  eta: 0:06:59  lr: 0.000188  loss: 1.1040 (1.1295)  time: 1.1501  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:156]  [ 80/431]  eta: 0:06:46  lr: 0.000188  loss: 1.1335 (1.1323)  time: 1.1392  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:156]  [ 90/431]  eta: 0:06:34  lr: 0.000188  loss: 1.1201 (1.1351)  time: 1.1365  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [100/431]  eta: 0:06:21  lr: 0.000188  loss: 1.1178 (1.1381)  time: 1.1307  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [110/431]  eta: 0:06:09  lr: 0.000188  loss: 1.1145 (1.1402)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [120/431]  eta: 0:05:57  lr: 0.000188  loss: 1.0608 (1.1345)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [130/431]  eta: 0:05:45  lr: 0.000188  loss: 1.0498 (1.1304)  time: 1.1340  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [140/431]  eta: 0:05:33  lr: 0.000188  loss: 1.0756 (1.1293)  time: 1.1358  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [150/431]  eta: 0:05:21  lr: 0.000188  loss: 1.1077 (1.1293)  time: 1.1245  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [160/431]  eta: 0:05:10  lr: 0.000188  loss: 1.1231 (1.1282)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [170/431]  eta: 0:04:58  lr: 0.000188  loss: 1.1265 (1.1301)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [180/431]  eta: 0:04:46  lr: 0.000188  loss: 1.1265 (1.1315)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [190/431]  eta: 0:04:34  lr: 0.000188  loss: 1.0928 (1.1301)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [200/431]  eta: 0:04:22  lr: 0.000188  loss: 1.0928 (1.1288)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [210/431]  eta: 0:04:11  lr: 0.000188  loss: 1.1087 (1.1298)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [220/431]  eta: 0:03:59  lr: 0.000188  loss: 1.1105 (1.1290)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [230/431]  eta: 0:03:48  lr: 0.000188  loss: 1.0896 (1.1277)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [240/431]  eta: 0:03:36  lr: 0.000188  loss: 1.0896 (1.1269)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [250/431]  eta: 0:03:25  lr: 0.000188  loss: 1.1080 (1.1294)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [260/431]  eta: 0:03:14  lr: 0.000188  loss: 1.0810 (1.1280)  time: 1.1353  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [270/431]  eta: 0:03:02  lr: 0.000188  loss: 1.0645 (1.1266)  time: 1.1275  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:156]  [280/431]  eta: 0:02:51  lr: 0.000188  loss: 1.0873 (1.1259)  time: 1.1266  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:156]  [290/431]  eta: 0:02:39  lr: 0.000188  loss: 1.1162 (1.1258)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [300/431]  eta: 0:02:28  lr: 0.000188  loss: 1.1306 (1.1279)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [310/431]  eta: 0:02:17  lr: 0.000188  loss: 1.0990 (1.1265)  time: 1.1286  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [320/431]  eta: 0:02:05  lr: 0.000188  loss: 1.0711 (1.1246)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [330/431]  eta: 0:01:54  lr: 0.000188  loss: 1.1325 (1.1265)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [340/431]  eta: 0:01:42  lr: 0.000188  loss: 1.1625 (1.1274)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [350/431]  eta: 0:01:31  lr: 0.000188  loss: 1.1373 (1.1276)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [360/431]  eta: 0:01:20  lr: 0.000188  loss: 1.1218 (1.1274)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [370/431]  eta: 0:01:09  lr: 0.000188  loss: 1.1196 (1.1276)  time: 1.1336  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [380/431]  eta: 0:00:57  lr: 0.000188  loss: 1.1435 (1.1299)  time: 1.1355  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [390/431]  eta: 0:00:46  lr: 0.000188  loss: 1.1399 (1.1293)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [400/431]  eta: 0:00:35  lr: 0.000188  loss: 1.1064 (1.1289)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [410/431]  eta: 0:00:23  lr: 0.000188  loss: 1.1193 (1.1295)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:156]  [420/431]  eta: 0:00:12  lr: 0.000188  loss: 1.1042 (1.1282)  time: 1.1125  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:156]  [430/431]  eta: 0:00:01  lr: 0.000188  loss: 1.0890 (1.1278)  time: 1.1121  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:156] Total time: 0:08:07 (1.1302 s / it)\n",
      "Averaged stats: lr: 0.000188  loss: 1.0890 (1.1278)\n",
      "Valid: [epoch:156]  [ 0/14]  eta: 0:00:38  loss: 1.1366 (1.1366)  time: 2.7460  data: 2.5856  max mem: 15925\n",
      "Valid: [epoch:156]  [13/14]  eta: 0:00:00  loss: 1.0858 (1.0914)  time: 0.2949  data: 0.1848  max mem: 15925\n",
      "Valid: [epoch:156] Total time: 0:00:04 (0.3120 s / it)\n",
      "Averaged stats: loss: 1.0858 (1.0914)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_156_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.091%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:157]  [  0/431]  eta: 0:34:28  lr: 0.000188  loss: 1.1134 (1.1134)  time: 4.7992  data: 3.5564  max mem: 15925\n",
      "Train: [epoch:157]  [ 10/431]  eta: 0:09:57  lr: 0.000188  loss: 1.1789 (1.1803)  time: 1.4194  data: 0.3235  max mem: 15925\n",
      "Train: [epoch:157]  [ 20/431]  eta: 0:08:40  lr: 0.000188  loss: 1.1789 (1.1803)  time: 1.0886  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [ 30/431]  eta: 0:08:08  lr: 0.000188  loss: 1.0620 (1.1339)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [ 40/431]  eta: 0:07:45  lr: 0.000188  loss: 1.0461 (1.1181)  time: 1.1136  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [ 50/431]  eta: 0:07:27  lr: 0.000188  loss: 1.0979 (1.1242)  time: 1.1093  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [ 60/431]  eta: 0:07:11  lr: 0.000188  loss: 1.1427 (1.1233)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [ 70/431]  eta: 0:06:57  lr: 0.000188  loss: 1.1486 (1.1333)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [ 80/431]  eta: 0:06:43  lr: 0.000188  loss: 1.1709 (1.1332)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [ 90/431]  eta: 0:06:30  lr: 0.000188  loss: 1.0963 (1.1318)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [100/431]  eta: 0:06:18  lr: 0.000188  loss: 1.0915 (1.1307)  time: 1.1178  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [110/431]  eta: 0:06:06  lr: 0.000188  loss: 1.0866 (1.1288)  time: 1.1254  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [120/431]  eta: 0:05:55  lr: 0.000188  loss: 1.0970 (1.1330)  time: 1.1315  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [130/431]  eta: 0:05:43  lr: 0.000188  loss: 1.0970 (1.1281)  time: 1.1385  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [140/431]  eta: 0:05:31  lr: 0.000188  loss: 1.0476 (1.1237)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [150/431]  eta: 0:05:19  lr: 0.000188  loss: 1.0696 (1.1246)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [160/431]  eta: 0:05:07  lr: 0.000188  loss: 1.0883 (1.1245)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [170/431]  eta: 0:04:55  lr: 0.000188  loss: 1.0593 (1.1224)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [180/431]  eta: 0:04:44  lr: 0.000188  loss: 1.1223 (1.1233)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [190/431]  eta: 0:04:32  lr: 0.000188  loss: 1.1495 (1.1271)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [200/431]  eta: 0:04:21  lr: 0.000188  loss: 1.1207 (1.1250)  time: 1.1160  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:157]  [210/431]  eta: 0:04:09  lr: 0.000188  loss: 1.0627 (1.1232)  time: 1.1253  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [220/431]  eta: 0:03:58  lr: 0.000188  loss: 1.0851 (1.1240)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [230/431]  eta: 0:03:46  lr: 0.000188  loss: 1.1730 (1.1285)  time: 1.1087  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [240/431]  eta: 0:03:35  lr: 0.000188  loss: 1.1940 (1.1287)  time: 1.1093  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [250/431]  eta: 0:03:24  lr: 0.000188  loss: 1.1021 (1.1294)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [260/431]  eta: 0:03:12  lr: 0.000188  loss: 1.1028 (1.1306)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [270/431]  eta: 0:03:01  lr: 0.000188  loss: 1.0697 (1.1290)  time: 1.1265  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [280/431]  eta: 0:02:50  lr: 0.000188  loss: 1.0550 (1.1277)  time: 1.1243  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [290/431]  eta: 0:02:39  lr: 0.000188  loss: 1.0702 (1.1272)  time: 1.1272  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [300/431]  eta: 0:02:27  lr: 0.000188  loss: 1.1503 (1.1283)  time: 1.1279  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [310/431]  eta: 0:02:16  lr: 0.000188  loss: 1.1203 (1.1269)  time: 1.1336  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [320/431]  eta: 0:02:05  lr: 0.000188  loss: 1.1203 (1.1273)  time: 1.1327  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [330/431]  eta: 0:01:53  lr: 0.000188  loss: 1.1555 (1.1284)  time: 1.1191  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:157]  [340/431]  eta: 0:01:42  lr: 0.000188  loss: 1.1555 (1.1294)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [350/431]  eta: 0:01:31  lr: 0.000188  loss: 1.1481 (1.1303)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [360/431]  eta: 0:01:20  lr: 0.000188  loss: 1.1025 (1.1300)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [370/431]  eta: 0:01:08  lr: 0.000188  loss: 1.1231 (1.1313)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [380/431]  eta: 0:00:57  lr: 0.000188  loss: 1.1231 (1.1311)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [390/431]  eta: 0:00:46  lr: 0.000188  loss: 1.0939 (1.1310)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [400/431]  eta: 0:00:34  lr: 0.000188  loss: 1.0683 (1.1303)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [410/431]  eta: 0:00:23  lr: 0.000188  loss: 1.1161 (1.1320)  time: 1.1304  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [420/431]  eta: 0:00:12  lr: 0.000188  loss: 1.1161 (1.1314)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157]  [430/431]  eta: 0:00:01  lr: 0.000188  loss: 1.0919 (1.1310)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:157] Total time: 0:08:05 (1.1271 s / it)\n",
      "Averaged stats: lr: 0.000188  loss: 1.0919 (1.1310)\n",
      "Valid: [epoch:157]  [ 0/14]  eta: 0:00:33  loss: 1.1171 (1.1171)  time: 2.4001  data: 2.2592  max mem: 15925\n",
      "Valid: [epoch:157]  [13/14]  eta: 0:00:00  loss: 1.0561 (1.0685)  time: 0.2877  data: 0.1615  max mem: 15925\n",
      "Valid: [epoch:157] Total time: 0:00:04 (0.3059 s / it)\n",
      "Averaged stats: loss: 1.0561 (1.0685)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_157_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.068%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:158]  [  0/431]  eta: 0:33:39  lr: 0.000187  loss: 1.1448 (1.1448)  time: 4.6863  data: 3.4172  max mem: 15925\n",
      "Train: [epoch:158]  [ 10/431]  eta: 0:09:49  lr: 0.000187  loss: 1.1448 (1.1569)  time: 1.4007  data: 0.3109  max mem: 15925\n",
      "Train: [epoch:158]  [ 20/431]  eta: 0:08:35  lr: 0.000187  loss: 1.0855 (1.1316)  time: 1.0824  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [ 30/431]  eta: 0:08:06  lr: 0.000187  loss: 1.1066 (1.1410)  time: 1.1085  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [ 40/431]  eta: 0:07:42  lr: 0.000187  loss: 1.1483 (1.1487)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [ 50/431]  eta: 0:07:26  lr: 0.000187  loss: 1.1142 (1.1421)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [ 60/431]  eta: 0:07:13  lr: 0.000187  loss: 1.1142 (1.1358)  time: 1.1374  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [ 70/431]  eta: 0:07:00  lr: 0.000187  loss: 1.1271 (1.1334)  time: 1.1457  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [ 80/431]  eta: 0:06:46  lr: 0.000187  loss: 1.1476 (1.1424)  time: 1.1266  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [ 90/431]  eta: 0:06:33  lr: 0.000187  loss: 1.1051 (1.1440)  time: 1.1237  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [100/431]  eta: 0:06:20  lr: 0.000187  loss: 1.0957 (1.1406)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [110/431]  eta: 0:06:08  lr: 0.000187  loss: 1.0608 (1.1358)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [120/431]  eta: 0:05:55  lr: 0.000187  loss: 1.0436 (1.1339)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [130/431]  eta: 0:05:43  lr: 0.000187  loss: 1.0968 (1.1338)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [140/431]  eta: 0:05:32  lr: 0.000187  loss: 1.1125 (1.1349)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [150/431]  eta: 0:05:20  lr: 0.000187  loss: 1.1571 (1.1330)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [160/431]  eta: 0:05:08  lr: 0.000187  loss: 1.0954 (1.1336)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [170/431]  eta: 0:04:57  lr: 0.000187  loss: 1.0877 (1.1296)  time: 1.1277  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [180/431]  eta: 0:04:45  lr: 0.000187  loss: 1.0633 (1.1280)  time: 1.1294  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:158]  [190/431]  eta: 0:04:33  lr: 0.000187  loss: 1.1284 (1.1301)  time: 1.1185  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:158]  [200/431]  eta: 0:04:22  lr: 0.000187  loss: 1.1444 (1.1307)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [210/431]  eta: 0:04:10  lr: 0.000187  loss: 1.1165 (1.1293)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [220/431]  eta: 0:03:59  lr: 0.000187  loss: 1.1013 (1.1290)  time: 1.1347  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [230/431]  eta: 0:03:48  lr: 0.000187  loss: 1.1263 (1.1318)  time: 1.1328  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [240/431]  eta: 0:03:36  lr: 0.000187  loss: 1.1238 (1.1330)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [250/431]  eta: 0:03:25  lr: 0.000187  loss: 1.0999 (1.1334)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [260/431]  eta: 0:03:13  lr: 0.000187  loss: 1.0885 (1.1324)  time: 1.1278  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [270/431]  eta: 0:03:02  lr: 0.000187  loss: 1.0713 (1.1321)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [280/431]  eta: 0:02:51  lr: 0.000187  loss: 1.0694 (1.1300)  time: 1.1208  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [290/431]  eta: 0:02:39  lr: 0.000187  loss: 1.0402 (1.1282)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [300/431]  eta: 0:02:28  lr: 0.000187  loss: 1.1176 (1.1310)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [310/431]  eta: 0:02:16  lr: 0.000187  loss: 1.1390 (1.1312)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [320/431]  eta: 0:02:05  lr: 0.000187  loss: 1.0818 (1.1294)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [330/431]  eta: 0:01:54  lr: 0.000187  loss: 1.0590 (1.1289)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [340/431]  eta: 0:01:42  lr: 0.000187  loss: 1.1445 (1.1317)  time: 1.1310  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [350/431]  eta: 0:01:31  lr: 0.000187  loss: 1.1858 (1.1324)  time: 1.1213  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [360/431]  eta: 0:01:20  lr: 0.000187  loss: 1.1561 (1.1328)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [370/431]  eta: 0:01:08  lr: 0.000187  loss: 1.1471 (1.1331)  time: 1.1104  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:158]  [380/431]  eta: 0:00:57  lr: 0.000187  loss: 1.0822 (1.1313)  time: 1.1205  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [390/431]  eta: 0:00:46  lr: 0.000187  loss: 1.0712 (1.1304)  time: 1.1137  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [400/431]  eta: 0:00:34  lr: 0.000187  loss: 1.0753 (1.1299)  time: 1.1248  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:158]  [410/431]  eta: 0:00:23  lr: 0.000187  loss: 1.0753 (1.1305)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [420/431]  eta: 0:00:12  lr: 0.000187  loss: 1.0680 (1.1293)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158]  [430/431]  eta: 0:00:01  lr: 0.000187  loss: 1.0432 (1.1290)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:158] Total time: 0:08:06 (1.1282 s / it)\n",
      "Averaged stats: lr: 0.000187  loss: 1.0432 (1.1290)\n",
      "Valid: [epoch:158]  [ 0/14]  eta: 0:00:33  loss: 1.0347 (1.0347)  time: 2.3745  data: 2.2370  max mem: 15925\n",
      "Valid: [epoch:158]  [13/14]  eta: 0:00:00  loss: 1.0563 (1.0690)  time: 0.2593  data: 0.1599  max mem: 15925\n",
      "Valid: [epoch:158] Total time: 0:00:03 (0.2756 s / it)\n",
      "Averaged stats: loss: 1.0563 (1.0690)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_158_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.069%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:159]  [  0/431]  eta: 0:32:11  lr: 0.000187  loss: 1.1355 (1.1355)  time: 4.4824  data: 3.2161  max mem: 15925\n",
      "Train: [epoch:159]  [ 10/431]  eta: 0:09:40  lr: 0.000187  loss: 1.1355 (1.1257)  time: 1.3785  data: 0.2927  max mem: 15925\n",
      "Train: [epoch:159]  [ 20/431]  eta: 0:08:26  lr: 0.000187  loss: 1.0714 (1.1045)  time: 1.0687  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:159]  [ 30/431]  eta: 0:07:54  lr: 0.000187  loss: 1.0423 (1.0878)  time: 1.0748  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [ 40/431]  eta: 0:07:33  lr: 0.000187  loss: 1.0767 (1.0948)  time: 1.0846  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [ 50/431]  eta: 0:07:18  lr: 0.000187  loss: 1.1552 (1.1045)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [ 60/431]  eta: 0:07:05  lr: 0.000187  loss: 1.1456 (1.1079)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [ 70/431]  eta: 0:06:53  lr: 0.000187  loss: 1.1456 (1.1188)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [ 80/431]  eta: 0:06:40  lr: 0.000187  loss: 1.1788 (1.1258)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [ 90/431]  eta: 0:06:28  lr: 0.000187  loss: 1.1686 (1.1288)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [100/431]  eta: 0:06:16  lr: 0.000187  loss: 1.1625 (1.1344)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [110/431]  eta: 0:06:04  lr: 0.000187  loss: 1.1725 (1.1405)  time: 1.1290  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [120/431]  eta: 0:05:52  lr: 0.000187  loss: 1.1076 (1.1340)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [130/431]  eta: 0:05:41  lr: 0.000187  loss: 1.0600 (1.1285)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [140/431]  eta: 0:05:30  lr: 0.000187  loss: 1.0528 (1.1260)  time: 1.1331  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [150/431]  eta: 0:05:18  lr: 0.000187  loss: 1.1702 (1.1292)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [160/431]  eta: 0:05:07  lr: 0.000187  loss: 1.1712 (1.1300)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [170/431]  eta: 0:04:55  lr: 0.000187  loss: 1.1246 (1.1280)  time: 1.1279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [180/431]  eta: 0:04:44  lr: 0.000187  loss: 1.0767 (1.1254)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [190/431]  eta: 0:04:32  lr: 0.000187  loss: 1.0451 (1.1229)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [200/431]  eta: 0:04:21  lr: 0.000187  loss: 1.1207 (1.1265)  time: 1.1386  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [210/431]  eta: 0:04:10  lr: 0.000187  loss: 1.1207 (1.1232)  time: 1.1303  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [220/431]  eta: 0:03:58  lr: 0.000187  loss: 1.0705 (1.1231)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [230/431]  eta: 0:03:47  lr: 0.000187  loss: 1.1576 (1.1259)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [240/431]  eta: 0:03:35  lr: 0.000187  loss: 1.1315 (1.1263)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [250/431]  eta: 0:03:24  lr: 0.000187  loss: 1.0698 (1.1254)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [260/431]  eta: 0:03:12  lr: 0.000187  loss: 1.0992 (1.1262)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [270/431]  eta: 0:03:01  lr: 0.000187  loss: 1.1324 (1.1287)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [280/431]  eta: 0:02:50  lr: 0.000187  loss: 1.1318 (1.1273)  time: 1.1201  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:159]  [290/431]  eta: 0:02:39  lr: 0.000187  loss: 1.0505 (1.1266)  time: 1.1322  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [300/431]  eta: 0:02:27  lr: 0.000187  loss: 1.1171 (1.1288)  time: 1.1411  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [310/431]  eta: 0:02:16  lr: 0.000187  loss: 1.1661 (1.1289)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [320/431]  eta: 0:02:05  lr: 0.000187  loss: 1.1661 (1.1298)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [330/431]  eta: 0:01:53  lr: 0.000187  loss: 1.1599 (1.1300)  time: 1.1289  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:159]  [340/431]  eta: 0:01:42  lr: 0.000187  loss: 1.1218 (1.1307)  time: 1.1344  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:159]  [350/431]  eta: 0:01:31  lr: 0.000187  loss: 1.1038 (1.1299)  time: 1.1281  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:159]  [360/431]  eta: 0:01:20  lr: 0.000187  loss: 1.0670 (1.1294)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [370/431]  eta: 0:01:08  lr: 0.000187  loss: 1.1233 (1.1300)  time: 1.1411  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [380/431]  eta: 0:00:57  lr: 0.000187  loss: 1.1233 (1.1300)  time: 1.1505  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [390/431]  eta: 0:00:46  lr: 0.000187  loss: 1.0728 (1.1299)  time: 1.1358  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:159]  [400/431]  eta: 0:00:34  lr: 0.000187  loss: 1.1409 (1.1308)  time: 1.1224  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:159]  [410/431]  eta: 0:00:23  lr: 0.000187  loss: 1.1479 (1.1308)  time: 1.1300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [420/431]  eta: 0:00:12  lr: 0.000187  loss: 1.0917 (1.1297)  time: 1.1443  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159]  [430/431]  eta: 0:00:01  lr: 0.000187  loss: 1.0860 (1.1293)  time: 1.1320  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:159] Total time: 0:08:07 (1.1301 s / it)\n",
      "Averaged stats: lr: 0.000187  loss: 1.0860 (1.1293)\n",
      "Valid: [epoch:159]  [ 0/14]  eta: 0:00:35  loss: 1.0368 (1.0368)  time: 2.5331  data: 2.3374  max mem: 15925\n",
      "Valid: [epoch:159]  [13/14]  eta: 0:00:00  loss: 1.0590 (1.0695)  time: 0.2671  data: 0.1671  max mem: 15925\n",
      "Valid: [epoch:159] Total time: 0:00:03 (0.2835 s / it)\n",
      "Averaged stats: loss: 1.0590 (1.0695)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_159_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.069%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:160]  [  0/431]  eta: 0:34:29  lr: 0.000187  loss: 1.1110 (1.1110)  time: 4.8006  data: 3.6173  max mem: 15925\n",
      "Train: [epoch:160]  [ 10/431]  eta: 0:09:46  lr: 0.000187  loss: 1.1708 (1.1736)  time: 1.3942  data: 0.3291  max mem: 15925\n",
      "Train: [epoch:160]  [ 20/431]  eta: 0:08:29  lr: 0.000187  loss: 1.1333 (1.1677)  time: 1.0617  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [ 30/431]  eta: 0:07:56  lr: 0.000187  loss: 1.1493 (1.1690)  time: 1.0766  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [ 40/431]  eta: 0:07:37  lr: 0.000187  loss: 1.1787 (1.1735)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [ 50/431]  eta: 0:07:23  lr: 0.000187  loss: 1.1726 (1.1687)  time: 1.1275  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:160]  [ 60/431]  eta: 0:07:08  lr: 0.000187  loss: 1.0950 (1.1551)  time: 1.1289  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [ 70/431]  eta: 0:06:56  lr: 0.000187  loss: 1.0754 (1.1494)  time: 1.1232  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [ 80/431]  eta: 0:06:44  lr: 0.000187  loss: 1.0752 (1.1416)  time: 1.1387  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [ 90/431]  eta: 0:06:31  lr: 0.000187  loss: 1.0752 (1.1393)  time: 1.1326  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [100/431]  eta: 0:06:19  lr: 0.000187  loss: 1.0685 (1.1366)  time: 1.1217  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [110/431]  eta: 0:06:07  lr: 0.000187  loss: 1.0685 (1.1375)  time: 1.1296  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [120/431]  eta: 0:05:56  lr: 0.000187  loss: 1.0933 (1.1348)  time: 1.1453  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:160]  [130/431]  eta: 0:05:44  lr: 0.000187  loss: 1.1224 (1.1385)  time: 1.1389  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:160]  [140/431]  eta: 0:05:32  lr: 0.000187  loss: 1.1372 (1.1380)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [150/431]  eta: 0:05:20  lr: 0.000187  loss: 1.0972 (1.1368)  time: 1.1248  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [160/431]  eta: 0:05:09  lr: 0.000187  loss: 1.0512 (1.1321)  time: 1.1192  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [170/431]  eta: 0:04:57  lr: 0.000187  loss: 1.0757 (1.1327)  time: 1.1362  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [180/431]  eta: 0:04:46  lr: 0.000187  loss: 1.1622 (1.1328)  time: 1.1366  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [190/431]  eta: 0:04:34  lr: 0.000187  loss: 1.1735 (1.1341)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [200/431]  eta: 0:04:22  lr: 0.000187  loss: 1.1062 (1.1343)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [210/431]  eta: 0:04:10  lr: 0.000187  loss: 1.0777 (1.1345)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [220/431]  eta: 0:03:59  lr: 0.000187  loss: 1.0768 (1.1326)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [230/431]  eta: 0:03:47  lr: 0.000187  loss: 1.0856 (1.1313)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [240/431]  eta: 0:03:36  lr: 0.000187  loss: 1.1043 (1.1313)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [250/431]  eta: 0:03:25  lr: 0.000187  loss: 1.1126 (1.1305)  time: 1.1324  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [260/431]  eta: 0:03:13  lr: 0.000187  loss: 1.1126 (1.1294)  time: 1.1332  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [270/431]  eta: 0:03:02  lr: 0.000187  loss: 1.0641 (1.1281)  time: 1.1351  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [280/431]  eta: 0:02:51  lr: 0.000187  loss: 1.1566 (1.1303)  time: 1.1369  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [290/431]  eta: 0:02:39  lr: 0.000187  loss: 1.1593 (1.1310)  time: 1.1309  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [300/431]  eta: 0:02:28  lr: 0.000187  loss: 1.1400 (1.1320)  time: 1.1275  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [310/431]  eta: 0:02:17  lr: 0.000187  loss: 1.0651 (1.1301)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [320/431]  eta: 0:02:05  lr: 0.000187  loss: 1.1380 (1.1317)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [330/431]  eta: 0:01:54  lr: 0.000187  loss: 1.1574 (1.1317)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [340/431]  eta: 0:01:43  lr: 0.000187  loss: 1.0926 (1.1322)  time: 1.1319  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [350/431]  eta: 0:01:31  lr: 0.000187  loss: 1.0713 (1.1318)  time: 1.1409  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [360/431]  eta: 0:01:20  lr: 0.000187  loss: 1.0713 (1.1316)  time: 1.1324  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [370/431]  eta: 0:01:09  lr: 0.000187  loss: 1.0969 (1.1309)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [380/431]  eta: 0:00:57  lr: 0.000187  loss: 1.0927 (1.1304)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [390/431]  eta: 0:00:46  lr: 0.000187  loss: 1.0859 (1.1294)  time: 1.1091  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:160]  [400/431]  eta: 0:00:35  lr: 0.000187  loss: 1.0446 (1.1293)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [410/431]  eta: 0:00:23  lr: 0.000187  loss: 1.1000 (1.1307)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [420/431]  eta: 0:00:12  lr: 0.000187  loss: 1.1128 (1.1307)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160]  [430/431]  eta: 0:00:01  lr: 0.000187  loss: 1.0945 (1.1295)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:160] Total time: 0:08:07 (1.1310 s / it)\n",
      "Averaged stats: lr: 0.000187  loss: 1.0945 (1.1295)\n",
      "Valid: [epoch:160]  [ 0/14]  eta: 0:00:37  loss: 1.1266 (1.1266)  time: 2.6619  data: 2.4849  max mem: 15925\n",
      "Valid: [epoch:160]  [13/14]  eta: 0:00:00  loss: 1.0626 (1.0724)  time: 0.2965  data: 0.1776  max mem: 15925\n",
      "Valid: [epoch:160] Total time: 0:00:04 (0.3138 s / it)\n",
      "Averaged stats: loss: 1.0626 (1.0724)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_160_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:161]  [  0/431]  eta: 0:31:10  lr: 0.000187  loss: 1.2158 (1.2158)  time: 4.3389  data: 3.2159  max mem: 15925\n",
      "Train: [epoch:161]  [ 10/431]  eta: 0:09:42  lr: 0.000187  loss: 1.1366 (1.1658)  time: 1.3832  data: 0.2926  max mem: 15925\n",
      "Train: [epoch:161]  [ 20/431]  eta: 0:08:28  lr: 0.000187  loss: 1.1387 (1.1877)  time: 1.0817  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [ 30/431]  eta: 0:07:59  lr: 0.000187  loss: 1.1387 (1.1545)  time: 1.0920  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [ 40/431]  eta: 0:07:40  lr: 0.000187  loss: 1.0745 (1.1343)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [ 50/431]  eta: 0:07:25  lr: 0.000187  loss: 1.0531 (1.1332)  time: 1.1280  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [ 60/431]  eta: 0:07:09  lr: 0.000187  loss: 1.1128 (1.1278)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [ 70/431]  eta: 0:06:57  lr: 0.000187  loss: 1.0995 (1.1335)  time: 1.1242  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [ 80/431]  eta: 0:06:44  lr: 0.000187  loss: 1.1128 (1.1375)  time: 1.1385  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [ 90/431]  eta: 0:06:32  lr: 0.000187  loss: 1.1037 (1.1302)  time: 1.1282  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [100/431]  eta: 0:06:20  lr: 0.000187  loss: 1.0492 (1.1209)  time: 1.1323  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [110/431]  eta: 0:06:08  lr: 0.000187  loss: 1.0852 (1.1238)  time: 1.1356  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [120/431]  eta: 0:05:56  lr: 0.000187  loss: 1.1562 (1.1283)  time: 1.1329  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [130/431]  eta: 0:05:44  lr: 0.000187  loss: 1.1301 (1.1259)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [140/431]  eta: 0:05:32  lr: 0.000187  loss: 1.1267 (1.1242)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [150/431]  eta: 0:05:20  lr: 0.000187  loss: 1.1204 (1.1230)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [160/431]  eta: 0:05:08  lr: 0.000187  loss: 1.1324 (1.1260)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [170/431]  eta: 0:04:56  lr: 0.000187  loss: 1.0748 (1.1224)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [180/431]  eta: 0:04:44  lr: 0.000187  loss: 1.0414 (1.1237)  time: 1.1110  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [190/431]  eta: 0:04:33  lr: 0.000187  loss: 1.1204 (1.1238)  time: 1.1272  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [200/431]  eta: 0:04:22  lr: 0.000187  loss: 1.0914 (1.1230)  time: 1.1410  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:161]  [210/431]  eta: 0:04:10  lr: 0.000187  loss: 1.0900 (1.1240)  time: 1.1258  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:161]  [220/431]  eta: 0:03:59  lr: 0.000187  loss: 1.0884 (1.1238)  time: 1.1136  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:161]  [230/431]  eta: 0:03:47  lr: 0.000187  loss: 1.1021 (1.1260)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [240/431]  eta: 0:03:36  lr: 0.000187  loss: 1.1247 (1.1276)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [250/431]  eta: 0:03:25  lr: 0.000187  loss: 1.1233 (1.1277)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [260/431]  eta: 0:03:13  lr: 0.000187  loss: 1.1408 (1.1275)  time: 1.1283  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [270/431]  eta: 0:03:02  lr: 0.000187  loss: 1.1408 (1.1285)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [280/431]  eta: 0:02:50  lr: 0.000187  loss: 1.1188 (1.1275)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [290/431]  eta: 0:02:39  lr: 0.000187  loss: 1.1072 (1.1309)  time: 1.1325  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [300/431]  eta: 0:02:28  lr: 0.000187  loss: 1.1479 (1.1316)  time: 1.1345  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [310/431]  eta: 0:02:16  lr: 0.000187  loss: 1.1325 (1.1326)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [320/431]  eta: 0:02:05  lr: 0.000187  loss: 1.0859 (1.1312)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [330/431]  eta: 0:01:54  lr: 0.000187  loss: 1.1099 (1.1319)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [340/431]  eta: 0:01:42  lr: 0.000187  loss: 1.1401 (1.1345)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [350/431]  eta: 0:01:31  lr: 0.000187  loss: 1.2047 (1.1361)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [360/431]  eta: 0:01:20  lr: 0.000187  loss: 1.1560 (1.1363)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [370/431]  eta: 0:01:08  lr: 0.000187  loss: 1.0851 (1.1350)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [380/431]  eta: 0:00:57  lr: 0.000187  loss: 1.0603 (1.1338)  time: 1.1170  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [390/431]  eta: 0:00:46  lr: 0.000187  loss: 1.0649 (1.1328)  time: 1.1140  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:161]  [400/431]  eta: 0:00:34  lr: 0.000187  loss: 1.0681 (1.1323)  time: 1.1244  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [410/431]  eta: 0:00:23  lr: 0.000187  loss: 1.1166 (1.1329)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [420/431]  eta: 0:00:12  lr: 0.000187  loss: 1.1120 (1.1318)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:161]  [430/431]  eta: 0:00:01  lr: 0.000187  loss: 1.0601 (1.1316)  time: 1.1186  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:161] Total time: 0:08:06 (1.1284 s / it)\n",
      "Averaged stats: lr: 0.000187  loss: 1.0601 (1.1316)\n",
      "Valid: [epoch:161]  [ 0/14]  eta: 0:00:34  loss: 1.0253 (1.0253)  time: 2.4860  data: 2.3343  max mem: 15925\n",
      "Valid: [epoch:161]  [13/14]  eta: 0:00:00  loss: 1.0539 (1.0673)  time: 0.2628  data: 0.1678  max mem: 15925\n",
      "Valid: [epoch:161] Total time: 0:00:03 (0.2790 s / it)\n",
      "Averaged stats: loss: 1.0539 (1.0673)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_161_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.067%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:162]  [  0/431]  eta: 0:29:38  lr: 0.000186  loss: 1.0375 (1.0375)  time: 4.1270  data: 2.9297  max mem: 15925\n",
      "Train: [epoch:162]  [ 10/431]  eta: 0:09:25  lr: 0.000186  loss: 1.1549 (1.1253)  time: 1.3425  data: 0.2666  max mem: 15925\n",
      "Train: [epoch:162]  [ 20/431]  eta: 0:08:23  lr: 0.000186  loss: 1.1549 (1.1344)  time: 1.0794  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [ 30/431]  eta: 0:07:54  lr: 0.000186  loss: 1.0821 (1.1248)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [ 40/431]  eta: 0:07:37  lr: 0.000186  loss: 1.0821 (1.1337)  time: 1.1137  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [ 50/431]  eta: 0:07:21  lr: 0.000186  loss: 1.1392 (1.1339)  time: 1.1174  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:162]  [ 60/431]  eta: 0:07:07  lr: 0.000186  loss: 1.1016 (1.1302)  time: 1.1180  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:162]  [ 70/431]  eta: 0:06:54  lr: 0.000186  loss: 1.0969 (1.1327)  time: 1.1190  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [ 80/431]  eta: 0:06:42  lr: 0.000186  loss: 1.1424 (1.1355)  time: 1.1212  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [ 90/431]  eta: 0:06:29  lr: 0.000186  loss: 1.1316 (1.1334)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [100/431]  eta: 0:06:16  lr: 0.000186  loss: 1.0775 (1.1325)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [110/431]  eta: 0:06:04  lr: 0.000186  loss: 1.0908 (1.1311)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [120/431]  eta: 0:05:52  lr: 0.000186  loss: 1.0951 (1.1325)  time: 1.1075  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [130/431]  eta: 0:05:41  lr: 0.000186  loss: 1.1011 (1.1317)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [140/431]  eta: 0:05:29  lr: 0.000186  loss: 1.0527 (1.1262)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [150/431]  eta: 0:05:17  lr: 0.000186  loss: 1.0506 (1.1275)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [160/431]  eta: 0:05:06  lr: 0.000186  loss: 1.1495 (1.1312)  time: 1.1152  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [170/431]  eta: 0:04:54  lr: 0.000186  loss: 1.1495 (1.1314)  time: 1.1267  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [180/431]  eta: 0:04:43  lr: 0.000186  loss: 1.1080 (1.1307)  time: 1.1375  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [190/431]  eta: 0:04:32  lr: 0.000186  loss: 1.0742 (1.1292)  time: 1.1433  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [200/431]  eta: 0:04:21  lr: 0.000186  loss: 1.0961 (1.1283)  time: 1.1352  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [210/431]  eta: 0:04:10  lr: 0.000186  loss: 1.0932 (1.1276)  time: 1.1325  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [220/431]  eta: 0:03:58  lr: 0.000186  loss: 1.0765 (1.1252)  time: 1.1167  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [230/431]  eta: 0:03:47  lr: 0.000186  loss: 1.0612 (1.1247)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [240/431]  eta: 0:03:35  lr: 0.000186  loss: 1.1388 (1.1258)  time: 1.1325  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [250/431]  eta: 0:03:24  lr: 0.000186  loss: 1.1262 (1.1247)  time: 1.1417  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [260/431]  eta: 0:03:13  lr: 0.000186  loss: 1.1171 (1.1260)  time: 1.1381  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [270/431]  eta: 0:03:02  lr: 0.000186  loss: 1.1621 (1.1267)  time: 1.1259  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [280/431]  eta: 0:02:50  lr: 0.000186  loss: 1.1302 (1.1263)  time: 1.1373  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [290/431]  eta: 0:02:39  lr: 0.000186  loss: 1.1255 (1.1258)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [300/431]  eta: 0:02:28  lr: 0.000186  loss: 1.0846 (1.1253)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [310/431]  eta: 0:02:16  lr: 0.000186  loss: 1.0965 (1.1269)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [320/431]  eta: 0:02:05  lr: 0.000186  loss: 1.1008 (1.1262)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [330/431]  eta: 0:01:54  lr: 0.000186  loss: 1.0948 (1.1277)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [340/431]  eta: 0:01:42  lr: 0.000186  loss: 1.1289 (1.1294)  time: 1.1342  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [350/431]  eta: 0:01:31  lr: 0.000186  loss: 1.1744 (1.1313)  time: 1.1392  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [360/431]  eta: 0:01:20  lr: 0.000186  loss: 1.1143 (1.1302)  time: 1.1343  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [370/431]  eta: 0:01:08  lr: 0.000186  loss: 1.0598 (1.1288)  time: 1.1277  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [380/431]  eta: 0:00:57  lr: 0.000186  loss: 1.0931 (1.1279)  time: 1.1251  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [390/431]  eta: 0:00:46  lr: 0.000186  loss: 1.0931 (1.1284)  time: 1.1311  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:162]  [400/431]  eta: 0:00:35  lr: 0.000186  loss: 1.0710 (1.1264)  time: 1.1315  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:162]  [410/431]  eta: 0:00:23  lr: 0.000186  loss: 1.0824 (1.1272)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [420/431]  eta: 0:00:12  lr: 0.000186  loss: 1.1696 (1.1287)  time: 1.1404  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162]  [430/431]  eta: 0:00:01  lr: 0.000186  loss: 1.1585 (1.1293)  time: 1.1544  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:162] Total time: 0:08:07 (1.1316 s / it)\n",
      "Averaged stats: lr: 0.000186  loss: 1.1585 (1.1293)\n",
      "Valid: [epoch:162]  [ 0/14]  eta: 0:00:33  loss: 1.1273 (1.1273)  time: 2.3788  data: 2.2142  max mem: 15925\n",
      "Valid: [epoch:162]  [13/14]  eta: 0:00:00  loss: 1.0695 (1.0780)  time: 0.2756  data: 0.1584  max mem: 15925\n",
      "Valid: [epoch:162] Total time: 0:00:04 (0.2929 s / it)\n",
      "Averaged stats: loss: 1.0695 (1.0780)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_162_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.078%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:163]  [  0/431]  eta: 0:36:55  lr: 0.000186  loss: 1.0414 (1.0414)  time: 5.1398  data: 3.7691  max mem: 15925\n",
      "Train: [epoch:163]  [ 10/431]  eta: 0:10:06  lr: 0.000186  loss: 1.1979 (1.1925)  time: 1.4397  data: 0.3430  max mem: 15925\n",
      "Train: [epoch:163]  [ 20/431]  eta: 0:08:39  lr: 0.000186  loss: 1.1979 (1.1890)  time: 1.0697  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [ 30/431]  eta: 0:08:05  lr: 0.000186  loss: 1.1412 (1.1715)  time: 1.0842  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [ 40/431]  eta: 0:07:42  lr: 0.000186  loss: 1.0810 (1.1529)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [ 50/431]  eta: 0:07:23  lr: 0.000186  loss: 1.0920 (1.1492)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [ 60/431]  eta: 0:07:09  lr: 0.000186  loss: 1.0895 (1.1366)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [ 70/431]  eta: 0:06:55  lr: 0.000186  loss: 1.0976 (1.1435)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [ 80/431]  eta: 0:06:43  lr: 0.000186  loss: 1.1694 (1.1490)  time: 1.1293  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [ 90/431]  eta: 0:06:30  lr: 0.000186  loss: 1.1694 (1.1501)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [100/431]  eta: 0:06:18  lr: 0.000186  loss: 1.1115 (1.1485)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [110/431]  eta: 0:06:07  lr: 0.000186  loss: 1.1849 (1.1494)  time: 1.1406  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [120/431]  eta: 0:05:55  lr: 0.000186  loss: 1.1795 (1.1505)  time: 1.1332  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [130/431]  eta: 0:05:43  lr: 0.000186  loss: 1.1416 (1.1434)  time: 1.1271  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [140/431]  eta: 0:05:31  lr: 0.000186  loss: 1.0525 (1.1395)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [150/431]  eta: 0:05:20  lr: 0.000186  loss: 1.0747 (1.1385)  time: 1.1317  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [160/431]  eta: 0:05:08  lr: 0.000186  loss: 1.0796 (1.1348)  time: 1.1370  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [170/431]  eta: 0:04:57  lr: 0.000186  loss: 1.0924 (1.1353)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [180/431]  eta: 0:04:45  lr: 0.000186  loss: 1.1316 (1.1374)  time: 1.1315  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:163]  [190/431]  eta: 0:04:34  lr: 0.000186  loss: 1.0977 (1.1364)  time: 1.1374  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:163]  [200/431]  eta: 0:04:22  lr: 0.000186  loss: 1.1057 (1.1355)  time: 1.1327  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [210/431]  eta: 0:04:11  lr: 0.000186  loss: 1.1140 (1.1323)  time: 1.1346  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [220/431]  eta: 0:04:00  lr: 0.000186  loss: 1.0628 (1.1303)  time: 1.1355  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [230/431]  eta: 0:03:48  lr: 0.000186  loss: 1.0619 (1.1278)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [240/431]  eta: 0:03:36  lr: 0.000186  loss: 1.0578 (1.1261)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [250/431]  eta: 0:03:25  lr: 0.000186  loss: 1.0994 (1.1265)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [260/431]  eta: 0:03:13  lr: 0.000186  loss: 1.1156 (1.1258)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [270/431]  eta: 0:03:02  lr: 0.000186  loss: 1.1092 (1.1266)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [280/431]  eta: 0:02:51  lr: 0.000186  loss: 1.0809 (1.1262)  time: 1.1209  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [290/431]  eta: 0:02:39  lr: 0.000186  loss: 1.0834 (1.1250)  time: 1.1237  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [300/431]  eta: 0:02:28  lr: 0.000186  loss: 1.0834 (1.1253)  time: 1.1339  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [310/431]  eta: 0:02:17  lr: 0.000186  loss: 1.1227 (1.1252)  time: 1.1330  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [320/431]  eta: 0:02:05  lr: 0.000186  loss: 1.1227 (1.1266)  time: 1.1284  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [330/431]  eta: 0:01:54  lr: 0.000186  loss: 1.2015 (1.1289)  time: 1.1293  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [340/431]  eta: 0:01:43  lr: 0.000186  loss: 1.1182 (1.1288)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [350/431]  eta: 0:01:31  lr: 0.000186  loss: 1.1238 (1.1299)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [360/431]  eta: 0:01:20  lr: 0.000186  loss: 1.1686 (1.1298)  time: 1.1411  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:163]  [370/431]  eta: 0:01:09  lr: 0.000186  loss: 1.1085 (1.1300)  time: 1.1377  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [380/431]  eta: 0:00:57  lr: 0.000186  loss: 1.1085 (1.1300)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [390/431]  eta: 0:00:46  lr: 0.000186  loss: 1.1335 (1.1307)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [400/431]  eta: 0:00:35  lr: 0.000186  loss: 1.0948 (1.1298)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [410/431]  eta: 0:00:23  lr: 0.000186  loss: 1.0546 (1.1286)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [420/431]  eta: 0:00:12  lr: 0.000186  loss: 1.0859 (1.1289)  time: 1.1286  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163]  [430/431]  eta: 0:00:01  lr: 0.000186  loss: 1.1084 (1.1293)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:163] Total time: 0:08:07 (1.1322 s / it)\n",
      "Averaged stats: lr: 0.000186  loss: 1.1084 (1.1293)\n",
      "Valid: [epoch:163]  [ 0/14]  eta: 0:00:34  loss: 1.0387 (1.0387)  time: 2.4874  data: 2.2941  max mem: 15925\n",
      "Valid: [epoch:163]  [13/14]  eta: 0:00:00  loss: 1.0609 (1.0712)  time: 0.2640  data: 0.1640  max mem: 15925\n",
      "Valid: [epoch:163] Total time: 0:00:03 (0.2791 s / it)\n",
      "Averaged stats: loss: 1.0609 (1.0712)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_163_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.071%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:164]  [  0/431]  eta: 0:29:24  lr: 0.000186  loss: 1.1458 (1.1458)  time: 4.0946  data: 2.8801  max mem: 15925\n",
      "Train: [epoch:164]  [ 10/431]  eta: 0:09:24  lr: 0.000186  loss: 1.0866 (1.1037)  time: 1.3403  data: 0.2620  max mem: 15925\n",
      "Train: [epoch:164]  [ 20/431]  eta: 0:08:21  lr: 0.000186  loss: 1.1306 (1.1539)  time: 1.0777  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [ 30/431]  eta: 0:07:55  lr: 0.000186  loss: 1.1306 (1.1384)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [ 40/431]  eta: 0:07:38  lr: 0.000186  loss: 1.0818 (1.1409)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [ 50/431]  eta: 0:07:21  lr: 0.000186  loss: 1.1246 (1.1450)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [ 60/431]  eta: 0:07:07  lr: 0.000186  loss: 1.0804 (1.1280)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [ 70/431]  eta: 0:06:54  lr: 0.000186  loss: 1.0933 (1.1311)  time: 1.1145  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:164]  [ 80/431]  eta: 0:06:42  lr: 0.000186  loss: 1.1516 (1.1347)  time: 1.1307  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [ 90/431]  eta: 0:06:30  lr: 0.000186  loss: 1.1734 (1.1398)  time: 1.1390  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [100/431]  eta: 0:06:18  lr: 0.000186  loss: 1.0978 (1.1319)  time: 1.1287  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [110/431]  eta: 0:06:07  lr: 0.000186  loss: 1.0476 (1.1293)  time: 1.1389  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [120/431]  eta: 0:05:55  lr: 0.000186  loss: 1.0752 (1.1325)  time: 1.1405  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [130/431]  eta: 0:05:44  lr: 0.000186  loss: 1.0933 (1.1302)  time: 1.1427  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [140/431]  eta: 0:05:32  lr: 0.000186  loss: 1.0933 (1.1298)  time: 1.1449  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [150/431]  eta: 0:05:20  lr: 0.000186  loss: 1.0842 (1.1299)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [160/431]  eta: 0:05:09  lr: 0.000186  loss: 1.0892 (1.1298)  time: 1.1264  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [170/431]  eta: 0:04:57  lr: 0.000186  loss: 1.0892 (1.1296)  time: 1.1275  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [180/431]  eta: 0:04:45  lr: 0.000186  loss: 1.1035 (1.1306)  time: 1.1178  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [190/431]  eta: 0:04:34  lr: 0.000186  loss: 1.0997 (1.1292)  time: 1.1143  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [200/431]  eta: 0:04:22  lr: 0.000186  loss: 1.0847 (1.1270)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [210/431]  eta: 0:04:10  lr: 0.000186  loss: 1.0655 (1.1245)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [220/431]  eta: 0:03:59  lr: 0.000186  loss: 1.0605 (1.1235)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [230/431]  eta: 0:03:48  lr: 0.000186  loss: 1.0975 (1.1253)  time: 1.1389  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [240/431]  eta: 0:03:36  lr: 0.000186  loss: 1.1613 (1.1277)  time: 1.1339  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:164]  [250/431]  eta: 0:03:25  lr: 0.000186  loss: 1.1613 (1.1294)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [260/431]  eta: 0:03:14  lr: 0.000186  loss: 1.1196 (1.1302)  time: 1.1322  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [270/431]  eta: 0:03:02  lr: 0.000186  loss: 1.1151 (1.1307)  time: 1.1393  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [280/431]  eta: 0:02:51  lr: 0.000186  loss: 1.1079 (1.1304)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [290/431]  eta: 0:02:39  lr: 0.000186  loss: 1.1004 (1.1308)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [300/431]  eta: 0:02:28  lr: 0.000186  loss: 1.0883 (1.1318)  time: 1.1263  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [310/431]  eta: 0:02:17  lr: 0.000186  loss: 1.0883 (1.1312)  time: 1.1307  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [320/431]  eta: 0:02:05  lr: 0.000186  loss: 1.0630 (1.1309)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [330/431]  eta: 0:01:54  lr: 0.000186  loss: 1.0691 (1.1312)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [340/431]  eta: 0:01:43  lr: 0.000186  loss: 1.1542 (1.1328)  time: 1.1426  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [350/431]  eta: 0:01:31  lr: 0.000186  loss: 1.1627 (1.1339)  time: 1.1517  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:164]  [360/431]  eta: 0:01:20  lr: 0.000186  loss: 1.1098 (1.1329)  time: 1.1428  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [370/431]  eta: 0:01:09  lr: 0.000186  loss: 1.0704 (1.1323)  time: 1.1391  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [380/431]  eta: 0:00:57  lr: 0.000186  loss: 1.0636 (1.1310)  time: 1.1337  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [390/431]  eta: 0:00:46  lr: 0.000186  loss: 1.1155 (1.1313)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [400/431]  eta: 0:00:35  lr: 0.000186  loss: 1.1227 (1.1313)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [410/431]  eta: 0:00:23  lr: 0.000186  loss: 1.1115 (1.1306)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [420/431]  eta: 0:00:12  lr: 0.000186  loss: 1.1095 (1.1302)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:164]  [430/431]  eta: 0:00:01  lr: 0.000186  loss: 1.1172 (1.1301)  time: 1.1249  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:164] Total time: 0:08:08 (1.1329 s / it)\n",
      "Averaged stats: lr: 0.000186  loss: 1.1172 (1.1301)\n",
      "Valid: [epoch:164]  [ 0/14]  eta: 0:00:37  loss: 1.0294 (1.0294)  time: 2.6701  data: 2.5426  max mem: 15925\n",
      "Valid: [epoch:164]  [13/14]  eta: 0:00:00  loss: 1.0594 (1.0698)  time: 0.2947  data: 0.1817  max mem: 15925\n",
      "Valid: [epoch:164] Total time: 0:00:04 (0.3132 s / it)\n",
      "Averaged stats: loss: 1.0594 (1.0698)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_164_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.070%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:165]  [  0/431]  eta: 0:30:49  lr: 0.000186  loss: 1.1063 (1.1063)  time: 4.2920  data: 3.0899  max mem: 15925\n",
      "Train: [epoch:165]  [ 10/431]  eta: 0:09:24  lr: 0.000186  loss: 1.1063 (1.1099)  time: 1.3397  data: 0.2811  max mem: 15925\n",
      "Train: [epoch:165]  [ 20/431]  eta: 0:08:23  lr: 0.000186  loss: 1.0987 (1.1364)  time: 1.0714  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [ 30/431]  eta: 0:07:55  lr: 0.000186  loss: 1.0987 (1.1290)  time: 1.1023  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [ 40/431]  eta: 0:07:36  lr: 0.000186  loss: 1.1260 (1.1254)  time: 1.1073  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [ 50/431]  eta: 0:07:20  lr: 0.000186  loss: 1.0866 (1.1267)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [ 60/431]  eta: 0:07:06  lr: 0.000186  loss: 1.0463 (1.1202)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [ 70/431]  eta: 0:06:52  lr: 0.000186  loss: 1.0979 (1.1253)  time: 1.1100  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [ 80/431]  eta: 0:06:40  lr: 0.000186  loss: 1.1003 (1.1277)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [ 90/431]  eta: 0:06:28  lr: 0.000186  loss: 1.0945 (1.1263)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [100/431]  eta: 0:06:16  lr: 0.000186  loss: 1.1054 (1.1234)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [110/431]  eta: 0:06:04  lr: 0.000186  loss: 1.0856 (1.1207)  time: 1.1230  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:165]  [120/431]  eta: 0:05:53  lr: 0.000186  loss: 1.0856 (1.1202)  time: 1.1303  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:165]  [130/431]  eta: 0:05:41  lr: 0.000186  loss: 1.1100 (1.1190)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [140/431]  eta: 0:05:29  lr: 0.000186  loss: 1.1162 (1.1229)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [150/431]  eta: 0:05:17  lr: 0.000186  loss: 1.1162 (1.1238)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [160/431]  eta: 0:05:05  lr: 0.000186  loss: 1.0725 (1.1244)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [170/431]  eta: 0:04:54  lr: 0.000186  loss: 1.1222 (1.1253)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [180/431]  eta: 0:04:43  lr: 0.000186  loss: 1.1222 (1.1239)  time: 1.1213  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [190/431]  eta: 0:04:31  lr: 0.000186  loss: 1.0534 (1.1216)  time: 1.1294  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [200/431]  eta: 0:04:20  lr: 0.000186  loss: 1.0797 (1.1203)  time: 1.1352  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [210/431]  eta: 0:04:09  lr: 0.000186  loss: 1.0966 (1.1195)  time: 1.1296  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [220/431]  eta: 0:03:58  lr: 0.000186  loss: 1.1246 (1.1231)  time: 1.1219  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [230/431]  eta: 0:03:46  lr: 0.000186  loss: 1.1356 (1.1237)  time: 1.1267  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [240/431]  eta: 0:03:35  lr: 0.000186  loss: 1.1320 (1.1249)  time: 1.1382  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:165]  [250/431]  eta: 0:03:24  lr: 0.000186  loss: 1.0877 (1.1231)  time: 1.1249  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:165]  [260/431]  eta: 0:03:13  lr: 0.000186  loss: 1.0829 (1.1243)  time: 1.1290  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [270/431]  eta: 0:03:01  lr: 0.000186  loss: 1.1107 (1.1253)  time: 1.1349  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [280/431]  eta: 0:02:50  lr: 0.000186  loss: 1.0880 (1.1264)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [290/431]  eta: 0:02:38  lr: 0.000186  loss: 1.1195 (1.1274)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [300/431]  eta: 0:02:27  lr: 0.000186  loss: 1.1328 (1.1282)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [310/431]  eta: 0:02:16  lr: 0.000186  loss: 1.0993 (1.1278)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [320/431]  eta: 0:02:05  lr: 0.000186  loss: 1.0975 (1.1272)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [330/431]  eta: 0:01:53  lr: 0.000186  loss: 1.1347 (1.1303)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [340/431]  eta: 0:01:42  lr: 0.000186  loss: 1.1648 (1.1313)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [350/431]  eta: 0:01:31  lr: 0.000186  loss: 1.1484 (1.1317)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [360/431]  eta: 0:01:19  lr: 0.000186  loss: 1.0912 (1.1308)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [370/431]  eta: 0:01:08  lr: 0.000186  loss: 1.0802 (1.1306)  time: 1.1297  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [380/431]  eta: 0:00:57  lr: 0.000186  loss: 1.0876 (1.1302)  time: 1.1320  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [390/431]  eta: 0:00:46  lr: 0.000186  loss: 1.0876 (1.1298)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [400/431]  eta: 0:00:34  lr: 0.000186  loss: 1.0850 (1.1303)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [410/431]  eta: 0:00:23  lr: 0.000186  loss: 1.0838 (1.1291)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [420/431]  eta: 0:00:12  lr: 0.000186  loss: 1.0575 (1.1300)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:165]  [430/431]  eta: 0:00:01  lr: 0.000186  loss: 1.1431 (1.1294)  time: 1.1262  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:165] Total time: 0:08:05 (1.1264 s / it)\n",
      "Averaged stats: lr: 0.000186  loss: 1.1431 (1.1294)\n",
      "Valid: [epoch:165]  [ 0/14]  eta: 0:00:36  loss: 1.0372 (1.0372)  time: 2.5751  data: 2.4087  max mem: 15925\n",
      "Valid: [epoch:165]  [13/14]  eta: 0:00:00  loss: 1.0596 (1.0711)  time: 0.2744  data: 0.1722  max mem: 15925\n",
      "Valid: [epoch:165] Total time: 0:00:04 (0.2905 s / it)\n",
      "Averaged stats: loss: 1.0596 (1.0711)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_165_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.071%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:166]  [  0/431]  eta: 0:36:23  lr: 0.000186  loss: 1.2921 (1.2921)  time: 5.0652  data: 3.6377  max mem: 15925\n",
      "Train: [epoch:166]  [ 10/431]  eta: 0:10:00  lr: 0.000186  loss: 1.1377 (1.2094)  time: 1.4269  data: 0.3310  max mem: 15925\n",
      "Train: [epoch:166]  [ 20/431]  eta: 0:08:42  lr: 0.000186  loss: 1.1147 (1.1584)  time: 1.0805  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [ 30/431]  eta: 0:08:06  lr: 0.000186  loss: 1.0915 (1.1419)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [ 40/431]  eta: 0:07:46  lr: 0.000186  loss: 1.1165 (1.1534)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [ 50/431]  eta: 0:07:30  lr: 0.000186  loss: 1.0999 (1.1391)  time: 1.1333  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [ 60/431]  eta: 0:07:15  lr: 0.000186  loss: 1.0483 (1.1334)  time: 1.1357  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [ 70/431]  eta: 0:07:02  lr: 0.000186  loss: 1.1031 (1.1333)  time: 1.1377  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [ 80/431]  eta: 0:06:47  lr: 0.000186  loss: 1.1031 (1.1277)  time: 1.1210  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [ 90/431]  eta: 0:06:34  lr: 0.000186  loss: 1.0595 (1.1208)  time: 1.1086  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [100/431]  eta: 0:06:21  lr: 0.000186  loss: 1.0691 (1.1188)  time: 1.1131  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [110/431]  eta: 0:06:08  lr: 0.000186  loss: 1.0839 (1.1155)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [120/431]  eta: 0:05:56  lr: 0.000186  loss: 1.0879 (1.1166)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [130/431]  eta: 0:05:44  lr: 0.000186  loss: 1.0976 (1.1187)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [140/431]  eta: 0:05:32  lr: 0.000186  loss: 1.0687 (1.1165)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [150/431]  eta: 0:05:20  lr: 0.000186  loss: 1.0758 (1.1163)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [160/431]  eta: 0:05:08  lr: 0.000186  loss: 1.1359 (1.1221)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [170/431]  eta: 0:04:57  lr: 0.000186  loss: 1.0922 (1.1195)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [180/431]  eta: 0:04:45  lr: 0.000186  loss: 1.0617 (1.1217)  time: 1.1309  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:166]  [190/431]  eta: 0:04:34  lr: 0.000186  loss: 1.1172 (1.1226)  time: 1.1350  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [200/431]  eta: 0:04:22  lr: 0.000186  loss: 1.0833 (1.1224)  time: 1.1275  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [210/431]  eta: 0:04:11  lr: 0.000186  loss: 1.1153 (1.1244)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [220/431]  eta: 0:03:59  lr: 0.000186  loss: 1.1516 (1.1260)  time: 1.1294  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [230/431]  eta: 0:03:48  lr: 0.000186  loss: 1.1341 (1.1285)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [240/431]  eta: 0:03:36  lr: 0.000186  loss: 1.1407 (1.1304)  time: 1.1260  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [250/431]  eta: 0:03:25  lr: 0.000186  loss: 1.1767 (1.1329)  time: 1.1348  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [260/431]  eta: 0:03:14  lr: 0.000186  loss: 1.1030 (1.1326)  time: 1.1385  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [270/431]  eta: 0:03:02  lr: 0.000186  loss: 1.0957 (1.1332)  time: 1.1346  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [280/431]  eta: 0:02:51  lr: 0.000186  loss: 1.1253 (1.1336)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [290/431]  eta: 0:02:39  lr: 0.000186  loss: 1.0992 (1.1327)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [300/431]  eta: 0:02:28  lr: 0.000186  loss: 1.0830 (1.1323)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [310/431]  eta: 0:02:17  lr: 0.000186  loss: 1.1207 (1.1330)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [320/431]  eta: 0:02:05  lr: 0.000186  loss: 1.1207 (1.1323)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [330/431]  eta: 0:01:54  lr: 0.000186  loss: 1.1374 (1.1332)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [340/431]  eta: 0:01:43  lr: 0.000186  loss: 1.1019 (1.1321)  time: 1.1335  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [350/431]  eta: 0:01:31  lr: 0.000186  loss: 1.1196 (1.1338)  time: 1.1364  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [360/431]  eta: 0:01:20  lr: 0.000186  loss: 1.1545 (1.1349)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [370/431]  eta: 0:01:09  lr: 0.000186  loss: 1.1135 (1.1337)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [380/431]  eta: 0:00:57  lr: 0.000186  loss: 1.0623 (1.1313)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [390/431]  eta: 0:00:46  lr: 0.000186  loss: 1.0608 (1.1312)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [400/431]  eta: 0:00:35  lr: 0.000186  loss: 1.0645 (1.1296)  time: 1.1225  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:166]  [410/431]  eta: 0:00:23  lr: 0.000186  loss: 1.1124 (1.1302)  time: 1.1349  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:166]  [420/431]  eta: 0:00:12  lr: 0.000186  loss: 1.1380 (1.1301)  time: 1.1364  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166]  [430/431]  eta: 0:00:01  lr: 0.000186  loss: 1.0597 (1.1293)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:166] Total time: 0:08:08 (1.1330 s / it)\n",
      "Averaged stats: lr: 0.000186  loss: 1.0597 (1.1293)\n",
      "Valid: [epoch:166]  [ 0/14]  eta: 0:00:37  loss: 0.9482 (0.9482)  time: 2.6664  data: 2.5060  max mem: 15925\n",
      "Valid: [epoch:166]  [13/14]  eta: 0:00:00  loss: 1.0560 (1.0673)  time: 0.2794  data: 0.1791  max mem: 15925\n",
      "Valid: [epoch:166] Total time: 0:00:04 (0.2946 s / it)\n",
      "Averaged stats: loss: 1.0560 (1.0673)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_166_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.067%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:167]  [  0/431]  eta: 0:39:25  lr: 0.000185  loss: 1.0118 (1.0118)  time: 5.4881  data: 4.2933  max mem: 15925\n",
      "Train: [epoch:167]  [ 10/431]  eta: 0:10:18  lr: 0.000185  loss: 1.1656 (1.1927)  time: 1.4680  data: 0.3905  max mem: 15925\n",
      "Train: [epoch:167]  [ 20/431]  eta: 0:08:48  lr: 0.000185  loss: 1.1497 (1.1791)  time: 1.0755  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [ 30/431]  eta: 0:08:09  lr: 0.000185  loss: 1.1157 (1.1577)  time: 1.0855  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [ 40/431]  eta: 0:07:43  lr: 0.000185  loss: 1.0811 (1.1420)  time: 1.0818  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [ 50/431]  eta: 0:07:26  lr: 0.000185  loss: 1.0598 (1.1269)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [ 60/431]  eta: 0:07:12  lr: 0.000185  loss: 1.0779 (1.1309)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [ 70/431]  eta: 0:06:59  lr: 0.000185  loss: 1.1251 (1.1365)  time: 1.1419  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [ 80/431]  eta: 0:06:46  lr: 0.000185  loss: 1.1251 (1.1332)  time: 1.1339  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [ 90/431]  eta: 0:06:33  lr: 0.000185  loss: 1.0787 (1.1269)  time: 1.1246  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [100/431]  eta: 0:06:21  lr: 0.000185  loss: 1.0569 (1.1221)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [110/431]  eta: 0:06:09  lr: 0.000185  loss: 1.0592 (1.1205)  time: 1.1276  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [120/431]  eta: 0:05:57  lr: 0.000185  loss: 1.0637 (1.1180)  time: 1.1319  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [130/431]  eta: 0:05:45  lr: 0.000185  loss: 1.1006 (1.1180)  time: 1.1285  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:167]  [140/431]  eta: 0:05:33  lr: 0.000185  loss: 1.1487 (1.1214)  time: 1.1214  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:167]  [150/431]  eta: 0:05:21  lr: 0.000185  loss: 1.1211 (1.1205)  time: 1.1251  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [160/431]  eta: 0:05:09  lr: 0.000185  loss: 1.1133 (1.1222)  time: 1.1354  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [170/431]  eta: 0:04:58  lr: 0.000185  loss: 1.1122 (1.1220)  time: 1.1356  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [180/431]  eta: 0:04:46  lr: 0.000185  loss: 1.1300 (1.1231)  time: 1.1339  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [190/431]  eta: 0:04:35  lr: 0.000185  loss: 1.1281 (1.1227)  time: 1.1297  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [200/431]  eta: 0:04:23  lr: 0.000185  loss: 1.0567 (1.1204)  time: 1.1265  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [210/431]  eta: 0:04:11  lr: 0.000185  loss: 1.0685 (1.1192)  time: 1.1285  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [220/431]  eta: 0:04:00  lr: 0.000185  loss: 1.0884 (1.1197)  time: 1.1272  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [230/431]  eta: 0:03:48  lr: 0.000185  loss: 1.1490 (1.1209)  time: 1.1235  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [240/431]  eta: 0:03:37  lr: 0.000185  loss: 1.1651 (1.1257)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [250/431]  eta: 0:03:25  lr: 0.000185  loss: 1.2190 (1.1273)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [260/431]  eta: 0:03:14  lr: 0.000185  loss: 1.1287 (1.1277)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [270/431]  eta: 0:03:02  lr: 0.000185  loss: 1.1221 (1.1286)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [280/431]  eta: 0:02:51  lr: 0.000185  loss: 1.1330 (1.1274)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [290/431]  eta: 0:02:39  lr: 0.000185  loss: 1.1330 (1.1267)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [300/431]  eta: 0:02:28  lr: 0.000185  loss: 1.0876 (1.1265)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [310/431]  eta: 0:02:17  lr: 0.000185  loss: 1.0950 (1.1287)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [320/431]  eta: 0:02:05  lr: 0.000185  loss: 1.0889 (1.1279)  time: 1.1235  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:167]  [330/431]  eta: 0:01:54  lr: 0.000185  loss: 1.0940 (1.1279)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [340/431]  eta: 0:01:43  lr: 0.000185  loss: 1.1143 (1.1277)  time: 1.1234  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [350/431]  eta: 0:01:31  lr: 0.000185  loss: 1.1132 (1.1278)  time: 1.1356  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [360/431]  eta: 0:01:20  lr: 0.000185  loss: 1.1116 (1.1290)  time: 1.1352  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [370/431]  eta: 0:01:09  lr: 0.000185  loss: 1.0995 (1.1287)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [380/431]  eta: 0:00:57  lr: 0.000185  loss: 1.0738 (1.1278)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [390/431]  eta: 0:00:46  lr: 0.000185  loss: 1.0904 (1.1280)  time: 1.1296  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [400/431]  eta: 0:00:35  lr: 0.000185  loss: 1.1701 (1.1290)  time: 1.1429  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [410/431]  eta: 0:00:23  lr: 0.000185  loss: 1.0885 (1.1304)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [420/431]  eta: 0:00:12  lr: 0.000185  loss: 1.0885 (1.1308)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:167]  [430/431]  eta: 0:00:01  lr: 0.000185  loss: 1.1317 (1.1311)  time: 1.1142  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:167] Total time: 0:08:07 (1.1319 s / it)\n",
      "Averaged stats: lr: 0.000185  loss: 1.1317 (1.1311)\n",
      "Valid: [epoch:167]  [ 0/14]  eta: 0:00:34  loss: 1.1197 (1.1197)  time: 2.4543  data: 2.2988  max mem: 15925\n",
      "Valid: [epoch:167]  [13/14]  eta: 0:00:00  loss: 1.0599 (1.0713)  time: 0.2566  data: 0.1643  max mem: 15925\n",
      "Valid: [epoch:167] Total time: 0:00:03 (0.2701 s / it)\n",
      "Averaged stats: loss: 1.0599 (1.0713)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_167_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.071%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:168]  [  0/431]  eta: 0:30:07  lr: 0.000185  loss: 1.1147 (1.1147)  time: 4.1943  data: 2.9862  max mem: 15925\n",
      "Train: [epoch:168]  [ 10/431]  eta: 0:09:38  lr: 0.000185  loss: 1.1138 (1.1200)  time: 1.3735  data: 0.2717  max mem: 15925\n",
      "Train: [epoch:168]  [ 20/431]  eta: 0:08:30  lr: 0.000185  loss: 1.1138 (1.1341)  time: 1.0943  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [ 30/431]  eta: 0:07:58  lr: 0.000185  loss: 1.0824 (1.1189)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [ 40/431]  eta: 0:07:40  lr: 0.000185  loss: 1.0840 (1.1193)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [ 50/431]  eta: 0:07:24  lr: 0.000185  loss: 1.0992 (1.1206)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [ 60/431]  eta: 0:07:10  lr: 0.000185  loss: 1.0814 (1.1156)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [ 70/431]  eta: 0:06:56  lr: 0.000185  loss: 1.1144 (1.1148)  time: 1.1264  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [ 80/431]  eta: 0:06:44  lr: 0.000185  loss: 1.1531 (1.1244)  time: 1.1257  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [ 90/431]  eta: 0:06:31  lr: 0.000185  loss: 1.1510 (1.1248)  time: 1.1294  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:168]  [100/431]  eta: 0:06:19  lr: 0.000185  loss: 1.0968 (1.1250)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [110/431]  eta: 0:06:07  lr: 0.000185  loss: 1.0968 (1.1229)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [120/431]  eta: 0:05:55  lr: 0.000185  loss: 1.0902 (1.1219)  time: 1.1300  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [130/431]  eta: 0:05:43  lr: 0.000185  loss: 1.0902 (1.1221)  time: 1.1361  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [140/431]  eta: 0:05:32  lr: 0.000185  loss: 1.1157 (1.1230)  time: 1.1347  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [150/431]  eta: 0:05:20  lr: 0.000185  loss: 1.0650 (1.1190)  time: 1.1329  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [160/431]  eta: 0:05:09  lr: 0.000185  loss: 1.0650 (1.1206)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [170/431]  eta: 0:04:57  lr: 0.000185  loss: 1.0615 (1.1154)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [180/431]  eta: 0:04:45  lr: 0.000185  loss: 1.0877 (1.1179)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [190/431]  eta: 0:04:33  lr: 0.000185  loss: 1.1103 (1.1177)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [200/431]  eta: 0:04:22  lr: 0.000185  loss: 1.0737 (1.1179)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [210/431]  eta: 0:04:10  lr: 0.000185  loss: 1.0889 (1.1176)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [220/431]  eta: 0:03:58  lr: 0.000185  loss: 1.1417 (1.1181)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [230/431]  eta: 0:03:47  lr: 0.000185  loss: 1.1531 (1.1191)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [240/431]  eta: 0:03:36  lr: 0.000185  loss: 1.1290 (1.1208)  time: 1.1131  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [250/431]  eta: 0:03:24  lr: 0.000185  loss: 1.1208 (1.1214)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [260/431]  eta: 0:03:13  lr: 0.000185  loss: 1.0803 (1.1227)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [270/431]  eta: 0:03:01  lr: 0.000185  loss: 1.0895 (1.1230)  time: 1.1192  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [280/431]  eta: 0:02:50  lr: 0.000185  loss: 1.1154 (1.1247)  time: 1.1162  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [290/431]  eta: 0:02:39  lr: 0.000185  loss: 1.0868 (1.1250)  time: 1.1252  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [300/431]  eta: 0:02:27  lr: 0.000185  loss: 1.1423 (1.1269)  time: 1.1324  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [310/431]  eta: 0:02:16  lr: 0.000185  loss: 1.1506 (1.1258)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [320/431]  eta: 0:02:05  lr: 0.000185  loss: 1.0642 (1.1261)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [330/431]  eta: 0:01:54  lr: 0.000185  loss: 1.1587 (1.1272)  time: 1.1282  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [340/431]  eta: 0:01:42  lr: 0.000185  loss: 1.1587 (1.1284)  time: 1.1238  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [350/431]  eta: 0:01:31  lr: 0.000185  loss: 1.1213 (1.1286)  time: 1.1218  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [360/431]  eta: 0:01:20  lr: 0.000185  loss: 1.1011 (1.1292)  time: 1.1391  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:168]  [370/431]  eta: 0:01:08  lr: 0.000185  loss: 1.0968 (1.1284)  time: 1.1494  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [380/431]  eta: 0:00:57  lr: 0.000185  loss: 1.0954 (1.1282)  time: 1.1339  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [390/431]  eta: 0:00:46  lr: 0.000185  loss: 1.0954 (1.1276)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [400/431]  eta: 0:00:34  lr: 0.000185  loss: 1.0833 (1.1276)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [410/431]  eta: 0:00:23  lr: 0.000185  loss: 1.0833 (1.1277)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [420/431]  eta: 0:00:12  lr: 0.000185  loss: 1.0788 (1.1274)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:168]  [430/431]  eta: 0:00:01  lr: 0.000185  loss: 1.1323 (1.1275)  time: 1.1194  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:168] Total time: 0:08:06 (1.1283 s / it)\n",
      "Averaged stats: lr: 0.000185  loss: 1.1323 (1.1275)\n",
      "Valid: [epoch:168]  [ 0/14]  eta: 0:00:36  loss: 1.0014 (1.0014)  time: 2.5815  data: 2.4132  max mem: 15925\n",
      "Valid: [epoch:168]  [13/14]  eta: 0:00:00  loss: 1.0594 (1.0702)  time: 0.2757  data: 0.1725  max mem: 15925\n",
      "Valid: [epoch:168] Total time: 0:00:04 (0.2945 s / it)\n",
      "Averaged stats: loss: 1.0594 (1.0702)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_168_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.070%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:169]  [  0/431]  eta: 0:32:52  lr: 0.000185  loss: 1.1842 (1.1842)  time: 4.5764  data: 3.3464  max mem: 15925\n",
      "Train: [epoch:169]  [ 10/431]  eta: 0:09:44  lr: 0.000185  loss: 1.1842 (1.1641)  time: 1.3877  data: 0.3045  max mem: 15925\n",
      "Train: [epoch:169]  [ 20/431]  eta: 0:08:31  lr: 0.000185  loss: 1.1963 (1.1820)  time: 1.0788  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [ 30/431]  eta: 0:08:01  lr: 0.000185  loss: 1.1743 (1.1586)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [ 40/431]  eta: 0:07:41  lr: 0.000185  loss: 1.1153 (1.1566)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [ 50/431]  eta: 0:07:28  lr: 0.000185  loss: 1.1137 (1.1446)  time: 1.1379  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [ 60/431]  eta: 0:07:14  lr: 0.000185  loss: 1.0912 (1.1380)  time: 1.1510  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [ 70/431]  eta: 0:07:00  lr: 0.000185  loss: 1.1293 (1.1386)  time: 1.1315  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [ 80/431]  eta: 0:06:46  lr: 0.000185  loss: 1.1394 (1.1387)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [ 90/431]  eta: 0:06:34  lr: 0.000185  loss: 1.1021 (1.1324)  time: 1.1293  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [100/431]  eta: 0:06:21  lr: 0.000185  loss: 1.0524 (1.1284)  time: 1.1306  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [110/431]  eta: 0:06:09  lr: 0.000185  loss: 1.1003 (1.1334)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [120/431]  eta: 0:05:57  lr: 0.000185  loss: 1.1003 (1.1291)  time: 1.1306  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [130/431]  eta: 0:05:45  lr: 0.000185  loss: 1.0636 (1.1306)  time: 1.1322  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [140/431]  eta: 0:05:33  lr: 0.000185  loss: 1.1002 (1.1328)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [150/431]  eta: 0:05:21  lr: 0.000185  loss: 1.0896 (1.1328)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [160/431]  eta: 0:05:09  lr: 0.000185  loss: 1.0896 (1.1305)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [170/431]  eta: 0:04:57  lr: 0.000185  loss: 1.0765 (1.1262)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [180/431]  eta: 0:04:46  lr: 0.000185  loss: 1.0696 (1.1269)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [190/431]  eta: 0:04:34  lr: 0.000185  loss: 1.0991 (1.1286)  time: 1.1354  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [200/431]  eta: 0:04:23  lr: 0.000185  loss: 1.1447 (1.1290)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [210/431]  eta: 0:04:11  lr: 0.000185  loss: 1.1253 (1.1289)  time: 1.1307  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [220/431]  eta: 0:04:00  lr: 0.000185  loss: 1.1112 (1.1264)  time: 1.1320  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [230/431]  eta: 0:03:48  lr: 0.000185  loss: 1.0498 (1.1269)  time: 1.1233  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [240/431]  eta: 0:03:37  lr: 0.000185  loss: 1.1293 (1.1271)  time: 1.1268  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [250/431]  eta: 0:03:25  lr: 0.000185  loss: 1.1612 (1.1291)  time: 1.1209  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [260/431]  eta: 0:03:14  lr: 0.000185  loss: 1.1969 (1.1312)  time: 1.1292  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:169]  [270/431]  eta: 0:03:02  lr: 0.000185  loss: 1.1517 (1.1308)  time: 1.1397  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [280/431]  eta: 0:02:51  lr: 0.000185  loss: 1.1216 (1.1302)  time: 1.1402  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [290/431]  eta: 0:02:40  lr: 0.000185  loss: 1.1246 (1.1314)  time: 1.1444  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [300/431]  eta: 0:02:28  lr: 0.000185  loss: 1.1219 (1.1317)  time: 1.1333  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [310/431]  eta: 0:02:17  lr: 0.000185  loss: 1.1148 (1.1312)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [320/431]  eta: 0:02:06  lr: 0.000185  loss: 1.0947 (1.1291)  time: 1.1222  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [330/431]  eta: 0:01:54  lr: 0.000185  loss: 1.1063 (1.1299)  time: 1.1231  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [340/431]  eta: 0:01:43  lr: 0.000185  loss: 1.1105 (1.1296)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [350/431]  eta: 0:01:31  lr: 0.000185  loss: 1.1286 (1.1311)  time: 1.1148  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [360/431]  eta: 0:01:20  lr: 0.000185  loss: 1.0954 (1.1295)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [370/431]  eta: 0:01:09  lr: 0.000185  loss: 1.0829 (1.1289)  time: 1.1240  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [380/431]  eta: 0:00:57  lr: 0.000185  loss: 1.1256 (1.1292)  time: 1.1221  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:169]  [390/431]  eta: 0:00:46  lr: 0.000185  loss: 1.1275 (1.1292)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [400/431]  eta: 0:00:35  lr: 0.000185  loss: 1.1016 (1.1298)  time: 1.1338  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [410/431]  eta: 0:00:23  lr: 0.000185  loss: 1.1267 (1.1306)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169]  [420/431]  eta: 0:00:12  lr: 0.000185  loss: 1.0829 (1.1293)  time: 1.1175  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:169]  [430/431]  eta: 0:00:01  lr: 0.000185  loss: 1.0483 (1.1285)  time: 1.1339  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:169] Total time: 0:08:08 (1.1332 s / it)\n",
      "Averaged stats: lr: 0.000185  loss: 1.0483 (1.1285)\n",
      "Valid: [epoch:169]  [ 0/14]  eta: 0:00:35  loss: 1.1264 (1.1264)  time: 2.5105  data: 2.3479  max mem: 15925\n",
      "Valid: [epoch:169]  [13/14]  eta: 0:00:00  loss: 1.0588 (1.0693)  time: 0.2959  data: 0.1678  max mem: 15925\n",
      "Valid: [epoch:169] Total time: 0:00:04 (0.3142 s / it)\n",
      "Averaged stats: loss: 1.0588 (1.0693)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_169_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.069%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:170]  [  0/431]  eta: 0:34:01  lr: 0.000185  loss: 1.2785 (1.2785)  time: 4.7366  data: 3.5852  max mem: 15925\n",
      "Train: [epoch:170]  [ 10/431]  eta: 0:09:51  lr: 0.000185  loss: 1.1709 (1.1424)  time: 1.4043  data: 0.3262  max mem: 15925\n",
      "Train: [epoch:170]  [ 20/431]  eta: 0:08:38  lr: 0.000185  loss: 1.1696 (1.1666)  time: 1.0888  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [ 30/431]  eta: 0:08:06  lr: 0.000185  loss: 1.1684 (1.1638)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [ 40/431]  eta: 0:07:46  lr: 0.000185  loss: 1.1191 (1.1486)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [ 50/431]  eta: 0:07:28  lr: 0.000185  loss: 1.1145 (1.1433)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [ 60/431]  eta: 0:07:14  lr: 0.000185  loss: 1.0936 (1.1404)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [ 70/431]  eta: 0:07:01  lr: 0.000185  loss: 1.0848 (1.1358)  time: 1.1435  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [ 80/431]  eta: 0:06:47  lr: 0.000185  loss: 1.0803 (1.1333)  time: 1.1323  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [ 90/431]  eta: 0:06:34  lr: 0.000185  loss: 1.0692 (1.1270)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [100/431]  eta: 0:06:21  lr: 0.000185  loss: 1.0566 (1.1243)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [110/431]  eta: 0:06:08  lr: 0.000185  loss: 1.0579 (1.1197)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [120/431]  eta: 0:05:56  lr: 0.000185  loss: 1.0604 (1.1142)  time: 1.1242  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [130/431]  eta: 0:05:45  lr: 0.000185  loss: 1.0745 (1.1155)  time: 1.1430  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [140/431]  eta: 0:05:33  lr: 0.000185  loss: 1.1005 (1.1141)  time: 1.1425  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [150/431]  eta: 0:05:22  lr: 0.000185  loss: 1.1168 (1.1161)  time: 1.1385  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [160/431]  eta: 0:05:10  lr: 0.000185  loss: 1.1283 (1.1167)  time: 1.1362  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [170/431]  eta: 0:04:59  lr: 0.000185  loss: 1.0734 (1.1149)  time: 1.1375  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [180/431]  eta: 0:04:47  lr: 0.000185  loss: 1.0712 (1.1142)  time: 1.1414  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [190/431]  eta: 0:04:35  lr: 0.000185  loss: 1.0886 (1.1149)  time: 1.1344  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [200/431]  eta: 0:04:23  lr: 0.000185  loss: 1.0785 (1.1128)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [210/431]  eta: 0:04:12  lr: 0.000185  loss: 1.0711 (1.1134)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [220/431]  eta: 0:04:00  lr: 0.000185  loss: 1.0983 (1.1148)  time: 1.1313  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [230/431]  eta: 0:03:49  lr: 0.000185  loss: 1.1519 (1.1165)  time: 1.1361  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [240/431]  eta: 0:03:38  lr: 0.000185  loss: 1.1481 (1.1164)  time: 1.1389  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [250/431]  eta: 0:03:26  lr: 0.000185  loss: 1.1072 (1.1188)  time: 1.1285  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [260/431]  eta: 0:03:15  lr: 0.000185  loss: 1.0930 (1.1193)  time: 1.1375  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [270/431]  eta: 0:03:03  lr: 0.000185  loss: 1.1161 (1.1223)  time: 1.1422  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [280/431]  eta: 0:02:52  lr: 0.000185  loss: 1.1205 (1.1214)  time: 1.1251  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [290/431]  eta: 0:02:40  lr: 0.000185  loss: 1.1146 (1.1216)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [300/431]  eta: 0:02:29  lr: 0.000185  loss: 1.1014 (1.1216)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [310/431]  eta: 0:02:17  lr: 0.000185  loss: 1.0847 (1.1207)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [320/431]  eta: 0:02:06  lr: 0.000185  loss: 1.0786 (1.1214)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [330/431]  eta: 0:01:54  lr: 0.000185  loss: 1.0929 (1.1224)  time: 1.1206  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [340/431]  eta: 0:01:43  lr: 0.000185  loss: 1.1793 (1.1243)  time: 1.1204  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [350/431]  eta: 0:01:32  lr: 0.000185  loss: 1.1491 (1.1251)  time: 1.1315  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [360/431]  eta: 0:01:20  lr: 0.000185  loss: 1.1491 (1.1266)  time: 1.1284  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [370/431]  eta: 0:01:09  lr: 0.000185  loss: 1.1420 (1.1272)  time: 1.1290  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [380/431]  eta: 0:00:57  lr: 0.000185  loss: 1.1361 (1.1269)  time: 1.1347  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [390/431]  eta: 0:00:46  lr: 0.000185  loss: 1.1438 (1.1282)  time: 1.1335  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [400/431]  eta: 0:00:35  lr: 0.000185  loss: 1.1392 (1.1282)  time: 1.1418  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:170]  [410/431]  eta: 0:00:23  lr: 0.000185  loss: 1.0722 (1.1284)  time: 1.1390  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [420/431]  eta: 0:00:12  lr: 0.000185  loss: 1.1278 (1.1285)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:170]  [430/431]  eta: 0:00:01  lr: 0.000185  loss: 1.1221 (1.1281)  time: 1.1324  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:170] Total time: 0:08:09 (1.1366 s / it)\n",
      "Averaged stats: lr: 0.000185  loss: 1.1221 (1.1281)\n",
      "Valid: [epoch:170]  [ 0/14]  eta: 0:00:34  loss: 1.1109 (1.1109)  time: 2.4914  data: 2.3027  max mem: 15925\n",
      "Valid: [epoch:170]  [13/14]  eta: 0:00:00  loss: 1.0566 (1.0695)  time: 0.3067  data: 0.1646  max mem: 15925\n",
      "Valid: [epoch:170] Total time: 0:00:04 (0.3239 s / it)\n",
      "Averaged stats: loss: 1.0566 (1.0695)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_170_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.069%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:171]  [  0/431]  eta: 0:32:40  lr: 0.000184  loss: 1.1410 (1.1410)  time: 4.5493  data: 3.4110  max mem: 15925\n",
      "Train: [epoch:171]  [ 10/431]  eta: 0:09:26  lr: 0.000184  loss: 1.1338 (1.1413)  time: 1.3451  data: 0.3103  max mem: 15925\n",
      "Train: [epoch:171]  [ 20/431]  eta: 0:08:20  lr: 0.000184  loss: 1.0959 (1.1126)  time: 1.0500  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:171]  [ 30/431]  eta: 0:07:48  lr: 0.000184  loss: 1.0919 (1.1237)  time: 1.0726  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [ 40/431]  eta: 0:07:30  lr: 0.000184  loss: 1.1179 (1.1226)  time: 1.0856  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [ 50/431]  eta: 0:07:14  lr: 0.000184  loss: 1.0739 (1.1088)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [ 60/431]  eta: 0:07:01  lr: 0.000184  loss: 1.0238 (1.0951)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [ 70/431]  eta: 0:06:48  lr: 0.000184  loss: 1.0313 (1.0921)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [ 80/431]  eta: 0:06:37  lr: 0.000184  loss: 1.0876 (1.0948)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [ 90/431]  eta: 0:06:26  lr: 0.000184  loss: 1.1134 (1.0952)  time: 1.1327  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [100/431]  eta: 0:06:15  lr: 0.000184  loss: 1.0934 (1.0966)  time: 1.1437  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [110/431]  eta: 0:06:04  lr: 0.000184  loss: 1.0818 (1.0941)  time: 1.1424  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [120/431]  eta: 0:05:52  lr: 0.000184  loss: 1.0953 (1.0986)  time: 1.1337  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:171]  [130/431]  eta: 0:05:41  lr: 0.000184  loss: 1.1192 (1.0997)  time: 1.1405  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:171]  [140/431]  eta: 0:05:30  lr: 0.000184  loss: 1.1277 (1.1052)  time: 1.1360  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [150/431]  eta: 0:05:18  lr: 0.000184  loss: 1.1060 (1.1057)  time: 1.1331  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [160/431]  eta: 0:05:07  lr: 0.000184  loss: 1.0552 (1.1036)  time: 1.1410  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [170/431]  eta: 0:04:55  lr: 0.000184  loss: 1.0428 (1.1026)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [180/431]  eta: 0:04:44  lr: 0.000184  loss: 1.0751 (1.1058)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [190/431]  eta: 0:04:33  lr: 0.000184  loss: 1.1208 (1.1109)  time: 1.1350  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [200/431]  eta: 0:04:22  lr: 0.000184  loss: 1.1208 (1.1126)  time: 1.1493  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [210/431]  eta: 0:04:10  lr: 0.000184  loss: 1.0994 (1.1123)  time: 1.1453  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [220/431]  eta: 0:03:59  lr: 0.000184  loss: 1.0994 (1.1128)  time: 1.1302  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [230/431]  eta: 0:03:47  lr: 0.000184  loss: 1.1018 (1.1147)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [240/431]  eta: 0:03:36  lr: 0.000184  loss: 1.1568 (1.1199)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [250/431]  eta: 0:03:25  lr: 0.000184  loss: 1.1285 (1.1194)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [260/431]  eta: 0:03:13  lr: 0.000184  loss: 1.1145 (1.1206)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [270/431]  eta: 0:03:02  lr: 0.000184  loss: 1.1361 (1.1216)  time: 1.1295  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [280/431]  eta: 0:02:50  lr: 0.000184  loss: 1.0991 (1.1228)  time: 1.1229  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:171]  [290/431]  eta: 0:02:39  lr: 0.000184  loss: 1.0823 (1.1225)  time: 1.1162  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:171]  [300/431]  eta: 0:02:28  lr: 0.000184  loss: 1.0823 (1.1232)  time: 1.1156  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:171]  [310/431]  eta: 0:02:16  lr: 0.000184  loss: 1.0904 (1.1230)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [320/431]  eta: 0:02:05  lr: 0.000184  loss: 1.1280 (1.1248)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [330/431]  eta: 0:01:54  lr: 0.000184  loss: 1.2163 (1.1262)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [340/431]  eta: 0:01:42  lr: 0.000184  loss: 1.1365 (1.1264)  time: 1.1244  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [350/431]  eta: 0:01:31  lr: 0.000184  loss: 1.1534 (1.1287)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [360/431]  eta: 0:01:20  lr: 0.000184  loss: 1.1442 (1.1283)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [370/431]  eta: 0:01:08  lr: 0.000184  loss: 1.0781 (1.1282)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [380/431]  eta: 0:00:57  lr: 0.000184  loss: 1.1144 (1.1300)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [390/431]  eta: 0:00:46  lr: 0.000184  loss: 1.1448 (1.1294)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [400/431]  eta: 0:00:34  lr: 0.000184  loss: 1.0899 (1.1287)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [410/431]  eta: 0:00:23  lr: 0.000184  loss: 1.0899 (1.1288)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:171]  [420/431]  eta: 0:00:12  lr: 0.000184  loss: 1.0686 (1.1280)  time: 1.1162  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:171]  [430/431]  eta: 0:00:01  lr: 0.000184  loss: 1.1071 (1.1285)  time: 1.1217  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:171] Total time: 0:08:06 (1.1277 s / it)\n",
      "Averaged stats: lr: 0.000184  loss: 1.1071 (1.1285)\n",
      "Valid: [epoch:171]  [ 0/14]  eta: 0:00:35  loss: 1.1161 (1.1161)  time: 2.5622  data: 2.3748  max mem: 15925\n",
      "Valid: [epoch:171]  [13/14]  eta: 0:00:00  loss: 1.0656 (1.0763)  time: 0.2885  data: 0.1697  max mem: 15925\n",
      "Valid: [epoch:171] Total time: 0:00:04 (0.3044 s / it)\n",
      "Averaged stats: loss: 1.0656 (1.0763)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_171_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.076%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:172]  [  0/431]  eta: 0:31:43  lr: 0.000184  loss: 1.1280 (1.1280)  time: 4.4156  data: 3.2081  max mem: 15925\n",
      "Train: [epoch:172]  [ 10/431]  eta: 0:09:34  lr: 0.000184  loss: 1.1651 (1.1876)  time: 1.3658  data: 0.2919  max mem: 15925\n",
      "Train: [epoch:172]  [ 20/431]  eta: 0:08:27  lr: 0.000184  loss: 1.1443 (1.1424)  time: 1.0750  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [ 30/431]  eta: 0:07:58  lr: 0.000184  loss: 1.1443 (1.1451)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [ 40/431]  eta: 0:07:40  lr: 0.000184  loss: 1.1140 (1.1343)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [ 50/431]  eta: 0:07:23  lr: 0.000184  loss: 1.1005 (1.1309)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [ 60/431]  eta: 0:07:09  lr: 0.000184  loss: 1.0523 (1.1239)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [ 70/431]  eta: 0:06:56  lr: 0.000184  loss: 1.0833 (1.1250)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [ 80/431]  eta: 0:06:44  lr: 0.000184  loss: 1.0890 (1.1234)  time: 1.1322  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [ 90/431]  eta: 0:06:32  lr: 0.000184  loss: 1.0598 (1.1169)  time: 1.1408  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [100/431]  eta: 0:06:20  lr: 0.000184  loss: 1.0542 (1.1132)  time: 1.1352  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [110/431]  eta: 0:06:08  lr: 0.000184  loss: 1.0542 (1.1107)  time: 1.1321  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:172]  [120/431]  eta: 0:05:56  lr: 0.000184  loss: 1.0717 (1.1117)  time: 1.1360  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [130/431]  eta: 0:05:44  lr: 0.000184  loss: 1.1037 (1.1122)  time: 1.1245  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [140/431]  eta: 0:05:32  lr: 0.000184  loss: 1.0809 (1.1084)  time: 1.1255  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [150/431]  eta: 0:05:20  lr: 0.000184  loss: 1.1210 (1.1117)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [160/431]  eta: 0:05:09  lr: 0.000184  loss: 1.1328 (1.1152)  time: 1.1309  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [170/431]  eta: 0:04:57  lr: 0.000184  loss: 1.1575 (1.1176)  time: 1.1362  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [180/431]  eta: 0:04:45  lr: 0.000184  loss: 1.1064 (1.1165)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [190/431]  eta: 0:04:34  lr: 0.000184  loss: 1.1087 (1.1204)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [200/431]  eta: 0:04:22  lr: 0.000184  loss: 1.1698 (1.1234)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [210/431]  eta: 0:04:11  lr: 0.000184  loss: 1.1240 (1.1215)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [220/431]  eta: 0:03:59  lr: 0.000184  loss: 1.0521 (1.1215)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [230/431]  eta: 0:03:48  lr: 0.000184  loss: 1.0800 (1.1204)  time: 1.1229  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [240/431]  eta: 0:03:36  lr: 0.000184  loss: 1.1051 (1.1229)  time: 1.1322  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [250/431]  eta: 0:03:25  lr: 0.000184  loss: 1.2321 (1.1261)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [260/431]  eta: 0:03:13  lr: 0.000184  loss: 1.1626 (1.1248)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [270/431]  eta: 0:03:02  lr: 0.000184  loss: 1.0884 (1.1257)  time: 1.1335  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [280/431]  eta: 0:02:51  lr: 0.000184  loss: 1.1399 (1.1261)  time: 1.1273  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [290/431]  eta: 0:02:39  lr: 0.000184  loss: 1.1205 (1.1251)  time: 1.1301  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [300/431]  eta: 0:02:28  lr: 0.000184  loss: 1.1205 (1.1262)  time: 1.1372  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [310/431]  eta: 0:02:17  lr: 0.000184  loss: 1.1239 (1.1258)  time: 1.1444  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [320/431]  eta: 0:02:05  lr: 0.000184  loss: 1.0927 (1.1249)  time: 1.1457  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [330/431]  eta: 0:01:54  lr: 0.000184  loss: 1.1022 (1.1256)  time: 1.1479  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [340/431]  eta: 0:01:43  lr: 0.000184  loss: 1.1124 (1.1265)  time: 1.1417  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [350/431]  eta: 0:01:31  lr: 0.000184  loss: 1.1363 (1.1265)  time: 1.1436  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [360/431]  eta: 0:01:20  lr: 0.000184  loss: 1.1337 (1.1278)  time: 1.1472  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:172]  [370/431]  eta: 0:01:09  lr: 0.000184  loss: 1.1646 (1.1292)  time: 1.1426  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [380/431]  eta: 0:00:57  lr: 0.000184  loss: 1.1691 (1.1304)  time: 1.1293  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:172]  [390/431]  eta: 0:00:46  lr: 0.000184  loss: 1.0901 (1.1289)  time: 1.1169  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:172]  [400/431]  eta: 0:00:35  lr: 0.000184  loss: 1.0666 (1.1280)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [410/431]  eta: 0:00:23  lr: 0.000184  loss: 1.0706 (1.1275)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:172]  [420/431]  eta: 0:00:12  lr: 0.000184  loss: 1.1262 (1.1281)  time: 1.1190  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:172]  [430/431]  eta: 0:00:01  lr: 0.000184  loss: 1.1302 (1.1280)  time: 1.1253  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:172] Total time: 0:08:08 (1.1339 s / it)\n",
      "Averaged stats: lr: 0.000184  loss: 1.1302 (1.1280)\n",
      "Valid: [epoch:172]  [ 0/14]  eta: 0:00:37  loss: 1.0071 (1.0071)  time: 2.6561  data: 2.4760  max mem: 15925\n",
      "Valid: [epoch:172]  [13/14]  eta: 0:00:00  loss: 1.0635 (1.0736)  time: 0.3007  data: 0.1770  max mem: 15925\n",
      "Valid: [epoch:172] Total time: 0:00:04 (0.3174 s / it)\n",
      "Averaged stats: loss: 1.0635 (1.0736)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_172_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:173]  [  0/431]  eta: 0:34:51  lr: 0.000184  loss: 1.1040 (1.1040)  time: 4.8538  data: 3.6854  max mem: 15925\n",
      "Train: [epoch:173]  [ 10/431]  eta: 0:09:48  lr: 0.000184  loss: 1.0683 (1.1071)  time: 1.3988  data: 0.3352  max mem: 15925\n",
      "Train: [epoch:173]  [ 20/431]  eta: 0:08:29  lr: 0.000184  loss: 1.0800 (1.1131)  time: 1.0600  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [ 30/431]  eta: 0:07:58  lr: 0.000184  loss: 1.1029 (1.1054)  time: 1.0801  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [ 40/431]  eta: 0:07:42  lr: 0.000184  loss: 1.0518 (1.1085)  time: 1.1209  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [ 50/431]  eta: 0:07:27  lr: 0.000184  loss: 1.1203 (1.1138)  time: 1.1485  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [ 60/431]  eta: 0:07:13  lr: 0.000184  loss: 1.1309 (1.1182)  time: 1.1391  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [ 70/431]  eta: 0:07:01  lr: 0.000184  loss: 1.1472 (1.1207)  time: 1.1427  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [ 80/431]  eta: 0:06:47  lr: 0.000184  loss: 1.1239 (1.1239)  time: 1.1403  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [ 90/431]  eta: 0:06:35  lr: 0.000184  loss: 1.1378 (1.1300)  time: 1.1342  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [100/431]  eta: 0:06:23  lr: 0.000184  loss: 1.1661 (1.1315)  time: 1.1458  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [110/431]  eta: 0:06:11  lr: 0.000184  loss: 1.1630 (1.1319)  time: 1.1502  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [120/431]  eta: 0:05:59  lr: 0.000184  loss: 1.0604 (1.1271)  time: 1.1457  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [130/431]  eta: 0:05:47  lr: 0.000184  loss: 1.0581 (1.1239)  time: 1.1347  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [140/431]  eta: 0:05:35  lr: 0.000184  loss: 1.0596 (1.1219)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [150/431]  eta: 0:05:23  lr: 0.000184  loss: 1.1084 (1.1257)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [160/431]  eta: 0:05:11  lr: 0.000184  loss: 1.0962 (1.1283)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [170/431]  eta: 0:04:59  lr: 0.000184  loss: 1.1286 (1.1296)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [180/431]  eta: 0:04:47  lr: 0.000184  loss: 1.1353 (1.1320)  time: 1.1268  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [190/431]  eta: 0:04:35  lr: 0.000184  loss: 1.1739 (1.1344)  time: 1.1300  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [200/431]  eta: 0:04:24  lr: 0.000184  loss: 1.1171 (1.1332)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [210/431]  eta: 0:04:12  lr: 0.000184  loss: 1.0884 (1.1316)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [220/431]  eta: 0:04:00  lr: 0.000184  loss: 1.0805 (1.1309)  time: 1.1278  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [230/431]  eta: 0:03:49  lr: 0.000184  loss: 1.1319 (1.1325)  time: 1.1402  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [240/431]  eta: 0:03:38  lr: 0.000184  loss: 1.1319 (1.1314)  time: 1.1456  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [250/431]  eta: 0:03:26  lr: 0.000184  loss: 1.1024 (1.1301)  time: 1.1535  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [260/431]  eta: 0:03:15  lr: 0.000184  loss: 1.0721 (1.1300)  time: 1.1477  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [270/431]  eta: 0:03:03  lr: 0.000184  loss: 1.0923 (1.1280)  time: 1.1383  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [280/431]  eta: 0:02:52  lr: 0.000184  loss: 1.0789 (1.1263)  time: 1.1332  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:173]  [290/431]  eta: 0:02:40  lr: 0.000184  loss: 1.1147 (1.1256)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [300/431]  eta: 0:02:29  lr: 0.000184  loss: 1.1147 (1.1255)  time: 1.1292  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [310/431]  eta: 0:02:18  lr: 0.000184  loss: 1.1002 (1.1252)  time: 1.1300  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [320/431]  eta: 0:02:06  lr: 0.000184  loss: 1.1009 (1.1261)  time: 1.1234  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:173]  [330/431]  eta: 0:01:55  lr: 0.000184  loss: 1.1793 (1.1287)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [340/431]  eta: 0:01:43  lr: 0.000184  loss: 1.1573 (1.1302)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [350/431]  eta: 0:01:32  lr: 0.000184  loss: 1.1331 (1.1306)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [360/431]  eta: 0:01:20  lr: 0.000184  loss: 1.0856 (1.1292)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [370/431]  eta: 0:01:09  lr: 0.000184  loss: 1.0874 (1.1296)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [380/431]  eta: 0:00:57  lr: 0.000184  loss: 1.1364 (1.1289)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [390/431]  eta: 0:00:46  lr: 0.000184  loss: 1.0600 (1.1279)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [400/431]  eta: 0:00:35  lr: 0.000184  loss: 1.0617 (1.1272)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [410/431]  eta: 0:00:23  lr: 0.000184  loss: 1.0761 (1.1278)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [420/431]  eta: 0:00:12  lr: 0.000184  loss: 1.0905 (1.1274)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173]  [430/431]  eta: 0:00:01  lr: 0.000184  loss: 1.1003 (1.1274)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:173] Total time: 0:08:09 (1.1352 s / it)\n",
      "Averaged stats: lr: 0.000184  loss: 1.1003 (1.1274)\n",
      "Valid: [epoch:173]  [ 0/14]  eta: 0:00:34  loss: 1.0461 (1.0461)  time: 2.4376  data: 2.2841  max mem: 15925\n",
      "Valid: [epoch:173]  [13/14]  eta: 0:00:00  loss: 1.0588 (1.0695)  time: 0.2786  data: 0.1633  max mem: 15925\n",
      "Valid: [epoch:173] Total time: 0:00:04 (0.2954 s / it)\n",
      "Averaged stats: loss: 1.0588 (1.0695)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_173_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.070%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:174]  [  0/431]  eta: 0:35:09  lr: 0.000184  loss: 1.0658 (1.0658)  time: 4.8941  data: 3.7435  max mem: 15925\n",
      "Train: [epoch:174]  [ 10/431]  eta: 0:09:43  lr: 0.000184  loss: 1.1049 (1.1714)  time: 1.3855  data: 0.3405  max mem: 15925\n",
      "Train: [epoch:174]  [ 20/431]  eta: 0:08:30  lr: 0.000184  loss: 1.1049 (1.1501)  time: 1.0597  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [ 30/431]  eta: 0:07:57  lr: 0.000184  loss: 1.0834 (1.1543)  time: 1.0830  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [ 40/431]  eta: 0:07:38  lr: 0.000184  loss: 1.0834 (1.1428)  time: 1.0982  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [ 50/431]  eta: 0:07:23  lr: 0.000184  loss: 1.0616 (1.1279)  time: 1.1219  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [ 60/431]  eta: 0:07:11  lr: 0.000184  loss: 1.0601 (1.1247)  time: 1.1443  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [ 70/431]  eta: 0:06:57  lr: 0.000184  loss: 1.0864 (1.1189)  time: 1.1436  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [ 80/431]  eta: 0:06:44  lr: 0.000184  loss: 1.1273 (1.1219)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [ 90/431]  eta: 0:06:31  lr: 0.000184  loss: 1.0996 (1.1194)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [100/431]  eta: 0:06:18  lr: 0.000184  loss: 1.0438 (1.1187)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [110/431]  eta: 0:06:06  lr: 0.000184  loss: 1.0455 (1.1193)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [120/431]  eta: 0:05:54  lr: 0.000184  loss: 1.0878 (1.1191)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [130/431]  eta: 0:05:43  lr: 0.000184  loss: 1.1338 (1.1238)  time: 1.1347  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [140/431]  eta: 0:05:31  lr: 0.000184  loss: 1.1519 (1.1274)  time: 1.1393  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [150/431]  eta: 0:05:20  lr: 0.000184  loss: 1.1142 (1.1268)  time: 1.1279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [160/431]  eta: 0:05:08  lr: 0.000184  loss: 1.1209 (1.1276)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [170/431]  eta: 0:04:56  lr: 0.000184  loss: 1.1406 (1.1291)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [180/431]  eta: 0:04:45  lr: 0.000184  loss: 1.1158 (1.1279)  time: 1.1349  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [190/431]  eta: 0:04:34  lr: 0.000184  loss: 1.1012 (1.1286)  time: 1.1442  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [200/431]  eta: 0:04:22  lr: 0.000184  loss: 1.1184 (1.1290)  time: 1.1392  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [210/431]  eta: 0:04:11  lr: 0.000184  loss: 1.1348 (1.1314)  time: 1.1315  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:174]  [220/431]  eta: 0:03:59  lr: 0.000184  loss: 1.1434 (1.1313)  time: 1.1201  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:174]  [230/431]  eta: 0:03:48  lr: 0.000184  loss: 1.1370 (1.1326)  time: 1.1272  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [240/431]  eta: 0:03:36  lr: 0.000184  loss: 1.1370 (1.1326)  time: 1.1381  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [250/431]  eta: 0:03:25  lr: 0.000184  loss: 1.0740 (1.1300)  time: 1.1389  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [260/431]  eta: 0:03:14  lr: 0.000184  loss: 1.0892 (1.1302)  time: 1.1385  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [270/431]  eta: 0:03:02  lr: 0.000184  loss: 1.0736 (1.1295)  time: 1.1334  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [280/431]  eta: 0:02:51  lr: 0.000184  loss: 1.1159 (1.1309)  time: 1.1295  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [290/431]  eta: 0:02:39  lr: 0.000184  loss: 1.1400 (1.1315)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [300/431]  eta: 0:02:28  lr: 0.000184  loss: 1.1771 (1.1326)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [310/431]  eta: 0:02:17  lr: 0.000184  loss: 1.0814 (1.1307)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [320/431]  eta: 0:02:05  lr: 0.000184  loss: 1.0625 (1.1289)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [330/431]  eta: 0:01:54  lr: 0.000184  loss: 1.1019 (1.1297)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [340/431]  eta: 0:01:43  lr: 0.000184  loss: 1.1213 (1.1291)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [350/431]  eta: 0:01:31  lr: 0.000184  loss: 1.1377 (1.1300)  time: 1.1233  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [360/431]  eta: 0:01:20  lr: 0.000184  loss: 1.1390 (1.1301)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [370/431]  eta: 0:01:09  lr: 0.000184  loss: 1.0853 (1.1285)  time: 1.1300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [380/431]  eta: 0:00:57  lr: 0.000184  loss: 1.0853 (1.1287)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [390/431]  eta: 0:00:46  lr: 0.000184  loss: 1.1086 (1.1287)  time: 1.1202  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [400/431]  eta: 0:00:35  lr: 0.000184  loss: 1.1086 (1.1281)  time: 1.1196  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:174]  [410/431]  eta: 0:00:23  lr: 0.000184  loss: 1.0980 (1.1275)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [420/431]  eta: 0:00:12  lr: 0.000184  loss: 1.0963 (1.1272)  time: 1.1285  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174]  [430/431]  eta: 0:00:01  lr: 0.000184  loss: 1.1058 (1.1280)  time: 1.1308  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:174] Total time: 0:08:07 (1.1319 s / it)\n",
      "Averaged stats: lr: 0.000184  loss: 1.1058 (1.1280)\n",
      "Valid: [epoch:174]  [ 0/14]  eta: 0:00:35  loss: 1.0405 (1.0405)  time: 2.5588  data: 2.3803  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:174]  [13/14]  eta: 0:00:00  loss: 1.0705 (1.0780)  time: 0.2960  data: 0.1702  max mem: 15925\n",
      "Valid: [epoch:174] Total time: 0:00:04 (0.3121 s / it)\n",
      "Averaged stats: loss: 1.0705 (1.0780)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_174_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.078%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:175]  [  0/431]  eta: 0:31:46  lr: 0.000184  loss: 1.1399 (1.1399)  time: 4.4225  data: 2.9896  max mem: 15925\n",
      "Train: [epoch:175]  [ 10/431]  eta: 0:09:30  lr: 0.000184  loss: 1.0952 (1.0838)  time: 1.3557  data: 0.2721  max mem: 15925\n",
      "Train: [epoch:175]  [ 20/431]  eta: 0:08:24  lr: 0.000184  loss: 1.0828 (1.0961)  time: 1.0681  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [ 30/431]  eta: 0:07:54  lr: 0.000184  loss: 1.1049 (1.1066)  time: 1.0877  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [ 40/431]  eta: 0:07:35  lr: 0.000184  loss: 1.1080 (1.1204)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [ 50/431]  eta: 0:07:19  lr: 0.000184  loss: 1.1080 (1.1320)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [ 60/431]  eta: 0:07:05  lr: 0.000184  loss: 1.0658 (1.1226)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [ 70/431]  eta: 0:06:52  lr: 0.000184  loss: 1.0782 (1.1240)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [ 80/431]  eta: 0:06:40  lr: 0.000184  loss: 1.1138 (1.1259)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [ 90/431]  eta: 0:06:28  lr: 0.000184  loss: 1.1284 (1.1288)  time: 1.1286  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [100/431]  eta: 0:06:17  lr: 0.000184  loss: 1.1670 (1.1363)  time: 1.1349  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [110/431]  eta: 0:06:06  lr: 0.000184  loss: 1.1349 (1.1357)  time: 1.1406  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:175]  [120/431]  eta: 0:05:54  lr: 0.000184  loss: 1.0924 (1.1320)  time: 1.1320  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:175]  [130/431]  eta: 0:05:43  lr: 0.000184  loss: 1.0530 (1.1284)  time: 1.1449  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [140/431]  eta: 0:05:32  lr: 0.000184  loss: 1.0700 (1.1275)  time: 1.1540  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [150/431]  eta: 0:05:20  lr: 0.000184  loss: 1.0700 (1.1251)  time: 1.1326  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [160/431]  eta: 0:05:08  lr: 0.000184  loss: 1.0673 (1.1235)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [170/431]  eta: 0:04:57  lr: 0.000184  loss: 1.0776 (1.1233)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [180/431]  eta: 0:04:46  lr: 0.000184  loss: 1.0776 (1.1223)  time: 1.1550  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [190/431]  eta: 0:04:34  lr: 0.000184  loss: 1.0876 (1.1252)  time: 1.1473  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [200/431]  eta: 0:04:23  lr: 0.000184  loss: 1.0852 (1.1262)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [210/431]  eta: 0:04:11  lr: 0.000184  loss: 1.0747 (1.1253)  time: 1.1318  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [220/431]  eta: 0:04:00  lr: 0.000184  loss: 1.0430 (1.1243)  time: 1.1486  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [230/431]  eta: 0:03:48  lr: 0.000184  loss: 1.0627 (1.1251)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [240/431]  eta: 0:03:37  lr: 0.000184  loss: 1.1159 (1.1277)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [250/431]  eta: 0:03:25  lr: 0.000184  loss: 1.1613 (1.1297)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [260/431]  eta: 0:03:14  lr: 0.000184  loss: 1.0867 (1.1266)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [270/431]  eta: 0:03:02  lr: 0.000184  loss: 1.0630 (1.1266)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [280/431]  eta: 0:02:51  lr: 0.000184  loss: 1.0891 (1.1267)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [290/431]  eta: 0:02:39  lr: 0.000184  loss: 1.0594 (1.1272)  time: 1.1315  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [300/431]  eta: 0:02:28  lr: 0.000184  loss: 1.0915 (1.1262)  time: 1.1294  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [310/431]  eta: 0:02:17  lr: 0.000184  loss: 1.0538 (1.1255)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [320/431]  eta: 0:02:05  lr: 0.000184  loss: 1.0564 (1.1249)  time: 1.1300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [330/431]  eta: 0:01:54  lr: 0.000184  loss: 1.1003 (1.1255)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [340/431]  eta: 0:01:43  lr: 0.000184  loss: 1.1351 (1.1261)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [350/431]  eta: 0:01:31  lr: 0.000184  loss: 1.1464 (1.1271)  time: 1.1276  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [360/431]  eta: 0:01:20  lr: 0.000184  loss: 1.1464 (1.1282)  time: 1.1361  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [370/431]  eta: 0:01:09  lr: 0.000184  loss: 1.1493 (1.1285)  time: 1.1528  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [380/431]  eta: 0:00:57  lr: 0.000184  loss: 1.1450 (1.1285)  time: 1.1499  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [390/431]  eta: 0:00:46  lr: 0.000184  loss: 1.0886 (1.1287)  time: 1.1364  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [400/431]  eta: 0:00:35  lr: 0.000184  loss: 1.1074 (1.1284)  time: 1.1381  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:175]  [410/431]  eta: 0:00:23  lr: 0.000184  loss: 1.0628 (1.1263)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [420/431]  eta: 0:00:12  lr: 0.000184  loss: 1.0628 (1.1267)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:175]  [430/431]  eta: 0:00:01  lr: 0.000184  loss: 1.0998 (1.1269)  time: 1.1241  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:175] Total time: 0:08:08 (1.1340 s / it)\n",
      "Averaged stats: lr: 0.000184  loss: 1.0998 (1.1269)\n",
      "Valid: [epoch:175]  [ 0/14]  eta: 0:00:34  loss: 1.0475 (1.0475)  time: 2.4734  data: 2.2937  max mem: 15925\n",
      "Valid: [epoch:175]  [13/14]  eta: 0:00:00  loss: 1.0573 (1.0722)  time: 0.2773  data: 0.1639  max mem: 15925\n",
      "Valid: [epoch:175] Total time: 0:00:04 (0.2932 s / it)\n",
      "Averaged stats: loss: 1.0573 (1.0722)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_175_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.072%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:176]  [  0/431]  eta: 0:29:36  lr: 0.000183  loss: 1.2797 (1.2797)  time: 4.1209  data: 2.9537  max mem: 15925\n",
      "Train: [epoch:176]  [ 10/431]  eta: 0:09:16  lr: 0.000183  loss: 1.1523 (1.2008)  time: 1.3221  data: 0.2687  max mem: 15925\n",
      "Train: [epoch:176]  [ 20/431]  eta: 0:08:17  lr: 0.000183  loss: 1.1079 (1.1501)  time: 1.0649  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [ 30/431]  eta: 0:07:52  lr: 0.000183  loss: 1.1163 (1.1486)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [ 40/431]  eta: 0:07:33  lr: 0.000183  loss: 1.1354 (1.1349)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [ 50/431]  eta: 0:07:19  lr: 0.000183  loss: 1.0849 (1.1307)  time: 1.1150  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [ 60/431]  eta: 0:07:04  lr: 0.000183  loss: 1.0605 (1.1176)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [ 70/431]  eta: 0:06:52  lr: 0.000183  loss: 1.1052 (1.1307)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [ 80/431]  eta: 0:06:41  lr: 0.000183  loss: 1.1674 (1.1371)  time: 1.1356  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [ 90/431]  eta: 0:06:29  lr: 0.000183  loss: 1.1478 (1.1327)  time: 1.1450  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [100/431]  eta: 0:06:18  lr: 0.000183  loss: 1.0737 (1.1308)  time: 1.1420  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [110/431]  eta: 0:06:06  lr: 0.000183  loss: 1.1012 (1.1310)  time: 1.1278  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [120/431]  eta: 0:05:54  lr: 0.000183  loss: 1.1012 (1.1269)  time: 1.1283  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [130/431]  eta: 0:05:43  lr: 0.000183  loss: 1.0568 (1.1239)  time: 1.1400  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:176]  [140/431]  eta: 0:05:31  lr: 0.000183  loss: 1.0641 (1.1235)  time: 1.1410  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [150/431]  eta: 0:05:20  lr: 0.000183  loss: 1.0778 (1.1246)  time: 1.1416  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [160/431]  eta: 0:05:08  lr: 0.000183  loss: 1.1416 (1.1287)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [170/431]  eta: 0:04:56  lr: 0.000183  loss: 1.1242 (1.1284)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [180/431]  eta: 0:04:45  lr: 0.000183  loss: 1.0939 (1.1290)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [190/431]  eta: 0:04:33  lr: 0.000183  loss: 1.0511 (1.1250)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [200/431]  eta: 0:04:22  lr: 0.000183  loss: 1.0607 (1.1233)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [210/431]  eta: 0:04:10  lr: 0.000183  loss: 1.0721 (1.1213)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [220/431]  eta: 0:03:58  lr: 0.000183  loss: 1.0670 (1.1197)  time: 1.1092  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:176]  [230/431]  eta: 0:03:47  lr: 0.000183  loss: 1.1128 (1.1204)  time: 1.1151  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:176]  [240/431]  eta: 0:03:35  lr: 0.000183  loss: 1.1704 (1.1224)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [250/431]  eta: 0:03:24  lr: 0.000183  loss: 1.1953 (1.1263)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [260/431]  eta: 0:03:13  lr: 0.000183  loss: 1.1015 (1.1260)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [270/431]  eta: 0:03:01  lr: 0.000183  loss: 1.1015 (1.1268)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [280/431]  eta: 0:02:50  lr: 0.000183  loss: 1.1258 (1.1264)  time: 1.1217  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [290/431]  eta: 0:02:39  lr: 0.000183  loss: 1.1258 (1.1260)  time: 1.1359  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [300/431]  eta: 0:02:27  lr: 0.000183  loss: 1.0733 (1.1262)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [310/431]  eta: 0:02:16  lr: 0.000183  loss: 1.0354 (1.1256)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [320/431]  eta: 0:02:05  lr: 0.000183  loss: 1.0655 (1.1244)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [330/431]  eta: 0:01:53  lr: 0.000183  loss: 1.1266 (1.1277)  time: 1.1299  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [340/431]  eta: 0:01:42  lr: 0.000183  loss: 1.1136 (1.1270)  time: 1.1291  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [350/431]  eta: 0:01:31  lr: 0.000183  loss: 1.1127 (1.1280)  time: 1.1330  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [360/431]  eta: 0:01:20  lr: 0.000183  loss: 1.1127 (1.1272)  time: 1.1457  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:176]  [370/431]  eta: 0:01:08  lr: 0.000183  loss: 1.0899 (1.1273)  time: 1.1424  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [380/431]  eta: 0:00:57  lr: 0.000183  loss: 1.1132 (1.1279)  time: 1.1244  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [390/431]  eta: 0:00:46  lr: 0.000183  loss: 1.1194 (1.1279)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [400/431]  eta: 0:00:34  lr: 0.000183  loss: 1.1261 (1.1269)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [410/431]  eta: 0:00:23  lr: 0.000183  loss: 1.1348 (1.1284)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [420/431]  eta: 0:00:12  lr: 0.000183  loss: 1.1410 (1.1291)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:176]  [430/431]  eta: 0:00:01  lr: 0.000183  loss: 1.1365 (1.1290)  time: 1.1282  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:176] Total time: 0:08:06 (1.1285 s / it)\n",
      "Averaged stats: lr: 0.000183  loss: 1.1365 (1.1290)\n",
      "Valid: [epoch:176]  [ 0/14]  eta: 0:00:37  loss: 1.1638 (1.1638)  time: 2.6554  data: 2.4945  max mem: 15925\n",
      "Valid: [epoch:176]  [13/14]  eta: 0:00:00  loss: 1.0562 (1.0669)  time: 0.2854  data: 0.1783  max mem: 15925\n",
      "Valid: [epoch:176] Total time: 0:00:04 (0.3040 s / it)\n",
      "Averaged stats: loss: 1.0562 (1.0669)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_176_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.067%\n",
      "Min loss: 1.067\n",
      "Best Epoch: 154.000\n",
      "Train: [epoch:177]  [  0/431]  eta: 0:33:47  lr: 0.000183  loss: 1.0855 (1.0855)  time: 4.7046  data: 3.3205  max mem: 15925\n",
      "Train: [epoch:177]  [ 10/431]  eta: 0:09:38  lr: 0.000183  loss: 1.1675 (1.2092)  time: 1.3750  data: 0.3021  max mem: 15925\n",
      "Train: [epoch:177]  [ 20/431]  eta: 0:08:32  lr: 0.000183  loss: 1.1453 (1.1857)  time: 1.0745  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:177]  [ 30/431]  eta: 0:07:58  lr: 0.000183  loss: 1.1091 (1.1646)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [ 40/431]  eta: 0:07:40  lr: 0.000183  loss: 1.0827 (1.1489)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [ 50/431]  eta: 0:07:23  lr: 0.000183  loss: 1.0791 (1.1388)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [ 60/431]  eta: 0:07:10  lr: 0.000183  loss: 1.0810 (1.1258)  time: 1.1253  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:177]  [ 70/431]  eta: 0:06:56  lr: 0.000183  loss: 1.0926 (1.1277)  time: 1.1218  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:177]  [ 80/431]  eta: 0:06:44  lr: 0.000183  loss: 1.0932 (1.1289)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [ 90/431]  eta: 0:06:31  lr: 0.000183  loss: 1.0755 (1.1239)  time: 1.1354  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [100/431]  eta: 0:06:19  lr: 0.000183  loss: 1.0991 (1.1208)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [110/431]  eta: 0:06:07  lr: 0.000183  loss: 1.0884 (1.1182)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [120/431]  eta: 0:05:55  lr: 0.000183  loss: 1.0876 (1.1190)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [130/431]  eta: 0:05:43  lr: 0.000183  loss: 1.1034 (1.1177)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [140/431]  eta: 0:05:31  lr: 0.000183  loss: 1.0890 (1.1194)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [150/431]  eta: 0:05:19  lr: 0.000183  loss: 1.1102 (1.1222)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [160/431]  eta: 0:05:07  lr: 0.000183  loss: 1.1096 (1.1212)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [170/431]  eta: 0:04:56  lr: 0.000183  loss: 1.1096 (1.1267)  time: 1.1291  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [180/431]  eta: 0:04:44  lr: 0.000183  loss: 1.1492 (1.1289)  time: 1.1315  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [190/431]  eta: 0:04:33  lr: 0.000183  loss: 1.1060 (1.1296)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [200/431]  eta: 0:04:21  lr: 0.000183  loss: 1.1057 (1.1293)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [210/431]  eta: 0:04:10  lr: 0.000183  loss: 1.1121 (1.1301)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [220/431]  eta: 0:03:58  lr: 0.000183  loss: 1.0960 (1.1298)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [230/431]  eta: 0:03:47  lr: 0.000183  loss: 1.0600 (1.1279)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [240/431]  eta: 0:03:35  lr: 0.000183  loss: 1.0719 (1.1274)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [250/431]  eta: 0:03:24  lr: 0.000183  loss: 1.1099 (1.1265)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [260/431]  eta: 0:03:13  lr: 0.000183  loss: 1.1099 (1.1263)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [270/431]  eta: 0:03:01  lr: 0.000183  loss: 1.0547 (1.1238)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [280/431]  eta: 0:02:50  lr: 0.000183  loss: 1.0042 (1.1214)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [290/431]  eta: 0:02:39  lr: 0.000183  loss: 1.0612 (1.1219)  time: 1.1253  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:177]  [300/431]  eta: 0:02:27  lr: 0.000183  loss: 1.1073 (1.1225)  time: 1.1270  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:177]  [310/431]  eta: 0:02:16  lr: 0.000183  loss: 1.1330 (1.1225)  time: 1.1283  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:177]  [320/431]  eta: 0:02:05  lr: 0.000183  loss: 1.1357 (1.1243)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [330/431]  eta: 0:01:53  lr: 0.000183  loss: 1.1459 (1.1261)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [340/431]  eta: 0:01:42  lr: 0.000183  loss: 1.1561 (1.1280)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [350/431]  eta: 0:01:31  lr: 0.000183  loss: 1.1379 (1.1283)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [360/431]  eta: 0:01:20  lr: 0.000183  loss: 1.0839 (1.1282)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [370/431]  eta: 0:01:08  lr: 0.000183  loss: 1.0839 (1.1281)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [380/431]  eta: 0:00:57  lr: 0.000183  loss: 1.1434 (1.1295)  time: 1.1218  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:177]  [390/431]  eta: 0:00:46  lr: 0.000183  loss: 1.0795 (1.1268)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [400/431]  eta: 0:00:34  lr: 0.000183  loss: 1.0734 (1.1266)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [410/431]  eta: 0:00:23  lr: 0.000183  loss: 1.0841 (1.1258)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:177]  [420/431]  eta: 0:00:12  lr: 0.000183  loss: 1.0945 (1.1267)  time: 1.1249  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:177]  [430/431]  eta: 0:00:01  lr: 0.000183  loss: 1.1308 (1.1267)  time: 1.1265  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:177] Total time: 0:08:05 (1.1267 s / it)\n",
      "Averaged stats: lr: 0.000183  loss: 1.1308 (1.1267)\n",
      "Valid: [epoch:177]  [ 0/14]  eta: 0:00:37  loss: 1.0529 (1.0529)  time: 2.6777  data: 2.5113  max mem: 15925\n",
      "Valid: [epoch:177]  [13/14]  eta: 0:00:00  loss: 1.0529 (1.0661)  time: 0.2861  data: 0.1795  max mem: 15925\n",
      "Valid: [epoch:177] Total time: 0:00:04 (0.3045 s / it)\n",
      "Averaged stats: loss: 1.0529 (1.0661)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_177_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.066%\n",
      "Min loss: 1.066\n",
      "Best Epoch: 177.000\n",
      "Train: [epoch:178]  [  0/431]  eta: 0:33:00  lr: 0.000183  loss: 1.1804 (1.1804)  time: 4.5942  data: 3.1147  max mem: 15925\n",
      "Train: [epoch:178]  [ 10/431]  eta: 0:09:45  lr: 0.000183  loss: 1.2072 (1.2569)  time: 1.3918  data: 0.2834  max mem: 15925\n",
      "Train: [epoch:178]  [ 20/431]  eta: 0:08:34  lr: 0.000183  loss: 1.1702 (1.2096)  time: 1.0835  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:178]  [ 30/431]  eta: 0:08:02  lr: 0.000183  loss: 1.0872 (1.1638)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [ 40/431]  eta: 0:07:40  lr: 0.000183  loss: 1.0862 (1.1428)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [ 50/431]  eta: 0:07:24  lr: 0.000183  loss: 1.0863 (1.1316)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [ 60/431]  eta: 0:07:10  lr: 0.000183  loss: 1.0440 (1.1248)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [ 70/431]  eta: 0:06:56  lr: 0.000183  loss: 1.0456 (1.1128)  time: 1.1211  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:178]  [ 80/431]  eta: 0:06:42  lr: 0.000183  loss: 1.0456 (1.1142)  time: 1.1139  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:178]  [ 90/431]  eta: 0:06:30  lr: 0.000183  loss: 1.0998 (1.1191)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [100/431]  eta: 0:06:17  lr: 0.000183  loss: 1.0825 (1.1155)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [110/431]  eta: 0:06:05  lr: 0.000183  loss: 1.0618 (1.1138)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [120/431]  eta: 0:05:53  lr: 0.000183  loss: 1.0618 (1.1095)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [130/431]  eta: 0:05:42  lr: 0.000183  loss: 1.1149 (1.1160)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [140/431]  eta: 0:05:30  lr: 0.000183  loss: 1.1768 (1.1196)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [150/431]  eta: 0:05:19  lr: 0.000183  loss: 1.1414 (1.1217)  time: 1.1277  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [160/431]  eta: 0:05:07  lr: 0.000183  loss: 1.0696 (1.1205)  time: 1.1234  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [170/431]  eta: 0:04:55  lr: 0.000183  loss: 1.0564 (1.1191)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [180/431]  eta: 0:04:44  lr: 0.000183  loss: 1.0690 (1.1199)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [190/431]  eta: 0:04:32  lr: 0.000183  loss: 1.1227 (1.1215)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [200/431]  eta: 0:04:21  lr: 0.000183  loss: 1.0959 (1.1201)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [210/431]  eta: 0:04:09  lr: 0.000183  loss: 1.1088 (1.1222)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [220/431]  eta: 0:03:58  lr: 0.000183  loss: 1.1519 (1.1224)  time: 1.1294  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [230/431]  eta: 0:03:47  lr: 0.000183  loss: 1.0708 (1.1204)  time: 1.1339  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [240/431]  eta: 0:03:35  lr: 0.000183  loss: 1.1028 (1.1221)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [250/431]  eta: 0:03:24  lr: 0.000183  loss: 1.1393 (1.1246)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [260/431]  eta: 0:03:13  lr: 0.000183  loss: 1.1393 (1.1262)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [270/431]  eta: 0:03:01  lr: 0.000183  loss: 1.1058 (1.1256)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [280/431]  eta: 0:02:50  lr: 0.000183  loss: 1.0684 (1.1231)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [290/431]  eta: 0:02:39  lr: 0.000183  loss: 1.0719 (1.1248)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [300/431]  eta: 0:02:27  lr: 0.000183  loss: 1.1931 (1.1276)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [310/431]  eta: 0:02:16  lr: 0.000183  loss: 1.0763 (1.1254)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [320/431]  eta: 0:02:05  lr: 0.000183  loss: 1.0826 (1.1273)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [330/431]  eta: 0:01:53  lr: 0.000183  loss: 1.1356 (1.1291)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [340/431]  eta: 0:01:42  lr: 0.000183  loss: 1.1459 (1.1305)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [350/431]  eta: 0:01:31  lr: 0.000183  loss: 1.1259 (1.1304)  time: 1.1234  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:178]  [360/431]  eta: 0:01:19  lr: 0.000183  loss: 1.0831 (1.1306)  time: 1.1255  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:178]  [370/431]  eta: 0:01:08  lr: 0.000183  loss: 1.0759 (1.1295)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [380/431]  eta: 0:00:57  lr: 0.000183  loss: 1.0735 (1.1288)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [390/431]  eta: 0:00:46  lr: 0.000183  loss: 1.1005 (1.1297)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [400/431]  eta: 0:00:34  lr: 0.000183  loss: 1.1128 (1.1299)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [410/431]  eta: 0:00:23  lr: 0.000183  loss: 1.0965 (1.1289)  time: 1.1269  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178]  [420/431]  eta: 0:00:12  lr: 0.000183  loss: 1.0722 (1.1282)  time: 1.1274  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:178]  [430/431]  eta: 0:00:01  lr: 0.000183  loss: 1.0606 (1.1270)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:178] Total time: 0:08:05 (1.1264 s / it)\n",
      "Averaged stats: lr: 0.000183  loss: 1.0606 (1.1270)\n",
      "Valid: [epoch:178]  [ 0/14]  eta: 0:00:32  loss: 1.0524 (1.0524)  time: 2.3492  data: 2.1664  max mem: 15925\n",
      "Valid: [epoch:178]  [13/14]  eta: 0:00:00  loss: 1.0524 (1.0643)  time: 0.2728  data: 0.1549  max mem: 15925\n",
      "Valid: [epoch:178] Total time: 0:00:04 (0.2902 s / it)\n",
      "Averaged stats: loss: 1.0524 (1.0643)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_178_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 1.064%\n",
      "Min loss: 1.064\n",
      "Best Epoch: 178.000\n",
      "Train: [epoch:179]  [  0/431]  eta: 0:34:02  lr: 0.000183  loss: 1.1481 (1.1481)  time: 4.7384  data: 3.5001  max mem: 15925\n",
      "Train: [epoch:179]  [ 10/431]  eta: 0:09:51  lr: 0.000183  loss: 1.1481 (1.1621)  time: 1.4056  data: 0.3184  max mem: 15925\n",
      "Train: [epoch:179]  [ 20/431]  eta: 0:08:34  lr: 0.000183  loss: 1.1201 (1.1236)  time: 1.0773  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [ 30/431]  eta: 0:08:02  lr: 0.000183  loss: 1.0227 (1.0970)  time: 1.0912  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [ 40/431]  eta: 0:07:41  lr: 0.000183  loss: 1.0461 (1.0933)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [ 50/431]  eta: 0:07:23  lr: 0.000183  loss: 1.0869 (1.1000)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [ 60/431]  eta: 0:07:09  lr: 0.000183  loss: 1.1403 (1.1084)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [ 70/431]  eta: 0:06:55  lr: 0.000183  loss: 1.1236 (1.1119)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [ 80/431]  eta: 0:06:42  lr: 0.000183  loss: 1.1236 (1.1165)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [ 90/431]  eta: 0:06:31  lr: 0.000183  loss: 1.1548 (1.1205)  time: 1.1345  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [100/431]  eta: 0:06:18  lr: 0.000183  loss: 1.0681 (1.1137)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [110/431]  eta: 0:06:06  lr: 0.000183  loss: 1.0546 (1.1125)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [120/431]  eta: 0:05:55  lr: 0.000183  loss: 1.0875 (1.1156)  time: 1.1334  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [130/431]  eta: 0:05:43  lr: 0.000183  loss: 1.1157 (1.1186)  time: 1.1370  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [140/431]  eta: 0:05:32  lr: 0.000183  loss: 1.0931 (1.1157)  time: 1.1400  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [150/431]  eta: 0:05:20  lr: 0.000183  loss: 1.0624 (1.1164)  time: 1.1321  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [160/431]  eta: 0:05:08  lr: 0.000183  loss: 1.0976 (1.1167)  time: 1.1256  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [170/431]  eta: 0:04:57  lr: 0.000183  loss: 1.0843 (1.1140)  time: 1.1437  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [180/431]  eta: 0:04:46  lr: 0.000183  loss: 1.0843 (1.1138)  time: 1.1530  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [190/431]  eta: 0:04:34  lr: 0.000183  loss: 1.0798 (1.1137)  time: 1.1480  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [200/431]  eta: 0:04:23  lr: 0.000183  loss: 1.0798 (1.1151)  time: 1.1526  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [210/431]  eta: 0:04:12  lr: 0.000183  loss: 1.1434 (1.1198)  time: 1.1533  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [220/431]  eta: 0:04:00  lr: 0.000183  loss: 1.1434 (1.1215)  time: 1.1428  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [230/431]  eta: 0:03:49  lr: 0.000183  loss: 1.0984 (1.1216)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [240/431]  eta: 0:03:37  lr: 0.000183  loss: 1.1001 (1.1231)  time: 1.1162  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [250/431]  eta: 0:03:26  lr: 0.000183  loss: 1.1502 (1.1244)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [260/431]  eta: 0:03:14  lr: 0.000183  loss: 1.1476 (1.1233)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [270/431]  eta: 0:03:03  lr: 0.000183  loss: 1.0639 (1.1215)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [280/431]  eta: 0:02:51  lr: 0.000183  loss: 1.0301 (1.1190)  time: 1.1407  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [290/431]  eta: 0:02:40  lr: 0.000183  loss: 1.0133 (1.1185)  time: 1.1295  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [300/431]  eta: 0:02:28  lr: 0.000183  loss: 1.0633 (1.1192)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [310/431]  eta: 0:02:17  lr: 0.000183  loss: 1.1238 (1.1200)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [320/431]  eta: 0:02:06  lr: 0.000183  loss: 1.1106 (1.1202)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [330/431]  eta: 0:01:54  lr: 0.000183  loss: 1.1961 (1.1249)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [340/431]  eta: 0:01:43  lr: 0.000183  loss: 1.1961 (1.1255)  time: 1.1367  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [350/431]  eta: 0:01:31  lr: 0.000183  loss: 1.1027 (1.1262)  time: 1.1354  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [360/431]  eta: 0:01:20  lr: 0.000183  loss: 1.1027 (1.1266)  time: 1.1458  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [370/431]  eta: 0:01:09  lr: 0.000183  loss: 1.1701 (1.1294)  time: 1.1518  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [380/431]  eta: 0:00:57  lr: 0.000183  loss: 1.0921 (1.1276)  time: 1.1450  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [390/431]  eta: 0:00:46  lr: 0.000183  loss: 1.0715 (1.1265)  time: 1.1441  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [400/431]  eta: 0:00:35  lr: 0.000183  loss: 1.0670 (1.1258)  time: 1.1389  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:179]  [410/431]  eta: 0:00:23  lr: 0.000183  loss: 1.0739 (1.1250)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:179]  [420/431]  eta: 0:00:12  lr: 0.000183  loss: 1.1294 (1.1260)  time: 1.1171  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:179]  [430/431]  eta: 0:00:01  lr: 0.000183  loss: 1.1335 (1.1258)  time: 1.1177  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:179] Total time: 0:08:09 (1.1360 s / it)\n",
      "Averaged stats: lr: 0.000183  loss: 1.1335 (1.1258)\n",
      "Valid: [epoch:179]  [ 0/14]  eta: 0:00:29  loss: 1.1250 (1.1250)  time: 2.1054  data: 1.9725  max mem: 15925\n",
      "Valid: [epoch:179]  [13/14]  eta: 0:00:00  loss: 1.0547 (1.0665)  time: 0.2418  data: 0.1410  max mem: 15925\n",
      "Valid: [epoch:179] Total time: 0:00:03 (0.2565 s / it)\n",
      "Averaged stats: loss: 1.0547 (1.0665)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_179_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.067%\n",
      "Min loss: 1.064\n",
      "Best Epoch: 178.000\n",
      "Train: [epoch:180]  [  0/431]  eta: 0:28:40  lr: 0.000182  loss: 1.1462 (1.1462)  time: 3.9921  data: 2.8331  max mem: 15925\n",
      "Train: [epoch:180]  [ 10/431]  eta: 0:09:08  lr: 0.000182  loss: 1.1892 (1.2030)  time: 1.3017  data: 0.2578  max mem: 15925\n",
      "Train: [epoch:180]  [ 20/431]  eta: 0:08:09  lr: 0.000182  loss: 1.1621 (1.1539)  time: 1.0507  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [ 30/431]  eta: 0:07:45  lr: 0.000182  loss: 1.0712 (1.1201)  time: 1.0833  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [ 40/431]  eta: 0:07:27  lr: 0.000182  loss: 1.0712 (1.1159)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [ 50/431]  eta: 0:07:12  lr: 0.000182  loss: 1.1118 (1.1224)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [ 60/431]  eta: 0:07:00  lr: 0.000182  loss: 1.1098 (1.1174)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [ 70/431]  eta: 0:06:49  lr: 0.000182  loss: 1.0576 (1.1126)  time: 1.1300  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [ 80/431]  eta: 0:06:37  lr: 0.000182  loss: 1.0602 (1.1191)  time: 1.1258  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [ 90/431]  eta: 0:06:25  lr: 0.000182  loss: 1.1016 (1.1223)  time: 1.1191  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [100/431]  eta: 0:06:13  lr: 0.000182  loss: 1.1101 (1.1219)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [110/431]  eta: 0:06:02  lr: 0.000182  loss: 1.1101 (1.1221)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [120/431]  eta: 0:05:51  lr: 0.000182  loss: 1.1103 (1.1192)  time: 1.1379  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [130/431]  eta: 0:05:40  lr: 0.000182  loss: 1.0491 (1.1138)  time: 1.1393  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [140/431]  eta: 0:05:28  lr: 0.000182  loss: 1.0393 (1.1126)  time: 1.1273  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [150/431]  eta: 0:05:17  lr: 0.000182  loss: 1.0593 (1.1108)  time: 1.1223  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:180]  [160/431]  eta: 0:05:05  lr: 0.000182  loss: 1.0941 (1.1103)  time: 1.1259  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [170/431]  eta: 0:04:54  lr: 0.000182  loss: 1.1028 (1.1088)  time: 1.1245  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [180/431]  eta: 0:04:43  lr: 0.000182  loss: 1.0941 (1.1089)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [190/431]  eta: 0:04:31  lr: 0.000182  loss: 1.0825 (1.1083)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [200/431]  eta: 0:04:20  lr: 0.000182  loss: 1.0994 (1.1096)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [210/431]  eta: 0:04:08  lr: 0.000182  loss: 1.0994 (1.1105)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [220/431]  eta: 0:03:57  lr: 0.000182  loss: 1.1521 (1.1142)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [230/431]  eta: 0:03:45  lr: 0.000182  loss: 1.1261 (1.1137)  time: 1.1123  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [240/431]  eta: 0:03:34  lr: 0.000182  loss: 1.1043 (1.1151)  time: 1.1247  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [250/431]  eta: 0:03:23  lr: 0.000182  loss: 1.1187 (1.1155)  time: 1.1404  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [260/431]  eta: 0:03:12  lr: 0.000182  loss: 1.1131 (1.1154)  time: 1.1445  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [270/431]  eta: 0:03:01  lr: 0.000182  loss: 1.1131 (1.1165)  time: 1.1381  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [280/431]  eta: 0:02:50  lr: 0.000182  loss: 1.1427 (1.1181)  time: 1.1424  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [290/431]  eta: 0:02:39  lr: 0.000182  loss: 1.1297 (1.1185)  time: 1.1455  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [300/431]  eta: 0:02:27  lr: 0.000182  loss: 1.1121 (1.1188)  time: 1.1437  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [310/431]  eta: 0:02:16  lr: 0.000182  loss: 1.1129 (1.1197)  time: 1.1413  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [320/431]  eta: 0:02:05  lr: 0.000182  loss: 1.1129 (1.1186)  time: 1.1344  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [330/431]  eta: 0:01:54  lr: 0.000182  loss: 1.1446 (1.1211)  time: 1.1340  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [340/431]  eta: 0:01:42  lr: 0.000182  loss: 1.1930 (1.1222)  time: 1.1484  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [350/431]  eta: 0:01:31  lr: 0.000182  loss: 1.1575 (1.1234)  time: 1.1430  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [360/431]  eta: 0:01:20  lr: 0.000182  loss: 1.1268 (1.1241)  time: 1.1276  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [370/431]  eta: 0:01:08  lr: 0.000182  loss: 1.1090 (1.1239)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [380/431]  eta: 0:00:57  lr: 0.000182  loss: 1.1204 (1.1242)  time: 1.1243  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [390/431]  eta: 0:00:46  lr: 0.000182  loss: 1.1150 (1.1239)  time: 1.1149  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [400/431]  eta: 0:00:34  lr: 0.000182  loss: 1.1445 (1.1261)  time: 1.1073  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:180]  [410/431]  eta: 0:00:23  lr: 0.000182  loss: 1.1445 (1.1261)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:180]  [420/431]  eta: 0:00:12  lr: 0.000182  loss: 1.1102 (1.1259)  time: 1.1103  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:180]  [430/431]  eta: 0:00:01  lr: 0.000182  loss: 1.0935 (1.1249)  time: 1.1223  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:180] Total time: 0:08:06 (1.1280 s / it)\n",
      "Averaged stats: lr: 0.000182  loss: 1.0935 (1.1249)\n",
      "Valid: [epoch:180]  [ 0/14]  eta: 0:00:34  loss: 1.0212 (1.0212)  time: 2.4736  data: 2.3324  max mem: 15925\n",
      "Valid: [epoch:180]  [13/14]  eta: 0:00:00  loss: 1.0512 (1.0626)  time: 0.2822  data: 0.1667  max mem: 15925\n",
      "Valid: [epoch:180] Total time: 0:00:04 (0.2965 s / it)\n",
      "Averaged stats: loss: 1.0512 (1.0626)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_180_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.063%\n",
      "Min loss: 1.063\n",
      "Best Epoch: 180.000\n",
      "Train: [epoch:181]  [  0/431]  eta: 0:33:02  lr: 0.000182  loss: 1.2130 (1.2130)  time: 4.5991  data: 3.4045  max mem: 15925\n",
      "Train: [epoch:181]  [ 10/431]  eta: 0:10:00  lr: 0.000182  loss: 1.1812 (1.1852)  time: 1.4263  data: 0.3098  max mem: 15925\n",
      "Train: [epoch:181]  [ 20/431]  eta: 0:08:38  lr: 0.000182  loss: 1.1672 (1.1713)  time: 1.0943  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [ 30/431]  eta: 0:08:06  lr: 0.000182  loss: 1.1267 (1.1510)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [ 40/431]  eta: 0:07:43  lr: 0.000182  loss: 1.0765 (1.1270)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [ 50/431]  eta: 0:07:28  lr: 0.000182  loss: 1.0592 (1.1247)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [ 60/431]  eta: 0:07:14  lr: 0.000182  loss: 1.0605 (1.1155)  time: 1.1430  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [ 70/431]  eta: 0:07:00  lr: 0.000182  loss: 1.0608 (1.1145)  time: 1.1337  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [ 80/431]  eta: 0:06:47  lr: 0.000182  loss: 1.1247 (1.1204)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [ 90/431]  eta: 0:06:35  lr: 0.000182  loss: 1.1247 (1.1163)  time: 1.1395  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [100/431]  eta: 0:06:22  lr: 0.000182  loss: 1.0321 (1.1160)  time: 1.1339  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [110/431]  eta: 0:06:10  lr: 0.000182  loss: 1.0689 (1.1166)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [120/431]  eta: 0:05:57  lr: 0.000182  loss: 1.1252 (1.1177)  time: 1.1270  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [130/431]  eta: 0:05:45  lr: 0.000182  loss: 1.1252 (1.1175)  time: 1.1290  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [140/431]  eta: 0:05:33  lr: 0.000182  loss: 1.0633 (1.1134)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [150/431]  eta: 0:05:21  lr: 0.000182  loss: 1.0633 (1.1143)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [160/431]  eta: 0:05:09  lr: 0.000182  loss: 1.0659 (1.1104)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [170/431]  eta: 0:04:57  lr: 0.000182  loss: 1.0479 (1.1094)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [180/431]  eta: 0:04:46  lr: 0.000182  loss: 1.0950 (1.1140)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [190/431]  eta: 0:04:34  lr: 0.000182  loss: 1.1325 (1.1156)  time: 1.1345  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [200/431]  eta: 0:04:23  lr: 0.000182  loss: 1.0950 (1.1143)  time: 1.1355  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [210/431]  eta: 0:04:11  lr: 0.000182  loss: 1.1167 (1.1164)  time: 1.1308  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [220/431]  eta: 0:04:00  lr: 0.000182  loss: 1.1618 (1.1178)  time: 1.1320  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [230/431]  eta: 0:03:48  lr: 0.000182  loss: 1.1415 (1.1191)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [240/431]  eta: 0:03:37  lr: 0.000182  loss: 1.1833 (1.1230)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [250/431]  eta: 0:03:25  lr: 0.000182  loss: 1.1562 (1.1215)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [260/431]  eta: 0:03:14  lr: 0.000182  loss: 1.1143 (1.1218)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [270/431]  eta: 0:03:02  lr: 0.000182  loss: 1.1143 (1.1217)  time: 1.1274  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [280/431]  eta: 0:02:51  lr: 0.000182  loss: 1.1471 (1.1242)  time: 1.1280  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [290/431]  eta: 0:02:40  lr: 0.000182  loss: 1.1383 (1.1230)  time: 1.1450  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [300/431]  eta: 0:02:28  lr: 0.000182  loss: 1.0928 (1.1243)  time: 1.1405  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [310/431]  eta: 0:02:17  lr: 0.000182  loss: 1.1362 (1.1246)  time: 1.1298  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [320/431]  eta: 0:02:06  lr: 0.000182  loss: 1.0943 (1.1248)  time: 1.1254  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:181]  [330/431]  eta: 0:01:54  lr: 0.000182  loss: 1.1071 (1.1266)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [340/431]  eta: 0:01:43  lr: 0.000182  loss: 1.1363 (1.1279)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [350/431]  eta: 0:01:31  lr: 0.000182  loss: 1.1773 (1.1284)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [360/431]  eta: 0:01:20  lr: 0.000182  loss: 1.0959 (1.1270)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [370/431]  eta: 0:01:09  lr: 0.000182  loss: 1.0583 (1.1255)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [380/431]  eta: 0:00:57  lr: 0.000182  loss: 1.0590 (1.1252)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [390/431]  eta: 0:00:46  lr: 0.000182  loss: 1.1094 (1.1257)  time: 1.1361  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [400/431]  eta: 0:00:35  lr: 0.000182  loss: 1.1286 (1.1259)  time: 1.1239  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:181]  [410/431]  eta: 0:00:23  lr: 0.000182  loss: 1.1115 (1.1251)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [420/431]  eta: 0:00:12  lr: 0.000182  loss: 1.1065 (1.1264)  time: 1.1269  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181]  [430/431]  eta: 0:00:01  lr: 0.000182  loss: 1.0923 (1.1257)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:181] Total time: 0:08:08 (1.1328 s / it)\n",
      "Averaged stats: lr: 0.000182  loss: 1.0923 (1.1257)\n",
      "Valid: [epoch:181]  [ 0/14]  eta: 0:00:32  loss: 1.1063 (1.1063)  time: 2.3117  data: 2.1537  max mem: 15925\n",
      "Valid: [epoch:181]  [13/14]  eta: 0:00:00  loss: 1.0523 (1.0655)  time: 0.2504  data: 0.1539  max mem: 15925\n",
      "Valid: [epoch:181] Total time: 0:00:03 (0.2687 s / it)\n",
      "Averaged stats: loss: 1.0523 (1.0655)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_181_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.065%\n",
      "Min loss: 1.063\n",
      "Best Epoch: 180.000\n",
      "Train: [epoch:182]  [  0/431]  eta: 0:33:35  lr: 0.000182  loss: 1.2520 (1.2520)  time: 4.6754  data: 3.4976  max mem: 15925\n",
      "Train: [epoch:182]  [ 10/431]  eta: 0:09:42  lr: 0.000182  loss: 1.1621 (1.1495)  time: 1.3828  data: 0.3182  max mem: 15925\n",
      "Train: [epoch:182]  [ 20/431]  eta: 0:08:30  lr: 0.000182  loss: 1.1309 (1.1654)  time: 1.0716  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [ 30/431]  eta: 0:08:01  lr: 0.000182  loss: 1.0997 (1.1418)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [ 40/431]  eta: 0:07:41  lr: 0.000182  loss: 1.0486 (1.1378)  time: 1.1132  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [ 50/431]  eta: 0:07:24  lr: 0.000182  loss: 1.0537 (1.1318)  time: 1.1097  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [ 60/431]  eta: 0:07:10  lr: 0.000182  loss: 1.0501 (1.1208)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [ 70/431]  eta: 0:06:57  lr: 0.000182  loss: 1.0538 (1.1166)  time: 1.1346  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [ 80/431]  eta: 0:06:44  lr: 0.000182  loss: 1.0823 (1.1137)  time: 1.1295  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [ 90/431]  eta: 0:06:31  lr: 0.000182  loss: 1.0823 (1.1125)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [100/431]  eta: 0:06:19  lr: 0.000182  loss: 1.0769 (1.1141)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [110/431]  eta: 0:06:06  lr: 0.000182  loss: 1.0659 (1.1132)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [120/431]  eta: 0:05:54  lr: 0.000182  loss: 1.1141 (1.1126)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [130/431]  eta: 0:05:43  lr: 0.000182  loss: 1.0894 (1.1099)  time: 1.1279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [140/431]  eta: 0:05:31  lr: 0.000182  loss: 1.0894 (1.1099)  time: 1.1308  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [150/431]  eta: 0:05:20  lr: 0.000182  loss: 1.0969 (1.1120)  time: 1.1306  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [160/431]  eta: 0:05:08  lr: 0.000182  loss: 1.0822 (1.1110)  time: 1.1312  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [170/431]  eta: 0:04:56  lr: 0.000182  loss: 1.0822 (1.1104)  time: 1.1231  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [180/431]  eta: 0:04:45  lr: 0.000182  loss: 1.1281 (1.1147)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [190/431]  eta: 0:04:33  lr: 0.000182  loss: 1.1338 (1.1164)  time: 1.1217  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [200/431]  eta: 0:04:22  lr: 0.000182  loss: 1.1122 (1.1178)  time: 1.1297  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [210/431]  eta: 0:04:10  lr: 0.000182  loss: 1.1122 (1.1194)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [220/431]  eta: 0:03:58  lr: 0.000182  loss: 1.1024 (1.1220)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [230/431]  eta: 0:03:47  lr: 0.000182  loss: 1.0845 (1.1202)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [240/431]  eta: 0:03:36  lr: 0.000182  loss: 1.0904 (1.1219)  time: 1.1262  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [250/431]  eta: 0:03:24  lr: 0.000182  loss: 1.1412 (1.1233)  time: 1.1331  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [260/431]  eta: 0:03:13  lr: 0.000182  loss: 1.0749 (1.1214)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [270/431]  eta: 0:03:02  lr: 0.000182  loss: 1.1369 (1.1248)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [280/431]  eta: 0:02:50  lr: 0.000182  loss: 1.1229 (1.1221)  time: 1.1300  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [290/431]  eta: 0:02:39  lr: 0.000182  loss: 1.0871 (1.1237)  time: 1.1243  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [300/431]  eta: 0:02:28  lr: 0.000182  loss: 1.1482 (1.1259)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [310/431]  eta: 0:02:16  lr: 0.000182  loss: 1.1387 (1.1268)  time: 1.1260  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [320/431]  eta: 0:02:05  lr: 0.000182  loss: 1.0917 (1.1261)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [330/431]  eta: 0:01:54  lr: 0.000182  loss: 1.0911 (1.1265)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [340/431]  eta: 0:01:42  lr: 0.000182  loss: 1.1299 (1.1273)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [350/431]  eta: 0:01:31  lr: 0.000182  loss: 1.1275 (1.1270)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [360/431]  eta: 0:01:20  lr: 0.000182  loss: 1.1275 (1.1271)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [370/431]  eta: 0:01:08  lr: 0.000182  loss: 1.1105 (1.1261)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [380/431]  eta: 0:00:57  lr: 0.000182  loss: 1.0884 (1.1253)  time: 1.1259  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [390/431]  eta: 0:00:46  lr: 0.000182  loss: 1.1267 (1.1262)  time: 1.1349  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:182]  [400/431]  eta: 0:00:34  lr: 0.000182  loss: 1.1658 (1.1266)  time: 1.1293  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [410/431]  eta: 0:00:23  lr: 0.000182  loss: 1.1127 (1.1258)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:182]  [420/431]  eta: 0:00:12  lr: 0.000182  loss: 1.0777 (1.1248)  time: 1.1245  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:182]  [430/431]  eta: 0:00:01  lr: 0.000182  loss: 1.0784 (1.1252)  time: 1.1238  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:182] Total time: 0:08:06 (1.1291 s / it)\n",
      "Averaged stats: lr: 0.000182  loss: 1.0784 (1.1252)\n",
      "Valid: [epoch:182]  [ 0/14]  eta: 0:00:38  loss: 1.0302 (1.0302)  time: 2.7409  data: 2.5687  max mem: 15925\n",
      "Valid: [epoch:182]  [13/14]  eta: 0:00:00  loss: 1.0508 (1.0644)  time: 0.2862  data: 0.1836  max mem: 15925\n",
      "Valid: [epoch:182] Total time: 0:00:04 (0.3038 s / it)\n",
      "Averaged stats: loss: 1.0508 (1.0644)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_182_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.064%\n",
      "Min loss: 1.063\n",
      "Best Epoch: 180.000\n",
      "Train: [epoch:183]  [  0/431]  eta: 0:33:51  lr: 0.000182  loss: 1.4223 (1.4223)  time: 4.7126  data: 3.5315  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:183]  [ 10/431]  eta: 0:09:42  lr: 0.000182  loss: 1.1085 (1.1624)  time: 1.3837  data: 0.3214  max mem: 15925\n",
      "Train: [epoch:183]  [ 20/431]  eta: 0:08:29  lr: 0.000182  loss: 1.0769 (1.1284)  time: 1.0653  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [ 30/431]  eta: 0:08:00  lr: 0.000182  loss: 1.0799 (1.1261)  time: 1.0961  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [ 40/431]  eta: 0:07:39  lr: 0.000182  loss: 1.1108 (1.1190)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [ 50/431]  eta: 0:07:22  lr: 0.000182  loss: 1.0913 (1.1201)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [ 60/431]  eta: 0:07:08  lr: 0.000182  loss: 1.0913 (1.1197)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [ 70/431]  eta: 0:06:54  lr: 0.000182  loss: 1.1221 (1.1279)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [ 80/431]  eta: 0:06:41  lr: 0.000182  loss: 1.1529 (1.1295)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [ 90/431]  eta: 0:06:29  lr: 0.000182  loss: 1.0907 (1.1240)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [100/431]  eta: 0:06:17  lr: 0.000182  loss: 1.1050 (1.1255)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [110/431]  eta: 0:06:05  lr: 0.000182  loss: 1.1050 (1.1241)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [120/431]  eta: 0:05:53  lr: 0.000182  loss: 1.1413 (1.1271)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [130/431]  eta: 0:05:41  lr: 0.000182  loss: 1.1284 (1.1283)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [140/431]  eta: 0:05:30  lr: 0.000182  loss: 1.1098 (1.1325)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [150/431]  eta: 0:05:18  lr: 0.000182  loss: 1.1098 (1.1326)  time: 1.1139  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [160/431]  eta: 0:05:06  lr: 0.000182  loss: 1.1050 (1.1325)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [170/431]  eta: 0:04:55  lr: 0.000182  loss: 1.1397 (1.1346)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [180/431]  eta: 0:04:43  lr: 0.000182  loss: 1.1355 (1.1332)  time: 1.1206  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [190/431]  eta: 0:04:32  lr: 0.000182  loss: 1.0914 (1.1310)  time: 1.1276  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [200/431]  eta: 0:04:21  lr: 0.000182  loss: 1.0931 (1.1299)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [210/431]  eta: 0:04:09  lr: 0.000182  loss: 1.0758 (1.1279)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [220/431]  eta: 0:03:58  lr: 0.000182  loss: 1.0585 (1.1264)  time: 1.1242  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [230/431]  eta: 0:03:47  lr: 0.000182  loss: 1.0896 (1.1257)  time: 1.1208  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [240/431]  eta: 0:03:35  lr: 0.000182  loss: 1.1108 (1.1259)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [250/431]  eta: 0:03:24  lr: 0.000182  loss: 1.1108 (1.1259)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [260/431]  eta: 0:03:12  lr: 0.000182  loss: 1.1478 (1.1283)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [270/431]  eta: 0:03:01  lr: 0.000182  loss: 1.1538 (1.1292)  time: 1.1078  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [280/431]  eta: 0:02:50  lr: 0.000182  loss: 1.1536 (1.1292)  time: 1.1232  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:183]  [290/431]  eta: 0:02:39  lr: 0.000182  loss: 1.1536 (1.1286)  time: 1.1336  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [300/431]  eta: 0:02:27  lr: 0.000182  loss: 1.1284 (1.1295)  time: 1.1349  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [310/431]  eta: 0:02:16  lr: 0.000182  loss: 1.0925 (1.1289)  time: 1.1316  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [320/431]  eta: 0:02:05  lr: 0.000182  loss: 1.0775 (1.1274)  time: 1.1356  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [330/431]  eta: 0:01:54  lr: 0.000182  loss: 1.0758 (1.1273)  time: 1.1419  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [340/431]  eta: 0:01:42  lr: 0.000182  loss: 1.0670 (1.1252)  time: 1.1395  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [350/431]  eta: 0:01:31  lr: 0.000182  loss: 1.0764 (1.1257)  time: 1.1425  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [360/431]  eta: 0:01:20  lr: 0.000182  loss: 1.1525 (1.1258)  time: 1.1398  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [370/431]  eta: 0:01:08  lr: 0.000182  loss: 1.0681 (1.1239)  time: 1.1208  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [380/431]  eta: 0:00:57  lr: 0.000182  loss: 1.0644 (1.1234)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [390/431]  eta: 0:00:46  lr: 0.000182  loss: 1.0992 (1.1245)  time: 1.1275  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [400/431]  eta: 0:00:35  lr: 0.000182  loss: 1.1312 (1.1252)  time: 1.1280  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:183]  [410/431]  eta: 0:00:23  lr: 0.000182  loss: 1.0977 (1.1248)  time: 1.1375  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [420/431]  eta: 0:00:12  lr: 0.000182  loss: 1.1180 (1.1250)  time: 1.1396  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183]  [430/431]  eta: 0:00:01  lr: 0.000182  loss: 1.0862 (1.1239)  time: 1.1364  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:183] Total time: 0:08:07 (1.1306 s / it)\n",
      "Averaged stats: lr: 0.000182  loss: 1.0862 (1.1239)\n",
      "Valid: [epoch:183]  [ 0/14]  eta: 0:00:33  loss: 1.0309 (1.0309)  time: 2.3837  data: 2.1893  max mem: 15925\n",
      "Valid: [epoch:183]  [13/14]  eta: 0:00:00  loss: 1.0518 (1.0630)  time: 0.2729  data: 0.1565  max mem: 15925\n",
      "Valid: [epoch:183] Total time: 0:00:04 (0.2891 s / it)\n",
      "Averaged stats: loss: 1.0518 (1.0630)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_183_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.063%\n",
      "Min loss: 1.063\n",
      "Best Epoch: 180.000\n",
      "Train: [epoch:184]  [  0/431]  eta: 0:33:00  lr: 0.000182  loss: 1.1701 (1.1701)  time: 4.5954  data: 3.4215  max mem: 15925\n",
      "Train: [epoch:184]  [ 10/431]  eta: 0:09:38  lr: 0.000182  loss: 1.0724 (1.0814)  time: 1.3736  data: 0.3113  max mem: 15925\n",
      "Train: [epoch:184]  [ 20/431]  eta: 0:08:26  lr: 0.000182  loss: 1.0856 (1.1242)  time: 1.0655  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [ 30/431]  eta: 0:07:58  lr: 0.000182  loss: 1.1234 (1.1194)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [ 40/431]  eta: 0:07:39  lr: 0.000182  loss: 1.0999 (1.1167)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [ 50/431]  eta: 0:07:25  lr: 0.000182  loss: 1.0969 (1.1220)  time: 1.1351  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [ 60/431]  eta: 0:07:09  lr: 0.000182  loss: 1.0436 (1.1054)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [ 70/431]  eta: 0:06:56  lr: 0.000182  loss: 1.0833 (1.1165)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [ 80/431]  eta: 0:06:43  lr: 0.000182  loss: 1.1290 (1.1118)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [ 90/431]  eta: 0:06:30  lr: 0.000182  loss: 1.1215 (1.1153)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [100/431]  eta: 0:06:19  lr: 0.000182  loss: 1.1215 (1.1132)  time: 1.1335  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [110/431]  eta: 0:06:06  lr: 0.000182  loss: 1.0434 (1.1084)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [120/431]  eta: 0:05:54  lr: 0.000182  loss: 1.0994 (1.1106)  time: 1.1160  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [130/431]  eta: 0:05:42  lr: 0.000182  loss: 1.0994 (1.1099)  time: 1.1143  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [140/431]  eta: 0:05:30  lr: 0.000182  loss: 1.0910 (1.1091)  time: 1.1156  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [150/431]  eta: 0:05:19  lr: 0.000182  loss: 1.1030 (1.1114)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [160/431]  eta: 0:05:07  lr: 0.000182  loss: 1.1030 (1.1109)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [170/431]  eta: 0:04:56  lr: 0.000182  loss: 1.0610 (1.1101)  time: 1.1293  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:184]  [180/431]  eta: 0:04:44  lr: 0.000182  loss: 1.1294 (1.1132)  time: 1.1296  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [190/431]  eta: 0:04:33  lr: 0.000182  loss: 1.1766 (1.1175)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [200/431]  eta: 0:04:21  lr: 0.000182  loss: 1.1397 (1.1173)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [210/431]  eta: 0:04:10  lr: 0.000182  loss: 1.1059 (1.1182)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [220/431]  eta: 0:03:58  lr: 0.000182  loss: 1.0704 (1.1157)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [230/431]  eta: 0:03:47  lr: 0.000182  loss: 1.0335 (1.1132)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [240/431]  eta: 0:03:35  lr: 0.000182  loss: 1.0374 (1.1127)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [250/431]  eta: 0:03:24  lr: 0.000182  loss: 1.1113 (1.1127)  time: 1.1215  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [260/431]  eta: 0:03:13  lr: 0.000182  loss: 1.1506 (1.1142)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [270/431]  eta: 0:03:01  lr: 0.000182  loss: 1.1247 (1.1143)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [280/431]  eta: 0:02:50  lr: 0.000182  loss: 1.0879 (1.1149)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [290/431]  eta: 0:02:39  lr: 0.000182  loss: 1.1527 (1.1164)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [300/431]  eta: 0:02:27  lr: 0.000182  loss: 1.1574 (1.1173)  time: 1.1182  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [310/431]  eta: 0:02:16  lr: 0.000182  loss: 1.1464 (1.1175)  time: 1.1177  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [320/431]  eta: 0:02:05  lr: 0.000182  loss: 1.1198 (1.1171)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [330/431]  eta: 0:01:53  lr: 0.000182  loss: 1.1198 (1.1204)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [340/431]  eta: 0:01:42  lr: 0.000182  loss: 1.1851 (1.1217)  time: 1.1284  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:184]  [350/431]  eta: 0:01:31  lr: 0.000182  loss: 1.1113 (1.1218)  time: 1.1227  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [360/431]  eta: 0:01:20  lr: 0.000182  loss: 1.0846 (1.1208)  time: 1.1292  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [370/431]  eta: 0:01:08  lr: 0.000182  loss: 1.0799 (1.1209)  time: 1.1349  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [380/431]  eta: 0:00:57  lr: 0.000182  loss: 1.1034 (1.1208)  time: 1.1316  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [390/431]  eta: 0:00:46  lr: 0.000182  loss: 1.1077 (1.1224)  time: 1.1192  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:184]  [400/431]  eta: 0:00:34  lr: 0.000182  loss: 1.1162 (1.1228)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [410/431]  eta: 0:00:23  lr: 0.000182  loss: 1.1323 (1.1236)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:184]  [420/431]  eta: 0:00:12  lr: 0.000182  loss: 1.1145 (1.1229)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:184]  [430/431]  eta: 0:00:01  lr: 0.000182  loss: 1.1143 (1.1231)  time: 1.1088  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:184] Total time: 0:08:05 (1.1266 s / it)\n",
      "Averaged stats: lr: 0.000182  loss: 1.1143 (1.1231)\n",
      "Valid: [epoch:184]  [ 0/14]  eta: 0:00:34  loss: 1.1084 (1.1084)  time: 2.4868  data: 2.3435  max mem: 15925\n",
      "Valid: [epoch:184]  [13/14]  eta: 0:00:00  loss: 1.0636 (1.0711)  time: 0.2596  data: 0.1676  max mem: 15925\n",
      "Valid: [epoch:184] Total time: 0:00:03 (0.2773 s / it)\n",
      "Averaged stats: loss: 1.0636 (1.0711)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_184_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.071%\n",
      "Min loss: 1.063\n",
      "Best Epoch: 180.000\n",
      "Train: [epoch:185]  [  0/431]  eta: 0:33:13  lr: 0.000181  loss: 1.0491 (1.0491)  time: 4.6245  data: 3.3884  max mem: 15925\n",
      "Train: [epoch:185]  [ 10/431]  eta: 0:09:53  lr: 0.000181  loss: 1.1978 (1.1761)  time: 1.4104  data: 0.3083  max mem: 15925\n",
      "Train: [epoch:185]  [ 20/431]  eta: 0:08:34  lr: 0.000181  loss: 1.1367 (1.1616)  time: 1.0837  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [ 30/431]  eta: 0:08:01  lr: 0.000181  loss: 1.0931 (1.1479)  time: 1.0865  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [ 40/431]  eta: 0:07:42  lr: 0.000181  loss: 1.1019 (1.1415)  time: 1.1090  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [ 50/431]  eta: 0:07:24  lr: 0.000181  loss: 1.1133 (1.1433)  time: 1.1140  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [ 60/431]  eta: 0:07:11  lr: 0.000181  loss: 1.1040 (1.1288)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [ 70/431]  eta: 0:06:58  lr: 0.000181  loss: 1.0486 (1.1258)  time: 1.1411  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [ 80/431]  eta: 0:06:45  lr: 0.000181  loss: 1.0997 (1.1251)  time: 1.1303  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [ 90/431]  eta: 0:06:33  lr: 0.000181  loss: 1.0638 (1.1183)  time: 1.1284  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [100/431]  eta: 0:06:20  lr: 0.000181  loss: 1.0264 (1.1136)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [110/431]  eta: 0:06:08  lr: 0.000181  loss: 1.0332 (1.1100)  time: 1.1343  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [120/431]  eta: 0:05:57  lr: 0.000181  loss: 1.0430 (1.1069)  time: 1.1399  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [130/431]  eta: 0:05:45  lr: 0.000181  loss: 1.1066 (1.1148)  time: 1.1279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [140/431]  eta: 0:05:32  lr: 0.000181  loss: 1.1066 (1.1137)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [150/431]  eta: 0:05:20  lr: 0.000181  loss: 1.1048 (1.1174)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [160/431]  eta: 0:05:08  lr: 0.000181  loss: 1.1674 (1.1192)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [170/431]  eta: 0:04:57  lr: 0.000181  loss: 1.0992 (1.1196)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [180/431]  eta: 0:04:45  lr: 0.000181  loss: 1.0893 (1.1198)  time: 1.1201  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:185]  [190/431]  eta: 0:04:34  lr: 0.000181  loss: 1.1156 (1.1224)  time: 1.1249  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:185]  [200/431]  eta: 0:04:22  lr: 0.000181  loss: 1.1129 (1.1223)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [210/431]  eta: 0:04:11  lr: 0.000181  loss: 1.1129 (1.1238)  time: 1.1279  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [220/431]  eta: 0:03:59  lr: 0.000181  loss: 1.0816 (1.1221)  time: 1.1337  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [230/431]  eta: 0:03:48  lr: 0.000181  loss: 1.0469 (1.1216)  time: 1.1267  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [240/431]  eta: 0:03:36  lr: 0.000181  loss: 1.0643 (1.1213)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [250/431]  eta: 0:03:25  lr: 0.000181  loss: 1.0674 (1.1195)  time: 1.1340  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [260/431]  eta: 0:03:14  lr: 0.000181  loss: 1.0716 (1.1195)  time: 1.1291  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [270/431]  eta: 0:03:02  lr: 0.000181  loss: 1.1037 (1.1205)  time: 1.1330  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [280/431]  eta: 0:02:51  lr: 0.000181  loss: 1.0865 (1.1189)  time: 1.1364  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [290/431]  eta: 0:02:40  lr: 0.000181  loss: 1.0683 (1.1193)  time: 1.1346  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [300/431]  eta: 0:02:28  lr: 0.000181  loss: 1.1145 (1.1201)  time: 1.1329  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [310/431]  eta: 0:02:17  lr: 0.000181  loss: 1.1194 (1.1197)  time: 1.1360  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [320/431]  eta: 0:02:06  lr: 0.000181  loss: 1.1194 (1.1202)  time: 1.1429  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [330/431]  eta: 0:01:54  lr: 0.000181  loss: 1.1234 (1.1209)  time: 1.1340  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [340/431]  eta: 0:01:43  lr: 0.000181  loss: 1.0942 (1.1209)  time: 1.1307  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:185]  [350/431]  eta: 0:01:31  lr: 0.000181  loss: 1.0900 (1.1211)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [360/431]  eta: 0:01:20  lr: 0.000181  loss: 1.0942 (1.1210)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [370/431]  eta: 0:01:09  lr: 0.000181  loss: 1.0913 (1.1208)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [380/431]  eta: 0:00:57  lr: 0.000181  loss: 1.0708 (1.1202)  time: 1.1295  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [390/431]  eta: 0:00:46  lr: 0.000181  loss: 1.1094 (1.1210)  time: 1.1245  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [400/431]  eta: 0:00:35  lr: 0.000181  loss: 1.0773 (1.1197)  time: 1.1294  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:185]  [410/431]  eta: 0:00:23  lr: 0.000181  loss: 1.0860 (1.1224)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [420/431]  eta: 0:00:12  lr: 0.000181  loss: 1.1522 (1.1233)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185]  [430/431]  eta: 0:00:01  lr: 0.000181  loss: 1.0892 (1.1222)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:185] Total time: 0:08:08 (1.1330 s / it)\n",
      "Averaged stats: lr: 0.000181  loss: 1.0892 (1.1222)\n",
      "Valid: [epoch:185]  [ 0/14]  eta: 0:00:35  loss: 0.9765 (0.9765)  time: 2.5632  data: 2.4606  max mem: 15925\n",
      "Valid: [epoch:185]  [13/14]  eta: 0:00:00  loss: 1.0612 (1.0699)  time: 0.2748  data: 0.1758  max mem: 15925\n",
      "Valid: [epoch:185] Total time: 0:00:04 (0.2892 s / it)\n",
      "Averaged stats: loss: 1.0612 (1.0699)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_185_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.070%\n",
      "Min loss: 1.063\n",
      "Best Epoch: 180.000\n",
      "Train: [epoch:186]  [  0/431]  eta: 0:32:52  lr: 0.000181  loss: 1.1117 (1.1117)  time: 4.5759  data: 3.4312  max mem: 15925\n",
      "Train: [epoch:186]  [ 10/431]  eta: 0:09:40  lr: 0.000181  loss: 1.1649 (1.1702)  time: 1.3789  data: 0.3122  max mem: 15925\n",
      "Train: [epoch:186]  [ 20/431]  eta: 0:08:29  lr: 0.000181  loss: 1.1644 (1.1648)  time: 1.0718  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [ 30/431]  eta: 0:07:59  lr: 0.000181  loss: 1.1370 (1.1614)  time: 1.0952  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [ 40/431]  eta: 0:07:38  lr: 0.000181  loss: 1.1518 (1.1592)  time: 1.1028  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [ 50/431]  eta: 0:07:22  lr: 0.000181  loss: 1.1004 (1.1479)  time: 1.1075  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [ 60/431]  eta: 0:07:07  lr: 0.000181  loss: 1.0932 (1.1413)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [ 70/431]  eta: 0:06:54  lr: 0.000181  loss: 1.0933 (1.1324)  time: 1.1166  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [ 80/431]  eta: 0:06:41  lr: 0.000181  loss: 1.1443 (1.1444)  time: 1.1213  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [ 90/431]  eta: 0:06:29  lr: 0.000181  loss: 1.1161 (1.1337)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [100/431]  eta: 0:06:17  lr: 0.000181  loss: 1.0433 (1.1298)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [110/431]  eta: 0:06:05  lr: 0.000181  loss: 1.0558 (1.1317)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [120/431]  eta: 0:05:53  lr: 0.000181  loss: 1.1527 (1.1360)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [130/431]  eta: 0:05:41  lr: 0.000181  loss: 1.1552 (1.1366)  time: 1.1108  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [140/431]  eta: 0:05:29  lr: 0.000181  loss: 1.1316 (1.1370)  time: 1.1120  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [150/431]  eta: 0:05:17  lr: 0.000181  loss: 1.1257 (1.1370)  time: 1.1181  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [160/431]  eta: 0:05:06  lr: 0.000181  loss: 1.1121 (1.1362)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [170/431]  eta: 0:04:54  lr: 0.000181  loss: 1.1121 (1.1351)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [180/431]  eta: 0:04:43  lr: 0.000181  loss: 1.0681 (1.1302)  time: 1.1261  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [190/431]  eta: 0:04:32  lr: 0.000181  loss: 1.0681 (1.1282)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [200/431]  eta: 0:04:20  lr: 0.000181  loss: 1.1081 (1.1287)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [210/431]  eta: 0:04:09  lr: 0.000181  loss: 1.1122 (1.1295)  time: 1.1300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [220/431]  eta: 0:03:58  lr: 0.000181  loss: 1.0528 (1.1259)  time: 1.1376  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [230/431]  eta: 0:03:47  lr: 0.000181  loss: 1.0528 (1.1243)  time: 1.1378  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [240/431]  eta: 0:03:35  lr: 0.000181  loss: 1.0849 (1.1236)  time: 1.1414  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [250/431]  eta: 0:03:24  lr: 0.000181  loss: 1.0876 (1.1229)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [260/431]  eta: 0:03:13  lr: 0.000181  loss: 1.1195 (1.1262)  time: 1.1267  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [270/431]  eta: 0:03:02  lr: 0.000181  loss: 1.1327 (1.1257)  time: 1.1464  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [280/431]  eta: 0:02:50  lr: 0.000181  loss: 1.1171 (1.1257)  time: 1.1325  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [290/431]  eta: 0:02:39  lr: 0.000181  loss: 1.0570 (1.1249)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [300/431]  eta: 0:02:27  lr: 0.000181  loss: 1.0430 (1.1242)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [310/431]  eta: 0:02:16  lr: 0.000181  loss: 1.0934 (1.1235)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [320/431]  eta: 0:02:05  lr: 0.000181  loss: 1.0889 (1.1232)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [330/431]  eta: 0:01:53  lr: 0.000181  loss: 1.0794 (1.1248)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [340/431]  eta: 0:01:42  lr: 0.000181  loss: 1.0794 (1.1242)  time: 1.1235  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [350/431]  eta: 0:01:31  lr: 0.000181  loss: 1.0787 (1.1245)  time: 1.1211  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [360/431]  eta: 0:01:20  lr: 0.000181  loss: 1.0622 (1.1230)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [370/431]  eta: 0:01:08  lr: 0.000181  loss: 1.0622 (1.1225)  time: 1.1361  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [380/431]  eta: 0:00:57  lr: 0.000181  loss: 1.0774 (1.1206)  time: 1.1301  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [390/431]  eta: 0:00:46  lr: 0.000181  loss: 1.0808 (1.1217)  time: 1.1188  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [400/431]  eta: 0:00:34  lr: 0.000181  loss: 1.1251 (1.1210)  time: 1.1212  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:186]  [410/431]  eta: 0:00:23  lr: 0.000181  loss: 1.0761 (1.1202)  time: 1.1246  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:186]  [420/431]  eta: 0:00:12  lr: 0.000181  loss: 1.0874 (1.1212)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186]  [430/431]  eta: 0:00:01  lr: 0.000181  loss: 1.1333 (1.1220)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:186] Total time: 0:08:06 (1.1278 s / it)\n",
      "Averaged stats: lr: 0.000181  loss: 1.1333 (1.1220)\n",
      "Valid: [epoch:186]  [ 0/14]  eta: 0:00:36  loss: 1.0247 (1.0247)  time: 2.6068  data: 2.4240  max mem: 15925\n",
      "Valid: [epoch:186]  [13/14]  eta: 0:00:00  loss: 1.0554 (1.0638)  time: 0.2964  data: 0.1732  max mem: 15925\n",
      "Valid: [epoch:186] Total time: 0:00:04 (0.3127 s / it)\n",
      "Averaged stats: loss: 1.0554 (1.0638)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_186_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.064%\n",
      "Min loss: 1.063\n",
      "Best Epoch: 180.000\n",
      "Train: [epoch:187]  [  0/431]  eta: 0:33:48  lr: 0.000181  loss: 1.2020 (1.2020)  time: 4.7059  data: 3.2436  max mem: 15925\n",
      "Train: [epoch:187]  [ 10/431]  eta: 0:09:36  lr: 0.000181  loss: 1.2187 (1.2136)  time: 1.3693  data: 0.2951  max mem: 15925\n",
      "Train: [epoch:187]  [ 20/431]  eta: 0:08:28  lr: 0.000181  loss: 1.1531 (1.1877)  time: 1.0649  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:187]  [ 30/431]  eta: 0:07:56  lr: 0.000181  loss: 1.1010 (1.1468)  time: 1.0906  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [ 40/431]  eta: 0:07:34  lr: 0.000181  loss: 1.0655 (1.1465)  time: 1.0853  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [ 50/431]  eta: 0:07:19  lr: 0.000181  loss: 1.0863 (1.1403)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [ 60/431]  eta: 0:07:04  lr: 0.000181  loss: 1.0514 (1.1280)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [ 70/431]  eta: 0:06:51  lr: 0.000181  loss: 1.0556 (1.1250)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [ 80/431]  eta: 0:06:40  lr: 0.000181  loss: 1.0873 (1.1243)  time: 1.1331  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [ 90/431]  eta: 0:06:28  lr: 0.000181  loss: 1.0847 (1.1217)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [100/431]  eta: 0:06:16  lr: 0.000181  loss: 1.0927 (1.1200)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [110/431]  eta: 0:06:04  lr: 0.000181  loss: 1.0963 (1.1150)  time: 1.1210  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [120/431]  eta: 0:05:52  lr: 0.000181  loss: 1.0426 (1.1102)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [130/431]  eta: 0:05:41  lr: 0.000181  loss: 1.0227 (1.1075)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [140/431]  eta: 0:05:29  lr: 0.000181  loss: 1.0606 (1.1048)  time: 1.1259  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [150/431]  eta: 0:05:18  lr: 0.000181  loss: 1.0476 (1.1011)  time: 1.1299  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [160/431]  eta: 0:05:06  lr: 0.000181  loss: 1.0395 (1.1000)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [170/431]  eta: 0:04:55  lr: 0.000181  loss: 1.0542 (1.0993)  time: 1.1276  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [180/431]  eta: 0:04:44  lr: 0.000181  loss: 1.0688 (1.1032)  time: 1.1364  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:187]  [190/431]  eta: 0:04:33  lr: 0.000181  loss: 1.1714 (1.1090)  time: 1.1418  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [200/431]  eta: 0:04:21  lr: 0.000181  loss: 1.1676 (1.1108)  time: 1.1405  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [210/431]  eta: 0:04:10  lr: 0.000181  loss: 1.0983 (1.1110)  time: 1.1270  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [220/431]  eta: 0:03:59  lr: 0.000181  loss: 1.1203 (1.1134)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [230/431]  eta: 0:03:47  lr: 0.000181  loss: 1.1302 (1.1136)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [240/431]  eta: 0:03:36  lr: 0.000181  loss: 1.1414 (1.1145)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [250/431]  eta: 0:03:24  lr: 0.000181  loss: 1.1414 (1.1154)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [260/431]  eta: 0:03:13  lr: 0.000181  loss: 1.1460 (1.1181)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [270/431]  eta: 0:03:01  lr: 0.000181  loss: 1.1490 (1.1187)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [280/431]  eta: 0:02:50  lr: 0.000181  loss: 1.0679 (1.1172)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [290/431]  eta: 0:02:39  lr: 0.000181  loss: 1.0662 (1.1161)  time: 1.1278  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [300/431]  eta: 0:02:27  lr: 0.000181  loss: 1.0889 (1.1175)  time: 1.1269  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [310/431]  eta: 0:02:16  lr: 0.000181  loss: 1.1173 (1.1193)  time: 1.1306  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [320/431]  eta: 0:02:05  lr: 0.000181  loss: 1.1757 (1.1206)  time: 1.1418  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [330/431]  eta: 0:01:54  lr: 0.000181  loss: 1.1620 (1.1222)  time: 1.1389  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [340/431]  eta: 0:01:42  lr: 0.000181  loss: 1.1188 (1.1217)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [350/431]  eta: 0:01:31  lr: 0.000181  loss: 1.1188 (1.1219)  time: 1.1251  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [360/431]  eta: 0:01:20  lr: 0.000181  loss: 1.1302 (1.1222)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [370/431]  eta: 0:01:08  lr: 0.000181  loss: 1.1053 (1.1234)  time: 1.1432  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [380/431]  eta: 0:00:57  lr: 0.000181  loss: 1.1053 (1.1244)  time: 1.1382  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [390/431]  eta: 0:00:46  lr: 0.000181  loss: 1.0250 (1.1224)  time: 1.1308  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [400/431]  eta: 0:00:35  lr: 0.000181  loss: 1.0571 (1.1224)  time: 1.1391  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:187]  [410/431]  eta: 0:00:23  lr: 0.000181  loss: 1.1153 (1.1224)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [420/431]  eta: 0:00:12  lr: 0.000181  loss: 1.1153 (1.1229)  time: 1.1300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187]  [430/431]  eta: 0:00:01  lr: 0.000181  loss: 1.0766 (1.1225)  time: 1.1349  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:187] Total time: 0:08:07 (1.1311 s / it)\n",
      "Averaged stats: lr: 0.000181  loss: 1.0766 (1.1225)\n",
      "Valid: [epoch:187]  [ 0/14]  eta: 0:00:35  loss: 1.1150 (1.1150)  time: 2.5435  data: 2.3881  max mem: 15925\n",
      "Valid: [epoch:187]  [13/14]  eta: 0:00:00  loss: 1.0497 (1.0602)  time: 0.2766  data: 0.1707  max mem: 15925\n",
      "Valid: [epoch:187] Total time: 0:00:04 (0.2918 s / it)\n",
      "Averaged stats: loss: 1.0497 (1.0602)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_187_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.060%\n",
      "Min loss: 1.060\n",
      "Best Epoch: 187.000\n",
      "Train: [epoch:188]  [  0/431]  eta: 0:30:45  lr: 0.000181  loss: 1.1769 (1.1769)  time: 4.2823  data: 3.0847  max mem: 15925\n",
      "Train: [epoch:188]  [ 10/431]  eta: 0:09:19  lr: 0.000181  loss: 1.1374 (1.1360)  time: 1.3282  data: 0.2806  max mem: 15925\n",
      "Train: [epoch:188]  [ 20/431]  eta: 0:08:17  lr: 0.000181  loss: 1.1189 (1.1382)  time: 1.0570  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [ 30/431]  eta: 0:07:52  lr: 0.000181  loss: 1.1189 (1.1477)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [ 40/431]  eta: 0:07:33  lr: 0.000181  loss: 1.1386 (1.1380)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [ 50/431]  eta: 0:07:18  lr: 0.000181  loss: 1.1057 (1.1291)  time: 1.1106  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [ 60/431]  eta: 0:07:06  lr: 0.000181  loss: 1.0411 (1.1215)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [ 70/431]  eta: 0:06:54  lr: 0.000181  loss: 1.0550 (1.1235)  time: 1.1344  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [ 80/431]  eta: 0:06:40  lr: 0.000181  loss: 1.1046 (1.1264)  time: 1.1162  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [ 90/431]  eta: 0:06:28  lr: 0.000181  loss: 1.1046 (1.1241)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [100/431]  eta: 0:06:16  lr: 0.000181  loss: 1.0659 (1.1245)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [110/431]  eta: 0:06:04  lr: 0.000181  loss: 1.0646 (1.1199)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [120/431]  eta: 0:05:53  lr: 0.000181  loss: 1.0331 (1.1137)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [130/431]  eta: 0:05:41  lr: 0.000181  loss: 1.0649 (1.1175)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [140/431]  eta: 0:05:29  lr: 0.000181  loss: 1.0802 (1.1161)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [150/431]  eta: 0:05:18  lr: 0.000181  loss: 1.0654 (1.1178)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [160/431]  eta: 0:05:06  lr: 0.000181  loss: 1.0746 (1.1192)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [170/431]  eta: 0:04:54  lr: 0.000181  loss: 1.0790 (1.1180)  time: 1.1169  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [180/431]  eta: 0:04:43  lr: 0.000181  loss: 1.1077 (1.1196)  time: 1.1182  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [190/431]  eta: 0:04:31  lr: 0.000181  loss: 1.1336 (1.1187)  time: 1.1112  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:188]  [200/431]  eta: 0:04:20  lr: 0.000181  loss: 1.1239 (1.1217)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [210/431]  eta: 0:04:09  lr: 0.000181  loss: 1.1158 (1.1197)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [220/431]  eta: 0:03:57  lr: 0.000181  loss: 1.0364 (1.1201)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [230/431]  eta: 0:03:46  lr: 0.000181  loss: 1.0798 (1.1201)  time: 1.1307  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [240/431]  eta: 0:03:35  lr: 0.000181  loss: 1.0798 (1.1185)  time: 1.1304  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [250/431]  eta: 0:03:24  lr: 0.000181  loss: 1.0999 (1.1209)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [260/431]  eta: 0:03:12  lr: 0.000181  loss: 1.1460 (1.1191)  time: 1.1370  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [270/431]  eta: 0:03:01  lr: 0.000181  loss: 1.0611 (1.1188)  time: 1.1234  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [280/431]  eta: 0:02:50  lr: 0.000181  loss: 1.0424 (1.1179)  time: 1.1125  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [290/431]  eta: 0:02:38  lr: 0.000181  loss: 1.0549 (1.1185)  time: 1.1163  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [300/431]  eta: 0:02:27  lr: 0.000181  loss: 1.1457 (1.1195)  time: 1.1288  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:188]  [310/431]  eta: 0:02:16  lr: 0.000181  loss: 1.1351 (1.1190)  time: 1.1325  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [320/431]  eta: 0:02:05  lr: 0.000181  loss: 1.0901 (1.1190)  time: 1.1174  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [330/431]  eta: 0:01:53  lr: 0.000181  loss: 1.0978 (1.1208)  time: 1.1205  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [340/431]  eta: 0:01:42  lr: 0.000181  loss: 1.1144 (1.1211)  time: 1.1226  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [350/431]  eta: 0:01:31  lr: 0.000181  loss: 1.1138 (1.1226)  time: 1.1082  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [360/431]  eta: 0:01:19  lr: 0.000181  loss: 1.1421 (1.1229)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [370/431]  eta: 0:01:08  lr: 0.000181  loss: 1.1524 (1.1245)  time: 1.1359  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [380/431]  eta: 0:00:57  lr: 0.000181  loss: 1.0967 (1.1235)  time: 1.1281  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:188]  [390/431]  eta: 0:00:46  lr: 0.000181  loss: 1.0766 (1.1228)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [400/431]  eta: 0:00:34  lr: 0.000181  loss: 1.0705 (1.1228)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [410/431]  eta: 0:00:23  lr: 0.000181  loss: 1.0857 (1.1229)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [420/431]  eta: 0:00:12  lr: 0.000181  loss: 1.1026 (1.1235)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:188]  [430/431]  eta: 0:00:01  lr: 0.000181  loss: 1.0800 (1.1218)  time: 1.1049  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:188] Total time: 0:08:04 (1.1250 s / it)\n",
      "Averaged stats: lr: 0.000181  loss: 1.0800 (1.1218)\n",
      "Valid: [epoch:188]  [ 0/14]  eta: 0:00:38  loss: 1.1278 (1.1278)  time: 2.7390  data: 2.5922  max mem: 15925\n",
      "Valid: [epoch:188]  [13/14]  eta: 0:00:00  loss: 1.0672 (1.0740)  time: 0.2803  data: 0.1852  max mem: 15925\n",
      "Valid: [epoch:188] Total time: 0:00:04 (0.2970 s / it)\n",
      "Averaged stats: loss: 1.0672 (1.0740)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_188_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.074%\n",
      "Min loss: 1.060\n",
      "Best Epoch: 187.000\n",
      "Train: [epoch:189]  [  0/431]  eta: 0:30:25  lr: 0.000180  loss: 1.0592 (1.0592)  time: 4.2345  data: 3.0120  max mem: 15925\n",
      "Train: [epoch:189]  [ 10/431]  eta: 0:09:31  lr: 0.000180  loss: 1.1603 (1.1850)  time: 1.3563  data: 0.2741  max mem: 15925\n",
      "Train: [epoch:189]  [ 20/431]  eta: 0:08:19  lr: 0.000180  loss: 1.1603 (1.1829)  time: 1.0639  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [ 30/431]  eta: 0:07:51  lr: 0.000180  loss: 1.1333 (1.1697)  time: 1.0779  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [ 40/431]  eta: 0:07:33  lr: 0.000180  loss: 1.0727 (1.1551)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [ 50/431]  eta: 0:07:18  lr: 0.000180  loss: 1.0724 (1.1364)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [ 60/431]  eta: 0:07:05  lr: 0.000180  loss: 1.0727 (1.1316)  time: 1.1178  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [ 70/431]  eta: 0:06:52  lr: 0.000180  loss: 1.1174 (1.1350)  time: 1.1260  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [ 80/431]  eta: 0:06:40  lr: 0.000180  loss: 1.1147 (1.1329)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [ 90/431]  eta: 0:06:28  lr: 0.000180  loss: 1.0776 (1.1274)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [100/431]  eta: 0:06:16  lr: 0.000180  loss: 1.0510 (1.1264)  time: 1.1300  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [110/431]  eta: 0:06:04  lr: 0.000180  loss: 1.1016 (1.1255)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [120/431]  eta: 0:05:52  lr: 0.000180  loss: 1.1153 (1.1278)  time: 1.1192  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [130/431]  eta: 0:05:41  lr: 0.000180  loss: 1.1346 (1.1277)  time: 1.1152  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [140/431]  eta: 0:05:29  lr: 0.000180  loss: 1.1298 (1.1310)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [150/431]  eta: 0:05:17  lr: 0.000180  loss: 1.0968 (1.1291)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [160/431]  eta: 0:05:06  lr: 0.000180  loss: 1.0598 (1.1265)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [170/431]  eta: 0:04:54  lr: 0.000180  loss: 1.0598 (1.1239)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [180/431]  eta: 0:04:43  lr: 0.000180  loss: 1.0718 (1.1246)  time: 1.1240  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [190/431]  eta: 0:04:31  lr: 0.000180  loss: 1.1017 (1.1247)  time: 1.1231  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [200/431]  eta: 0:04:20  lr: 0.000180  loss: 1.0627 (1.1230)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [210/431]  eta: 0:04:09  lr: 0.000180  loss: 1.0627 (1.1241)  time: 1.1189  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [220/431]  eta: 0:03:57  lr: 0.000180  loss: 1.0277 (1.1199)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [230/431]  eta: 0:03:46  lr: 0.000180  loss: 1.0549 (1.1207)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [240/431]  eta: 0:03:35  lr: 0.000180  loss: 1.1054 (1.1210)  time: 1.1242  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [250/431]  eta: 0:03:23  lr: 0.000180  loss: 1.0757 (1.1198)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [260/431]  eta: 0:03:12  lr: 0.000180  loss: 1.0757 (1.1190)  time: 1.1209  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [270/431]  eta: 0:03:01  lr: 0.000180  loss: 1.1083 (1.1201)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [280/431]  eta: 0:02:50  lr: 0.000180  loss: 1.1050 (1.1180)  time: 1.1249  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [290/431]  eta: 0:02:38  lr: 0.000180  loss: 1.0563 (1.1188)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [300/431]  eta: 0:02:27  lr: 0.000180  loss: 1.1246 (1.1204)  time: 1.1285  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [310/431]  eta: 0:02:16  lr: 0.000180  loss: 1.0658 (1.1192)  time: 1.1343  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [320/431]  eta: 0:02:04  lr: 0.000180  loss: 1.0480 (1.1181)  time: 1.1246  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [330/431]  eta: 0:01:53  lr: 0.000180  loss: 1.0972 (1.1183)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [340/431]  eta: 0:01:42  lr: 0.000180  loss: 1.1214 (1.1196)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [350/431]  eta: 0:01:31  lr: 0.000180  loss: 1.1396 (1.1200)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [360/431]  eta: 0:01:19  lr: 0.000180  loss: 1.1092 (1.1197)  time: 1.1189  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:189]  [370/431]  eta: 0:01:08  lr: 0.000180  loss: 1.0804 (1.1194)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [380/431]  eta: 0:00:57  lr: 0.000180  loss: 1.1435 (1.1206)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [390/431]  eta: 0:00:46  lr: 0.000180  loss: 1.1618 (1.1216)  time: 1.1246  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [400/431]  eta: 0:00:34  lr: 0.000180  loss: 1.1164 (1.1207)  time: 1.1215  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:189]  [410/431]  eta: 0:00:23  lr: 0.000180  loss: 1.0780 (1.1203)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:189]  [420/431]  eta: 0:00:12  lr: 0.000180  loss: 1.0823 (1.1206)  time: 1.1017  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:189]  [430/431]  eta: 0:00:01  lr: 0.000180  loss: 1.0823 (1.1199)  time: 1.1115  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:189] Total time: 0:08:04 (1.1240 s / it)\n",
      "Averaged stats: lr: 0.000180  loss: 1.0823 (1.1199)\n",
      "Valid: [epoch:189]  [ 0/14]  eta: 0:00:34  loss: 1.0174 (1.0174)  time: 2.4927  data: 2.3634  max mem: 15925\n",
      "Valid: [epoch:189]  [13/14]  eta: 0:00:00  loss: 1.0470 (1.0596)  time: 0.2583  data: 0.1689  max mem: 15925\n",
      "Valid: [epoch:189] Total time: 0:00:03 (0.2758 s / it)\n",
      "Averaged stats: loss: 1.0470 (1.0596)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_189_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.060%\n",
      "Min loss: 1.060\n",
      "Best Epoch: 189.000\n",
      "Train: [epoch:190]  [  0/431]  eta: 0:37:19  lr: 0.000180  loss: 1.0474 (1.0474)  time: 5.1950  data: 3.7592  max mem: 15925\n",
      "Train: [epoch:190]  [ 10/431]  eta: 0:09:58  lr: 0.000180  loss: 1.1063 (1.1248)  time: 1.4208  data: 0.3420  max mem: 15925\n",
      "Train: [epoch:190]  [ 20/431]  eta: 0:08:37  lr: 0.000180  loss: 1.1098 (1.1378)  time: 1.0628  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [ 30/431]  eta: 0:08:06  lr: 0.000180  loss: 1.1179 (1.1392)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [ 40/431]  eta: 0:07:42  lr: 0.000180  loss: 1.1305 (1.1271)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [ 50/431]  eta: 0:07:25  lr: 0.000180  loss: 1.1305 (1.1286)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [ 60/431]  eta: 0:07:09  lr: 0.000180  loss: 1.1376 (1.1303)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [ 70/431]  eta: 0:06:56  lr: 0.000180  loss: 1.1153 (1.1330)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [ 80/431]  eta: 0:06:42  lr: 0.000180  loss: 1.1334 (1.1404)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [ 90/431]  eta: 0:06:30  lr: 0.000180  loss: 1.1403 (1.1395)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [100/431]  eta: 0:06:17  lr: 0.000180  loss: 1.1539 (1.1367)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [110/431]  eta: 0:06:05  lr: 0.000180  loss: 1.1200 (1.1394)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [120/431]  eta: 0:05:53  lr: 0.000180  loss: 1.0949 (1.1336)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [130/431]  eta: 0:05:42  lr: 0.000180  loss: 1.0742 (1.1360)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [140/431]  eta: 0:05:30  lr: 0.000180  loss: 1.0778 (1.1334)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [150/431]  eta: 0:05:18  lr: 0.000180  loss: 1.0777 (1.1313)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [160/431]  eta: 0:05:06  lr: 0.000180  loss: 1.1023 (1.1312)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [170/431]  eta: 0:04:55  lr: 0.000180  loss: 1.1075 (1.1311)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [180/431]  eta: 0:04:43  lr: 0.000180  loss: 1.1075 (1.1329)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [190/431]  eta: 0:04:31  lr: 0.000180  loss: 1.1253 (1.1339)  time: 1.1108  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [200/431]  eta: 0:04:20  lr: 0.000180  loss: 1.0974 (1.1322)  time: 1.1205  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [210/431]  eta: 0:04:09  lr: 0.000180  loss: 1.0650 (1.1318)  time: 1.1350  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [220/431]  eta: 0:03:58  lr: 0.000180  loss: 1.0808 (1.1304)  time: 1.1342  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [230/431]  eta: 0:03:46  lr: 0.000180  loss: 1.0452 (1.1291)  time: 1.1325  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [240/431]  eta: 0:03:35  lr: 0.000180  loss: 1.0921 (1.1295)  time: 1.1340  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [250/431]  eta: 0:03:24  lr: 0.000180  loss: 1.1078 (1.1283)  time: 1.1307  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [260/431]  eta: 0:03:13  lr: 0.000180  loss: 1.1052 (1.1258)  time: 1.1291  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [270/431]  eta: 0:03:01  lr: 0.000180  loss: 1.0729 (1.1240)  time: 1.1274  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [280/431]  eta: 0:02:50  lr: 0.000180  loss: 1.0477 (1.1215)  time: 1.1292  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [290/431]  eta: 0:02:39  lr: 0.000180  loss: 1.0692 (1.1215)  time: 1.1313  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [300/431]  eta: 0:02:27  lr: 0.000180  loss: 1.0907 (1.1227)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [310/431]  eta: 0:02:16  lr: 0.000180  loss: 1.0907 (1.1231)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [320/431]  eta: 0:02:05  lr: 0.000180  loss: 1.0672 (1.1216)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [330/431]  eta: 0:01:53  lr: 0.000180  loss: 1.0827 (1.1234)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [340/431]  eta: 0:01:42  lr: 0.000180  loss: 1.0879 (1.1226)  time: 1.1242  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [350/431]  eta: 0:01:31  lr: 0.000180  loss: 1.1404 (1.1257)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [360/431]  eta: 0:01:20  lr: 0.000180  loss: 1.1399 (1.1251)  time: 1.1118  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [370/431]  eta: 0:01:08  lr: 0.000180  loss: 1.0721 (1.1234)  time: 1.1319  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [380/431]  eta: 0:00:57  lr: 0.000180  loss: 1.0508 (1.1224)  time: 1.1399  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [390/431]  eta: 0:00:46  lr: 0.000180  loss: 1.0748 (1.1219)  time: 1.1331  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [400/431]  eta: 0:00:34  lr: 0.000180  loss: 1.0357 (1.1200)  time: 1.1231  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [410/431]  eta: 0:00:23  lr: 0.000180  loss: 1.0181 (1.1205)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:190]  [420/431]  eta: 0:00:12  lr: 0.000180  loss: 1.0490 (1.1206)  time: 1.1257  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190]  [430/431]  eta: 0:00:01  lr: 0.000180  loss: 1.0449 (1.1201)  time: 1.1324  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:190] Total time: 0:08:06 (1.1284 s / it)\n",
      "Averaged stats: lr: 0.000180  loss: 1.0449 (1.1201)\n",
      "Valid: [epoch:190]  [ 0/14]  eta: 0:00:36  loss: 1.1487 (1.1487)  time: 2.6317  data: 2.4516  max mem: 15925\n",
      "Valid: [epoch:190]  [13/14]  eta: 0:00:00  loss: 1.0423 (1.0548)  time: 0.2804  data: 0.1752  max mem: 15925\n",
      "Valid: [epoch:190] Total time: 0:00:04 (0.2969 s / it)\n",
      "Averaged stats: loss: 1.0423 (1.0548)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_190_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.055%\n",
      "Min loss: 1.055\n",
      "Best Epoch: 190.000\n",
      "Train: [epoch:191]  [  0/431]  eta: 0:32:01  lr: 0.000180  loss: 1.0656 (1.0656)  time: 4.4594  data: 3.1587  max mem: 15925\n",
      "Train: [epoch:191]  [ 10/431]  eta: 0:09:39  lr: 0.000180  loss: 1.1534 (1.1644)  time: 1.3774  data: 0.2874  max mem: 15925\n",
      "Train: [epoch:191]  [ 20/431]  eta: 0:08:29  lr: 0.000180  loss: 1.1107 (1.1315)  time: 1.0784  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [ 30/431]  eta: 0:07:59  lr: 0.000180  loss: 1.0745 (1.1250)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [ 40/431]  eta: 0:07:37  lr: 0.000180  loss: 1.0893 (1.1285)  time: 1.0963  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:191]  [ 50/431]  eta: 0:07:21  lr: 0.000180  loss: 1.0893 (1.1191)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [ 60/431]  eta: 0:07:06  lr: 0.000180  loss: 1.0921 (1.1243)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [ 70/431]  eta: 0:06:53  lr: 0.000180  loss: 1.0712 (1.1150)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [ 80/431]  eta: 0:06:40  lr: 0.000180  loss: 1.0712 (1.1190)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [ 90/431]  eta: 0:06:28  lr: 0.000180  loss: 1.0796 (1.1134)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [100/431]  eta: 0:06:15  lr: 0.000180  loss: 1.0626 (1.1126)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [110/431]  eta: 0:06:04  lr: 0.000180  loss: 1.0804 (1.1111)  time: 1.1255  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [120/431]  eta: 0:05:52  lr: 0.000180  loss: 1.0804 (1.1116)  time: 1.1244  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [130/431]  eta: 0:05:41  lr: 0.000180  loss: 1.0755 (1.1095)  time: 1.1165  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [140/431]  eta: 0:05:29  lr: 0.000180  loss: 1.0987 (1.1086)  time: 1.1189  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [150/431]  eta: 0:05:17  lr: 0.000180  loss: 1.1050 (1.1130)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [160/431]  eta: 0:05:06  lr: 0.000180  loss: 1.1050 (1.1141)  time: 1.1180  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [170/431]  eta: 0:04:54  lr: 0.000180  loss: 1.1365 (1.1150)  time: 1.1233  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [180/431]  eta: 0:04:43  lr: 0.000180  loss: 1.1276 (1.1140)  time: 1.1296  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [190/431]  eta: 0:04:32  lr: 0.000180  loss: 1.1233 (1.1180)  time: 1.1345  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [200/431]  eta: 0:04:21  lr: 0.000180  loss: 1.1030 (1.1154)  time: 1.1334  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [210/431]  eta: 0:04:09  lr: 0.000180  loss: 1.0568 (1.1136)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [220/431]  eta: 0:03:58  lr: 0.000180  loss: 1.0432 (1.1125)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [230/431]  eta: 0:03:47  lr: 0.000180  loss: 1.0531 (1.1132)  time: 1.1312  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [240/431]  eta: 0:03:35  lr: 0.000180  loss: 1.0646 (1.1114)  time: 1.1170  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [250/431]  eta: 0:03:24  lr: 0.000180  loss: 1.0702 (1.1124)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [260/431]  eta: 0:03:12  lr: 0.000180  loss: 1.0700 (1.1113)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [270/431]  eta: 0:03:01  lr: 0.000180  loss: 1.0700 (1.1122)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [280/431]  eta: 0:02:50  lr: 0.000180  loss: 1.0484 (1.1107)  time: 1.1375  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [290/431]  eta: 0:02:38  lr: 0.000180  loss: 1.0413 (1.1113)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [300/431]  eta: 0:02:27  lr: 0.000180  loss: 1.1508 (1.1130)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [310/431]  eta: 0:02:16  lr: 0.000180  loss: 1.1397 (1.1144)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [320/431]  eta: 0:02:05  lr: 0.000180  loss: 1.0863 (1.1141)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [330/431]  eta: 0:01:53  lr: 0.000180  loss: 1.0679 (1.1147)  time: 1.1135  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [340/431]  eta: 0:01:42  lr: 0.000180  loss: 1.1338 (1.1168)  time: 1.1278  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [350/431]  eta: 0:01:31  lr: 0.000180  loss: 1.1087 (1.1163)  time: 1.1416  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [360/431]  eta: 0:01:20  lr: 0.000180  loss: 1.0741 (1.1166)  time: 1.1351  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [370/431]  eta: 0:01:08  lr: 0.000180  loss: 1.0610 (1.1157)  time: 1.1309  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [380/431]  eta: 0:00:57  lr: 0.000180  loss: 1.0916 (1.1159)  time: 1.1400  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [390/431]  eta: 0:00:46  lr: 0.000180  loss: 1.1010 (1.1168)  time: 1.1377  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [400/431]  eta: 0:00:34  lr: 0.000180  loss: 1.0850 (1.1168)  time: 1.1354  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:191]  [410/431]  eta: 0:00:23  lr: 0.000180  loss: 1.0909 (1.1161)  time: 1.1254  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [420/431]  eta: 0:00:12  lr: 0.000180  loss: 1.1223 (1.1167)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191]  [430/431]  eta: 0:00:01  lr: 0.000180  loss: 1.0951 (1.1167)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:191] Total time: 0:08:06 (1.1278 s / it)\n",
      "Averaged stats: lr: 0.000180  loss: 1.0951 (1.1167)\n",
      "Valid: [epoch:191]  [ 0/14]  eta: 0:00:35  loss: 1.0472 (1.0472)  time: 2.5010  data: 2.3284  max mem: 15925\n",
      "Valid: [epoch:191]  [13/14]  eta: 0:00:00  loss: 1.0472 (1.0574)  time: 0.2633  data: 0.1664  max mem: 15925\n",
      "Valid: [epoch:191] Total time: 0:00:03 (0.2788 s / it)\n",
      "Averaged stats: loss: 1.0472 (1.0574)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_191_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.057%\n",
      "Min loss: 1.055\n",
      "Best Epoch: 190.000\n",
      "Train: [epoch:192]  [  0/431]  eta: 0:36:48  lr: 0.000180  loss: 1.0586 (1.0586)  time: 5.1249  data: 3.8098  max mem: 15925\n",
      "Train: [epoch:192]  [ 10/431]  eta: 0:09:47  lr: 0.000180  loss: 1.1056 (1.1357)  time: 1.3965  data: 0.3466  max mem: 15925\n",
      "Train: [epoch:192]  [ 20/431]  eta: 0:08:33  lr: 0.000180  loss: 1.1732 (1.1653)  time: 1.0544  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [ 30/431]  eta: 0:08:01  lr: 0.000180  loss: 1.1327 (1.1414)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [ 40/431]  eta: 0:07:41  lr: 0.000180  loss: 1.1007 (1.1308)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [ 50/431]  eta: 0:07:24  lr: 0.000180  loss: 1.1053 (1.1327)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [ 60/431]  eta: 0:07:10  lr: 0.000180  loss: 1.1117 (1.1312)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [ 70/431]  eta: 0:06:58  lr: 0.000180  loss: 1.0749 (1.1247)  time: 1.1448  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [ 80/431]  eta: 0:06:47  lr: 0.000180  loss: 1.0946 (1.1247)  time: 1.1554  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [ 90/431]  eta: 0:06:34  lr: 0.000180  loss: 1.0946 (1.1243)  time: 1.1398  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [100/431]  eta: 0:06:21  lr: 0.000180  loss: 1.0565 (1.1168)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [110/431]  eta: 0:06:09  lr: 0.000180  loss: 1.0091 (1.1090)  time: 1.1260  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [120/431]  eta: 0:05:57  lr: 0.000180  loss: 1.0342 (1.1098)  time: 1.1266  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [130/431]  eta: 0:05:45  lr: 0.000180  loss: 1.0467 (1.1069)  time: 1.1258  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [140/431]  eta: 0:05:33  lr: 0.000180  loss: 1.0786 (1.1088)  time: 1.1246  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [150/431]  eta: 0:05:21  lr: 0.000180  loss: 1.1160 (1.1117)  time: 1.1361  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [160/431]  eta: 0:05:09  lr: 0.000180  loss: 1.1309 (1.1121)  time: 1.1339  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [170/431]  eta: 0:04:58  lr: 0.000180  loss: 1.1255 (1.1133)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [180/431]  eta: 0:04:46  lr: 0.000180  loss: 1.1255 (1.1153)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [190/431]  eta: 0:04:34  lr: 0.000180  loss: 1.1110 (1.1154)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [200/431]  eta: 0:04:23  lr: 0.000180  loss: 1.0675 (1.1139)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [210/431]  eta: 0:04:11  lr: 0.000180  loss: 1.0403 (1.1138)  time: 1.1097  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:192]  [220/431]  eta: 0:03:59  lr: 0.000180  loss: 1.1162 (1.1137)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [230/431]  eta: 0:03:48  lr: 0.000180  loss: 1.0634 (1.1116)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [240/431]  eta: 0:03:36  lr: 0.000180  loss: 1.0420 (1.1112)  time: 1.1244  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [250/431]  eta: 0:03:25  lr: 0.000180  loss: 1.0913 (1.1113)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [260/431]  eta: 0:03:13  lr: 0.000180  loss: 1.1486 (1.1132)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [270/431]  eta: 0:03:02  lr: 0.000180  loss: 1.1047 (1.1124)  time: 1.1170  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [280/431]  eta: 0:02:51  lr: 0.000180  loss: 1.0349 (1.1106)  time: 1.1199  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [290/431]  eta: 0:02:39  lr: 0.000180  loss: 1.0604 (1.1110)  time: 1.1121  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [300/431]  eta: 0:02:28  lr: 0.000180  loss: 1.0867 (1.1115)  time: 1.1245  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [310/431]  eta: 0:02:17  lr: 0.000180  loss: 1.0851 (1.1108)  time: 1.1252  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [320/431]  eta: 0:02:05  lr: 0.000180  loss: 1.0722 (1.1129)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [330/431]  eta: 0:01:54  lr: 0.000180  loss: 1.1194 (1.1148)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [340/431]  eta: 0:01:42  lr: 0.000180  loss: 1.1623 (1.1169)  time: 1.1338  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [350/431]  eta: 0:01:31  lr: 0.000180  loss: 1.0751 (1.1155)  time: 1.1449  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [360/431]  eta: 0:01:20  lr: 0.000180  loss: 1.0751 (1.1149)  time: 1.1487  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [370/431]  eta: 0:01:09  lr: 0.000180  loss: 1.1074 (1.1160)  time: 1.1414  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [380/431]  eta: 0:00:57  lr: 0.000180  loss: 1.1390 (1.1162)  time: 1.1349  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [390/431]  eta: 0:00:46  lr: 0.000180  loss: 1.1717 (1.1184)  time: 1.1308  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:192]  [400/431]  eta: 0:00:35  lr: 0.000180  loss: 1.1325 (1.1176)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [410/431]  eta: 0:00:23  lr: 0.000180  loss: 1.0641 (1.1180)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192]  [420/431]  eta: 0:00:12  lr: 0.000180  loss: 1.0795 (1.1165)  time: 1.1010  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:192]  [430/431]  eta: 0:00:01  lr: 0.000180  loss: 1.0345 (1.1168)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:192] Total time: 0:08:07 (1.1313 s / it)\n",
      "Averaged stats: lr: 0.000180  loss: 1.0345 (1.1168)\n",
      "Valid: [epoch:192]  [ 0/14]  eta: 0:00:34  loss: 1.1067 (1.1067)  time: 2.4884  data: 2.3210  max mem: 15925\n",
      "Valid: [epoch:192]  [13/14]  eta: 0:00:00  loss: 1.0421 (1.0554)  time: 0.2967  data: 0.1659  max mem: 15925\n",
      "Valid: [epoch:192] Total time: 0:00:04 (0.3115 s / it)\n",
      "Averaged stats: loss: 1.0421 (1.0554)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_192_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.055%\n",
      "Min loss: 1.055\n",
      "Best Epoch: 190.000\n",
      "Train: [epoch:193]  [  0/431]  eta: 0:32:16  lr: 0.000180  loss: 0.9854 (0.9854)  time: 4.4940  data: 3.3312  max mem: 15925\n",
      "Train: [epoch:193]  [ 10/431]  eta: 0:09:34  lr: 0.000180  loss: 1.1072 (1.1308)  time: 1.3656  data: 0.3031  max mem: 15925\n",
      "Train: [epoch:193]  [ 20/431]  eta: 0:08:25  lr: 0.000180  loss: 1.1525 (1.1556)  time: 1.0670  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [ 30/431]  eta: 0:07:58  lr: 0.000180  loss: 1.1309 (1.1250)  time: 1.0984  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [ 40/431]  eta: 0:07:37  lr: 0.000180  loss: 1.0293 (1.1123)  time: 1.1087  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:193]  [ 50/431]  eta: 0:07:23  lr: 0.000180  loss: 1.0577 (1.1060)  time: 1.1169  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [ 60/431]  eta: 0:07:08  lr: 0.000180  loss: 1.0523 (1.1080)  time: 1.1236  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [ 70/431]  eta: 0:06:56  lr: 0.000180  loss: 1.0517 (1.1141)  time: 1.1246  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [ 80/431]  eta: 0:06:43  lr: 0.000180  loss: 1.0564 (1.1103)  time: 1.1336  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [ 90/431]  eta: 0:06:31  lr: 0.000180  loss: 1.0673 (1.1110)  time: 1.1371  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [100/431]  eta: 0:06:19  lr: 0.000180  loss: 1.0676 (1.1096)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [110/431]  eta: 0:06:07  lr: 0.000180  loss: 1.1070 (1.1094)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [120/431]  eta: 0:05:55  lr: 0.000180  loss: 1.0967 (1.1074)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [130/431]  eta: 0:05:43  lr: 0.000180  loss: 1.1102 (1.1120)  time: 1.1346  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [140/431]  eta: 0:05:31  lr: 0.000180  loss: 1.1548 (1.1119)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [150/431]  eta: 0:05:20  lr: 0.000180  loss: 1.0570 (1.1107)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [160/431]  eta: 0:05:08  lr: 0.000180  loss: 1.1222 (1.1147)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [170/431]  eta: 0:04:56  lr: 0.000180  loss: 1.1222 (1.1148)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [180/431]  eta: 0:04:45  lr: 0.000180  loss: 1.0582 (1.1124)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [190/431]  eta: 0:04:33  lr: 0.000180  loss: 1.0582 (1.1146)  time: 1.1293  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [200/431]  eta: 0:04:22  lr: 0.000180  loss: 1.0684 (1.1125)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [210/431]  eta: 0:04:10  lr: 0.000180  loss: 1.0684 (1.1134)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [220/431]  eta: 0:03:59  lr: 0.000180  loss: 1.0552 (1.1115)  time: 1.1317  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [230/431]  eta: 0:03:47  lr: 0.000180  loss: 1.0172 (1.1119)  time: 1.1296  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [240/431]  eta: 0:03:36  lr: 0.000180  loss: 1.0797 (1.1131)  time: 1.1278  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [250/431]  eta: 0:03:25  lr: 0.000180  loss: 1.1070 (1.1161)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [260/431]  eta: 0:03:13  lr: 0.000180  loss: 1.1554 (1.1178)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [270/431]  eta: 0:03:02  lr: 0.000180  loss: 1.1554 (1.1185)  time: 1.1283  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [280/431]  eta: 0:02:51  lr: 0.000180  loss: 1.0819 (1.1152)  time: 1.1421  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [290/431]  eta: 0:02:39  lr: 0.000180  loss: 1.0781 (1.1164)  time: 1.1446  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [300/431]  eta: 0:02:28  lr: 0.000180  loss: 1.1017 (1.1172)  time: 1.1372  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [310/431]  eta: 0:02:17  lr: 0.000180  loss: 1.0781 (1.1165)  time: 1.1310  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [320/431]  eta: 0:02:05  lr: 0.000180  loss: 1.0911 (1.1189)  time: 1.1324  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [330/431]  eta: 0:01:54  lr: 0.000180  loss: 1.1686 (1.1204)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [340/431]  eta: 0:01:43  lr: 0.000180  loss: 1.1686 (1.1219)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [350/431]  eta: 0:01:31  lr: 0.000180  loss: 1.1054 (1.1211)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [360/431]  eta: 0:01:20  lr: 0.000180  loss: 1.0541 (1.1195)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [370/431]  eta: 0:01:09  lr: 0.000180  loss: 1.0793 (1.1189)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [380/431]  eta: 0:00:57  lr: 0.000180  loss: 1.0848 (1.1183)  time: 1.1288  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:193]  [390/431]  eta: 0:00:46  lr: 0.000180  loss: 1.0942 (1.1192)  time: 1.1217  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:193]  [400/431]  eta: 0:00:35  lr: 0.000180  loss: 1.1217 (1.1193)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [410/431]  eta: 0:00:23  lr: 0.000180  loss: 1.0636 (1.1179)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [420/431]  eta: 0:00:12  lr: 0.000180  loss: 1.0636 (1.1176)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193]  [430/431]  eta: 0:00:01  lr: 0.000180  loss: 1.0345 (1.1150)  time: 1.1274  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:193] Total time: 0:08:07 (1.1304 s / it)\n",
      "Averaged stats: lr: 0.000180  loss: 1.0345 (1.1150)\n",
      "Valid: [epoch:193]  [ 0/14]  eta: 0:00:35  loss: 0.9898 (0.9898)  time: 2.5219  data: 2.3611  max mem: 15925\n",
      "Valid: [epoch:193]  [13/14]  eta: 0:00:00  loss: 1.0427 (1.0546)  time: 0.2825  data: 0.1687  max mem: 15925\n",
      "Valid: [epoch:193] Total time: 0:00:04 (0.2994 s / it)\n",
      "Averaged stats: loss: 1.0427 (1.0546)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_193_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.055%\n",
      "Min loss: 1.055\n",
      "Best Epoch: 193.000\n",
      "Train: [epoch:194]  [  0/431]  eta: 0:31:09  lr: 0.000179  loss: 1.1762 (1.1762)  time: 4.3375  data: 3.0792  max mem: 15925\n",
      "Train: [epoch:194]  [ 10/431]  eta: 0:09:29  lr: 0.000179  loss: 1.1388 (1.1604)  time: 1.3536  data: 0.2802  max mem: 15925\n",
      "Train: [epoch:194]  [ 20/431]  eta: 0:08:24  lr: 0.000179  loss: 1.1430 (1.1560)  time: 1.0724  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [ 30/431]  eta: 0:07:55  lr: 0.000179  loss: 1.1338 (1.1390)  time: 1.0941  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [ 40/431]  eta: 0:07:38  lr: 0.000179  loss: 1.0847 (1.1237)  time: 1.1130  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [ 50/431]  eta: 0:07:23  lr: 0.000179  loss: 1.0614 (1.1118)  time: 1.1297  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [ 60/431]  eta: 0:07:08  lr: 0.000179  loss: 1.0239 (1.0982)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [ 70/431]  eta: 0:06:56  lr: 0.000179  loss: 1.0215 (1.0990)  time: 1.1237  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [ 80/431]  eta: 0:06:42  lr: 0.000179  loss: 1.1021 (1.1038)  time: 1.1171  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [ 90/431]  eta: 0:06:29  lr: 0.000179  loss: 1.0649 (1.0964)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [100/431]  eta: 0:06:17  lr: 0.000179  loss: 1.0113 (1.0948)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [110/431]  eta: 0:06:04  lr: 0.000179  loss: 1.0974 (1.0998)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [120/431]  eta: 0:05:53  lr: 0.000179  loss: 1.1047 (1.0988)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [130/431]  eta: 0:05:41  lr: 0.000179  loss: 1.0670 (1.1008)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [140/431]  eta: 0:05:29  lr: 0.000179  loss: 1.0670 (1.1013)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [150/431]  eta: 0:05:17  lr: 0.000179  loss: 1.0597 (1.0992)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [160/431]  eta: 0:05:06  lr: 0.000179  loss: 1.0687 (1.1018)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [170/431]  eta: 0:04:55  lr: 0.000179  loss: 1.1346 (1.1031)  time: 1.1391  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [180/431]  eta: 0:04:44  lr: 0.000179  loss: 1.0718 (1.1013)  time: 1.1389  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [190/431]  eta: 0:04:32  lr: 0.000179  loss: 1.0724 (1.1037)  time: 1.1280  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [200/431]  eta: 0:04:21  lr: 0.000179  loss: 1.1275 (1.1059)  time: 1.1235  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [210/431]  eta: 0:04:10  lr: 0.000179  loss: 1.1076 (1.1072)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [220/431]  eta: 0:03:59  lr: 0.000179  loss: 1.0766 (1.1054)  time: 1.1446  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [230/431]  eta: 0:03:47  lr: 0.000179  loss: 1.0491 (1.1060)  time: 1.1465  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [240/431]  eta: 0:03:36  lr: 0.000179  loss: 1.1439 (1.1090)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [250/431]  eta: 0:03:24  lr: 0.000179  loss: 1.1574 (1.1099)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [260/431]  eta: 0:03:13  lr: 0.000179  loss: 1.0912 (1.1097)  time: 1.1467  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [270/431]  eta: 0:03:02  lr: 0.000179  loss: 1.0919 (1.1112)  time: 1.1321  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [280/431]  eta: 0:02:50  lr: 0.000179  loss: 1.1409 (1.1133)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [290/431]  eta: 0:02:39  lr: 0.000179  loss: 1.0975 (1.1125)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [300/431]  eta: 0:02:28  lr: 0.000179  loss: 1.0913 (1.1141)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [310/431]  eta: 0:02:16  lr: 0.000179  loss: 1.0695 (1.1125)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [320/431]  eta: 0:02:05  lr: 0.000179  loss: 1.0637 (1.1123)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [330/431]  eta: 0:01:54  lr: 0.000179  loss: 1.0792 (1.1123)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [340/431]  eta: 0:01:42  lr: 0.000179  loss: 1.0818 (1.1138)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [350/431]  eta: 0:01:31  lr: 0.000179  loss: 1.1983 (1.1154)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [360/431]  eta: 0:01:20  lr: 0.000179  loss: 1.1583 (1.1170)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [370/431]  eta: 0:01:08  lr: 0.000179  loss: 1.1429 (1.1175)  time: 1.1346  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [380/431]  eta: 0:00:57  lr: 0.000179  loss: 1.0813 (1.1163)  time: 1.1313  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [390/431]  eta: 0:00:46  lr: 0.000179  loss: 1.0453 (1.1160)  time: 1.1144  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [400/431]  eta: 0:00:34  lr: 0.000179  loss: 1.0781 (1.1163)  time: 1.1158  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:194]  [410/431]  eta: 0:00:23  lr: 0.000179  loss: 1.0692 (1.1155)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:194]  [420/431]  eta: 0:00:12  lr: 0.000179  loss: 1.0652 (1.1147)  time: 1.1126  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:194]  [430/431]  eta: 0:00:01  lr: 0.000179  loss: 1.0569 (1.1135)  time: 1.1207  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:194] Total time: 0:08:06 (1.1277 s / it)\n",
      "Averaged stats: lr: 0.000179  loss: 1.0569 (1.1135)\n",
      "Valid: [epoch:194]  [ 0/14]  eta: 0:00:37  loss: 0.9873 (0.9873)  time: 2.6994  data: 2.5724  max mem: 15925\n",
      "Valid: [epoch:194]  [13/14]  eta: 0:00:00  loss: 1.0393 (1.0530)  time: 0.3194  data: 0.1838  max mem: 15925\n",
      "Valid: [epoch:194] Total time: 0:00:04 (0.3345 s / it)\n",
      "Averaged stats: loss: 1.0393 (1.0530)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_194_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.053%\n",
      "Min loss: 1.053\n",
      "Best Epoch: 194.000\n",
      "Train: [epoch:195]  [  0/431]  eta: 0:34:18  lr: 0.000179  loss: 1.1314 (1.1314)  time: 4.7756  data: 3.5588  max mem: 15925\n",
      "Train: [epoch:195]  [ 10/431]  eta: 0:09:55  lr: 0.000179  loss: 1.1314 (1.1672)  time: 1.4152  data: 0.3238  max mem: 15925\n",
      "Train: [epoch:195]  [ 20/431]  eta: 0:08:34  lr: 0.000179  loss: 1.1000 (1.1270)  time: 1.0754  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:195]  [ 30/431]  eta: 0:07:58  lr: 0.000179  loss: 1.0463 (1.0932)  time: 1.0720  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:195]  [ 40/431]  eta: 0:07:37  lr: 0.000179  loss: 1.0212 (1.0804)  time: 1.0855  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [ 50/431]  eta: 0:07:19  lr: 0.000179  loss: 1.0281 (1.0784)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [ 60/431]  eta: 0:07:05  lr: 0.000179  loss: 1.0656 (1.0816)  time: 1.0984  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:195]  [ 70/431]  eta: 0:06:54  lr: 0.000179  loss: 1.0656 (1.0790)  time: 1.1292  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [ 80/431]  eta: 0:06:41  lr: 0.000179  loss: 1.0557 (1.0861)  time: 1.1337  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [ 90/431]  eta: 0:06:28  lr: 0.000179  loss: 1.0868 (1.0939)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [100/431]  eta: 0:06:16  lr: 0.000179  loss: 1.1094 (1.1017)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [110/431]  eta: 0:06:04  lr: 0.000179  loss: 1.1220 (1.1036)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [120/431]  eta: 0:05:52  lr: 0.000179  loss: 1.0994 (1.1047)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [130/431]  eta: 0:05:41  lr: 0.000179  loss: 1.0761 (1.1022)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [140/431]  eta: 0:05:30  lr: 0.000179  loss: 1.0830 (1.1034)  time: 1.1347  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [150/431]  eta: 0:05:18  lr: 0.000179  loss: 1.1072 (1.1046)  time: 1.1245  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [160/431]  eta: 0:05:06  lr: 0.000179  loss: 1.0658 (1.1030)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [170/431]  eta: 0:04:55  lr: 0.000179  loss: 1.0854 (1.1067)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [180/431]  eta: 0:04:43  lr: 0.000179  loss: 1.1037 (1.1071)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [190/431]  eta: 0:04:32  lr: 0.000179  loss: 1.0490 (1.1056)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [200/431]  eta: 0:04:21  lr: 0.000179  loss: 1.0178 (1.1028)  time: 1.1285  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [210/431]  eta: 0:04:10  lr: 0.000179  loss: 1.0917 (1.1034)  time: 1.1384  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [220/431]  eta: 0:03:58  lr: 0.000179  loss: 1.1105 (1.1050)  time: 1.1285  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [230/431]  eta: 0:03:47  lr: 0.000179  loss: 1.1213 (1.1044)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [240/431]  eta: 0:03:35  lr: 0.000179  loss: 1.0850 (1.1051)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [250/431]  eta: 0:03:24  lr: 0.000179  loss: 1.0730 (1.1048)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [260/431]  eta: 0:03:12  lr: 0.000179  loss: 1.1074 (1.1069)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [270/431]  eta: 0:03:01  lr: 0.000179  loss: 1.0866 (1.1059)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [280/431]  eta: 0:02:50  lr: 0.000179  loss: 1.0675 (1.1061)  time: 1.1099  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:195]  [290/431]  eta: 0:02:38  lr: 0.000179  loss: 1.0788 (1.1064)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [300/431]  eta: 0:02:27  lr: 0.000179  loss: 1.0940 (1.1069)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [310/431]  eta: 0:02:16  lr: 0.000179  loss: 1.0830 (1.1065)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [320/431]  eta: 0:02:04  lr: 0.000179  loss: 1.0830 (1.1067)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [330/431]  eta: 0:01:53  lr: 0.000179  loss: 1.1100 (1.1083)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [340/431]  eta: 0:01:42  lr: 0.000179  loss: 1.1007 (1.1089)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [350/431]  eta: 0:01:31  lr: 0.000179  loss: 1.1115 (1.1090)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [360/431]  eta: 0:01:19  lr: 0.000179  loss: 1.1250 (1.1088)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [370/431]  eta: 0:01:08  lr: 0.000179  loss: 1.1162 (1.1088)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [380/431]  eta: 0:00:57  lr: 0.000179  loss: 1.0723 (1.1076)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [390/431]  eta: 0:00:46  lr: 0.000179  loss: 1.0548 (1.1077)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [400/431]  eta: 0:00:34  lr: 0.000179  loss: 1.1100 (1.1079)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [410/431]  eta: 0:00:23  lr: 0.000179  loss: 1.1668 (1.1099)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [420/431]  eta: 0:00:12  lr: 0.000179  loss: 1.1934 (1.1116)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:195]  [430/431]  eta: 0:00:01  lr: 0.000179  loss: 1.1410 (1.1122)  time: 1.1102  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:195] Total time: 0:08:03 (1.1222 s / it)\n",
      "Averaged stats: lr: 0.000179  loss: 1.1410 (1.1122)\n",
      "Valid: [epoch:195]  [ 0/14]  eta: 0:00:37  loss: 1.1032 (1.1032)  time: 2.6645  data: 2.5244  max mem: 15925\n",
      "Valid: [epoch:195]  [13/14]  eta: 0:00:00  loss: 1.0498 (1.0629)  time: 0.2747  data: 0.1804  max mem: 15925\n",
      "Valid: [epoch:195] Total time: 0:00:04 (0.2921 s / it)\n",
      "Averaged stats: loss: 1.0498 (1.0629)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_195_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.063%\n",
      "Min loss: 1.053\n",
      "Best Epoch: 194.000\n",
      "Train: [epoch:196]  [  0/431]  eta: 0:31:10  lr: 0.000179  loss: 1.1642 (1.1642)  time: 4.3405  data: 3.1636  max mem: 15925\n",
      "Train: [epoch:196]  [ 10/431]  eta: 0:09:29  lr: 0.000179  loss: 1.1036 (1.1432)  time: 1.3533  data: 0.2878  max mem: 15925\n",
      "Train: [epoch:196]  [ 20/431]  eta: 0:08:22  lr: 0.000179  loss: 1.0853 (1.1311)  time: 1.0665  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [ 30/431]  eta: 0:07:54  lr: 0.000179  loss: 1.0836 (1.1280)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [ 40/431]  eta: 0:07:32  lr: 0.000179  loss: 1.1286 (1.1362)  time: 1.0890  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [ 50/431]  eta: 0:07:17  lr: 0.000179  loss: 1.1361 (1.1300)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [ 60/431]  eta: 0:07:03  lr: 0.000179  loss: 1.0595 (1.1231)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [ 70/431]  eta: 0:06:51  lr: 0.000179  loss: 1.0484 (1.1196)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [ 80/431]  eta: 0:06:40  lr: 0.000179  loss: 1.0719 (1.1217)  time: 1.1377  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [ 90/431]  eta: 0:06:28  lr: 0.000179  loss: 1.1193 (1.1232)  time: 1.1390  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [100/431]  eta: 0:06:16  lr: 0.000179  loss: 1.0838 (1.1179)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [110/431]  eta: 0:06:04  lr: 0.000179  loss: 1.0428 (1.1130)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [120/431]  eta: 0:05:52  lr: 0.000179  loss: 1.0372 (1.1095)  time: 1.1212  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [130/431]  eta: 0:05:41  lr: 0.000179  loss: 1.0716 (1.1083)  time: 1.1288  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [140/431]  eta: 0:05:30  lr: 0.000179  loss: 1.0876 (1.1089)  time: 1.1369  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [150/431]  eta: 0:05:18  lr: 0.000179  loss: 1.0828 (1.1066)  time: 1.1193  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [160/431]  eta: 0:05:06  lr: 0.000179  loss: 1.1237 (1.1094)  time: 1.1197  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [170/431]  eta: 0:04:55  lr: 0.000179  loss: 1.0624 (1.1063)  time: 1.1312  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [180/431]  eta: 0:04:44  lr: 0.000179  loss: 1.0687 (1.1100)  time: 1.1283  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [190/431]  eta: 0:04:32  lr: 0.000179  loss: 1.1335 (1.1097)  time: 1.1233  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [200/431]  eta: 0:04:21  lr: 0.000179  loss: 1.0940 (1.1093)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [210/431]  eta: 0:04:09  lr: 0.000179  loss: 1.0926 (1.1108)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [220/431]  eta: 0:03:58  lr: 0.000179  loss: 1.0701 (1.1093)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [230/431]  eta: 0:03:46  lr: 0.000179  loss: 1.0846 (1.1092)  time: 1.1119  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:196]  [240/431]  eta: 0:03:35  lr: 0.000179  loss: 1.1216 (1.1110)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [250/431]  eta: 0:03:24  lr: 0.000179  loss: 1.1152 (1.1104)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [260/431]  eta: 0:03:12  lr: 0.000179  loss: 1.0519 (1.1103)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [270/431]  eta: 0:03:01  lr: 0.000179  loss: 1.0689 (1.1092)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [280/431]  eta: 0:02:50  lr: 0.000179  loss: 1.0961 (1.1105)  time: 1.1239  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [290/431]  eta: 0:02:38  lr: 0.000179  loss: 1.0961 (1.1114)  time: 1.1404  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [300/431]  eta: 0:02:27  lr: 0.000179  loss: 1.0918 (1.1120)  time: 1.1308  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [310/431]  eta: 0:02:16  lr: 0.000179  loss: 1.0918 (1.1121)  time: 1.1359  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [320/431]  eta: 0:02:05  lr: 0.000179  loss: 1.0685 (1.1102)  time: 1.1523  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [330/431]  eta: 0:01:53  lr: 0.000179  loss: 1.0947 (1.1124)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [340/431]  eta: 0:01:42  lr: 0.000179  loss: 1.1384 (1.1138)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [350/431]  eta: 0:01:31  lr: 0.000179  loss: 1.1034 (1.1129)  time: 1.1216  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [360/431]  eta: 0:01:20  lr: 0.000179  loss: 1.0836 (1.1119)  time: 1.1163  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:196]  [370/431]  eta: 0:01:08  lr: 0.000179  loss: 1.0511 (1.1112)  time: 1.1265  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [380/431]  eta: 0:00:57  lr: 0.000179  loss: 1.0394 (1.1099)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [390/431]  eta: 0:00:46  lr: 0.000179  loss: 1.0446 (1.1109)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [400/431]  eta: 0:00:34  lr: 0.000179  loss: 1.1590 (1.1117)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [410/431]  eta: 0:00:23  lr: 0.000179  loss: 1.1141 (1.1120)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196]  [420/431]  eta: 0:00:12  lr: 0.000179  loss: 1.0840 (1.1116)  time: 1.1083  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:196]  [430/431]  eta: 0:00:01  lr: 0.000179  loss: 1.0840 (1.1117)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:196] Total time: 0:08:05 (1.1267 s / it)\n",
      "Averaged stats: lr: 0.000179  loss: 1.0840 (1.1117)\n",
      "Valid: [epoch:196]  [ 0/14]  eta: 0:00:35  loss: 0.9363 (0.9363)  time: 2.5249  data: 2.3718  max mem: 15925\n",
      "Valid: [epoch:196]  [13/14]  eta: 0:00:00  loss: 1.0380 (1.0494)  time: 0.2761  data: 0.1695  max mem: 15925\n",
      "Valid: [epoch:196] Total time: 0:00:04 (0.2931 s / it)\n",
      "Averaged stats: loss: 1.0380 (1.0494)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_196_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.049\n",
      "Best Epoch: 196.000\n",
      "Train: [epoch:197]  [  0/431]  eta: 0:30:42  lr: 0.000179  loss: 1.2183 (1.2183)  time: 4.2753  data: 3.0606  max mem: 15925\n",
      "Train: [epoch:197]  [ 10/431]  eta: 0:09:28  lr: 0.000179  loss: 1.2183 (1.2011)  time: 1.3493  data: 0.2785  max mem: 15925\n",
      "Train: [epoch:197]  [ 20/431]  eta: 0:08:23  lr: 0.000179  loss: 1.1370 (1.1639)  time: 1.0729  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [ 30/431]  eta: 0:07:52  lr: 0.000179  loss: 1.0934 (1.1404)  time: 1.0856  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [ 40/431]  eta: 0:07:34  lr: 0.000179  loss: 1.0842 (1.1252)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [ 50/431]  eta: 0:07:19  lr: 0.000179  loss: 1.0448 (1.1159)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [ 60/431]  eta: 0:07:05  lr: 0.000179  loss: 1.0491 (1.1074)  time: 1.1155  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [ 70/431]  eta: 0:06:52  lr: 0.000179  loss: 1.0685 (1.1152)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [ 80/431]  eta: 0:06:40  lr: 0.000179  loss: 1.0647 (1.1141)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [ 90/431]  eta: 0:06:27  lr: 0.000179  loss: 1.0626 (1.1102)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [100/431]  eta: 0:06:15  lr: 0.000179  loss: 1.0707 (1.1062)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [110/431]  eta: 0:06:04  lr: 0.000179  loss: 1.0688 (1.1064)  time: 1.1269  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [120/431]  eta: 0:05:52  lr: 0.000179  loss: 1.0848 (1.1055)  time: 1.1299  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [130/431]  eta: 0:05:41  lr: 0.000179  loss: 1.0848 (1.1053)  time: 1.1307  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [140/431]  eta: 0:05:29  lr: 0.000179  loss: 1.1093 (1.1070)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [150/431]  eta: 0:05:18  lr: 0.000179  loss: 1.1038 (1.1069)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [160/431]  eta: 0:05:06  lr: 0.000179  loss: 1.1038 (1.1107)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [170/431]  eta: 0:04:54  lr: 0.000179  loss: 1.0630 (1.1050)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [180/431]  eta: 0:04:43  lr: 0.000179  loss: 1.0509 (1.1057)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [190/431]  eta: 0:04:32  lr: 0.000179  loss: 1.1042 (1.1071)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [200/431]  eta: 0:04:20  lr: 0.000179  loss: 1.1042 (1.1086)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [210/431]  eta: 0:04:09  lr: 0.000179  loss: 1.0700 (1.1120)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [220/431]  eta: 0:03:57  lr: 0.000179  loss: 1.1291 (1.1127)  time: 1.1196  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [230/431]  eta: 0:03:46  lr: 0.000179  loss: 1.1291 (1.1133)  time: 1.1192  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [240/431]  eta: 0:03:35  lr: 0.000179  loss: 1.0921 (1.1125)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [250/431]  eta: 0:03:23  lr: 0.000179  loss: 1.0608 (1.1106)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [260/431]  eta: 0:03:12  lr: 0.000179  loss: 1.0608 (1.1101)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [270/431]  eta: 0:03:01  lr: 0.000179  loss: 1.0697 (1.1093)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [280/431]  eta: 0:02:49  lr: 0.000179  loss: 1.0784 (1.1083)  time: 1.1149  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [290/431]  eta: 0:02:38  lr: 0.000179  loss: 1.1286 (1.1110)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [300/431]  eta: 0:02:27  lr: 0.000179  loss: 1.1286 (1.1112)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [310/431]  eta: 0:02:16  lr: 0.000179  loss: 1.0613 (1.1103)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [320/431]  eta: 0:02:04  lr: 0.000179  loss: 1.0880 (1.1120)  time: 1.1170  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [330/431]  eta: 0:01:53  lr: 0.000179  loss: 1.1363 (1.1139)  time: 1.1132  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [340/431]  eta: 0:01:42  lr: 0.000179  loss: 1.1363 (1.1160)  time: 1.1158  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:197]  [350/431]  eta: 0:01:31  lr: 0.000179  loss: 1.1368 (1.1166)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [360/431]  eta: 0:01:19  lr: 0.000179  loss: 1.1140 (1.1156)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [370/431]  eta: 0:01:08  lr: 0.000179  loss: 1.1140 (1.1154)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [380/431]  eta: 0:00:57  lr: 0.000179  loss: 1.0680 (1.1146)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [390/431]  eta: 0:00:46  lr: 0.000179  loss: 1.0391 (1.1145)  time: 1.1145  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:197]  [400/431]  eta: 0:00:34  lr: 0.000179  loss: 1.0394 (1.1131)  time: 1.1210  data: 0.0004  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:197]  [410/431]  eta: 0:00:23  lr: 0.000179  loss: 1.0538 (1.1124)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197]  [420/431]  eta: 0:00:12  lr: 0.000179  loss: 1.0538 (1.1111)  time: 1.1192  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:197]  [430/431]  eta: 0:00:01  lr: 0.000179  loss: 1.0493 (1.1102)  time: 1.1306  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:197] Total time: 0:08:04 (1.1237 s / it)\n",
      "Averaged stats: lr: 0.000179  loss: 1.0493 (1.1102)\n",
      "Valid: [epoch:197]  [ 0/14]  eta: 0:00:37  loss: 1.0958 (1.0958)  time: 2.7094  data: 2.5048  max mem: 15925\n",
      "Valid: [epoch:197]  [13/14]  eta: 0:00:00  loss: 1.0322 (1.0436)  time: 0.2945  data: 0.1790  max mem: 15925\n",
      "Valid: [epoch:197] Total time: 0:00:04 (0.3111 s / it)\n",
      "Averaged stats: loss: 1.0322 (1.0436)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_197_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:198]  [  0/431]  eta: 0:28:37  lr: 0.000178  loss: 0.9442 (0.9442)  time: 3.9856  data: 2.7233  max mem: 15925\n",
      "Train: [epoch:198]  [ 10/431]  eta: 0:09:23  lr: 0.000178  loss: 1.1885 (1.1694)  time: 1.3391  data: 0.2478  max mem: 15925\n",
      "Train: [epoch:198]  [ 20/431]  eta: 0:08:19  lr: 0.000178  loss: 1.1318 (1.1243)  time: 1.0766  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [ 30/431]  eta: 0:07:54  lr: 0.000178  loss: 1.0518 (1.1096)  time: 1.0968  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [ 40/431]  eta: 0:07:36  lr: 0.000178  loss: 1.0452 (1.0965)  time: 1.1172  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [ 50/431]  eta: 0:07:19  lr: 0.000178  loss: 1.0618 (1.1021)  time: 1.1073  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [ 60/431]  eta: 0:07:05  lr: 0.000178  loss: 1.0981 (1.1022)  time: 1.1039  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [ 70/431]  eta: 0:06:52  lr: 0.000178  loss: 1.0869 (1.0982)  time: 1.1193  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [ 80/431]  eta: 0:06:40  lr: 0.000178  loss: 1.1002 (1.1150)  time: 1.1277  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [ 90/431]  eta: 0:06:27  lr: 0.000178  loss: 1.1701 (1.1116)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [100/431]  eta: 0:06:15  lr: 0.000178  loss: 1.0563 (1.1089)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [110/431]  eta: 0:06:03  lr: 0.000178  loss: 1.0875 (1.1113)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [120/431]  eta: 0:05:51  lr: 0.000178  loss: 1.0973 (1.1130)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [130/431]  eta: 0:05:39  lr: 0.000178  loss: 1.0430 (1.1073)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [140/431]  eta: 0:05:28  lr: 0.000178  loss: 1.0430 (1.1073)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [150/431]  eta: 0:05:17  lr: 0.000178  loss: 1.0482 (1.1040)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [160/431]  eta: 0:05:05  lr: 0.000178  loss: 1.0668 (1.1044)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [170/431]  eta: 0:04:54  lr: 0.000178  loss: 1.0743 (1.1045)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [180/431]  eta: 0:04:42  lr: 0.000178  loss: 1.0448 (1.1017)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [190/431]  eta: 0:04:31  lr: 0.000178  loss: 1.0384 (1.1013)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [200/431]  eta: 0:04:19  lr: 0.000178  loss: 1.0681 (1.1010)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [210/431]  eta: 0:04:08  lr: 0.000178  loss: 1.0493 (1.1003)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [220/431]  eta: 0:03:57  lr: 0.000178  loss: 1.0756 (1.1010)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [230/431]  eta: 0:03:45  lr: 0.000178  loss: 1.0640 (1.0994)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [240/431]  eta: 0:03:34  lr: 0.000178  loss: 1.1237 (1.1016)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [250/431]  eta: 0:03:23  lr: 0.000178  loss: 1.1077 (1.1018)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [260/431]  eta: 0:03:12  lr: 0.000178  loss: 1.0738 (1.1012)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [270/431]  eta: 0:03:01  lr: 0.000178  loss: 1.0619 (1.1005)  time: 1.1383  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [280/431]  eta: 0:02:49  lr: 0.000178  loss: 1.0770 (1.1006)  time: 1.1447  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [290/431]  eta: 0:02:38  lr: 0.000178  loss: 1.0974 (1.1014)  time: 1.1256  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [300/431]  eta: 0:02:27  lr: 0.000178  loss: 1.1200 (1.1032)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [310/431]  eta: 0:02:16  lr: 0.000178  loss: 1.1043 (1.1025)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [320/431]  eta: 0:02:04  lr: 0.000178  loss: 1.1043 (1.1036)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [330/431]  eta: 0:01:53  lr: 0.000178  loss: 1.1317 (1.1059)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [340/431]  eta: 0:01:42  lr: 0.000178  loss: 1.1358 (1.1093)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [350/431]  eta: 0:01:30  lr: 0.000178  loss: 1.1358 (1.1105)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [360/431]  eta: 0:01:19  lr: 0.000178  loss: 1.1090 (1.1100)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [370/431]  eta: 0:01:08  lr: 0.000178  loss: 1.0752 (1.1106)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [380/431]  eta: 0:00:57  lr: 0.000178  loss: 1.0478 (1.1095)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [390/431]  eta: 0:00:46  lr: 0.000178  loss: 1.0521 (1.1095)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [400/431]  eta: 0:00:34  lr: 0.000178  loss: 1.0929 (1.1100)  time: 1.1275  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:198]  [410/431]  eta: 0:00:23  lr: 0.000178  loss: 1.1048 (1.1109)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [420/431]  eta: 0:00:12  lr: 0.000178  loss: 1.1299 (1.1107)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:198]  [430/431]  eta: 0:00:01  lr: 0.000178  loss: 1.0700 (1.1098)  time: 1.1161  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:198] Total time: 0:08:04 (1.1233 s / it)\n",
      "Averaged stats: lr: 0.000178  loss: 1.0700 (1.1098)\n",
      "Valid: [epoch:198]  [ 0/14]  eta: 0:00:36  loss: 1.0134 (1.0134)  time: 2.6254  data: 2.4189  max mem: 15925\n",
      "Valid: [epoch:198]  [13/14]  eta: 0:00:00  loss: 1.0422 (1.0499)  time: 0.2995  data: 0.1729  max mem: 15925\n",
      "Valid: [epoch:198] Total time: 0:00:04 (0.3165 s / it)\n",
      "Averaged stats: loss: 1.0422 (1.0499)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_198_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:199]  [  0/431]  eta: 0:32:54  lr: 0.000178  loss: 1.2644 (1.2644)  time: 4.5803  data: 3.3373  max mem: 15925\n",
      "Train: [epoch:199]  [ 10/431]  eta: 0:09:45  lr: 0.000178  loss: 1.2030 (1.1678)  time: 1.3910  data: 0.3037  max mem: 15925\n",
      "Train: [epoch:199]  [ 20/431]  eta: 0:08:29  lr: 0.000178  loss: 1.1003 (1.1361)  time: 1.0724  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:199]  [ 30/431]  eta: 0:07:57  lr: 0.000178  loss: 1.0629 (1.1104)  time: 1.0795  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [ 40/431]  eta: 0:07:36  lr: 0.000178  loss: 1.0680 (1.0975)  time: 1.0917  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [ 50/431]  eta: 0:07:19  lr: 0.000178  loss: 1.1005 (1.1060)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [ 60/431]  eta: 0:07:05  lr: 0.000178  loss: 1.1117 (1.1048)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [ 70/431]  eta: 0:06:52  lr: 0.000178  loss: 1.0601 (1.1001)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [ 80/431]  eta: 0:06:39  lr: 0.000178  loss: 1.0706 (1.0998)  time: 1.1103  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:199]  [ 90/431]  eta: 0:06:26  lr: 0.000178  loss: 1.0902 (1.1030)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [100/431]  eta: 0:06:14  lr: 0.000178  loss: 1.1057 (1.1064)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [110/431]  eta: 0:06:02  lr: 0.000178  loss: 1.0278 (1.1038)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [120/431]  eta: 0:05:51  lr: 0.000178  loss: 1.0390 (1.1038)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [130/431]  eta: 0:05:40  lr: 0.000178  loss: 1.1416 (1.1065)  time: 1.1353  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:199]  [140/431]  eta: 0:05:28  lr: 0.000178  loss: 1.0912 (1.1043)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [150/431]  eta: 0:05:17  lr: 0.000178  loss: 1.0965 (1.1054)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [160/431]  eta: 0:05:06  lr: 0.000178  loss: 1.1058 (1.1039)  time: 1.1377  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [170/431]  eta: 0:04:54  lr: 0.000178  loss: 1.0866 (1.1010)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [180/431]  eta: 0:04:42  lr: 0.000178  loss: 1.1037 (1.1041)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [190/431]  eta: 0:04:31  lr: 0.000178  loss: 1.1203 (1.1042)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [200/431]  eta: 0:04:20  lr: 0.000178  loss: 1.1110 (1.1059)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [210/431]  eta: 0:04:08  lr: 0.000178  loss: 1.1110 (1.1064)  time: 1.1236  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:199]  [220/431]  eta: 0:03:57  lr: 0.000178  loss: 1.1205 (1.1070)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [230/431]  eta: 0:03:46  lr: 0.000178  loss: 1.1277 (1.1076)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [240/431]  eta: 0:03:34  lr: 0.000178  loss: 1.0033 (1.1043)  time: 1.1244  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:199]  [250/431]  eta: 0:03:23  lr: 0.000178  loss: 1.0224 (1.1038)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [260/431]  eta: 0:03:12  lr: 0.000178  loss: 1.1002 (1.1043)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [270/431]  eta: 0:03:00  lr: 0.000178  loss: 1.1172 (1.1087)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [280/431]  eta: 0:02:49  lr: 0.000178  loss: 1.0781 (1.1078)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [290/431]  eta: 0:02:38  lr: 0.000178  loss: 1.0656 (1.1080)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [300/431]  eta: 0:02:27  lr: 0.000178  loss: 1.0654 (1.1075)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [310/431]  eta: 0:02:15  lr: 0.000178  loss: 1.0601 (1.1062)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [320/431]  eta: 0:02:04  lr: 0.000178  loss: 1.0846 (1.1064)  time: 1.1116  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:199]  [330/431]  eta: 0:01:53  lr: 0.000178  loss: 1.1033 (1.1071)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [340/431]  eta: 0:01:41  lr: 0.000178  loss: 1.1033 (1.1081)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [350/431]  eta: 0:01:30  lr: 0.000178  loss: 1.1255 (1.1097)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [360/431]  eta: 0:01:19  lr: 0.000178  loss: 1.0927 (1.1088)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [370/431]  eta: 0:01:08  lr: 0.000178  loss: 1.0433 (1.1085)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [380/431]  eta: 0:00:57  lr: 0.000178  loss: 1.0685 (1.1078)  time: 1.1224  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:199]  [390/431]  eta: 0:00:45  lr: 0.000178  loss: 1.0685 (1.1096)  time: 1.1231  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:199]  [400/431]  eta: 0:00:34  lr: 0.000178  loss: 1.0660 (1.1084)  time: 1.1180  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:199]  [410/431]  eta: 0:00:23  lr: 0.000178  loss: 1.0660 (1.1080)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:199]  [420/431]  eta: 0:00:12  lr: 0.000178  loss: 1.1122 (1.1080)  time: 1.1257  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:199]  [430/431]  eta: 0:00:01  lr: 0.000178  loss: 1.1552 (1.1094)  time: 1.1194  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:199] Total time: 0:08:03 (1.1214 s / it)\n",
      "Averaged stats: lr: 0.000178  loss: 1.1552 (1.1094)\n",
      "Valid: [epoch:199]  [ 0/14]  eta: 0:00:35  loss: 0.9837 (0.9837)  time: 2.5562  data: 2.4001  max mem: 15925\n",
      "Valid: [epoch:199]  [13/14]  eta: 0:00:00  loss: 1.0383 (1.0494)  time: 0.2738  data: 0.1715  max mem: 15925\n",
      "Valid: [epoch:199] Total time: 0:00:04 (0.2912 s / it)\n",
      "Averaged stats: loss: 1.0383 (1.0494)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_199_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:200]  [  0/431]  eta: 0:34:14  lr: 0.000178  loss: 1.0868 (1.0868)  time: 4.7679  data: 3.5831  max mem: 15925\n",
      "Train: [epoch:200]  [ 10/431]  eta: 0:09:46  lr: 0.000178  loss: 1.1406 (1.1771)  time: 1.3921  data: 0.3260  max mem: 15925\n",
      "Train: [epoch:200]  [ 20/431]  eta: 0:08:28  lr: 0.000178  loss: 1.1406 (1.1824)  time: 1.0620  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:200]  [ 30/431]  eta: 0:07:57  lr: 0.000178  loss: 1.1686 (1.1724)  time: 1.0802  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [ 40/431]  eta: 0:07:36  lr: 0.000178  loss: 1.1265 (1.1586)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [ 50/431]  eta: 0:07:20  lr: 0.000178  loss: 1.1032 (1.1465)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [ 60/431]  eta: 0:07:06  lr: 0.000178  loss: 1.0234 (1.1245)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [ 70/431]  eta: 0:06:53  lr: 0.000178  loss: 1.0577 (1.1248)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [ 80/431]  eta: 0:06:40  lr: 0.000178  loss: 1.0720 (1.1202)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [ 90/431]  eta: 0:06:28  lr: 0.000178  loss: 1.0510 (1.1142)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [100/431]  eta: 0:06:16  lr: 0.000178  loss: 1.0771 (1.1157)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [110/431]  eta: 0:06:04  lr: 0.000178  loss: 1.0771 (1.1123)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [120/431]  eta: 0:05:52  lr: 0.000178  loss: 1.0531 (1.1103)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [130/431]  eta: 0:05:41  lr: 0.000178  loss: 1.0447 (1.1067)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [140/431]  eta: 0:05:29  lr: 0.000178  loss: 1.0235 (1.1022)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [150/431]  eta: 0:05:18  lr: 0.000178  loss: 1.0530 (1.1012)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [160/431]  eta: 0:05:06  lr: 0.000178  loss: 1.0702 (1.1015)  time: 1.1308  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [170/431]  eta: 0:04:55  lr: 0.000178  loss: 1.0445 (1.1001)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [180/431]  eta: 0:04:43  lr: 0.000178  loss: 1.0485 (1.0987)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [190/431]  eta: 0:04:32  lr: 0.000178  loss: 1.0506 (1.1003)  time: 1.1244  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:200]  [200/431]  eta: 0:04:20  lr: 0.000178  loss: 1.0658 (1.0977)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [210/431]  eta: 0:04:09  lr: 0.000178  loss: 1.0563 (1.0988)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [220/431]  eta: 0:03:58  lr: 0.000178  loss: 1.0876 (1.1009)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [230/431]  eta: 0:03:46  lr: 0.000178  loss: 1.0990 (1.1014)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [240/431]  eta: 0:03:35  lr: 0.000178  loss: 1.0691 (1.1006)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [250/431]  eta: 0:03:24  lr: 0.000178  loss: 1.0709 (1.1006)  time: 1.1238  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:200]  [260/431]  eta: 0:03:12  lr: 0.000178  loss: 1.0469 (1.0997)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [270/431]  eta: 0:03:01  lr: 0.000178  loss: 1.0524 (1.0984)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [280/431]  eta: 0:02:50  lr: 0.000178  loss: 1.0524 (1.0977)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [290/431]  eta: 0:02:38  lr: 0.000178  loss: 1.0742 (1.0990)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [300/431]  eta: 0:02:27  lr: 0.000178  loss: 1.0720 (1.0986)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [310/431]  eta: 0:02:16  lr: 0.000178  loss: 1.0483 (1.1001)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [320/431]  eta: 0:02:04  lr: 0.000178  loss: 1.0106 (1.0977)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [330/431]  eta: 0:01:53  lr: 0.000178  loss: 1.0532 (1.0986)  time: 1.1175  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:200]  [340/431]  eta: 0:01:42  lr: 0.000178  loss: 1.1082 (1.1003)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [350/431]  eta: 0:01:31  lr: 0.000178  loss: 1.1581 (1.1019)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [360/431]  eta: 0:01:19  lr: 0.000178  loss: 1.1414 (1.1010)  time: 1.1303  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [370/431]  eta: 0:01:08  lr: 0.000178  loss: 1.1358 (1.1027)  time: 1.1267  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [380/431]  eta: 0:00:57  lr: 0.000178  loss: 1.1657 (1.1047)  time: 1.1388  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [390/431]  eta: 0:00:46  lr: 0.000178  loss: 1.1619 (1.1062)  time: 1.1388  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:200]  [400/431]  eta: 0:00:34  lr: 0.000178  loss: 1.1649 (1.1078)  time: 1.1276  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:200]  [410/431]  eta: 0:00:23  lr: 0.000178  loss: 1.0945 (1.1072)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:200]  [420/431]  eta: 0:00:12  lr: 0.000178  loss: 1.0945 (1.1074)  time: 1.1122  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:200]  [430/431]  eta: 0:00:01  lr: 0.000178  loss: 1.1226 (1.1079)  time: 1.1102  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:200] Total time: 0:08:05 (1.1256 s / it)\n",
      "Averaged stats: lr: 0.000178  loss: 1.1226 (1.1079)\n",
      "Valid: [epoch:200]  [ 0/14]  eta: 0:00:34  loss: 1.1021 (1.1021)  time: 2.4740  data: 2.3181  max mem: 15925\n",
      "Valid: [epoch:200]  [13/14]  eta: 0:00:00  loss: 1.0425 (1.0504)  time: 0.2748  data: 0.1666  max mem: 15925\n",
      "Valid: [epoch:200] Total time: 0:00:04 (0.2999 s / it)\n",
      "Averaged stats: loss: 1.0425 (1.0504)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_200_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:201]  [  0/431]  eta: 0:32:46  lr: 0.000178  loss: 1.2757 (1.2757)  time: 4.5626  data: 3.3609  max mem: 15925\n",
      "Train: [epoch:201]  [ 10/431]  eta: 0:09:32  lr: 0.000178  loss: 1.1096 (1.1320)  time: 1.3597  data: 0.3058  max mem: 15925\n",
      "Train: [epoch:201]  [ 20/431]  eta: 0:08:24  lr: 0.000178  loss: 1.0925 (1.1058)  time: 1.0618  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [ 30/431]  eta: 0:07:51  lr: 0.000178  loss: 1.0991 (1.1206)  time: 1.0759  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [ 40/431]  eta: 0:07:31  lr: 0.000178  loss: 1.1261 (1.1141)  time: 1.0791  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [ 50/431]  eta: 0:07:16  lr: 0.000178  loss: 1.0948 (1.1115)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [ 60/431]  eta: 0:07:03  lr: 0.000178  loss: 1.0820 (1.1049)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [ 70/431]  eta: 0:06:51  lr: 0.000178  loss: 1.0199 (1.1012)  time: 1.1198  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [ 80/431]  eta: 0:06:39  lr: 0.000178  loss: 1.0351 (1.0951)  time: 1.1226  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [ 90/431]  eta: 0:06:27  lr: 0.000178  loss: 1.0351 (1.0991)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [100/431]  eta: 0:06:15  lr: 0.000178  loss: 1.0421 (1.0971)  time: 1.1271  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [110/431]  eta: 0:06:03  lr: 0.000178  loss: 1.0375 (1.0949)  time: 1.1200  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [120/431]  eta: 0:05:52  lr: 0.000178  loss: 1.1033 (1.0980)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [130/431]  eta: 0:05:41  lr: 0.000178  loss: 1.0388 (1.0959)  time: 1.1347  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [140/431]  eta: 0:05:29  lr: 0.000178  loss: 1.0429 (1.0945)  time: 1.1368  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [150/431]  eta: 0:05:18  lr: 0.000178  loss: 1.0703 (1.0961)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [160/431]  eta: 0:05:06  lr: 0.000178  loss: 1.1046 (1.0989)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [170/431]  eta: 0:04:55  lr: 0.000178  loss: 1.1382 (1.1015)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [180/431]  eta: 0:04:43  lr: 0.000178  loss: 1.0663 (1.0999)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [190/431]  eta: 0:04:32  lr: 0.000178  loss: 1.0663 (1.1021)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [200/431]  eta: 0:04:20  lr: 0.000178  loss: 1.0978 (1.1028)  time: 1.1250  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [210/431]  eta: 0:04:09  lr: 0.000178  loss: 1.0978 (1.1007)  time: 1.1300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [220/431]  eta: 0:03:58  lr: 0.000178  loss: 1.0883 (1.1023)  time: 1.1310  data: 0.0006  max mem: 15925\n",
      "Train: [epoch:201]  [230/431]  eta: 0:03:47  lr: 0.000178  loss: 1.0890 (1.1034)  time: 1.1293  data: 0.0006  max mem: 15925\n",
      "Train: [epoch:201]  [240/431]  eta: 0:03:35  lr: 0.000178  loss: 1.0949 (1.1041)  time: 1.1275  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [250/431]  eta: 0:03:24  lr: 0.000178  loss: 1.1307 (1.1067)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [260/431]  eta: 0:03:13  lr: 0.000178  loss: 1.1307 (1.1080)  time: 1.1308  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [270/431]  eta: 0:03:01  lr: 0.000178  loss: 1.1271 (1.1095)  time: 1.1449  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [280/431]  eta: 0:02:50  lr: 0.000178  loss: 1.0724 (1.1090)  time: 1.1372  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [290/431]  eta: 0:02:39  lr: 0.000178  loss: 1.0743 (1.1098)  time: 1.1356  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [300/431]  eta: 0:02:28  lr: 0.000178  loss: 1.0988 (1.1108)  time: 1.1370  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [310/431]  eta: 0:02:16  lr: 0.000178  loss: 1.1232 (1.1119)  time: 1.1308  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [320/431]  eta: 0:02:05  lr: 0.000178  loss: 1.1464 (1.1134)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [330/431]  eta: 0:01:54  lr: 0.000178  loss: 1.1057 (1.1135)  time: 1.1239  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [340/431]  eta: 0:01:42  lr: 0.000178  loss: 1.0980 (1.1148)  time: 1.1292  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [350/431]  eta: 0:01:31  lr: 0.000178  loss: 1.0977 (1.1147)  time: 1.1110  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [360/431]  eta: 0:01:20  lr: 0.000178  loss: 1.0637 (1.1133)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [370/431]  eta: 0:01:08  lr: 0.000178  loss: 1.0515 (1.1128)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [380/431]  eta: 0:00:57  lr: 0.000178  loss: 1.0549 (1.1121)  time: 1.1234  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [390/431]  eta: 0:00:46  lr: 0.000178  loss: 1.0472 (1.1115)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [400/431]  eta: 0:00:34  lr: 0.000178  loss: 1.0547 (1.1114)  time: 1.1239  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:201]  [410/431]  eta: 0:00:23  lr: 0.000178  loss: 1.0547 (1.1102)  time: 1.1337  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201]  [420/431]  eta: 0:00:12  lr: 0.000178  loss: 1.0557 (1.1091)  time: 1.1208  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:201]  [430/431]  eta: 0:00:01  lr: 0.000178  loss: 1.0764 (1.1089)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:201] Total time: 0:08:06 (1.1283 s / it)\n",
      "Averaged stats: lr: 0.000178  loss: 1.0764 (1.1089)\n",
      "Valid: [epoch:201]  [ 0/14]  eta: 0:00:36  loss: 0.9814 (0.9814)  time: 2.5747  data: 2.4464  max mem: 15925\n",
      "Valid: [epoch:201]  [13/14]  eta: 0:00:00  loss: 1.0348 (1.0463)  time: 0.2858  data: 0.1748  max mem: 15925\n",
      "Valid: [epoch:201] Total time: 0:00:04 (0.3020 s / it)\n",
      "Averaged stats: loss: 1.0348 (1.0463)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_201_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:202]  [  0/431]  eta: 0:32:43  lr: 0.000178  loss: 1.2843 (1.2843)  time: 4.5548  data: 3.4093  max mem: 15925\n",
      "Train: [epoch:202]  [ 10/431]  eta: 0:09:40  lr: 0.000178  loss: 1.0957 (1.1196)  time: 1.3795  data: 0.3102  max mem: 15925\n",
      "Train: [epoch:202]  [ 20/431]  eta: 0:08:28  lr: 0.000178  loss: 1.1209 (1.1756)  time: 1.0705  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [ 30/431]  eta: 0:08:00  lr: 0.000178  loss: 1.1463 (1.1458)  time: 1.1004  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [ 40/431]  eta: 0:07:39  lr: 0.000178  loss: 1.0525 (1.1319)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [ 50/431]  eta: 0:07:24  lr: 0.000178  loss: 1.0525 (1.1237)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [ 60/431]  eta: 0:07:10  lr: 0.000178  loss: 1.0921 (1.1202)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [ 70/431]  eta: 0:06:56  lr: 0.000178  loss: 1.1163 (1.1212)  time: 1.1237  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [ 80/431]  eta: 0:06:42  lr: 0.000178  loss: 1.1218 (1.1231)  time: 1.1100  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [ 90/431]  eta: 0:06:30  lr: 0.000178  loss: 1.1249 (1.1288)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [100/431]  eta: 0:06:18  lr: 0.000178  loss: 1.0745 (1.1204)  time: 1.1305  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [110/431]  eta: 0:06:06  lr: 0.000178  loss: 1.0121 (1.1162)  time: 1.1292  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [120/431]  eta: 0:05:54  lr: 0.000178  loss: 1.0533 (1.1127)  time: 1.1140  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [130/431]  eta: 0:05:42  lr: 0.000178  loss: 1.0533 (1.1128)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [140/431]  eta: 0:05:30  lr: 0.000178  loss: 1.0373 (1.1072)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [150/431]  eta: 0:05:18  lr: 0.000178  loss: 1.0122 (1.1062)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [160/431]  eta: 0:05:07  lr: 0.000178  loss: 1.0982 (1.1075)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [170/431]  eta: 0:04:56  lr: 0.000178  loss: 1.0665 (1.1032)  time: 1.1416  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [180/431]  eta: 0:04:44  lr: 0.000178  loss: 1.0004 (1.0977)  time: 1.1445  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [190/431]  eta: 0:04:33  lr: 0.000178  loss: 1.0697 (1.0989)  time: 1.1387  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [200/431]  eta: 0:04:21  lr: 0.000178  loss: 1.0815 (1.0981)  time: 1.1296  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [210/431]  eta: 0:04:10  lr: 0.000178  loss: 1.1065 (1.1018)  time: 1.1322  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [220/431]  eta: 0:03:59  lr: 0.000178  loss: 1.1360 (1.1025)  time: 1.1371  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [230/431]  eta: 0:03:47  lr: 0.000178  loss: 1.1219 (1.1033)  time: 1.1271  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [240/431]  eta: 0:03:36  lr: 0.000178  loss: 1.1156 (1.1039)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [250/431]  eta: 0:03:25  lr: 0.000178  loss: 1.1447 (1.1080)  time: 1.1275  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [260/431]  eta: 0:03:13  lr: 0.000178  loss: 1.1471 (1.1079)  time: 1.1366  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [270/431]  eta: 0:03:02  lr: 0.000178  loss: 1.0978 (1.1080)  time: 1.1305  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [280/431]  eta: 0:02:51  lr: 0.000178  loss: 1.0991 (1.1088)  time: 1.1260  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [290/431]  eta: 0:02:39  lr: 0.000178  loss: 1.1118 (1.1079)  time: 1.1185  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [300/431]  eta: 0:02:28  lr: 0.000178  loss: 1.0696 (1.1067)  time: 1.1271  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [310/431]  eta: 0:02:16  lr: 0.000178  loss: 1.0677 (1.1056)  time: 1.1295  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [320/431]  eta: 0:02:05  lr: 0.000178  loss: 1.0933 (1.1062)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [330/431]  eta: 0:01:54  lr: 0.000178  loss: 1.1150 (1.1087)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [340/431]  eta: 0:01:42  lr: 0.000178  loss: 1.1150 (1.1083)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [350/431]  eta: 0:01:31  lr: 0.000178  loss: 1.1166 (1.1104)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [360/431]  eta: 0:01:20  lr: 0.000178  loss: 1.0985 (1.1085)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [370/431]  eta: 0:01:08  lr: 0.000178  loss: 1.0342 (1.1079)  time: 1.1122  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [380/431]  eta: 0:00:57  lr: 0.000178  loss: 1.0388 (1.1070)  time: 1.1098  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [390/431]  eta: 0:00:46  lr: 0.000178  loss: 1.0529 (1.1069)  time: 1.1280  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:202]  [400/431]  eta: 0:00:34  lr: 0.000178  loss: 1.0854 (1.1080)  time: 1.1319  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [410/431]  eta: 0:00:23  lr: 0.000178  loss: 1.1189 (1.1089)  time: 1.1294  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [420/431]  eta: 0:00:12  lr: 0.000178  loss: 1.1521 (1.1092)  time: 1.1331  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202]  [430/431]  eta: 0:00:01  lr: 0.000178  loss: 1.0726 (1.1089)  time: 1.1304  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:202] Total time: 0:08:06 (1.1292 s / it)\n",
      "Averaged stats: lr: 0.000178  loss: 1.0726 (1.1089)\n",
      "Valid: [epoch:202]  [ 0/14]  eta: 0:00:37  loss: 1.0024 (1.0024)  time: 2.6707  data: 2.5253  max mem: 15925\n",
      "Valid: [epoch:202]  [13/14]  eta: 0:00:00  loss: 1.0319 (1.0438)  time: 0.3013  data: 0.1805  max mem: 15925\n",
      "Valid: [epoch:202] Total time: 0:00:04 (0.3169 s / it)\n",
      "Averaged stats: loss: 1.0319 (1.0438)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_202_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:203]  [  0/431]  eta: 0:29:45  lr: 0.000177  loss: 1.1501 (1.1501)  time: 4.1421  data: 2.9429  max mem: 15925\n",
      "Train: [epoch:203]  [ 10/431]  eta: 0:09:31  lr: 0.000177  loss: 1.1501 (1.1629)  time: 1.3575  data: 0.2678  max mem: 15925\n",
      "Train: [epoch:203]  [ 20/431]  eta: 0:08:21  lr: 0.000177  loss: 1.1152 (1.1451)  time: 1.0748  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [ 30/431]  eta: 0:07:53  lr: 0.000177  loss: 1.1114 (1.1163)  time: 1.0845  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [ 40/431]  eta: 0:07:36  lr: 0.000177  loss: 1.1114 (1.1128)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [ 50/431]  eta: 0:07:20  lr: 0.000177  loss: 1.1216 (1.1151)  time: 1.1180  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:203]  [ 60/431]  eta: 0:07:06  lr: 0.000177  loss: 1.0494 (1.1019)  time: 1.1175  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:203]  [ 70/431]  eta: 0:06:53  lr: 0.000177  loss: 1.0494 (1.1049)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [ 80/431]  eta: 0:06:39  lr: 0.000177  loss: 1.0836 (1.1041)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [ 90/431]  eta: 0:06:27  lr: 0.000177  loss: 1.0742 (1.1045)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [100/431]  eta: 0:06:15  lr: 0.000177  loss: 1.1185 (1.1080)  time: 1.1155  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:203]  [110/431]  eta: 0:06:04  lr: 0.000177  loss: 1.0953 (1.1061)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [120/431]  eta: 0:05:52  lr: 0.000177  loss: 1.0556 (1.1052)  time: 1.1256  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [130/431]  eta: 0:05:40  lr: 0.000177  loss: 1.0568 (1.1052)  time: 1.1245  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [140/431]  eta: 0:05:29  lr: 0.000177  loss: 1.0577 (1.1015)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [150/431]  eta: 0:05:17  lr: 0.000177  loss: 1.0396 (1.1024)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [160/431]  eta: 0:05:06  lr: 0.000177  loss: 1.1070 (1.1049)  time: 1.1265  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [170/431]  eta: 0:04:55  lr: 0.000177  loss: 1.1323 (1.1082)  time: 1.1311  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [180/431]  eta: 0:04:43  lr: 0.000177  loss: 1.0791 (1.1067)  time: 1.1283  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [190/431]  eta: 0:04:32  lr: 0.000177  loss: 1.0704 (1.1072)  time: 1.1178  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [200/431]  eta: 0:04:20  lr: 0.000177  loss: 1.0531 (1.1024)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [210/431]  eta: 0:04:09  lr: 0.000177  loss: 1.0318 (1.1018)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [220/431]  eta: 0:03:57  lr: 0.000177  loss: 1.1079 (1.1023)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [230/431]  eta: 0:03:46  lr: 0.000177  loss: 1.0691 (1.1027)  time: 1.1313  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [240/431]  eta: 0:03:35  lr: 0.000177  loss: 1.0721 (1.1024)  time: 1.1346  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [250/431]  eta: 0:03:24  lr: 0.000177  loss: 1.0721 (1.1028)  time: 1.1296  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [260/431]  eta: 0:03:12  lr: 0.000177  loss: 1.0701 (1.1032)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [270/431]  eta: 0:03:01  lr: 0.000177  loss: 1.0773 (1.1045)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [280/431]  eta: 0:02:50  lr: 0.000177  loss: 1.0343 (1.1033)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [290/431]  eta: 0:02:38  lr: 0.000177  loss: 1.0334 (1.1015)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [300/431]  eta: 0:02:27  lr: 0.000177  loss: 1.0428 (1.1031)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [310/431]  eta: 0:02:16  lr: 0.000177  loss: 1.0646 (1.1028)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [320/431]  eta: 0:02:04  lr: 0.000177  loss: 1.0823 (1.1042)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [330/431]  eta: 0:01:53  lr: 0.000177  loss: 1.1212 (1.1057)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [340/431]  eta: 0:01:42  lr: 0.000177  loss: 1.1908 (1.1087)  time: 1.1148  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [350/431]  eta: 0:01:31  lr: 0.000177  loss: 1.1738 (1.1092)  time: 1.1197  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [360/431]  eta: 0:01:19  lr: 0.000177  loss: 1.0893 (1.1092)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [370/431]  eta: 0:01:08  lr: 0.000177  loss: 1.0924 (1.1096)  time: 1.1307  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [380/431]  eta: 0:00:57  lr: 0.000177  loss: 1.1258 (1.1111)  time: 1.1404  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [390/431]  eta: 0:00:46  lr: 0.000177  loss: 1.0972 (1.1098)  time: 1.1238  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [400/431]  eta: 0:00:34  lr: 0.000177  loss: 1.0596 (1.1093)  time: 1.1114  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:203]  [410/431]  eta: 0:00:23  lr: 0.000177  loss: 1.0923 (1.1096)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [420/431]  eta: 0:00:12  lr: 0.000177  loss: 1.0987 (1.1090)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203]  [430/431]  eta: 0:00:01  lr: 0.000177  loss: 1.1032 (1.1090)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:203] Total time: 0:08:04 (1.1250 s / it)\n",
      "Averaged stats: lr: 0.000177  loss: 1.1032 (1.1090)\n",
      "Valid: [epoch:203]  [ 0/14]  eta: 0:00:37  loss: 1.0270 (1.0270)  time: 2.6690  data: 2.5411  max mem: 15925\n",
      "Valid: [epoch:203]  [13/14]  eta: 0:00:00  loss: 1.0472 (1.0588)  time: 0.2777  data: 0.1816  max mem: 15925\n",
      "Valid: [epoch:203] Total time: 0:00:04 (0.2937 s / it)\n",
      "Averaged stats: loss: 1.0472 (1.0588)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_203_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.059%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:204]  [  0/431]  eta: 0:33:10  lr: 0.000177  loss: 1.2781 (1.2781)  time: 4.6192  data: 3.4800  max mem: 15925\n",
      "Train: [epoch:204]  [ 10/431]  eta: 0:09:48  lr: 0.000177  loss: 1.1694 (1.1568)  time: 1.3980  data: 0.3166  max mem: 15925\n",
      "Train: [epoch:204]  [ 20/431]  eta: 0:08:29  lr: 0.000177  loss: 1.0915 (1.1260)  time: 1.0718  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [ 30/431]  eta: 0:07:58  lr: 0.000177  loss: 1.0630 (1.1200)  time: 1.0793  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [ 40/431]  eta: 0:07:35  lr: 0.000177  loss: 1.0539 (1.1055)  time: 1.0841  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [ 50/431]  eta: 0:07:19  lr: 0.000177  loss: 1.0505 (1.1015)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [ 60/431]  eta: 0:07:04  lr: 0.000177  loss: 1.0504 (1.0992)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [ 70/431]  eta: 0:06:51  lr: 0.000177  loss: 1.0767 (1.0988)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [ 80/431]  eta: 0:06:39  lr: 0.000177  loss: 1.0899 (1.1045)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [ 90/431]  eta: 0:06:27  lr: 0.000177  loss: 1.0577 (1.0995)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [100/431]  eta: 0:06:15  lr: 0.000177  loss: 1.0577 (1.1021)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [110/431]  eta: 0:06:03  lr: 0.000177  loss: 1.0687 (1.1050)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [120/431]  eta: 0:05:51  lr: 0.000177  loss: 1.0620 (1.1036)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [130/431]  eta: 0:05:39  lr: 0.000177  loss: 1.0724 (1.1053)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [140/431]  eta: 0:05:28  lr: 0.000177  loss: 1.0860 (1.1052)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [150/431]  eta: 0:05:16  lr: 0.000177  loss: 1.0773 (1.1073)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [160/431]  eta: 0:05:05  lr: 0.000177  loss: 1.0773 (1.1057)  time: 1.1169  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:204]  [170/431]  eta: 0:04:53  lr: 0.000177  loss: 1.0647 (1.1028)  time: 1.1120  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:204]  [180/431]  eta: 0:04:42  lr: 0.000177  loss: 1.0306 (1.0994)  time: 1.1065  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:204]  [190/431]  eta: 0:04:30  lr: 0.000177  loss: 1.0701 (1.1063)  time: 1.1121  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:204]  [200/431]  eta: 0:04:19  lr: 0.000177  loss: 1.0973 (1.1045)  time: 1.1115  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:204]  [210/431]  eta: 0:04:08  lr: 0.000177  loss: 1.1035 (1.1069)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [220/431]  eta: 0:03:56  lr: 0.000177  loss: 1.0775 (1.1075)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [230/431]  eta: 0:03:45  lr: 0.000177  loss: 1.0952 (1.1102)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [240/431]  eta: 0:03:34  lr: 0.000177  loss: 1.1292 (1.1118)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [250/431]  eta: 0:03:22  lr: 0.000177  loss: 1.1292 (1.1112)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [260/431]  eta: 0:03:11  lr: 0.000177  loss: 1.0684 (1.1107)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [270/431]  eta: 0:03:00  lr: 0.000177  loss: 1.0753 (1.1114)  time: 1.1101  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:204]  [280/431]  eta: 0:02:49  lr: 0.000177  loss: 1.1213 (1.1110)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [290/431]  eta: 0:02:37  lr: 0.000177  loss: 1.0712 (1.1133)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [300/431]  eta: 0:02:26  lr: 0.000177  loss: 1.1331 (1.1143)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [310/431]  eta: 0:02:15  lr: 0.000177  loss: 1.0906 (1.1133)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [320/431]  eta: 0:02:04  lr: 0.000177  loss: 1.0593 (1.1132)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [330/431]  eta: 0:01:53  lr: 0.000177  loss: 1.1235 (1.1136)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [340/431]  eta: 0:01:41  lr: 0.000177  loss: 1.0918 (1.1137)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [350/431]  eta: 0:01:30  lr: 0.000177  loss: 1.0647 (1.1135)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [360/431]  eta: 0:01:19  lr: 0.000177  loss: 1.0592 (1.1126)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [370/431]  eta: 0:01:08  lr: 0.000177  loss: 1.0592 (1.1116)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [380/431]  eta: 0:00:57  lr: 0.000177  loss: 1.0491 (1.1112)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [390/431]  eta: 0:00:45  lr: 0.000177  loss: 1.0451 (1.1105)  time: 1.1214  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:204]  [400/431]  eta: 0:00:34  lr: 0.000177  loss: 1.0756 (1.1103)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [410/431]  eta: 0:00:23  lr: 0.000177  loss: 1.0784 (1.1097)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [420/431]  eta: 0:00:12  lr: 0.000177  loss: 1.0738 (1.1103)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:204]  [430/431]  eta: 0:00:01  lr: 0.000177  loss: 1.1048 (1.1114)  time: 1.1006  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:204] Total time: 0:08:01 (1.1182 s / it)\n",
      "Averaged stats: lr: 0.000177  loss: 1.1048 (1.1114)\n",
      "Valid: [epoch:204]  [ 0/14]  eta: 0:00:34  loss: 1.0376 (1.0376)  time: 2.4913  data: 2.3339  max mem: 15925\n",
      "Valid: [epoch:204]  [13/14]  eta: 0:00:00  loss: 1.0376 (1.0489)  time: 0.2582  data: 0.1668  max mem: 15925\n",
      "Valid: [epoch:204] Total time: 0:00:03 (0.2731 s / it)\n",
      "Averaged stats: loss: 1.0376 (1.0489)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_204_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:205]  [  0/431]  eta: 0:31:34  lr: 0.000177  loss: 1.0855 (1.0855)  time: 4.3967  data: 3.1649  max mem: 15925\n",
      "Train: [epoch:205]  [ 10/431]  eta: 0:09:34  lr: 0.000177  loss: 1.0164 (1.0904)  time: 1.3636  data: 0.2879  max mem: 15925\n",
      "Train: [epoch:205]  [ 20/431]  eta: 0:08:22  lr: 0.000177  loss: 1.0164 (1.0894)  time: 1.0631  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [ 30/431]  eta: 0:07:49  lr: 0.000177  loss: 1.0132 (1.0628)  time: 1.0655  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [ 40/431]  eta: 0:07:32  lr: 0.000177  loss: 1.0494 (1.0782)  time: 1.0885  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [ 50/431]  eta: 0:07:16  lr: 0.000177  loss: 1.1073 (1.0846)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [ 60/431]  eta: 0:07:02  lr: 0.000177  loss: 1.1084 (1.0894)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [ 70/431]  eta: 0:06:50  lr: 0.000177  loss: 1.0718 (1.0848)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [ 80/431]  eta: 0:06:38  lr: 0.000177  loss: 1.0749 (1.0929)  time: 1.1232  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [ 90/431]  eta: 0:06:26  lr: 0.000177  loss: 1.0749 (1.0887)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [100/431]  eta: 0:06:14  lr: 0.000177  loss: 1.0510 (1.0921)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [110/431]  eta: 0:06:03  lr: 0.000177  loss: 1.0510 (1.0898)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [120/431]  eta: 0:05:50  lr: 0.000177  loss: 1.1155 (1.0966)  time: 1.1120  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [130/431]  eta: 0:05:39  lr: 0.000177  loss: 1.0642 (1.0918)  time: 1.1099  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [140/431]  eta: 0:05:27  lr: 0.000177  loss: 1.0292 (1.0935)  time: 1.1176  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [150/431]  eta: 0:05:16  lr: 0.000177  loss: 1.1167 (1.0938)  time: 1.1147  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [160/431]  eta: 0:05:05  lr: 0.000177  loss: 1.0849 (1.0961)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [170/431]  eta: 0:04:53  lr: 0.000177  loss: 1.0334 (1.0957)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [180/431]  eta: 0:04:41  lr: 0.000177  loss: 1.0646 (1.0956)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [190/431]  eta: 0:04:30  lr: 0.000177  loss: 1.0743 (1.0954)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [200/431]  eta: 0:04:18  lr: 0.000177  loss: 1.1029 (1.0976)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [210/431]  eta: 0:04:07  lr: 0.000177  loss: 1.1029 (1.0985)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [220/431]  eta: 0:03:56  lr: 0.000177  loss: 1.0834 (1.0997)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [230/431]  eta: 0:03:44  lr: 0.000177  loss: 1.0465 (1.0971)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [240/431]  eta: 0:03:33  lr: 0.000177  loss: 1.0832 (1.0988)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [250/431]  eta: 0:03:22  lr: 0.000177  loss: 1.1281 (1.1009)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [260/431]  eta: 0:03:11  lr: 0.000177  loss: 1.1486 (1.1027)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [270/431]  eta: 0:02:59  lr: 0.000177  loss: 1.1278 (1.1039)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [280/431]  eta: 0:02:48  lr: 0.000177  loss: 1.1137 (1.1035)  time: 1.1139  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [290/431]  eta: 0:02:37  lr: 0.000177  loss: 1.0777 (1.1034)  time: 1.1138  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [300/431]  eta: 0:02:26  lr: 0.000177  loss: 1.0998 (1.1041)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [310/431]  eta: 0:02:15  lr: 0.000177  loss: 1.1223 (1.1049)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [320/431]  eta: 0:02:04  lr: 0.000177  loss: 1.0898 (1.1051)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [330/431]  eta: 0:01:52  lr: 0.000177  loss: 1.1023 (1.1086)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [340/431]  eta: 0:01:41  lr: 0.000177  loss: 1.1539 (1.1111)  time: 1.1195  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [350/431]  eta: 0:01:30  lr: 0.000177  loss: 1.1376 (1.1118)  time: 1.1379  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [360/431]  eta: 0:01:19  lr: 0.000177  loss: 1.0880 (1.1128)  time: 1.1430  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [370/431]  eta: 0:01:08  lr: 0.000177  loss: 1.0853 (1.1113)  time: 1.1284  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:205]  [380/431]  eta: 0:00:57  lr: 0.000177  loss: 1.0338 (1.1106)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [390/431]  eta: 0:00:45  lr: 0.000177  loss: 1.0506 (1.1098)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [400/431]  eta: 0:00:34  lr: 0.000177  loss: 1.0619 (1.1099)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [410/431]  eta: 0:00:23  lr: 0.000177  loss: 1.0619 (1.1103)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [420/431]  eta: 0:00:12  lr: 0.000177  loss: 1.0916 (1.1107)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205]  [430/431]  eta: 0:00:01  lr: 0.000177  loss: 1.1157 (1.1106)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:205] Total time: 0:08:02 (1.1194 s / it)\n",
      "Averaged stats: lr: 0.000177  loss: 1.1157 (1.1106)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:205]  [ 0/14]  eta: 0:00:35  loss: 0.9417 (0.9417)  time: 2.5489  data: 2.3915  max mem: 15925\n",
      "Valid: [epoch:205]  [13/14]  eta: 0:00:00  loss: 1.0418 (1.0516)  time: 0.2977  data: 0.1709  max mem: 15925\n",
      "Valid: [epoch:205] Total time: 0:00:04 (0.3141 s / it)\n",
      "Averaged stats: loss: 1.0418 (1.0516)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_205_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.052%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:206]  [  0/431]  eta: 0:31:40  lr: 0.000177  loss: 1.2160 (1.2160)  time: 4.4096  data: 3.1874  max mem: 15925\n",
      "Train: [epoch:206]  [ 10/431]  eta: 0:09:39  lr: 0.000177  loss: 1.1047 (1.1453)  time: 1.3770  data: 0.2900  max mem: 15925\n",
      "Train: [epoch:206]  [ 20/431]  eta: 0:08:26  lr: 0.000177  loss: 1.0894 (1.1341)  time: 1.0740  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [ 30/431]  eta: 0:07:56  lr: 0.000177  loss: 1.0677 (1.1242)  time: 1.0855  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [ 40/431]  eta: 0:07:36  lr: 0.000177  loss: 1.0522 (1.1263)  time: 1.1007  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [ 50/431]  eta: 0:07:21  lr: 0.000177  loss: 0.9990 (1.1182)  time: 1.1137  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [ 60/431]  eta: 0:07:08  lr: 0.000177  loss: 1.0012 (1.1102)  time: 1.1294  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [ 70/431]  eta: 0:06:56  lr: 0.000177  loss: 1.0726 (1.1155)  time: 1.1386  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [ 80/431]  eta: 0:06:42  lr: 0.000177  loss: 1.1058 (1.1133)  time: 1.1248  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [ 90/431]  eta: 0:06:30  lr: 0.000177  loss: 1.0801 (1.1116)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [100/431]  eta: 0:06:18  lr: 0.000177  loss: 1.0646 (1.1075)  time: 1.1269  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [110/431]  eta: 0:06:06  lr: 0.000177  loss: 1.0646 (1.1039)  time: 1.1264  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [120/431]  eta: 0:05:54  lr: 0.000177  loss: 1.0795 (1.1009)  time: 1.1145  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [130/431]  eta: 0:05:41  lr: 0.000177  loss: 1.0622 (1.0994)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [140/431]  eta: 0:05:29  lr: 0.000177  loss: 1.0623 (1.0981)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [150/431]  eta: 0:05:17  lr: 0.000177  loss: 1.0979 (1.1032)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [160/431]  eta: 0:05:06  lr: 0.000177  loss: 1.1181 (1.1053)  time: 1.1118  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [170/431]  eta: 0:04:54  lr: 0.000177  loss: 1.1476 (1.1080)  time: 1.1192  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [180/431]  eta: 0:04:43  lr: 0.000177  loss: 1.1476 (1.1084)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [190/431]  eta: 0:04:31  lr: 0.000177  loss: 1.0975 (1.1101)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [200/431]  eta: 0:04:20  lr: 0.000177  loss: 1.0582 (1.1091)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [210/431]  eta: 0:04:09  lr: 0.000177  loss: 1.1085 (1.1117)  time: 1.1293  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [220/431]  eta: 0:03:58  lr: 0.000177  loss: 1.1097 (1.1115)  time: 1.1339  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [230/431]  eta: 0:03:46  lr: 0.000177  loss: 1.0542 (1.1093)  time: 1.1306  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [240/431]  eta: 0:03:35  lr: 0.000177  loss: 1.0440 (1.1083)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [250/431]  eta: 0:03:24  lr: 0.000177  loss: 1.0990 (1.1098)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [260/431]  eta: 0:03:12  lr: 0.000177  loss: 1.1258 (1.1114)  time: 1.1362  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [270/431]  eta: 0:03:01  lr: 0.000177  loss: 1.1244 (1.1102)  time: 1.1380  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [280/431]  eta: 0:02:50  lr: 0.000177  loss: 1.0371 (1.1094)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [290/431]  eta: 0:02:38  lr: 0.000177  loss: 1.0847 (1.1087)  time: 1.1096  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [300/431]  eta: 0:02:27  lr: 0.000177  loss: 1.0466 (1.1075)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [310/431]  eta: 0:02:16  lr: 0.000177  loss: 1.0419 (1.1073)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [320/431]  eta: 0:02:04  lr: 0.000177  loss: 1.0839 (1.1095)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [330/431]  eta: 0:01:53  lr: 0.000177  loss: 1.1523 (1.1117)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [340/431]  eta: 0:01:42  lr: 0.000177  loss: 1.1982 (1.1136)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [350/431]  eta: 0:01:31  lr: 0.000177  loss: 1.1551 (1.1146)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [360/431]  eta: 0:01:19  lr: 0.000177  loss: 1.0857 (1.1127)  time: 1.1085  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [370/431]  eta: 0:01:08  lr: 0.000177  loss: 1.0206 (1.1122)  time: 1.1097  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:206]  [380/431]  eta: 0:00:57  lr: 0.000177  loss: 1.0918 (1.1134)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [390/431]  eta: 0:00:46  lr: 0.000177  loss: 1.1026 (1.1128)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [400/431]  eta: 0:00:34  lr: 0.000177  loss: 1.0190 (1.1116)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [410/431]  eta: 0:00:23  lr: 0.000177  loss: 1.0250 (1.1105)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206]  [420/431]  eta: 0:00:12  lr: 0.000177  loss: 1.0666 (1.1119)  time: 1.0957  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:206]  [430/431]  eta: 0:00:01  lr: 0.000177  loss: 1.1127 (1.1121)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:206] Total time: 0:08:03 (1.1217 s / it)\n",
      "Averaged stats: lr: 0.000177  loss: 1.1127 (1.1121)\n",
      "Valid: [epoch:206]  [ 0/14]  eta: 0:00:36  loss: 0.9478 (0.9478)  time: 2.6345  data: 2.4369  max mem: 15925\n",
      "Valid: [epoch:206]  [13/14]  eta: 0:00:00  loss: 1.0486 (1.0563)  time: 0.3080  data: 0.1742  max mem: 15925\n",
      "Valid: [epoch:206] Total time: 0:00:04 (0.3257 s / it)\n",
      "Averaged stats: loss: 1.0486 (1.0563)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_206_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.056%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:207]  [  0/431]  eta: 0:32:28  lr: 0.000176  loss: 1.1564 (1.1564)  time: 4.5211  data: 3.3444  max mem: 15925\n",
      "Train: [epoch:207]  [ 10/431]  eta: 0:09:42  lr: 0.000176  loss: 1.0888 (1.1270)  time: 1.3824  data: 0.3043  max mem: 15925\n",
      "Train: [epoch:207]  [ 20/431]  eta: 0:08:28  lr: 0.000176  loss: 1.1153 (1.1585)  time: 1.0738  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [ 30/431]  eta: 0:07:58  lr: 0.000176  loss: 1.0509 (1.1248)  time: 1.0878  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [ 40/431]  eta: 0:07:36  lr: 0.000176  loss: 1.0358 (1.1164)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [ 50/431]  eta: 0:07:20  lr: 0.000176  loss: 1.0986 (1.1180)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [ 60/431]  eta: 0:07:06  lr: 0.000176  loss: 1.0986 (1.1121)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [ 70/431]  eta: 0:06:52  lr: 0.000176  loss: 1.1217 (1.1170)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [ 80/431]  eta: 0:06:38  lr: 0.000176  loss: 1.0733 (1.1092)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [ 90/431]  eta: 0:06:26  lr: 0.000176  loss: 1.0820 (1.1110)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [100/431]  eta: 0:06:13  lr: 0.000176  loss: 1.1347 (1.1137)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [110/431]  eta: 0:06:01  lr: 0.000176  loss: 1.0869 (1.1082)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [120/431]  eta: 0:05:50  lr: 0.000176  loss: 1.0980 (1.1116)  time: 1.1132  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:207]  [130/431]  eta: 0:05:39  lr: 0.000176  loss: 1.1338 (1.1121)  time: 1.1224  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [140/431]  eta: 0:05:28  lr: 0.000176  loss: 1.0826 (1.1103)  time: 1.1329  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [150/431]  eta: 0:05:16  lr: 0.000176  loss: 1.0716 (1.1102)  time: 1.1306  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [160/431]  eta: 0:05:05  lr: 0.000176  loss: 1.0716 (1.1112)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [170/431]  eta: 0:04:53  lr: 0.000176  loss: 1.0614 (1.1101)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [180/431]  eta: 0:04:42  lr: 0.000176  loss: 1.0603 (1.1099)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [190/431]  eta: 0:04:31  lr: 0.000176  loss: 1.0851 (1.1107)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [200/431]  eta: 0:04:19  lr: 0.000176  loss: 1.1235 (1.1111)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [210/431]  eta: 0:04:08  lr: 0.000176  loss: 1.1280 (1.1133)  time: 1.1275  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [220/431]  eta: 0:03:57  lr: 0.000176  loss: 1.1426 (1.1141)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [230/431]  eta: 0:03:45  lr: 0.000176  loss: 1.0633 (1.1118)  time: 1.1158  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [240/431]  eta: 0:03:34  lr: 0.000176  loss: 1.0633 (1.1135)  time: 1.1193  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [250/431]  eta: 0:03:23  lr: 0.000176  loss: 1.0976 (1.1119)  time: 1.1261  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:207]  [260/431]  eta: 0:03:12  lr: 0.000176  loss: 1.1374 (1.1159)  time: 1.1355  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [270/431]  eta: 0:03:00  lr: 0.000176  loss: 1.1613 (1.1169)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [280/431]  eta: 0:02:49  lr: 0.000176  loss: 1.1069 (1.1156)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [290/431]  eta: 0:02:38  lr: 0.000176  loss: 1.0459 (1.1143)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [300/431]  eta: 0:02:27  lr: 0.000176  loss: 1.0944 (1.1147)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [310/431]  eta: 0:02:15  lr: 0.000176  loss: 1.0961 (1.1143)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [320/431]  eta: 0:02:04  lr: 0.000176  loss: 1.1048 (1.1149)  time: 1.1071  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [330/431]  eta: 0:01:53  lr: 0.000176  loss: 1.1219 (1.1148)  time: 1.1143  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [340/431]  eta: 0:01:42  lr: 0.000176  loss: 1.1117 (1.1150)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [350/431]  eta: 0:01:30  lr: 0.000176  loss: 1.0806 (1.1135)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [360/431]  eta: 0:01:19  lr: 0.000176  loss: 1.0674 (1.1121)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [370/431]  eta: 0:01:08  lr: 0.000176  loss: 1.0848 (1.1132)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [380/431]  eta: 0:00:57  lr: 0.000176  loss: 1.1092 (1.1142)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [390/431]  eta: 0:00:46  lr: 0.000176  loss: 1.1285 (1.1149)  time: 1.1328  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [400/431]  eta: 0:00:34  lr: 0.000176  loss: 1.0906 (1.1142)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:207]  [410/431]  eta: 0:00:23  lr: 0.000176  loss: 1.0495 (1.1130)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [420/431]  eta: 0:00:12  lr: 0.000176  loss: 1.0706 (1.1128)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207]  [430/431]  eta: 0:00:01  lr: 0.000176  loss: 1.0901 (1.1123)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:207] Total time: 0:08:03 (1.1224 s / it)\n",
      "Averaged stats: lr: 0.000176  loss: 1.0901 (1.1123)\n",
      "Valid: [epoch:207]  [ 0/14]  eta: 0:00:34  loss: 1.1040 (1.1040)  time: 2.4679  data: 2.2994  max mem: 15925\n",
      "Valid: [epoch:207]  [13/14]  eta: 0:00:00  loss: 1.0379 (1.0501)  time: 0.2903  data: 0.1644  max mem: 15925\n",
      "Valid: [epoch:207] Total time: 0:00:04 (0.3056 s / it)\n",
      "Averaged stats: loss: 1.0379 (1.0501)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_207_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:208]  [  0/431]  eta: 0:33:08  lr: 0.000176  loss: 1.1678 (1.1678)  time: 4.6127  data: 3.0690  max mem: 15925\n",
      "Train: [epoch:208]  [ 10/431]  eta: 0:09:37  lr: 0.000176  loss: 1.0683 (1.1034)  time: 1.3711  data: 0.2792  max mem: 15925\n",
      "Train: [epoch:208]  [ 20/431]  eta: 0:08:23  lr: 0.000176  loss: 1.1407 (1.1539)  time: 1.0556  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [ 30/431]  eta: 0:07:52  lr: 0.000176  loss: 1.1407 (1.1441)  time: 1.0735  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [ 40/431]  eta: 0:07:32  lr: 0.000176  loss: 1.0676 (1.1240)  time: 1.0850  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [ 50/431]  eta: 0:07:16  lr: 0.000176  loss: 1.0509 (1.1124)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [ 60/431]  eta: 0:07:03  lr: 0.000176  loss: 1.0302 (1.1063)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [ 70/431]  eta: 0:06:50  lr: 0.000176  loss: 1.0759 (1.1048)  time: 1.1200  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [ 80/431]  eta: 0:06:39  lr: 0.000176  loss: 1.1301 (1.1111)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [ 90/431]  eta: 0:06:27  lr: 0.000176  loss: 1.1100 (1.1099)  time: 1.1265  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [100/431]  eta: 0:06:15  lr: 0.000176  loss: 1.0892 (1.1104)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [110/431]  eta: 0:06:03  lr: 0.000176  loss: 1.0687 (1.1048)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [120/431]  eta: 0:05:52  lr: 0.000176  loss: 1.0744 (1.1033)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [130/431]  eta: 0:05:40  lr: 0.000176  loss: 1.1371 (1.1063)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [140/431]  eta: 0:05:29  lr: 0.000176  loss: 1.1621 (1.1079)  time: 1.1227  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [150/431]  eta: 0:05:17  lr: 0.000176  loss: 1.1105 (1.1084)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [160/431]  eta: 0:05:06  lr: 0.000176  loss: 1.1323 (1.1117)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [170/431]  eta: 0:04:54  lr: 0.000176  loss: 1.1186 (1.1103)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [180/431]  eta: 0:04:43  lr: 0.000176  loss: 1.0256 (1.1079)  time: 1.1337  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [190/431]  eta: 0:04:32  lr: 0.000176  loss: 1.0417 (1.1091)  time: 1.1279  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [200/431]  eta: 0:04:20  lr: 0.000176  loss: 1.0663 (1.1081)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [210/431]  eta: 0:04:09  lr: 0.000176  loss: 1.1006 (1.1100)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [220/431]  eta: 0:03:57  lr: 0.000176  loss: 1.1144 (1.1102)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [230/431]  eta: 0:03:46  lr: 0.000176  loss: 1.0716 (1.1082)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [240/431]  eta: 0:03:34  lr: 0.000176  loss: 1.0866 (1.1084)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [250/431]  eta: 0:03:23  lr: 0.000176  loss: 1.1365 (1.1089)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [260/431]  eta: 0:03:12  lr: 0.000176  loss: 1.1158 (1.1077)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [270/431]  eta: 0:03:00  lr: 0.000176  loss: 1.0878 (1.1094)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [280/431]  eta: 0:02:49  lr: 0.000176  loss: 1.0704 (1.1085)  time: 1.1237  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [290/431]  eta: 0:02:38  lr: 0.000176  loss: 1.0678 (1.1095)  time: 1.1145  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:208]  [300/431]  eta: 0:02:27  lr: 0.000176  loss: 1.1026 (1.1097)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [310/431]  eta: 0:02:15  lr: 0.000176  loss: 1.1063 (1.1090)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [320/431]  eta: 0:02:04  lr: 0.000176  loss: 1.1170 (1.1100)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [330/431]  eta: 0:01:53  lr: 0.000176  loss: 1.1403 (1.1122)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [340/431]  eta: 0:01:42  lr: 0.000176  loss: 1.1430 (1.1151)  time: 1.1139  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [350/431]  eta: 0:01:30  lr: 0.000176  loss: 1.1757 (1.1158)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [360/431]  eta: 0:01:19  lr: 0.000176  loss: 1.1363 (1.1155)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [370/431]  eta: 0:01:08  lr: 0.000176  loss: 1.0834 (1.1139)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [380/431]  eta: 0:00:57  lr: 0.000176  loss: 1.0213 (1.1131)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [390/431]  eta: 0:00:45  lr: 0.000176  loss: 1.0434 (1.1125)  time: 1.1318  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [400/431]  eta: 0:00:34  lr: 0.000176  loss: 1.0834 (1.1118)  time: 1.1259  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [410/431]  eta: 0:00:23  lr: 0.000176  loss: 1.1125 (1.1126)  time: 1.1148  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:208]  [420/431]  eta: 0:00:12  lr: 0.000176  loss: 1.1187 (1.1118)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:208]  [430/431]  eta: 0:00:01  lr: 0.000176  loss: 1.0307 (1.1107)  time: 1.1084  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:208] Total time: 0:08:02 (1.1199 s / it)\n",
      "Averaged stats: lr: 0.000176  loss: 1.0307 (1.1107)\n",
      "Valid: [epoch:208]  [ 0/14]  eta: 0:00:35  loss: 1.0302 (1.0302)  time: 2.5043  data: 2.3171  max mem: 15925\n",
      "Valid: [epoch:208]  [13/14]  eta: 0:00:00  loss: 1.0384 (1.0516)  time: 0.2707  data: 0.1656  max mem: 15925\n",
      "Valid: [epoch:208] Total time: 0:00:04 (0.2888 s / it)\n",
      "Averaged stats: loss: 1.0384 (1.0516)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_208_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.052%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:209]  [  0/431]  eta: 0:35:32  lr: 0.000176  loss: 1.0697 (1.0697)  time: 4.9483  data: 3.5487  max mem: 15925\n",
      "Train: [epoch:209]  [ 10/431]  eta: 0:09:41  lr: 0.000176  loss: 1.0697 (1.1109)  time: 1.3811  data: 0.3229  max mem: 15925\n",
      "Train: [epoch:209]  [ 20/431]  eta: 0:08:25  lr: 0.000176  loss: 1.0802 (1.1124)  time: 1.0447  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [ 30/431]  eta: 0:07:54  lr: 0.000176  loss: 1.0409 (1.0859)  time: 1.0762  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [ 40/431]  eta: 0:07:35  lr: 0.000176  loss: 1.0314 (1.0841)  time: 1.0952  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [ 50/431]  eta: 0:07:19  lr: 0.000176  loss: 1.0590 (1.0801)  time: 1.1035  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [ 60/431]  eta: 0:07:05  lr: 0.000176  loss: 1.1076 (1.0875)  time: 1.1070  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [ 70/431]  eta: 0:06:51  lr: 0.000176  loss: 1.0988 (1.0858)  time: 1.1080  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [ 80/431]  eta: 0:06:38  lr: 0.000176  loss: 1.0814 (1.0894)  time: 1.1032  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [ 90/431]  eta: 0:06:26  lr: 0.000176  loss: 1.0802 (1.0885)  time: 1.1070  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [100/431]  eta: 0:06:13  lr: 0.000176  loss: 1.0860 (1.0909)  time: 1.1077  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [110/431]  eta: 0:06:02  lr: 0.000176  loss: 1.1229 (1.0954)  time: 1.1102  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [120/431]  eta: 0:05:50  lr: 0.000176  loss: 1.0583 (1.0939)  time: 1.1179  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [130/431]  eta: 0:05:39  lr: 0.000176  loss: 1.1325 (1.0978)  time: 1.1177  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [140/431]  eta: 0:05:27  lr: 0.000176  loss: 1.0563 (1.0968)  time: 1.1198  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [150/431]  eta: 0:05:16  lr: 0.000176  loss: 1.1161 (1.1042)  time: 1.1288  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [160/431]  eta: 0:05:05  lr: 0.000176  loss: 1.1723 (1.1077)  time: 1.1213  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [170/431]  eta: 0:04:53  lr: 0.000176  loss: 1.0930 (1.1073)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [180/431]  eta: 0:04:41  lr: 0.000176  loss: 1.0852 (1.1088)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [190/431]  eta: 0:04:30  lr: 0.000176  loss: 1.1121 (1.1103)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [200/431]  eta: 0:04:18  lr: 0.000176  loss: 1.1109 (1.1086)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [210/431]  eta: 0:04:07  lr: 0.000176  loss: 1.0666 (1.1082)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [220/431]  eta: 0:03:56  lr: 0.000176  loss: 1.0585 (1.1087)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [230/431]  eta: 0:03:44  lr: 0.000176  loss: 1.0886 (1.1089)  time: 1.1021  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [240/431]  eta: 0:03:33  lr: 0.000176  loss: 1.1161 (1.1089)  time: 1.1043  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [250/431]  eta: 0:03:22  lr: 0.000176  loss: 1.0894 (1.1096)  time: 1.1185  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [260/431]  eta: 0:03:11  lr: 0.000176  loss: 1.0894 (1.1106)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [270/431]  eta: 0:03:00  lr: 0.000176  loss: 1.0979 (1.1103)  time: 1.1197  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [280/431]  eta: 0:02:48  lr: 0.000176  loss: 1.0809 (1.1104)  time: 1.1180  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [290/431]  eta: 0:02:37  lr: 0.000176  loss: 1.0481 (1.1086)  time: 1.1186  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [300/431]  eta: 0:02:26  lr: 0.000176  loss: 1.0800 (1.1102)  time: 1.1239  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [310/431]  eta: 0:02:15  lr: 0.000176  loss: 1.0845 (1.1095)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [320/431]  eta: 0:02:04  lr: 0.000176  loss: 1.0724 (1.1088)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [330/431]  eta: 0:01:52  lr: 0.000176  loss: 1.1391 (1.1113)  time: 1.1176  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [340/431]  eta: 0:01:41  lr: 0.000176  loss: 1.1391 (1.1119)  time: 1.1142  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [350/431]  eta: 0:01:30  lr: 0.000176  loss: 1.1109 (1.1120)  time: 1.1118  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [360/431]  eta: 0:01:19  lr: 0.000176  loss: 1.0867 (1.1119)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [370/431]  eta: 0:01:08  lr: 0.000176  loss: 1.0913 (1.1123)  time: 1.1089  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:209]  [380/431]  eta: 0:00:56  lr: 0.000176  loss: 1.0403 (1.1100)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [390/431]  eta: 0:00:45  lr: 0.000176  loss: 1.0403 (1.1102)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [400/431]  eta: 0:00:34  lr: 0.000176  loss: 1.0733 (1.1096)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [410/431]  eta: 0:00:23  lr: 0.000176  loss: 1.1190 (1.1102)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [420/431]  eta: 0:00:12  lr: 0.000176  loss: 1.1077 (1.1099)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209]  [430/431]  eta: 0:00:01  lr: 0.000176  loss: 1.0801 (1.1098)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:209] Total time: 0:08:01 (1.1173 s / it)\n",
      "Averaged stats: lr: 0.000176  loss: 1.0801 (1.1098)\n",
      "Valid: [epoch:209]  [ 0/14]  eta: 0:00:35  loss: 1.0104 (1.0104)  time: 2.5582  data: 2.3949  max mem: 15925\n",
      "Valid: [epoch:209]  [13/14]  eta: 0:00:00  loss: 1.0398 (1.0517)  time: 0.2785  data: 0.1712  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:209] Total time: 0:00:04 (0.2964 s / it)\n",
      "Averaged stats: loss: 1.0398 (1.0517)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_209_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.052%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:210]  [  0/431]  eta: 0:32:11  lr: 0.000176  loss: 1.1275 (1.1275)  time: 4.4821  data: 3.3270  max mem: 15925\n",
      "Train: [epoch:210]  [ 10/431]  eta: 0:09:44  lr: 0.000176  loss: 1.1275 (1.1103)  time: 1.3879  data: 0.3027  max mem: 15925\n",
      "Train: [epoch:210]  [ 20/431]  eta: 0:08:29  lr: 0.000176  loss: 1.0918 (1.0982)  time: 1.0772  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [ 30/431]  eta: 0:08:01  lr: 0.000176  loss: 1.0886 (1.0952)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [ 40/431]  eta: 0:07:37  lr: 0.000176  loss: 1.0747 (1.0911)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [ 50/431]  eta: 0:07:21  lr: 0.000176  loss: 1.0601 (1.0841)  time: 1.0941  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [ 60/431]  eta: 0:07:07  lr: 0.000176  loss: 1.0387 (1.0857)  time: 1.1165  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [ 70/431]  eta: 0:06:53  lr: 0.000176  loss: 1.0955 (1.0852)  time: 1.1143  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [ 80/431]  eta: 0:06:40  lr: 0.000176  loss: 1.0967 (1.0881)  time: 1.1094  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [ 90/431]  eta: 0:06:27  lr: 0.000176  loss: 1.1087 (1.0934)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [100/431]  eta: 0:06:15  lr: 0.000176  loss: 1.1205 (1.0955)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [110/431]  eta: 0:06:04  lr: 0.000176  loss: 1.1205 (1.0976)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [120/431]  eta: 0:05:51  lr: 0.000176  loss: 1.0726 (1.0975)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [130/431]  eta: 0:05:40  lr: 0.000176  loss: 1.0665 (1.1014)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [140/431]  eta: 0:05:27  lr: 0.000176  loss: 1.0665 (1.1015)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [150/431]  eta: 0:05:16  lr: 0.000176  loss: 1.0883 (1.1058)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [160/431]  eta: 0:05:04  lr: 0.000176  loss: 1.1040 (1.1059)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [170/431]  eta: 0:04:53  lr: 0.000176  loss: 1.1212 (1.1065)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [180/431]  eta: 0:04:41  lr: 0.000176  loss: 1.1259 (1.1072)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [190/431]  eta: 0:04:30  lr: 0.000176  loss: 1.0882 (1.1060)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [200/431]  eta: 0:04:19  lr: 0.000176  loss: 1.0723 (1.1049)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [210/431]  eta: 0:04:07  lr: 0.000176  loss: 1.0901 (1.1048)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [220/431]  eta: 0:03:56  lr: 0.000176  loss: 1.1126 (1.1052)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [230/431]  eta: 0:03:45  lr: 0.000176  loss: 1.0698 (1.1038)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [240/431]  eta: 0:03:33  lr: 0.000176  loss: 1.0753 (1.1063)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [250/431]  eta: 0:03:22  lr: 0.000176  loss: 1.1472 (1.1079)  time: 1.1323  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [260/431]  eta: 0:03:12  lr: 0.000176  loss: 1.1376 (1.1080)  time: 1.1519  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [270/431]  eta: 0:03:00  lr: 0.000176  loss: 1.0947 (1.1088)  time: 1.1394  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [280/431]  eta: 0:02:49  lr: 0.000176  loss: 1.0983 (1.1094)  time: 1.1196  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [290/431]  eta: 0:02:38  lr: 0.000176  loss: 1.0921 (1.1069)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [300/431]  eta: 0:02:27  lr: 0.000176  loss: 1.0770 (1.1078)  time: 1.1379  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [310/431]  eta: 0:02:15  lr: 0.000176  loss: 1.1626 (1.1104)  time: 1.1323  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [320/431]  eta: 0:02:04  lr: 0.000176  loss: 1.1039 (1.1095)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [330/431]  eta: 0:01:53  lr: 0.000176  loss: 1.0487 (1.1102)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [340/431]  eta: 0:01:42  lr: 0.000176  loss: 1.1049 (1.1104)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [350/431]  eta: 0:01:30  lr: 0.000176  loss: 1.1212 (1.1107)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [360/431]  eta: 0:01:19  lr: 0.000176  loss: 1.1659 (1.1124)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [370/431]  eta: 0:01:08  lr: 0.000176  loss: 1.0980 (1.1117)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [380/431]  eta: 0:00:57  lr: 0.000176  loss: 1.0722 (1.1111)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [390/431]  eta: 0:00:45  lr: 0.000176  loss: 1.0632 (1.1113)  time: 1.1037  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:210]  [400/431]  eta: 0:00:34  lr: 0.000176  loss: 1.1302 (1.1121)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [410/431]  eta: 0:00:23  lr: 0.000176  loss: 1.1243 (1.1112)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:210]  [420/431]  eta: 0:00:12  lr: 0.000176  loss: 1.0941 (1.1119)  time: 1.1068  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:210]  [430/431]  eta: 0:00:01  lr: 0.000176  loss: 1.1012 (1.1117)  time: 1.1155  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:210] Total time: 0:08:02 (1.1194 s / it)\n",
      "Averaged stats: lr: 0.000176  loss: 1.1012 (1.1117)\n",
      "Valid: [epoch:210]  [ 0/14]  eta: 0:00:35  loss: 1.1092 (1.1092)  time: 2.5430  data: 2.3864  max mem: 15925\n",
      "Valid: [epoch:210]  [13/14]  eta: 0:00:00  loss: 1.0376 (1.0528)  time: 0.2780  data: 0.1706  max mem: 15925\n",
      "Valid: [epoch:210] Total time: 0:00:04 (0.2949 s / it)\n",
      "Averaged stats: loss: 1.0376 (1.0528)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_210_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.053%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:211]  [  0/431]  eta: 0:33:37  lr: 0.000176  loss: 1.0887 (1.0887)  time: 4.6819  data: 3.2139  max mem: 15925\n",
      "Train: [epoch:211]  [ 10/431]  eta: 0:09:44  lr: 0.000176  loss: 1.0690 (1.1026)  time: 1.3883  data: 0.2925  max mem: 15925\n",
      "Train: [epoch:211]  [ 20/431]  eta: 0:08:33  lr: 0.000176  loss: 1.0636 (1.0871)  time: 1.0788  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:211]  [ 30/431]  eta: 0:08:00  lr: 0.000176  loss: 1.0636 (1.0856)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [ 40/431]  eta: 0:07:39  lr: 0.000176  loss: 1.0584 (1.0808)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [ 50/431]  eta: 0:07:22  lr: 0.000176  loss: 1.0717 (1.0943)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [ 60/431]  eta: 0:07:08  lr: 0.000176  loss: 1.1181 (1.0950)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [ 70/431]  eta: 0:06:55  lr: 0.000176  loss: 1.0464 (1.0936)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [ 80/431]  eta: 0:06:41  lr: 0.000176  loss: 1.0891 (1.1022)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [ 90/431]  eta: 0:06:28  lr: 0.000176  loss: 1.1273 (1.1075)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [100/431]  eta: 0:06:15  lr: 0.000176  loss: 1.0813 (1.1032)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [110/431]  eta: 0:06:03  lr: 0.000176  loss: 1.0338 (1.0990)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [120/431]  eta: 0:05:52  lr: 0.000176  loss: 1.0271 (1.0962)  time: 1.1180  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:211]  [130/431]  eta: 0:05:40  lr: 0.000176  loss: 1.0268 (1.0949)  time: 1.1167  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:211]  [140/431]  eta: 0:05:28  lr: 0.000176  loss: 1.0840 (1.0988)  time: 1.1085  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:211]  [150/431]  eta: 0:05:16  lr: 0.000176  loss: 1.0981 (1.0971)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [160/431]  eta: 0:05:05  lr: 0.000176  loss: 1.0724 (1.0959)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [170/431]  eta: 0:04:53  lr: 0.000176  loss: 1.0724 (1.0960)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [180/431]  eta: 0:04:42  lr: 0.000176  loss: 1.0542 (1.0960)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [190/431]  eta: 0:04:30  lr: 0.000176  loss: 1.0793 (1.0982)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [200/431]  eta: 0:04:18  lr: 0.000176  loss: 1.0719 (1.0952)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [210/431]  eta: 0:04:07  lr: 0.000176  loss: 1.0719 (1.0968)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [220/431]  eta: 0:03:56  lr: 0.000176  loss: 1.1071 (1.0966)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [230/431]  eta: 0:03:44  lr: 0.000176  loss: 1.0997 (1.0992)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [240/431]  eta: 0:03:33  lr: 0.000176  loss: 1.0997 (1.1003)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [250/431]  eta: 0:03:21  lr: 0.000176  loss: 1.1413 (1.1021)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [260/431]  eta: 0:03:10  lr: 0.000176  loss: 1.1431 (1.1041)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [270/431]  eta: 0:02:59  lr: 0.000176  loss: 1.0769 (1.1035)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [280/431]  eta: 0:02:48  lr: 0.000176  loss: 1.0639 (1.1015)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [290/431]  eta: 0:02:36  lr: 0.000176  loss: 1.0359 (1.1007)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [300/431]  eta: 0:02:25  lr: 0.000176  loss: 1.0885 (1.1034)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [310/431]  eta: 0:02:14  lr: 0.000176  loss: 1.1760 (1.1068)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [320/431]  eta: 0:02:03  lr: 0.000176  loss: 1.1579 (1.1064)  time: 1.0883  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [330/431]  eta: 0:01:52  lr: 0.000176  loss: 1.0913 (1.1075)  time: 1.0895  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [340/431]  eta: 0:01:40  lr: 0.000176  loss: 1.1553 (1.1086)  time: 1.0846  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [350/431]  eta: 0:01:29  lr: 0.000176  loss: 1.1304 (1.1093)  time: 1.0865  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [360/431]  eta: 0:01:18  lr: 0.000176  loss: 1.1420 (1.1104)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [370/431]  eta: 0:01:07  lr: 0.000176  loss: 1.1558 (1.1108)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [380/431]  eta: 0:00:56  lr: 0.000176  loss: 1.0581 (1.1094)  time: 1.0897  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [390/431]  eta: 0:00:45  lr: 0.000176  loss: 1.0288 (1.1091)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [400/431]  eta: 0:00:34  lr: 0.000176  loss: 1.1096 (1.1111)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:211]  [410/431]  eta: 0:00:23  lr: 0.000176  loss: 1.1559 (1.1118)  time: 1.0961  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:211]  [420/431]  eta: 0:00:12  lr: 0.000176  loss: 1.1559 (1.1121)  time: 1.0960  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:211]  [430/431]  eta: 0:00:01  lr: 0.000176  loss: 1.0648 (1.1103)  time: 1.0955  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:211] Total time: 0:07:57 (1.1074 s / it)\n",
      "Averaged stats: lr: 0.000176  loss: 1.0648 (1.1103)\n",
      "Valid: [epoch:211]  [ 0/14]  eta: 0:00:34  loss: 0.9546 (0.9546)  time: 2.4991  data: 2.3220  max mem: 15925\n",
      "Valid: [epoch:211]  [13/14]  eta: 0:00:00  loss: 1.0339 (1.0474)  time: 0.2625  data: 0.1660  max mem: 15925\n",
      "Valid: [epoch:211] Total time: 0:00:03 (0.2790 s / it)\n",
      "Averaged stats: loss: 1.0339 (1.0474)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_211_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:212]  [  0/431]  eta: 0:31:49  lr: 0.000175  loss: 1.2305 (1.2305)  time: 4.4310  data: 3.1934  max mem: 15925\n",
      "Train: [epoch:212]  [ 10/431]  eta: 0:09:31  lr: 0.000175  loss: 1.1643 (1.1368)  time: 1.3566  data: 0.2905  max mem: 15925\n",
      "Train: [epoch:212]  [ 20/431]  eta: 0:08:18  lr: 0.000175  loss: 1.0830 (1.1056)  time: 1.0511  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [ 30/431]  eta: 0:07:48  lr: 0.000175  loss: 1.0673 (1.1058)  time: 1.0641  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [ 40/431]  eta: 0:07:29  lr: 0.000175  loss: 1.0509 (1.0942)  time: 1.0818  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [ 50/431]  eta: 0:07:13  lr: 0.000175  loss: 1.0509 (1.1043)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [ 60/431]  eta: 0:06:59  lr: 0.000175  loss: 1.0623 (1.1060)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [ 70/431]  eta: 0:06:46  lr: 0.000175  loss: 1.1324 (1.1249)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [ 80/431]  eta: 0:06:34  lr: 0.000175  loss: 1.1566 (1.1288)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [ 90/431]  eta: 0:06:21  lr: 0.000175  loss: 1.0900 (1.1240)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [100/431]  eta: 0:06:09  lr: 0.000175  loss: 1.0631 (1.1182)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [110/431]  eta: 0:05:57  lr: 0.000175  loss: 1.0467 (1.1141)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [120/431]  eta: 0:05:46  lr: 0.000175  loss: 1.0555 (1.1119)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [130/431]  eta: 0:05:34  lr: 0.000175  loss: 1.0655 (1.1096)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [140/431]  eta: 0:05:23  lr: 0.000175  loss: 1.0474 (1.1133)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [150/431]  eta: 0:05:11  lr: 0.000175  loss: 1.1054 (1.1160)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [160/431]  eta: 0:05:00  lr: 0.000175  loss: 1.1198 (1.1192)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [170/431]  eta: 0:04:49  lr: 0.000175  loss: 1.1649 (1.1216)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [180/431]  eta: 0:04:37  lr: 0.000175  loss: 1.1127 (1.1189)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [190/431]  eta: 0:04:26  lr: 0.000175  loss: 1.1103 (1.1181)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [200/431]  eta: 0:04:15  lr: 0.000175  loss: 1.0435 (1.1159)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [210/431]  eta: 0:04:04  lr: 0.000175  loss: 1.0488 (1.1184)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [220/431]  eta: 0:03:53  lr: 0.000175  loss: 1.1270 (1.1177)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [230/431]  eta: 0:03:42  lr: 0.000175  loss: 1.0742 (1.1156)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [240/431]  eta: 0:03:31  lr: 0.000175  loss: 1.0805 (1.1159)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [250/431]  eta: 0:03:20  lr: 0.000175  loss: 1.0880 (1.1147)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [260/431]  eta: 0:03:08  lr: 0.000175  loss: 1.0844 (1.1146)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [270/431]  eta: 0:02:57  lr: 0.000175  loss: 1.0781 (1.1133)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [280/431]  eta: 0:02:46  lr: 0.000175  loss: 1.0676 (1.1121)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [290/431]  eta: 0:02:35  lr: 0.000175  loss: 1.1158 (1.1131)  time: 1.0884  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [300/431]  eta: 0:02:24  lr: 0.000175  loss: 1.1258 (1.1141)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [310/431]  eta: 0:02:13  lr: 0.000175  loss: 1.0687 (1.1148)  time: 1.0961  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:212]  [320/431]  eta: 0:02:02  lr: 0.000175  loss: 1.0730 (1.1138)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [330/431]  eta: 0:01:51  lr: 0.000175  loss: 1.1211 (1.1162)  time: 1.0893  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [340/431]  eta: 0:01:40  lr: 0.000175  loss: 1.1420 (1.1172)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [350/431]  eta: 0:01:29  lr: 0.000175  loss: 1.1332 (1.1174)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [360/431]  eta: 0:01:18  lr: 0.000175  loss: 1.0985 (1.1172)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [370/431]  eta: 0:01:07  lr: 0.000175  loss: 1.0985 (1.1164)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [380/431]  eta: 0:00:56  lr: 0.000175  loss: 1.0527 (1.1145)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [390/431]  eta: 0:00:45  lr: 0.000175  loss: 1.0441 (1.1136)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [400/431]  eta: 0:00:34  lr: 0.000175  loss: 1.0667 (1.1145)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [410/431]  eta: 0:00:23  lr: 0.000175  loss: 1.0782 (1.1145)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:212]  [420/431]  eta: 0:00:12  lr: 0.000175  loss: 1.0614 (1.1133)  time: 1.1027  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:212]  [430/431]  eta: 0:00:01  lr: 0.000175  loss: 1.0491 (1.1132)  time: 1.0990  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:212] Total time: 0:07:55 (1.1022 s / it)\n",
      "Averaged stats: lr: 0.000175  loss: 1.0491 (1.1132)\n",
      "Valid: [epoch:212]  [ 0/14]  eta: 0:00:37  loss: 1.0387 (1.0387)  time: 2.6829  data: 2.4912  max mem: 15925\n",
      "Valid: [epoch:212]  [13/14]  eta: 0:00:00  loss: 1.0387 (1.0507)  time: 0.2765  data: 0.1780  max mem: 15925\n",
      "Valid: [epoch:212] Total time: 0:00:04 (0.2930 s / it)\n",
      "Averaged stats: loss: 1.0387 (1.0507)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_212_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.051%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:213]  [  0/431]  eta: 0:33:29  lr: 0.000175  loss: 1.1941 (1.1941)  time: 4.6616  data: 3.4655  max mem: 15925\n",
      "Train: [epoch:213]  [ 10/431]  eta: 0:09:38  lr: 0.000175  loss: 1.1585 (1.1481)  time: 1.3739  data: 0.3153  max mem: 15925\n",
      "Train: [epoch:213]  [ 20/431]  eta: 0:08:24  lr: 0.000175  loss: 1.1491 (1.1647)  time: 1.0546  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [ 30/431]  eta: 0:07:49  lr: 0.000175  loss: 1.1051 (1.1549)  time: 1.0585  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [ 40/431]  eta: 0:07:29  lr: 0.000175  loss: 1.1062 (1.1515)  time: 1.0679  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [ 50/431]  eta: 0:07:13  lr: 0.000175  loss: 1.1013 (1.1429)  time: 1.0857  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [ 60/431]  eta: 0:06:58  lr: 0.000175  loss: 1.0589 (1.1296)  time: 1.0875  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [ 70/431]  eta: 0:06:45  lr: 0.000175  loss: 1.1249 (1.1384)  time: 1.0909  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [ 80/431]  eta: 0:06:34  lr: 0.000175  loss: 1.1387 (1.1365)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [ 90/431]  eta: 0:06:21  lr: 0.000175  loss: 1.0914 (1.1326)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [100/431]  eta: 0:06:09  lr: 0.000175  loss: 1.1123 (1.1300)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [110/431]  eta: 0:05:57  lr: 0.000175  loss: 1.1123 (1.1292)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [120/431]  eta: 0:05:46  lr: 0.000175  loss: 1.1136 (1.1287)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [130/431]  eta: 0:05:34  lr: 0.000175  loss: 1.0518 (1.1244)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [140/431]  eta: 0:05:23  lr: 0.000175  loss: 1.0369 (1.1197)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [150/431]  eta: 0:05:12  lr: 0.000175  loss: 1.0724 (1.1211)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [160/431]  eta: 0:05:00  lr: 0.000175  loss: 1.0992 (1.1227)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [170/431]  eta: 0:04:49  lr: 0.000175  loss: 1.0966 (1.1228)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [180/431]  eta: 0:04:38  lr: 0.000175  loss: 1.0994 (1.1226)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [190/431]  eta: 0:04:27  lr: 0.000175  loss: 1.0682 (1.1203)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [200/431]  eta: 0:04:16  lr: 0.000175  loss: 1.0638 (1.1161)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [210/431]  eta: 0:04:05  lr: 0.000175  loss: 1.0850 (1.1168)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [220/431]  eta: 0:03:53  lr: 0.000175  loss: 1.0675 (1.1126)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [230/431]  eta: 0:03:42  lr: 0.000175  loss: 1.0477 (1.1122)  time: 1.0845  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [240/431]  eta: 0:03:31  lr: 0.000175  loss: 1.0749 (1.1123)  time: 1.0885  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [250/431]  eta: 0:03:20  lr: 0.000175  loss: 1.1148 (1.1132)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [260/431]  eta: 0:03:09  lr: 0.000175  loss: 1.1304 (1.1139)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [270/431]  eta: 0:02:57  lr: 0.000175  loss: 1.1198 (1.1131)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [280/431]  eta: 0:02:46  lr: 0.000175  loss: 1.1198 (1.1143)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [290/431]  eta: 0:02:35  lr: 0.000175  loss: 1.1179 (1.1147)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [300/431]  eta: 0:02:24  lr: 0.000175  loss: 1.0324 (1.1124)  time: 1.0917  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [310/431]  eta: 0:02:13  lr: 0.000175  loss: 1.0383 (1.1121)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [320/431]  eta: 0:02:02  lr: 0.000175  loss: 1.0448 (1.1104)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [330/431]  eta: 0:01:51  lr: 0.000175  loss: 1.0563 (1.1094)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [340/431]  eta: 0:01:40  lr: 0.000175  loss: 1.1345 (1.1123)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [350/431]  eta: 0:01:29  lr: 0.000175  loss: 1.1477 (1.1124)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [360/431]  eta: 0:01:18  lr: 0.000175  loss: 1.0935 (1.1123)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [370/431]  eta: 0:01:07  lr: 0.000175  loss: 1.0876 (1.1129)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [380/431]  eta: 0:00:56  lr: 0.000175  loss: 1.0750 (1.1134)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [390/431]  eta: 0:00:45  lr: 0.000175  loss: 1.0750 (1.1121)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [400/431]  eta: 0:00:34  lr: 0.000175  loss: 1.1263 (1.1125)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [410/431]  eta: 0:00:23  lr: 0.000175  loss: 1.1263 (1.1119)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:213]  [420/431]  eta: 0:00:12  lr: 0.000175  loss: 1.0947 (1.1112)  time: 1.1034  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:213]  [430/431]  eta: 0:00:01  lr: 0.000175  loss: 1.0768 (1.1112)  time: 1.1006  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:213] Total time: 0:07:54 (1.1020 s / it)\n",
      "Averaged stats: lr: 0.000175  loss: 1.0768 (1.1112)\n",
      "Valid: [epoch:213]  [ 0/14]  eta: 0:00:36  loss: 1.1071 (1.1071)  time: 2.5845  data: 2.4566  max mem: 15925\n",
      "Valid: [epoch:213]  [13/14]  eta: 0:00:00  loss: 1.0452 (1.0553)  time: 0.2715  data: 0.1756  max mem: 15925\n",
      "Valid: [epoch:213] Total time: 0:00:04 (0.2896 s / it)\n",
      "Averaged stats: loss: 1.0452 (1.0553)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_213_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.055%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:214]  [  0/431]  eta: 0:36:26  lr: 0.000175  loss: 1.0430 (1.0430)  time: 5.0735  data: 3.8804  max mem: 15925\n",
      "Train: [epoch:214]  [ 10/431]  eta: 0:09:49  lr: 0.000175  loss: 1.0739 (1.0761)  time: 1.4006  data: 0.3529  max mem: 15925\n",
      "Train: [epoch:214]  [ 20/431]  eta: 0:08:26  lr: 0.000175  loss: 1.0739 (1.0936)  time: 1.0408  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [ 30/431]  eta: 0:07:54  lr: 0.000175  loss: 1.0907 (1.1084)  time: 1.0622  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [ 40/431]  eta: 0:07:32  lr: 0.000175  loss: 1.0907 (1.1006)  time: 1.0773  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [ 50/431]  eta: 0:07:15  lr: 0.000175  loss: 1.0770 (1.1053)  time: 1.0816  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [ 60/431]  eta: 0:07:01  lr: 0.000175  loss: 1.0988 (1.1070)  time: 1.0925  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [ 70/431]  eta: 0:06:47  lr: 0.000175  loss: 1.1345 (1.1157)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [ 80/431]  eta: 0:06:34  lr: 0.000175  loss: 1.1471 (1.1234)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [ 90/431]  eta: 0:06:22  lr: 0.000175  loss: 1.0855 (1.1224)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [100/431]  eta: 0:06:10  lr: 0.000175  loss: 1.0324 (1.1148)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [110/431]  eta: 0:05:58  lr: 0.000175  loss: 1.0989 (1.1202)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [120/431]  eta: 0:05:46  lr: 0.000175  loss: 1.1467 (1.1208)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [130/431]  eta: 0:05:35  lr: 0.000175  loss: 1.1302 (1.1207)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [140/431]  eta: 0:05:23  lr: 0.000175  loss: 1.1144 (1.1209)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [150/431]  eta: 0:05:12  lr: 0.000175  loss: 1.1144 (1.1253)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [160/431]  eta: 0:05:01  lr: 0.000175  loss: 1.1431 (1.1277)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [170/431]  eta: 0:04:50  lr: 0.000175  loss: 1.0991 (1.1250)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [180/431]  eta: 0:04:38  lr: 0.000175  loss: 1.0472 (1.1213)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [190/431]  eta: 0:04:27  lr: 0.000175  loss: 1.0329 (1.1187)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [200/431]  eta: 0:04:16  lr: 0.000175  loss: 1.0402 (1.1157)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [210/431]  eta: 0:04:05  lr: 0.000175  loss: 1.1149 (1.1160)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [220/431]  eta: 0:03:53  lr: 0.000175  loss: 1.1149 (1.1142)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [230/431]  eta: 0:03:42  lr: 0.000175  loss: 1.0487 (1.1136)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [240/431]  eta: 0:03:31  lr: 0.000175  loss: 1.0752 (1.1134)  time: 1.0885  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [250/431]  eta: 0:03:20  lr: 0.000175  loss: 1.0693 (1.1147)  time: 1.0905  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [260/431]  eta: 0:03:09  lr: 0.000175  loss: 1.0614 (1.1137)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [270/431]  eta: 0:02:58  lr: 0.000175  loss: 1.0824 (1.1138)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [280/431]  eta: 0:02:46  lr: 0.000175  loss: 1.1386 (1.1139)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [290/431]  eta: 0:02:35  lr: 0.000175  loss: 1.1288 (1.1139)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [300/431]  eta: 0:02:24  lr: 0.000175  loss: 1.1288 (1.1145)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [310/431]  eta: 0:02:13  lr: 0.000175  loss: 1.0926 (1.1141)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [320/431]  eta: 0:02:02  lr: 0.000175  loss: 1.0926 (1.1137)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [330/431]  eta: 0:01:51  lr: 0.000175  loss: 1.1076 (1.1142)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [340/431]  eta: 0:01:40  lr: 0.000175  loss: 1.0669 (1.1130)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [350/431]  eta: 0:01:29  lr: 0.000175  loss: 1.0710 (1.1134)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [360/431]  eta: 0:01:18  lr: 0.000175  loss: 1.1212 (1.1126)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [370/431]  eta: 0:01:07  lr: 0.000175  loss: 1.1224 (1.1139)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [380/431]  eta: 0:00:56  lr: 0.000175  loss: 1.1224 (1.1128)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [390/431]  eta: 0:00:45  lr: 0.000175  loss: 1.0849 (1.1137)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [400/431]  eta: 0:00:34  lr: 0.000175  loss: 1.0751 (1.1133)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [410/431]  eta: 0:00:23  lr: 0.000175  loss: 1.0562 (1.1124)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:214]  [420/431]  eta: 0:00:12  lr: 0.000175  loss: 1.0956 (1.1132)  time: 1.0960  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:214]  [430/431]  eta: 0:00:01  lr: 0.000175  loss: 1.1041 (1.1126)  time: 1.0893  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:214] Total time: 0:07:55 (1.1028 s / it)\n",
      "Averaged stats: lr: 0.000175  loss: 1.1041 (1.1126)\n",
      "Valid: [epoch:214]  [ 0/14]  eta: 0:00:36  loss: 1.0938 (1.0938)  time: 2.5942  data: 2.4466  max mem: 15925\n",
      "Valid: [epoch:214]  [13/14]  eta: 0:00:00  loss: 1.0379 (1.0530)  time: 0.2814  data: 0.1748  max mem: 15925\n",
      "Valid: [epoch:214] Total time: 0:00:04 (0.2996 s / it)\n",
      "Averaged stats: loss: 1.0379 (1.0530)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_214_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.053%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:215]  [  0/431]  eta: 0:35:27  lr: 0.000175  loss: 1.0443 (1.0443)  time: 4.9367  data: 3.7372  max mem: 15925\n",
      "Train: [epoch:215]  [ 10/431]  eta: 0:09:51  lr: 0.000175  loss: 1.1867 (1.1857)  time: 1.4053  data: 0.3399  max mem: 15925\n",
      "Train: [epoch:215]  [ 20/431]  eta: 0:08:26  lr: 0.000175  loss: 1.1315 (1.1618)  time: 1.0481  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [ 30/431]  eta: 0:07:54  lr: 0.000175  loss: 1.0422 (1.1108)  time: 1.0629  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [ 40/431]  eta: 0:07:33  lr: 0.000175  loss: 1.0175 (1.1135)  time: 1.0847  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [ 50/431]  eta: 0:07:16  lr: 0.000175  loss: 1.0562 (1.1036)  time: 1.0863  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [ 60/431]  eta: 0:07:02  lr: 0.000175  loss: 1.0454 (1.0906)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [ 70/431]  eta: 0:06:49  lr: 0.000175  loss: 1.0808 (1.0931)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [ 80/431]  eta: 0:06:36  lr: 0.000175  loss: 1.1133 (1.0980)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [ 90/431]  eta: 0:06:24  lr: 0.000175  loss: 1.1133 (1.1027)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [100/431]  eta: 0:06:12  lr: 0.000175  loss: 1.0984 (1.1029)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [110/431]  eta: 0:06:00  lr: 0.000175  loss: 1.0728 (1.0984)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [120/431]  eta: 0:05:48  lr: 0.000175  loss: 1.0023 (1.0937)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [130/431]  eta: 0:05:37  lr: 0.000175  loss: 1.0587 (1.0952)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [140/431]  eta: 0:05:25  lr: 0.000175  loss: 1.0441 (1.0938)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [150/431]  eta: 0:05:13  lr: 0.000175  loss: 1.0616 (1.0941)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [160/431]  eta: 0:05:02  lr: 0.000175  loss: 1.0766 (1.0947)  time: 1.1024  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:215]  [170/431]  eta: 0:04:51  lr: 0.000175  loss: 1.0703 (1.0932)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [180/431]  eta: 0:04:39  lr: 0.000175  loss: 1.0411 (1.0912)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [190/431]  eta: 0:04:28  lr: 0.000175  loss: 1.0729 (1.0952)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [200/431]  eta: 0:04:16  lr: 0.000175  loss: 1.1119 (1.0971)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [210/431]  eta: 0:04:05  lr: 0.000175  loss: 1.1342 (1.0988)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [220/431]  eta: 0:03:54  lr: 0.000175  loss: 1.0908 (1.1001)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [230/431]  eta: 0:03:43  lr: 0.000175  loss: 1.0875 (1.1006)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [240/431]  eta: 0:03:31  lr: 0.000175  loss: 1.0845 (1.1016)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [250/431]  eta: 0:03:20  lr: 0.000175  loss: 1.0693 (1.1016)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [260/431]  eta: 0:03:09  lr: 0.000175  loss: 1.1200 (1.1034)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [270/431]  eta: 0:02:58  lr: 0.000175  loss: 1.1297 (1.1046)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [280/431]  eta: 0:02:47  lr: 0.000175  loss: 1.0831 (1.1032)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [290/431]  eta: 0:02:36  lr: 0.000175  loss: 1.0700 (1.1043)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [300/431]  eta: 0:02:25  lr: 0.000175  loss: 1.1379 (1.1072)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [310/431]  eta: 0:02:14  lr: 0.000175  loss: 1.1436 (1.1096)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [320/431]  eta: 0:02:03  lr: 0.000175  loss: 1.1158 (1.1094)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [330/431]  eta: 0:01:51  lr: 0.000175  loss: 1.0742 (1.1094)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [340/431]  eta: 0:01:40  lr: 0.000175  loss: 1.0855 (1.1095)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [350/431]  eta: 0:01:29  lr: 0.000175  loss: 1.0854 (1.1089)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [360/431]  eta: 0:01:18  lr: 0.000175  loss: 1.1169 (1.1107)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [370/431]  eta: 0:01:07  lr: 0.000175  loss: 1.1367 (1.1117)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [380/431]  eta: 0:00:56  lr: 0.000175  loss: 1.0957 (1.1115)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [390/431]  eta: 0:00:45  lr: 0.000175  loss: 1.0520 (1.1098)  time: 1.0911  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [400/431]  eta: 0:00:34  lr: 0.000175  loss: 1.0381 (1.1096)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:215]  [410/431]  eta: 0:00:23  lr: 0.000175  loss: 1.1209 (1.1114)  time: 1.0918  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:215]  [420/431]  eta: 0:00:12  lr: 0.000175  loss: 1.1187 (1.1113)  time: 1.0971  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:215]  [430/431]  eta: 0:00:01  lr: 0.000175  loss: 1.0815 (1.1124)  time: 1.1018  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:215] Total time: 0:07:56 (1.1062 s / it)\n",
      "Averaged stats: lr: 0.000175  loss: 1.0815 (1.1124)\n",
      "Valid: [epoch:215]  [ 0/14]  eta: 0:00:37  loss: 1.0464 (1.0464)  time: 2.6481  data: 2.5126  max mem: 15925\n",
      "Valid: [epoch:215]  [13/14]  eta: 0:00:00  loss: 1.0464 (1.0571)  time: 0.2870  data: 0.1796  max mem: 15925\n",
      "Valid: [epoch:215] Total time: 0:00:04 (0.3015 s / it)\n",
      "Averaged stats: loss: 1.0464 (1.0571)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_215_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.057%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:216]  [  0/431]  eta: 0:35:35  lr: 0.000174  loss: 1.3304 (1.3304)  time: 4.9548  data: 3.7465  max mem: 15925\n",
      "Train: [epoch:216]  [ 10/431]  eta: 0:09:44  lr: 0.000174  loss: 1.2636 (1.2542)  time: 1.3889  data: 0.3407  max mem: 15925\n",
      "Train: [epoch:216]  [ 20/431]  eta: 0:08:24  lr: 0.000174  loss: 1.1072 (1.1734)  time: 1.0399  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [ 30/431]  eta: 0:07:51  lr: 0.000174  loss: 1.0608 (1.1298)  time: 1.0583  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [ 40/431]  eta: 0:07:30  lr: 0.000174  loss: 1.0238 (1.1167)  time: 1.0769  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [ 50/431]  eta: 0:07:14  lr: 0.000174  loss: 1.0270 (1.1093)  time: 1.0876  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [ 60/431]  eta: 0:07:01  lr: 0.000174  loss: 1.0503 (1.1049)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [ 70/431]  eta: 0:06:49  lr: 0.000174  loss: 1.0619 (1.1080)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [ 80/431]  eta: 0:06:36  lr: 0.000174  loss: 1.0652 (1.1072)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [ 90/431]  eta: 0:06:23  lr: 0.000174  loss: 1.0652 (1.1012)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [100/431]  eta: 0:06:11  lr: 0.000174  loss: 1.0528 (1.1008)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [110/431]  eta: 0:05:59  lr: 0.000174  loss: 1.0528 (1.0964)  time: 1.0917  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [120/431]  eta: 0:05:47  lr: 0.000174  loss: 1.0446 (1.0947)  time: 1.0872  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [130/431]  eta: 0:05:35  lr: 0.000174  loss: 1.0706 (1.0962)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [140/431]  eta: 0:05:24  lr: 0.000174  loss: 1.0833 (1.0939)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [150/431]  eta: 0:05:12  lr: 0.000174  loss: 1.0814 (1.0945)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [160/431]  eta: 0:05:01  lr: 0.000174  loss: 1.0390 (1.0927)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [170/431]  eta: 0:04:50  lr: 0.000174  loss: 1.0248 (1.0907)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [180/431]  eta: 0:04:38  lr: 0.000174  loss: 1.0995 (1.0969)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [190/431]  eta: 0:04:27  lr: 0.000174  loss: 1.1060 (1.0977)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [200/431]  eta: 0:04:16  lr: 0.000174  loss: 1.1060 (1.1006)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [210/431]  eta: 0:04:05  lr: 0.000174  loss: 1.1625 (1.1020)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [220/431]  eta: 0:03:53  lr: 0.000174  loss: 1.1136 (1.1037)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [230/431]  eta: 0:03:42  lr: 0.000174  loss: 1.1127 (1.1053)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [240/431]  eta: 0:03:31  lr: 0.000174  loss: 1.0489 (1.1041)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [250/431]  eta: 0:03:20  lr: 0.000174  loss: 1.0402 (1.1018)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [260/431]  eta: 0:03:09  lr: 0.000174  loss: 1.0441 (1.1014)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [270/431]  eta: 0:02:58  lr: 0.000174  loss: 1.0845 (1.1019)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [280/431]  eta: 0:02:47  lr: 0.000174  loss: 1.1298 (1.1050)  time: 1.1084  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:216]  [290/431]  eta: 0:02:35  lr: 0.000174  loss: 1.1140 (1.1040)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [300/431]  eta: 0:02:24  lr: 0.000174  loss: 1.1011 (1.1046)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [310/431]  eta: 0:02:13  lr: 0.000174  loss: 1.0982 (1.1035)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [320/431]  eta: 0:02:02  lr: 0.000174  loss: 1.0518 (1.1018)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [330/431]  eta: 0:01:51  lr: 0.000174  loss: 1.0693 (1.1027)  time: 1.0899  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:216]  [340/431]  eta: 0:01:40  lr: 0.000174  loss: 1.1025 (1.1034)  time: 1.0829  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [350/431]  eta: 0:01:29  lr: 0.000174  loss: 1.0789 (1.1035)  time: 1.0909  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [360/431]  eta: 0:01:18  lr: 0.000174  loss: 1.0692 (1.1036)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [370/431]  eta: 0:01:07  lr: 0.000174  loss: 1.0692 (1.1028)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [380/431]  eta: 0:00:56  lr: 0.000174  loss: 1.0928 (1.1031)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [390/431]  eta: 0:00:45  lr: 0.000174  loss: 1.1000 (1.1036)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [400/431]  eta: 0:00:34  lr: 0.000174  loss: 1.1823 (1.1071)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [410/431]  eta: 0:00:23  lr: 0.000174  loss: 1.2059 (1.1076)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:216]  [420/431]  eta: 0:00:12  lr: 0.000174  loss: 1.0939 (1.1086)  time: 1.1003  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:216]  [430/431]  eta: 0:00:01  lr: 0.000174  loss: 1.1021 (1.1091)  time: 1.0904  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:216] Total time: 0:07:55 (1.1030 s / it)\n",
      "Averaged stats: lr: 0.000174  loss: 1.1021 (1.1091)\n",
      "Valid: [epoch:216]  [ 0/14]  eta: 0:00:36  loss: 1.1025 (1.1025)  time: 2.6149  data: 2.4739  max mem: 15925\n",
      "Valid: [epoch:216]  [13/14]  eta: 0:00:00  loss: 1.0420 (1.0529)  time: 0.2659  data: 0.1768  max mem: 15925\n",
      "Valid: [epoch:216] Total time: 0:00:03 (0.2825 s / it)\n",
      "Averaged stats: loss: 1.0420 (1.0529)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_216_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.053%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:217]  [  0/431]  eta: 0:36:01  lr: 0.000174  loss: 1.0541 (1.0541)  time: 5.0159  data: 3.8292  max mem: 15925\n",
      "Train: [epoch:217]  [ 10/431]  eta: 0:09:47  lr: 0.000174  loss: 1.1740 (1.1602)  time: 1.3944  data: 0.3483  max mem: 15925\n",
      "Train: [epoch:217]  [ 20/431]  eta: 0:08:25  lr: 0.000174  loss: 1.1441 (1.1629)  time: 1.0403  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [ 30/431]  eta: 0:07:54  lr: 0.000174  loss: 1.0601 (1.1197)  time: 1.0660  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [ 40/431]  eta: 0:07:33  lr: 0.000174  loss: 1.0474 (1.1095)  time: 1.0860  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [ 50/431]  eta: 0:07:16  lr: 0.000174  loss: 1.1039 (1.1071)  time: 1.0865  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [ 60/431]  eta: 0:07:01  lr: 0.000174  loss: 1.1193 (1.1069)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [ 70/431]  eta: 0:06:46  lr: 0.000174  loss: 1.1265 (1.1093)  time: 1.0820  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [ 80/431]  eta: 0:06:34  lr: 0.000174  loss: 1.1196 (1.1057)  time: 1.0829  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [ 90/431]  eta: 0:06:21  lr: 0.000174  loss: 1.0703 (1.1029)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [100/431]  eta: 0:06:09  lr: 0.000174  loss: 1.0976 (1.1047)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [110/431]  eta: 0:05:57  lr: 0.000174  loss: 1.0919 (1.0986)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [120/431]  eta: 0:05:46  lr: 0.000174  loss: 1.0451 (1.1013)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [130/431]  eta: 0:05:35  lr: 0.000174  loss: 1.0575 (1.0976)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [140/431]  eta: 0:05:23  lr: 0.000174  loss: 1.0751 (1.1018)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [150/431]  eta: 0:05:11  lr: 0.000174  loss: 1.1418 (1.1040)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [160/431]  eta: 0:05:00  lr: 0.000174  loss: 1.0996 (1.1035)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [170/431]  eta: 0:04:49  lr: 0.000174  loss: 1.0535 (1.1027)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [180/431]  eta: 0:04:38  lr: 0.000174  loss: 1.1155 (1.1057)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [190/431]  eta: 0:04:26  lr: 0.000174  loss: 1.1165 (1.1080)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [200/431]  eta: 0:04:15  lr: 0.000174  loss: 1.0745 (1.1067)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [210/431]  eta: 0:04:04  lr: 0.000174  loss: 1.0532 (1.1069)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [220/431]  eta: 0:03:53  lr: 0.000174  loss: 1.0839 (1.1065)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [230/431]  eta: 0:03:42  lr: 0.000174  loss: 1.0948 (1.1093)  time: 1.0906  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [240/431]  eta: 0:03:30  lr: 0.000174  loss: 1.0831 (1.1073)  time: 1.0832  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [250/431]  eta: 0:03:19  lr: 0.000174  loss: 1.0750 (1.1082)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [260/431]  eta: 0:03:08  lr: 0.000174  loss: 1.1025 (1.1078)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [270/431]  eta: 0:02:57  lr: 0.000174  loss: 1.1087 (1.1094)  time: 1.0859  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [280/431]  eta: 0:02:46  lr: 0.000174  loss: 1.1181 (1.1093)  time: 1.0849  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [290/431]  eta: 0:02:35  lr: 0.000174  loss: 1.1032 (1.1082)  time: 1.0885  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [300/431]  eta: 0:02:24  lr: 0.000174  loss: 1.0540 (1.1104)  time: 1.0880  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [310/431]  eta: 0:02:13  lr: 0.000174  loss: 1.0987 (1.1104)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [320/431]  eta: 0:02:02  lr: 0.000174  loss: 1.1008 (1.1119)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [330/431]  eta: 0:01:51  lr: 0.000174  loss: 1.1485 (1.1137)  time: 1.0872  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [340/431]  eta: 0:01:40  lr: 0.000174  loss: 1.1069 (1.1133)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [350/431]  eta: 0:01:29  lr: 0.000174  loss: 1.0715 (1.1121)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [360/431]  eta: 0:01:18  lr: 0.000174  loss: 1.0504 (1.1105)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [370/431]  eta: 0:01:07  lr: 0.000174  loss: 1.0586 (1.1098)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [380/431]  eta: 0:00:56  lr: 0.000174  loss: 1.0648 (1.1097)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [390/431]  eta: 0:00:45  lr: 0.000174  loss: 1.0746 (1.1100)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [400/431]  eta: 0:00:34  lr: 0.000174  loss: 1.1131 (1.1109)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [410/431]  eta: 0:00:23  lr: 0.000174  loss: 1.1154 (1.1115)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [420/431]  eta: 0:00:12  lr: 0.000174  loss: 1.1122 (1.1109)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217]  [430/431]  eta: 0:00:01  lr: 0.000174  loss: 1.0646 (1.1105)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:217] Total time: 0:07:55 (1.1025 s / it)\n",
      "Averaged stats: lr: 0.000174  loss: 1.0646 (1.1105)\n",
      "Valid: [epoch:217]  [ 0/14]  eta: 0:00:31  loss: 1.0368 (1.0368)  time: 2.2667  data: 2.0785  max mem: 15925\n",
      "Valid: [epoch:217]  [13/14]  eta: 0:00:00  loss: 1.0368 (1.0502)  time: 0.2584  data: 0.1486  max mem: 15925\n",
      "Valid: [epoch:217] Total time: 0:00:03 (0.2722 s / it)\n",
      "Averaged stats: loss: 1.0368 (1.0502)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_217_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:218]  [  0/431]  eta: 0:31:54  lr: 0.000174  loss: 1.0426 (1.0426)  time: 4.4422  data: 3.2282  max mem: 15925\n",
      "Train: [epoch:218]  [ 10/431]  eta: 0:09:30  lr: 0.000174  loss: 1.1337 (1.1288)  time: 1.3547  data: 0.2937  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:218]  [ 20/431]  eta: 0:08:20  lr: 0.000174  loss: 1.1337 (1.1439)  time: 1.0562  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [ 30/431]  eta: 0:07:46  lr: 0.000174  loss: 1.1001 (1.1143)  time: 1.0598  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [ 40/431]  eta: 0:07:27  lr: 0.000174  loss: 1.0707 (1.1063)  time: 1.0688  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [ 50/431]  eta: 0:07:10  lr: 0.000174  loss: 1.0412 (1.0978)  time: 1.0792  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [ 60/431]  eta: 0:06:57  lr: 0.000174  loss: 1.0188 (1.0853)  time: 1.0841  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [ 70/431]  eta: 0:06:45  lr: 0.000174  loss: 1.0327 (1.0922)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [ 80/431]  eta: 0:06:33  lr: 0.000174  loss: 1.1255 (1.0964)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [ 90/431]  eta: 0:06:21  lr: 0.000174  loss: 1.1053 (1.1028)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [100/431]  eta: 0:06:08  lr: 0.000174  loss: 1.0483 (1.0984)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [110/431]  eta: 0:05:57  lr: 0.000174  loss: 1.0609 (1.0959)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [120/431]  eta: 0:05:45  lr: 0.000174  loss: 1.0701 (1.0932)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [130/431]  eta: 0:05:34  lr: 0.000174  loss: 1.1040 (1.0957)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [140/431]  eta: 0:05:22  lr: 0.000174  loss: 1.1082 (1.0966)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [150/431]  eta: 0:05:11  lr: 0.000174  loss: 1.0880 (1.1004)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [160/431]  eta: 0:05:00  lr: 0.000174  loss: 1.2003 (1.1056)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [170/431]  eta: 0:04:49  lr: 0.000174  loss: 1.1614 (1.1039)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [180/431]  eta: 0:04:38  lr: 0.000174  loss: 1.0364 (1.1016)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [190/431]  eta: 0:04:26  lr: 0.000174  loss: 1.0929 (1.1041)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [200/431]  eta: 0:04:15  lr: 0.000174  loss: 1.1169 (1.1044)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [210/431]  eta: 0:04:04  lr: 0.000174  loss: 1.1306 (1.1086)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [220/431]  eta: 0:03:53  lr: 0.000174  loss: 1.1423 (1.1093)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [230/431]  eta: 0:03:42  lr: 0.000174  loss: 1.1276 (1.1128)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [240/431]  eta: 0:03:30  lr: 0.000174  loss: 1.1402 (1.1152)  time: 1.0860  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [250/431]  eta: 0:03:19  lr: 0.000174  loss: 1.0979 (1.1137)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [260/431]  eta: 0:03:08  lr: 0.000174  loss: 1.0495 (1.1127)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [270/431]  eta: 0:02:57  lr: 0.000174  loss: 1.0618 (1.1124)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [280/431]  eta: 0:02:46  lr: 0.000174  loss: 1.0699 (1.1127)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [290/431]  eta: 0:02:35  lr: 0.000174  loss: 1.0838 (1.1131)  time: 1.0888  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [300/431]  eta: 0:02:24  lr: 0.000174  loss: 1.0878 (1.1145)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [310/431]  eta: 0:02:13  lr: 0.000174  loss: 1.0879 (1.1142)  time: 1.0875  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [320/431]  eta: 0:02:02  lr: 0.000174  loss: 1.0679 (1.1127)  time: 1.0925  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [330/431]  eta: 0:01:51  lr: 0.000174  loss: 1.1399 (1.1177)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [340/431]  eta: 0:01:40  lr: 0.000174  loss: 1.1612 (1.1171)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [350/431]  eta: 0:01:29  lr: 0.000174  loss: 1.1080 (1.1178)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [360/431]  eta: 0:01:18  lr: 0.000174  loss: 1.1201 (1.1182)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [370/431]  eta: 0:01:07  lr: 0.000174  loss: 1.1003 (1.1192)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [380/431]  eta: 0:00:56  lr: 0.000174  loss: 1.0865 (1.1170)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [390/431]  eta: 0:00:45  lr: 0.000174  loss: 1.0446 (1.1160)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [400/431]  eta: 0:00:34  lr: 0.000174  loss: 1.0716 (1.1143)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:218]  [410/431]  eta: 0:00:23  lr: 0.000174  loss: 1.0427 (1.1127)  time: 1.0932  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:218]  [420/431]  eta: 0:00:12  lr: 0.000174  loss: 1.0496 (1.1132)  time: 1.0955  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:218]  [430/431]  eta: 0:00:01  lr: 0.000174  loss: 1.0496 (1.1127)  time: 1.0961  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:218] Total time: 0:07:54 (1.1009 s / it)\n",
      "Averaged stats: lr: 0.000174  loss: 1.0496 (1.1127)\n",
      "Valid: [epoch:218]  [ 0/14]  eta: 0:00:35  loss: 1.0389 (1.0389)  time: 2.5293  data: 2.3978  max mem: 15925\n",
      "Valid: [epoch:218]  [13/14]  eta: 0:00:00  loss: 1.0482 (1.0576)  time: 0.2727  data: 0.1714  max mem: 15925\n",
      "Valid: [epoch:218] Total time: 0:00:04 (0.2885 s / it)\n",
      "Averaged stats: loss: 1.0482 (1.0576)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_218_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.058%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:219]  [  0/431]  eta: 0:33:21  lr: 0.000174  loss: 0.9391 (0.9391)  time: 4.6445  data: 3.4485  max mem: 15925\n",
      "Train: [epoch:219]  [ 10/431]  eta: 0:09:40  lr: 0.000174  loss: 1.1665 (1.1900)  time: 1.3783  data: 0.3137  max mem: 15925\n",
      "Train: [epoch:219]  [ 20/431]  eta: 0:08:24  lr: 0.000174  loss: 1.1665 (1.1655)  time: 1.0560  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [ 30/431]  eta: 0:07:50  lr: 0.000174  loss: 1.0727 (1.1277)  time: 1.0602  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [ 40/431]  eta: 0:07:29  lr: 0.000174  loss: 1.0710 (1.1174)  time: 1.0684  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [ 50/431]  eta: 0:07:12  lr: 0.000174  loss: 1.0992 (1.1150)  time: 1.0769  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [ 60/431]  eta: 0:06:59  lr: 0.000174  loss: 1.0838 (1.1060)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [ 70/431]  eta: 0:06:45  lr: 0.000174  loss: 1.0802 (1.1072)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [ 80/431]  eta: 0:06:32  lr: 0.000174  loss: 1.1747 (1.1204)  time: 1.0873  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [ 90/431]  eta: 0:06:20  lr: 0.000174  loss: 1.1404 (1.1183)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [100/431]  eta: 0:06:08  lr: 0.000174  loss: 1.0732 (1.1202)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [110/431]  eta: 0:05:57  lr: 0.000174  loss: 1.0732 (1.1152)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [120/431]  eta: 0:05:45  lr: 0.000174  loss: 1.1071 (1.1167)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [130/431]  eta: 0:05:34  lr: 0.000174  loss: 1.1222 (1.1192)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [140/431]  eta: 0:05:23  lr: 0.000174  loss: 1.0687 (1.1145)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [150/431]  eta: 0:05:11  lr: 0.000174  loss: 1.0442 (1.1132)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [160/431]  eta: 0:05:00  lr: 0.000174  loss: 1.0449 (1.1121)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [170/431]  eta: 0:04:49  lr: 0.000174  loss: 1.0188 (1.1078)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [180/431]  eta: 0:04:38  lr: 0.000174  loss: 1.0395 (1.1068)  time: 1.0960  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:219]  [190/431]  eta: 0:04:26  lr: 0.000174  loss: 1.0620 (1.1064)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [200/431]  eta: 0:04:15  lr: 0.000174  loss: 1.1501 (1.1105)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [210/431]  eta: 0:04:04  lr: 0.000174  loss: 1.1443 (1.1100)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [220/431]  eta: 0:03:53  lr: 0.000174  loss: 1.0611 (1.1069)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [230/431]  eta: 0:03:42  lr: 0.000174  loss: 1.0657 (1.1083)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [240/431]  eta: 0:03:30  lr: 0.000174  loss: 1.0705 (1.1080)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [250/431]  eta: 0:03:19  lr: 0.000174  loss: 1.0660 (1.1074)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [260/431]  eta: 0:03:08  lr: 0.000174  loss: 1.0602 (1.1054)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [270/431]  eta: 0:02:57  lr: 0.000174  loss: 1.0640 (1.1058)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [280/431]  eta: 0:02:46  lr: 0.000174  loss: 1.0885 (1.1048)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [290/431]  eta: 0:02:35  lr: 0.000174  loss: 1.1059 (1.1074)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [300/431]  eta: 0:02:24  lr: 0.000174  loss: 1.1465 (1.1112)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [310/431]  eta: 0:02:13  lr: 0.000174  loss: 1.1217 (1.1099)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [320/431]  eta: 0:02:02  lr: 0.000174  loss: 1.0409 (1.1091)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [330/431]  eta: 0:01:51  lr: 0.000174  loss: 1.1229 (1.1108)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [340/431]  eta: 0:01:40  lr: 0.000174  loss: 1.1323 (1.1112)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [350/431]  eta: 0:01:29  lr: 0.000174  loss: 1.0918 (1.1106)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [360/431]  eta: 0:01:18  lr: 0.000174  loss: 1.0827 (1.1112)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [370/431]  eta: 0:01:07  lr: 0.000174  loss: 1.0853 (1.1105)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [380/431]  eta: 0:00:56  lr: 0.000174  loss: 1.1103 (1.1113)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [390/431]  eta: 0:00:45  lr: 0.000174  loss: 1.1113 (1.1105)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [400/431]  eta: 0:00:34  lr: 0.000174  loss: 1.1225 (1.1113)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [410/431]  eta: 0:00:23  lr: 0.000174  loss: 1.1032 (1.1104)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:219]  [420/431]  eta: 0:00:12  lr: 0.000174  loss: 1.0558 (1.1103)  time: 1.0939  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:219]  [430/431]  eta: 0:00:01  lr: 0.000174  loss: 1.0801 (1.1095)  time: 1.0957  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:219] Total time: 0:07:54 (1.1017 s / it)\n",
      "Averaged stats: lr: 0.000174  loss: 1.0801 (1.1095)\n",
      "Valid: [epoch:219]  [ 0/14]  eta: 0:00:37  loss: 1.1001 (1.1001)  time: 2.6540  data: 2.5173  max mem: 15925\n",
      "Valid: [epoch:219]  [13/14]  eta: 0:00:00  loss: 1.0325 (1.0460)  time: 0.2740  data: 0.1799  max mem: 15925\n",
      "Valid: [epoch:219] Total time: 0:00:04 (0.2901 s / it)\n",
      "Averaged stats: loss: 1.0325 (1.0460)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_219_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:220]  [  0/431]  eta: 0:33:10  lr: 0.000174  loss: 1.1588 (1.1588)  time: 4.6178  data: 3.3985  max mem: 15925\n",
      "Train: [epoch:220]  [ 10/431]  eta: 0:09:34  lr: 0.000174  loss: 1.1588 (1.1608)  time: 1.3638  data: 0.3091  max mem: 15925\n",
      "Train: [epoch:220]  [ 20/431]  eta: 0:08:17  lr: 0.000174  loss: 1.1239 (1.1441)  time: 1.0393  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [ 30/431]  eta: 0:07:47  lr: 0.000174  loss: 1.1239 (1.1337)  time: 1.0556  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [ 40/431]  eta: 0:07:28  lr: 0.000174  loss: 1.1112 (1.1266)  time: 1.0806  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:220]  [ 50/431]  eta: 0:07:11  lr: 0.000174  loss: 1.0978 (1.1103)  time: 1.0841  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [ 60/431]  eta: 0:06:58  lr: 0.000174  loss: 1.0257 (1.1027)  time: 1.0876  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [ 70/431]  eta: 0:06:45  lr: 0.000174  loss: 1.0584 (1.1105)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [ 80/431]  eta: 0:06:32  lr: 0.000174  loss: 1.0898 (1.1105)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [ 90/431]  eta: 0:06:21  lr: 0.000174  loss: 1.0505 (1.1031)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [100/431]  eta: 0:06:08  lr: 0.000174  loss: 1.0509 (1.1037)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [110/431]  eta: 0:05:57  lr: 0.000174  loss: 1.0611 (1.1026)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [120/431]  eta: 0:05:45  lr: 0.000174  loss: 1.0553 (1.1004)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [130/431]  eta: 0:05:34  lr: 0.000174  loss: 1.0558 (1.0996)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [140/431]  eta: 0:05:23  lr: 0.000174  loss: 1.0558 (1.0985)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [150/431]  eta: 0:05:11  lr: 0.000174  loss: 1.0791 (1.1059)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [160/431]  eta: 0:05:00  lr: 0.000174  loss: 1.1056 (1.1069)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [170/431]  eta: 0:04:49  lr: 0.000174  loss: 1.0976 (1.1088)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [180/431]  eta: 0:04:37  lr: 0.000174  loss: 1.0976 (1.1108)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [190/431]  eta: 0:04:26  lr: 0.000174  loss: 1.1869 (1.1139)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [200/431]  eta: 0:04:15  lr: 0.000174  loss: 1.1703 (1.1139)  time: 1.0897  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [210/431]  eta: 0:04:04  lr: 0.000174  loss: 1.0808 (1.1140)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [220/431]  eta: 0:03:53  lr: 0.000174  loss: 1.0782 (1.1122)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [230/431]  eta: 0:03:41  lr: 0.000174  loss: 1.0580 (1.1121)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [240/431]  eta: 0:03:30  lr: 0.000174  loss: 1.0958 (1.1124)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [250/431]  eta: 0:03:19  lr: 0.000174  loss: 1.1013 (1.1132)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [260/431]  eta: 0:03:08  lr: 0.000174  loss: 1.1013 (1.1139)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [270/431]  eta: 0:02:57  lr: 0.000174  loss: 1.1486 (1.1159)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [280/431]  eta: 0:02:46  lr: 0.000174  loss: 1.0993 (1.1145)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [290/431]  eta: 0:02:35  lr: 0.000174  loss: 1.0993 (1.1163)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [300/431]  eta: 0:02:24  lr: 0.000174  loss: 1.1188 (1.1166)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [310/431]  eta: 0:02:13  lr: 0.000174  loss: 1.1028 (1.1156)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [320/431]  eta: 0:02:02  lr: 0.000174  loss: 1.0478 (1.1138)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [330/431]  eta: 0:01:51  lr: 0.000174  loss: 1.0958 (1.1165)  time: 1.0812  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [340/431]  eta: 0:01:40  lr: 0.000174  loss: 1.1260 (1.1153)  time: 1.0832  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [350/431]  eta: 0:01:29  lr: 0.000174  loss: 1.0577 (1.1143)  time: 1.0955  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:220]  [360/431]  eta: 0:01:18  lr: 0.000174  loss: 1.0577 (1.1141)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [370/431]  eta: 0:01:07  lr: 0.000174  loss: 1.0522 (1.1128)  time: 1.0864  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [380/431]  eta: 0:00:56  lr: 0.000174  loss: 1.0587 (1.1128)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [390/431]  eta: 0:00:45  lr: 0.000174  loss: 1.0475 (1.1112)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [400/431]  eta: 0:00:34  lr: 0.000174  loss: 1.0427 (1.1107)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:220]  [410/431]  eta: 0:00:23  lr: 0.000174  loss: 1.1040 (1.1127)  time: 1.0952  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:220]  [420/431]  eta: 0:00:12  lr: 0.000174  loss: 1.1374 (1.1127)  time: 1.0931  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:220]  [430/431]  eta: 0:00:01  lr: 0.000174  loss: 1.0894 (1.1117)  time: 1.0943  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:220] Total time: 0:07:54 (1.1004 s / it)\n",
      "Averaged stats: lr: 0.000174  loss: 1.0894 (1.1117)\n",
      "Valid: [epoch:220]  [ 0/14]  eta: 0:00:36  loss: 1.0040 (1.0040)  time: 2.5943  data: 2.3982  max mem: 15925\n",
      "Valid: [epoch:220]  [13/14]  eta: 0:00:00  loss: 1.0590 (1.0661)  time: 0.2727  data: 0.1714  max mem: 15925\n",
      "Valid: [epoch:220] Total time: 0:00:04 (0.2919 s / it)\n",
      "Averaged stats: loss: 1.0590 (1.0661)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_220_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.066%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:221]  [  0/431]  eta: 0:32:17  lr: 0.000173  loss: 1.2538 (1.2538)  time: 4.4965  data: 3.2779  max mem: 15925\n",
      "Train: [epoch:221]  [ 10/431]  eta: 0:09:31  lr: 0.000173  loss: 1.1381 (1.1210)  time: 1.3586  data: 0.2982  max mem: 15925\n",
      "Train: [epoch:221]  [ 20/431]  eta: 0:08:19  lr: 0.000173  loss: 1.0719 (1.1031)  time: 1.0502  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [ 30/431]  eta: 0:07:49  lr: 0.000173  loss: 1.0345 (1.0906)  time: 1.0674  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [ 40/431]  eta: 0:07:31  lr: 0.000173  loss: 1.1216 (1.1034)  time: 1.0906  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [ 50/431]  eta: 0:07:14  lr: 0.000173  loss: 1.0786 (1.0982)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [ 60/431]  eta: 0:06:59  lr: 0.000173  loss: 1.0408 (1.0880)  time: 1.0870  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [ 70/431]  eta: 0:06:47  lr: 0.000173  loss: 1.0538 (1.0920)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [ 80/431]  eta: 0:06:34  lr: 0.000173  loss: 1.1584 (1.1064)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [ 90/431]  eta: 0:06:22  lr: 0.000173  loss: 1.1469 (1.1092)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [100/431]  eta: 0:06:10  lr: 0.000173  loss: 1.1209 (1.1097)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [110/431]  eta: 0:05:58  lr: 0.000173  loss: 1.1152 (1.1158)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [120/431]  eta: 0:05:47  lr: 0.000173  loss: 1.0847 (1.1165)  time: 1.1029  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:221]  [130/431]  eta: 0:05:35  lr: 0.000173  loss: 1.0758 (1.1140)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [140/431]  eta: 0:05:24  lr: 0.000173  loss: 1.1099 (1.1160)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [150/431]  eta: 0:05:12  lr: 0.000173  loss: 1.1140 (1.1168)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [160/431]  eta: 0:05:01  lr: 0.000173  loss: 1.0895 (1.1148)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [170/431]  eta: 0:04:50  lr: 0.000173  loss: 1.0497 (1.1131)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [180/431]  eta: 0:04:39  lr: 0.000173  loss: 1.0588 (1.1112)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [190/431]  eta: 0:04:27  lr: 0.000173  loss: 1.0750 (1.1119)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [200/431]  eta: 0:04:16  lr: 0.000173  loss: 1.1188 (1.1128)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [210/431]  eta: 0:04:05  lr: 0.000173  loss: 1.1006 (1.1130)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [220/431]  eta: 0:03:54  lr: 0.000173  loss: 1.0372 (1.1124)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [230/431]  eta: 0:03:42  lr: 0.000173  loss: 1.0743 (1.1130)  time: 1.0962  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:221]  [240/431]  eta: 0:03:31  lr: 0.000173  loss: 1.0878 (1.1129)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [250/431]  eta: 0:03:20  lr: 0.000173  loss: 1.1023 (1.1158)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [260/431]  eta: 0:03:09  lr: 0.000173  loss: 1.1023 (1.1151)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [270/431]  eta: 0:02:58  lr: 0.000173  loss: 1.0874 (1.1143)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [280/431]  eta: 0:02:47  lr: 0.000173  loss: 1.1172 (1.1162)  time: 1.0920  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:221]  [290/431]  eta: 0:02:36  lr: 0.000173  loss: 1.1317 (1.1160)  time: 1.0945  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:221]  [300/431]  eta: 0:02:24  lr: 0.000173  loss: 1.1252 (1.1175)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [310/431]  eta: 0:02:13  lr: 0.000173  loss: 1.1133 (1.1182)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [320/431]  eta: 0:02:02  lr: 0.000173  loss: 1.0871 (1.1165)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [330/431]  eta: 0:01:51  lr: 0.000173  loss: 1.0712 (1.1153)  time: 1.0868  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [340/431]  eta: 0:01:40  lr: 0.000173  loss: 1.0835 (1.1167)  time: 1.0911  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [350/431]  eta: 0:01:29  lr: 0.000173  loss: 1.1521 (1.1180)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [360/431]  eta: 0:01:18  lr: 0.000173  loss: 1.0538 (1.1174)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [370/431]  eta: 0:01:07  lr: 0.000173  loss: 1.0348 (1.1168)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [380/431]  eta: 0:00:56  lr: 0.000173  loss: 1.0275 (1.1153)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [390/431]  eta: 0:00:45  lr: 0.000173  loss: 1.0194 (1.1134)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [400/431]  eta: 0:00:34  lr: 0.000173  loss: 1.0923 (1.1139)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:221]  [410/431]  eta: 0:00:23  lr: 0.000173  loss: 1.1198 (1.1139)  time: 1.0874  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:221]  [420/431]  eta: 0:00:12  lr: 0.000173  loss: 1.1133 (1.1128)  time: 1.0907  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:221]  [430/431]  eta: 0:00:01  lr: 0.000173  loss: 1.0437 (1.1116)  time: 1.1071  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:221] Total time: 0:07:55 (1.1042 s / it)\n",
      "Averaged stats: lr: 0.000173  loss: 1.0437 (1.1116)\n",
      "Valid: [epoch:221]  [ 0/14]  eta: 0:00:36  loss: 0.9802 (0.9802)  time: 2.5715  data: 2.4408  max mem: 15925\n",
      "Valid: [epoch:221]  [13/14]  eta: 0:00:00  loss: 1.0342 (1.0468)  time: 0.2678  data: 0.1744  max mem: 15925\n",
      "Valid: [epoch:221] Total time: 0:00:03 (0.2856 s / it)\n",
      "Averaged stats: loss: 1.0342 (1.0468)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_221_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:222]  [  0/431]  eta: 0:33:31  lr: 0.000173  loss: 1.0499 (1.0499)  time: 4.6668  data: 3.5373  max mem: 15925\n",
      "Train: [epoch:222]  [ 10/431]  eta: 0:09:36  lr: 0.000173  loss: 1.1434 (1.1794)  time: 1.3693  data: 0.3218  max mem: 15925\n",
      "Train: [epoch:222]  [ 20/431]  eta: 0:08:20  lr: 0.000173  loss: 1.1045 (1.1433)  time: 1.0459  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [ 30/431]  eta: 0:07:49  lr: 0.000173  loss: 1.0719 (1.1174)  time: 1.0633  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:222]  [ 40/431]  eta: 0:07:29  lr: 0.000173  loss: 1.0483 (1.1005)  time: 1.0795  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [ 50/431]  eta: 0:07:12  lr: 0.000173  loss: 1.0452 (1.0951)  time: 1.0764  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [ 60/431]  eta: 0:06:58  lr: 0.000173  loss: 1.0595 (1.0917)  time: 1.0801  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [ 70/431]  eta: 0:06:45  lr: 0.000173  loss: 1.1119 (1.0977)  time: 1.0909  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [ 80/431]  eta: 0:06:32  lr: 0.000173  loss: 1.1119 (1.1014)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [ 90/431]  eta: 0:06:20  lr: 0.000173  loss: 1.0815 (1.1013)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [100/431]  eta: 0:06:08  lr: 0.000173  loss: 1.0677 (1.0957)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [110/431]  eta: 0:05:57  lr: 0.000173  loss: 1.0355 (1.0980)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [120/431]  eta: 0:05:45  lr: 0.000173  loss: 1.0626 (1.0959)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [130/431]  eta: 0:05:34  lr: 0.000173  loss: 1.0626 (1.0983)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [140/431]  eta: 0:05:23  lr: 0.000173  loss: 1.0933 (1.1025)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [150/431]  eta: 0:05:11  lr: 0.000173  loss: 1.0918 (1.1042)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [160/431]  eta: 0:05:00  lr: 0.000173  loss: 1.0840 (1.1036)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [170/431]  eta: 0:04:49  lr: 0.000173  loss: 1.0872 (1.1065)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [180/431]  eta: 0:04:38  lr: 0.000173  loss: 1.1326 (1.1105)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [190/431]  eta: 0:04:26  lr: 0.000173  loss: 1.0736 (1.1077)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [200/431]  eta: 0:04:15  lr: 0.000173  loss: 1.0694 (1.1082)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [210/431]  eta: 0:04:04  lr: 0.000173  loss: 1.0706 (1.1080)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [220/431]  eta: 0:03:53  lr: 0.000173  loss: 1.0735 (1.1082)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [230/431]  eta: 0:03:42  lr: 0.000173  loss: 1.0735 (1.1092)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [240/431]  eta: 0:03:30  lr: 0.000173  loss: 1.0587 (1.1083)  time: 1.0884  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [250/431]  eta: 0:03:19  lr: 0.000173  loss: 1.0947 (1.1095)  time: 1.0844  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [260/431]  eta: 0:03:08  lr: 0.000173  loss: 1.0947 (1.1093)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [270/431]  eta: 0:02:57  lr: 0.000173  loss: 1.0417 (1.1068)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [280/431]  eta: 0:02:46  lr: 0.000173  loss: 1.0103 (1.1052)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [290/431]  eta: 0:02:35  lr: 0.000173  loss: 1.0586 (1.1042)  time: 1.0882  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [300/431]  eta: 0:02:24  lr: 0.000173  loss: 1.0701 (1.1050)  time: 1.0859  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [310/431]  eta: 0:02:13  lr: 0.000173  loss: 1.0701 (1.1052)  time: 1.0906  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [320/431]  eta: 0:02:02  lr: 0.000173  loss: 1.0800 (1.1045)  time: 1.1039  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:222]  [330/431]  eta: 0:01:51  lr: 0.000173  loss: 1.0800 (1.1072)  time: 1.1050  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:222]  [340/431]  eta: 0:01:40  lr: 0.000173  loss: 1.0787 (1.1089)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [350/431]  eta: 0:01:29  lr: 0.000173  loss: 1.1208 (1.1106)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [360/431]  eta: 0:01:18  lr: 0.000173  loss: 1.0923 (1.1093)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [370/431]  eta: 0:01:07  lr: 0.000173  loss: 1.0519 (1.1086)  time: 1.0857  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [380/431]  eta: 0:00:56  lr: 0.000173  loss: 1.0934 (1.1083)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [390/431]  eta: 0:00:45  lr: 0.000173  loss: 1.0934 (1.1091)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [400/431]  eta: 0:00:34  lr: 0.000173  loss: 1.0831 (1.1085)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [410/431]  eta: 0:00:23  lr: 0.000173  loss: 1.0864 (1.1087)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:222]  [420/431]  eta: 0:00:12  lr: 0.000173  loss: 1.0808 (1.1084)  time: 1.0909  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:222]  [430/431]  eta: 0:00:01  lr: 0.000173  loss: 1.0768 (1.1090)  time: 1.0976  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:222] Total time: 0:07:54 (1.1007 s / it)\n",
      "Averaged stats: lr: 0.000173  loss: 1.0768 (1.1090)\n",
      "Valid: [epoch:222]  [ 0/14]  eta: 0:00:38  loss: 1.0173 (1.0173)  time: 2.7788  data: 2.5703  max mem: 15925\n",
      "Valid: [epoch:222]  [13/14]  eta: 0:00:00  loss: 1.0474 (1.0583)  time: 0.2847  data: 0.1837  max mem: 15925\n",
      "Valid: [epoch:222] Total time: 0:00:04 (0.2987 s / it)\n",
      "Averaged stats: loss: 1.0474 (1.0583)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_222_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.058%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:223]  [  0/431]  eta: 0:39:19  lr: 0.000173  loss: 1.3383 (1.3383)  time: 5.4748  data: 4.3175  max mem: 15925\n",
      "Train: [epoch:223]  [ 10/431]  eta: 0:10:06  lr: 0.000173  loss: 1.0647 (1.1026)  time: 1.4411  data: 0.3927  max mem: 15925\n",
      "Train: [epoch:223]  [ 20/431]  eta: 0:08:35  lr: 0.000173  loss: 1.0926 (1.1101)  time: 1.0427  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [ 30/431]  eta: 0:07:59  lr: 0.000173  loss: 1.0926 (1.1194)  time: 1.0607  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [ 40/431]  eta: 0:07:36  lr: 0.000173  loss: 1.0663 (1.1165)  time: 1.0775  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [ 50/431]  eta: 0:07:19  lr: 0.000173  loss: 1.1339 (1.1223)  time: 1.0899  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:223]  [ 60/431]  eta: 0:07:04  lr: 0.000173  loss: 1.0578 (1.1096)  time: 1.0983  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:223]  [ 70/431]  eta: 0:06:51  lr: 0.000173  loss: 1.0429 (1.1126)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [ 80/431]  eta: 0:06:38  lr: 0.000173  loss: 1.0747 (1.1163)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [ 90/431]  eta: 0:06:25  lr: 0.000173  loss: 1.0671 (1.1130)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [100/431]  eta: 0:06:13  lr: 0.000173  loss: 1.0612 (1.1113)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [110/431]  eta: 0:06:01  lr: 0.000173  loss: 1.0843 (1.1120)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [120/431]  eta: 0:05:49  lr: 0.000173  loss: 1.1001 (1.1135)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [130/431]  eta: 0:05:38  lr: 0.000173  loss: 1.0995 (1.1125)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [140/431]  eta: 0:05:26  lr: 0.000173  loss: 1.0995 (1.1126)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [150/431]  eta: 0:05:14  lr: 0.000173  loss: 1.1156 (1.1157)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [160/431]  eta: 0:05:03  lr: 0.000173  loss: 1.1141 (1.1136)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [170/431]  eta: 0:04:51  lr: 0.000173  loss: 0.9993 (1.1092)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [180/431]  eta: 0:04:40  lr: 0.000173  loss: 1.0217 (1.1082)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [190/431]  eta: 0:04:29  lr: 0.000173  loss: 1.0646 (1.1096)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [200/431]  eta: 0:04:17  lr: 0.000173  loss: 1.0935 (1.1107)  time: 1.0938  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:223]  [210/431]  eta: 0:04:06  lr: 0.000173  loss: 1.1236 (1.1119)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [220/431]  eta: 0:03:55  lr: 0.000173  loss: 1.1236 (1.1149)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [230/431]  eta: 0:03:43  lr: 0.000173  loss: 1.1520 (1.1165)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [240/431]  eta: 0:03:32  lr: 0.000173  loss: 1.1089 (1.1170)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [250/431]  eta: 0:03:21  lr: 0.000173  loss: 1.1328 (1.1179)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [260/431]  eta: 0:03:10  lr: 0.000173  loss: 1.0921 (1.1171)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [270/431]  eta: 0:02:58  lr: 0.000173  loss: 1.0599 (1.1171)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [280/431]  eta: 0:02:47  lr: 0.000173  loss: 1.1186 (1.1155)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [290/431]  eta: 0:02:36  lr: 0.000173  loss: 1.0785 (1.1156)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [300/431]  eta: 0:02:25  lr: 0.000173  loss: 1.0433 (1.1131)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [310/431]  eta: 0:02:14  lr: 0.000173  loss: 1.0433 (1.1129)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [320/431]  eta: 0:02:03  lr: 0.000173  loss: 1.0857 (1.1116)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [330/431]  eta: 0:01:51  lr: 0.000173  loss: 1.0858 (1.1133)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [340/431]  eta: 0:01:40  lr: 0.000173  loss: 1.1051 (1.1133)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [350/431]  eta: 0:01:29  lr: 0.000173  loss: 1.1138 (1.1141)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [360/431]  eta: 0:01:18  lr: 0.000173  loss: 1.1138 (1.1150)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [370/431]  eta: 0:01:07  lr: 0.000173  loss: 1.0935 (1.1148)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [380/431]  eta: 0:00:56  lr: 0.000173  loss: 1.0786 (1.1133)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [390/431]  eta: 0:00:45  lr: 0.000173  loss: 1.0523 (1.1124)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [400/431]  eta: 0:00:34  lr: 0.000173  loss: 1.0648 (1.1132)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [410/431]  eta: 0:00:23  lr: 0.000173  loss: 1.0613 (1.1130)  time: 1.0877  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:223]  [420/431]  eta: 0:00:12  lr: 0.000173  loss: 1.0456 (1.1122)  time: 1.0893  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:223]  [430/431]  eta: 0:00:01  lr: 0.000173  loss: 1.0655 (1.1124)  time: 1.0960  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:223] Total time: 0:07:56 (1.1065 s / it)\n",
      "Averaged stats: lr: 0.000173  loss: 1.0655 (1.1124)\n",
      "Valid: [epoch:223]  [ 0/14]  eta: 0:00:36  loss: 0.9844 (0.9844)  time: 2.6126  data: 2.4615  max mem: 15925\n",
      "Valid: [epoch:223]  [13/14]  eta: 0:00:00  loss: 1.0392 (1.0504)  time: 0.2883  data: 0.1759  max mem: 15925\n",
      "Valid: [epoch:223] Total time: 0:00:04 (0.3056 s / it)\n",
      "Averaged stats: loss: 1.0392 (1.0504)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_223_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:224]  [  0/431]  eta: 0:31:15  lr: 0.000173  loss: 1.1873 (1.1873)  time: 4.3527  data: 3.1726  max mem: 15925\n",
      "Train: [epoch:224]  [ 10/431]  eta: 0:09:23  lr: 0.000173  loss: 1.1732 (1.1304)  time: 1.3379  data: 0.2886  max mem: 15925\n",
      "Train: [epoch:224]  [ 20/431]  eta: 0:08:12  lr: 0.000173  loss: 1.1077 (1.1269)  time: 1.0410  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [ 30/431]  eta: 0:07:43  lr: 0.000173  loss: 1.0813 (1.1191)  time: 1.0552  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [ 40/431]  eta: 0:07:24  lr: 0.000173  loss: 1.1070 (1.1210)  time: 1.0717  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [ 50/431]  eta: 0:07:08  lr: 0.000173  loss: 1.1040 (1.1085)  time: 1.0764  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [ 60/431]  eta: 0:06:55  lr: 0.000173  loss: 1.0442 (1.0966)  time: 1.0821  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [ 70/431]  eta: 0:06:42  lr: 0.000173  loss: 1.0323 (1.0973)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [ 80/431]  eta: 0:06:30  lr: 0.000173  loss: 1.0666 (1.0954)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [ 90/431]  eta: 0:06:18  lr: 0.000173  loss: 1.0646 (1.0949)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [100/431]  eta: 0:06:07  lr: 0.000173  loss: 1.0620 (1.0931)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [110/431]  eta: 0:05:56  lr: 0.000173  loss: 1.0579 (1.0922)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [120/431]  eta: 0:05:44  lr: 0.000173  loss: 1.0475 (1.0909)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [130/431]  eta: 0:05:33  lr: 0.000173  loss: 1.0735 (1.0900)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [140/431]  eta: 0:05:21  lr: 0.000173  loss: 1.0735 (1.0949)  time: 1.0874  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [150/431]  eta: 0:05:10  lr: 0.000173  loss: 1.0780 (1.1004)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [160/431]  eta: 0:04:59  lr: 0.000173  loss: 1.1347 (1.1052)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [170/431]  eta: 0:04:48  lr: 0.000173  loss: 1.1328 (1.1065)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [180/431]  eta: 0:04:37  lr: 0.000173  loss: 1.1328 (1.1073)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [190/431]  eta: 0:04:26  lr: 0.000173  loss: 1.1232 (1.1080)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [200/431]  eta: 0:04:15  lr: 0.000173  loss: 1.1232 (1.1097)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [210/431]  eta: 0:04:04  lr: 0.000173  loss: 1.1205 (1.1104)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [220/431]  eta: 0:03:53  lr: 0.000173  loss: 1.0886 (1.1087)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [230/431]  eta: 0:03:42  lr: 0.000173  loss: 1.0456 (1.1081)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [240/431]  eta: 0:03:31  lr: 0.000173  loss: 1.0887 (1.1077)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [250/431]  eta: 0:03:20  lr: 0.000173  loss: 1.1429 (1.1093)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [260/431]  eta: 0:03:08  lr: 0.000173  loss: 1.1205 (1.1087)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [270/431]  eta: 0:02:57  lr: 0.000173  loss: 1.1160 (1.1093)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [280/431]  eta: 0:02:46  lr: 0.000173  loss: 1.1074 (1.1087)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [290/431]  eta: 0:02:35  lr: 0.000173  loss: 1.0727 (1.1076)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [300/431]  eta: 0:02:24  lr: 0.000173  loss: 1.0727 (1.1076)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [310/431]  eta: 0:02:13  lr: 0.000173  loss: 1.0357 (1.1060)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [320/431]  eta: 0:02:02  lr: 0.000173  loss: 1.0687 (1.1060)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [330/431]  eta: 0:01:51  lr: 0.000173  loss: 1.1089 (1.1075)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [340/431]  eta: 0:01:40  lr: 0.000173  loss: 1.1323 (1.1085)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [350/431]  eta: 0:01:29  lr: 0.000173  loss: 1.0868 (1.1089)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [360/431]  eta: 0:01:18  lr: 0.000173  loss: 1.0694 (1.1073)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [370/431]  eta: 0:01:07  lr: 0.000173  loss: 1.0283 (1.1063)  time: 1.0917  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:224]  [380/431]  eta: 0:00:56  lr: 0.000173  loss: 1.0537 (1.1065)  time: 1.0886  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [390/431]  eta: 0:00:45  lr: 0.000173  loss: 1.1371 (1.1086)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [400/431]  eta: 0:00:34  lr: 0.000173  loss: 1.1037 (1.1082)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [410/431]  eta: 0:00:23  lr: 0.000173  loss: 1.0820 (1.1094)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:224]  [420/431]  eta: 0:00:12  lr: 0.000173  loss: 1.0650 (1.1094)  time: 1.0991  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:224]  [430/431]  eta: 0:00:01  lr: 0.000173  loss: 1.0794 (1.1097)  time: 1.1073  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:224] Total time: 0:07:56 (1.1057 s / it)\n",
      "Averaged stats: lr: 0.000173  loss: 1.0794 (1.1097)\n",
      "Valid: [epoch:224]  [ 0/14]  eta: 0:00:36  loss: 0.9927 (0.9927)  time: 2.5848  data: 2.4422  max mem: 15925\n",
      "Valid: [epoch:224]  [13/14]  eta: 0:00:00  loss: 1.0469 (1.0561)  time: 0.2713  data: 0.1745  max mem: 15925\n",
      "Valid: [epoch:224] Total time: 0:00:04 (0.2873 s / it)\n",
      "Averaged stats: loss: 1.0469 (1.0561)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_224_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.056%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:225]  [  0/431]  eta: 0:33:26  lr: 0.000172  loss: 1.1147 (1.1147)  time: 4.6561  data: 3.4881  max mem: 15925\n",
      "Train: [epoch:225]  [ 10/431]  eta: 0:09:46  lr: 0.000172  loss: 1.1543 (1.1623)  time: 1.3937  data: 0.3173  max mem: 15925\n",
      "Train: [epoch:225]  [ 20/431]  eta: 0:08:29  lr: 0.000172  loss: 1.1397 (1.1389)  time: 1.0701  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [ 30/431]  eta: 0:07:56  lr: 0.000172  loss: 1.0812 (1.1134)  time: 1.0741  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [ 40/431]  eta: 0:07:34  lr: 0.000172  loss: 1.0626 (1.1064)  time: 1.0817  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [ 50/431]  eta: 0:07:18  lr: 0.000172  loss: 1.0647 (1.1030)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [ 60/431]  eta: 0:07:04  lr: 0.000172  loss: 1.0647 (1.1069)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [ 70/431]  eta: 0:06:50  lr: 0.000172  loss: 1.0814 (1.1060)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [ 80/431]  eta: 0:06:37  lr: 0.000172  loss: 1.0814 (1.1079)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [ 90/431]  eta: 0:06:25  lr: 0.000172  loss: 1.0602 (1.1097)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [100/431]  eta: 0:06:12  lr: 0.000172  loss: 1.0602 (1.1092)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [110/431]  eta: 0:06:00  lr: 0.000172  loss: 1.0790 (1.1092)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [120/431]  eta: 0:05:49  lr: 0.000172  loss: 1.0597 (1.1037)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [130/431]  eta: 0:05:37  lr: 0.000172  loss: 1.0481 (1.1012)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [140/431]  eta: 0:05:26  lr: 0.000172  loss: 1.0663 (1.1036)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [150/431]  eta: 0:05:14  lr: 0.000172  loss: 1.1543 (1.1027)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [160/431]  eta: 0:05:02  lr: 0.000172  loss: 1.1506 (1.1092)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [170/431]  eta: 0:04:51  lr: 0.000172  loss: 1.1481 (1.1094)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [180/431]  eta: 0:04:40  lr: 0.000172  loss: 1.1457 (1.1140)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [190/431]  eta: 0:04:28  lr: 0.000172  loss: 1.1543 (1.1176)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [200/431]  eta: 0:04:17  lr: 0.000172  loss: 1.1078 (1.1158)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [210/431]  eta: 0:04:06  lr: 0.000172  loss: 1.0456 (1.1147)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [220/431]  eta: 0:03:54  lr: 0.000172  loss: 1.1017 (1.1142)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [230/431]  eta: 0:03:43  lr: 0.000172  loss: 1.1189 (1.1181)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [240/431]  eta: 0:03:32  lr: 0.000172  loss: 1.1329 (1.1194)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [250/431]  eta: 0:03:21  lr: 0.000172  loss: 1.1022 (1.1204)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [260/431]  eta: 0:03:09  lr: 0.000172  loss: 1.1001 (1.1191)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [270/431]  eta: 0:02:58  lr: 0.000172  loss: 1.0670 (1.1178)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [280/431]  eta: 0:02:47  lr: 0.000172  loss: 1.0670 (1.1165)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [290/431]  eta: 0:02:36  lr: 0.000172  loss: 1.0444 (1.1153)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [300/431]  eta: 0:02:25  lr: 0.000172  loss: 1.0929 (1.1154)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [310/431]  eta: 0:02:14  lr: 0.000172  loss: 1.0973 (1.1143)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [320/431]  eta: 0:02:03  lr: 0.000172  loss: 1.0405 (1.1121)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [330/431]  eta: 0:01:52  lr: 0.000172  loss: 1.0517 (1.1110)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [340/431]  eta: 0:01:40  lr: 0.000172  loss: 1.0896 (1.1108)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [350/431]  eta: 0:01:29  lr: 0.000172  loss: 1.1097 (1.1104)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [360/431]  eta: 0:01:18  lr: 0.000172  loss: 1.0863 (1.1110)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [370/431]  eta: 0:01:07  lr: 0.000172  loss: 1.0587 (1.1095)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [380/431]  eta: 0:00:56  lr: 0.000172  loss: 1.0587 (1.1092)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [390/431]  eta: 0:00:45  lr: 0.000172  loss: 1.0805 (1.1085)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [400/431]  eta: 0:00:34  lr: 0.000172  loss: 1.1046 (1.1091)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [410/431]  eta: 0:00:23  lr: 0.000172  loss: 1.0985 (1.1089)  time: 1.0906  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:225]  [420/431]  eta: 0:00:12  lr: 0.000172  loss: 1.0683 (1.1076)  time: 1.0980  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:225]  [430/431]  eta: 0:00:01  lr: 0.000172  loss: 1.0586 (1.1067)  time: 1.1104  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:225] Total time: 0:07:57 (1.1087 s / it)\n",
      "Averaged stats: lr: 0.000172  loss: 1.0586 (1.1067)\n",
      "Valid: [epoch:225]  [ 0/14]  eta: 0:00:33  loss: 0.9920 (0.9920)  time: 2.3674  data: 2.2083  max mem: 15925\n",
      "Valid: [epoch:225]  [13/14]  eta: 0:00:00  loss: 1.0471 (1.0556)  time: 0.2738  data: 0.1592  max mem: 15925\n",
      "Valid: [epoch:225] Total time: 0:00:04 (0.2889 s / it)\n",
      "Averaged stats: loss: 1.0471 (1.0556)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_225_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.056%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:226]  [  0/431]  eta: 0:30:39  lr: 0.000172  loss: 1.3225 (1.3225)  time: 4.2677  data: 3.0538  max mem: 15925\n",
      "Train: [epoch:226]  [ 10/431]  eta: 0:09:25  lr: 0.000172  loss: 1.0962 (1.0994)  time: 1.3434  data: 0.2778  max mem: 15925\n",
      "Train: [epoch:226]  [ 20/431]  eta: 0:08:17  lr: 0.000172  loss: 1.1095 (1.1458)  time: 1.0588  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:226]  [ 30/431]  eta: 0:07:48  lr: 0.000172  loss: 1.1211 (1.1277)  time: 1.0729  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:226]  [ 40/431]  eta: 0:07:30  lr: 0.000172  loss: 1.0495 (1.1102)  time: 1.0877  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [ 50/431]  eta: 0:07:15  lr: 0.000172  loss: 1.0921 (1.1175)  time: 1.1014  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:226]  [ 60/431]  eta: 0:07:01  lr: 0.000172  loss: 1.1035 (1.1067)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [ 70/431]  eta: 0:06:47  lr: 0.000172  loss: 1.0926 (1.1135)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [ 80/431]  eta: 0:06:35  lr: 0.000172  loss: 1.1455 (1.1128)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [ 90/431]  eta: 0:06:22  lr: 0.000172  loss: 1.1250 (1.1162)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [100/431]  eta: 0:06:10  lr: 0.000172  loss: 1.0888 (1.1154)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [110/431]  eta: 0:05:58  lr: 0.000172  loss: 1.0670 (1.1071)  time: 1.0893  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [120/431]  eta: 0:05:46  lr: 0.000172  loss: 1.0014 (1.1041)  time: 1.0912  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [130/431]  eta: 0:05:35  lr: 0.000172  loss: 1.0282 (1.1031)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [140/431]  eta: 0:05:23  lr: 0.000172  loss: 1.0604 (1.1026)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [150/431]  eta: 0:05:12  lr: 0.000172  loss: 1.0727 (1.1027)  time: 1.0878  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [160/431]  eta: 0:05:00  lr: 0.000172  loss: 1.0866 (1.1028)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [170/431]  eta: 0:04:49  lr: 0.000172  loss: 1.1122 (1.1067)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [180/431]  eta: 0:04:38  lr: 0.000172  loss: 1.1026 (1.1066)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [190/431]  eta: 0:04:27  lr: 0.000172  loss: 1.1027 (1.1081)  time: 1.1058  data: 0.0008  max mem: 15925\n",
      "Train: [epoch:226]  [200/431]  eta: 0:04:15  lr: 0.000172  loss: 1.1135 (1.1092)  time: 1.0932  data: 0.0008  max mem: 15925\n",
      "Train: [epoch:226]  [210/431]  eta: 0:04:04  lr: 0.000172  loss: 1.0788 (1.1078)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [220/431]  eta: 0:03:53  lr: 0.000172  loss: 1.0892 (1.1086)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [230/431]  eta: 0:03:42  lr: 0.000172  loss: 1.1118 (1.1090)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [240/431]  eta: 0:03:31  lr: 0.000172  loss: 1.0695 (1.1069)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [250/431]  eta: 0:03:20  lr: 0.000172  loss: 1.1009 (1.1079)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [260/431]  eta: 0:03:09  lr: 0.000172  loss: 1.1006 (1.1086)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [270/431]  eta: 0:02:57  lr: 0.000172  loss: 1.0512 (1.1077)  time: 1.0886  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [280/431]  eta: 0:02:46  lr: 0.000172  loss: 1.0496 (1.1066)  time: 1.0925  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [290/431]  eta: 0:02:35  lr: 0.000172  loss: 1.0642 (1.1051)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [300/431]  eta: 0:02:24  lr: 0.000172  loss: 1.1250 (1.1087)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [310/431]  eta: 0:02:13  lr: 0.000172  loss: 1.1266 (1.1081)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [320/431]  eta: 0:02:02  lr: 0.000172  loss: 1.0977 (1.1088)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [330/431]  eta: 0:01:51  lr: 0.000172  loss: 1.0838 (1.1078)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [340/431]  eta: 0:01:40  lr: 0.000172  loss: 1.0808 (1.1093)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [350/431]  eta: 0:01:29  lr: 0.000172  loss: 1.0808 (1.1091)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [360/431]  eta: 0:01:18  lr: 0.000172  loss: 1.0878 (1.1085)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [370/431]  eta: 0:01:07  lr: 0.000172  loss: 1.0900 (1.1094)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [380/431]  eta: 0:00:56  lr: 0.000172  loss: 1.1016 (1.1098)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [390/431]  eta: 0:00:45  lr: 0.000172  loss: 1.1191 (1.1095)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [400/431]  eta: 0:00:34  lr: 0.000172  loss: 1.1268 (1.1098)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:226]  [410/431]  eta: 0:00:23  lr: 0.000172  loss: 1.1334 (1.1103)  time: 1.1033  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:226]  [420/431]  eta: 0:00:12  lr: 0.000172  loss: 1.1129 (1.1097)  time: 1.1059  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:226]  [430/431]  eta: 0:00:01  lr: 0.000172  loss: 1.0679 (1.1096)  time: 1.1018  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:226] Total time: 0:07:55 (1.1043 s / it)\n",
      "Averaged stats: lr: 0.000172  loss: 1.0679 (1.1096)\n",
      "Valid: [epoch:226]  [ 0/14]  eta: 0:00:35  loss: 1.1007 (1.1007)  time: 2.5642  data: 2.4425  max mem: 15925\n",
      "Valid: [epoch:226]  [13/14]  eta: 0:00:00  loss: 1.0360 (1.0485)  time: 0.2733  data: 0.1745  max mem: 15925\n",
      "Valid: [epoch:226] Total time: 0:00:04 (0.2909 s / it)\n",
      "Averaged stats: loss: 1.0360 (1.0485)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_226_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:227]  [  0/431]  eta: 0:33:16  lr: 0.000172  loss: 1.5232 (1.5232)  time: 4.6322  data: 3.4485  max mem: 15925\n",
      "Train: [epoch:227]  [ 10/431]  eta: 0:09:39  lr: 0.000172  loss: 1.1440 (1.2202)  time: 1.3754  data: 0.3137  max mem: 15925\n",
      "Train: [epoch:227]  [ 20/431]  eta: 0:08:25  lr: 0.000172  loss: 1.1022 (1.1510)  time: 1.0609  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [ 30/431]  eta: 0:07:54  lr: 0.000172  loss: 1.0347 (1.1135)  time: 1.0759  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [ 40/431]  eta: 0:07:33  lr: 0.000172  loss: 1.0333 (1.1090)  time: 1.0847  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [ 50/431]  eta: 0:07:16  lr: 0.000172  loss: 1.0503 (1.0992)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [ 60/431]  eta: 0:07:02  lr: 0.000172  loss: 1.0503 (1.1009)  time: 1.0949  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:227]  [ 70/431]  eta: 0:06:48  lr: 0.000172  loss: 1.1335 (1.1058)  time: 1.0979  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:227]  [ 80/431]  eta: 0:06:36  lr: 0.000172  loss: 1.1084 (1.1081)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [ 90/431]  eta: 0:06:23  lr: 0.000172  loss: 1.0987 (1.1061)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [100/431]  eta: 0:06:11  lr: 0.000172  loss: 1.0566 (1.1023)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [110/431]  eta: 0:05:59  lr: 0.000172  loss: 1.0688 (1.0991)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [120/431]  eta: 0:05:48  lr: 0.000172  loss: 1.0688 (1.0949)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [130/431]  eta: 0:05:37  lr: 0.000172  loss: 1.0824 (1.0983)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [140/431]  eta: 0:05:25  lr: 0.000172  loss: 1.0951 (1.0971)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [150/431]  eta: 0:05:14  lr: 0.000172  loss: 1.0883 (1.0980)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [160/431]  eta: 0:05:03  lr: 0.000172  loss: 1.1364 (1.1046)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [170/431]  eta: 0:04:51  lr: 0.000172  loss: 1.0817 (1.1005)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [180/431]  eta: 0:04:40  lr: 0.000172  loss: 1.0412 (1.0996)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [190/431]  eta: 0:04:29  lr: 0.000172  loss: 1.0731 (1.1002)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [200/431]  eta: 0:04:18  lr: 0.000172  loss: 1.0731 (1.0984)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [210/431]  eta: 0:04:06  lr: 0.000172  loss: 1.0445 (1.0982)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [220/431]  eta: 0:03:55  lr: 0.000172  loss: 1.0414 (1.0963)  time: 1.1030  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:227]  [230/431]  eta: 0:03:44  lr: 0.000172  loss: 1.0742 (1.0982)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [240/431]  eta: 0:03:32  lr: 0.000172  loss: 1.1213 (1.1013)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [250/431]  eta: 0:03:21  lr: 0.000172  loss: 1.1202 (1.1037)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [260/431]  eta: 0:03:10  lr: 0.000172  loss: 1.1259 (1.1055)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [270/431]  eta: 0:02:59  lr: 0.000172  loss: 1.0763 (1.1047)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [280/431]  eta: 0:02:48  lr: 0.000172  loss: 1.0700 (1.1040)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [290/431]  eta: 0:02:37  lr: 0.000172  loss: 1.0931 (1.1060)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [300/431]  eta: 0:02:25  lr: 0.000172  loss: 1.1465 (1.1088)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [310/431]  eta: 0:02:14  lr: 0.000172  loss: 1.0810 (1.1090)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [320/431]  eta: 0:02:03  lr: 0.000172  loss: 1.0810 (1.1082)  time: 1.1166  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:227]  [330/431]  eta: 0:01:52  lr: 0.000172  loss: 1.1080 (1.1104)  time: 1.1175  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:227]  [340/431]  eta: 0:01:41  lr: 0.000172  loss: 1.1329 (1.1108)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [350/431]  eta: 0:01:30  lr: 0.000172  loss: 1.1036 (1.1099)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [360/431]  eta: 0:01:19  lr: 0.000172  loss: 1.0484 (1.1103)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [370/431]  eta: 0:01:07  lr: 0.000172  loss: 1.0784 (1.1110)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [380/431]  eta: 0:00:56  lr: 0.000172  loss: 1.1034 (1.1113)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [390/431]  eta: 0:00:45  lr: 0.000172  loss: 1.1034 (1.1122)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [400/431]  eta: 0:00:34  lr: 0.000172  loss: 1.0685 (1.1113)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [410/431]  eta: 0:00:23  lr: 0.000172  loss: 1.0414 (1.1102)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:227]  [420/431]  eta: 0:00:12  lr: 0.000172  loss: 1.0206 (1.1098)  time: 1.1005  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:227]  [430/431]  eta: 0:00:01  lr: 0.000172  loss: 1.0419 (1.1094)  time: 1.0987  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:227] Total time: 0:07:59 (1.1131 s / it)\n",
      "Averaged stats: lr: 0.000172  loss: 1.0419 (1.1094)\n",
      "Valid: [epoch:227]  [ 0/14]  eta: 0:00:36  loss: 1.0729 (1.0729)  time: 2.6329  data: 2.4818  max mem: 15925\n",
      "Valid: [epoch:227]  [13/14]  eta: 0:00:00  loss: 1.0729 (1.0805)  time: 0.2758  data: 0.1774  max mem: 15925\n",
      "Valid: [epoch:227] Total time: 0:00:04 (0.2909 s / it)\n",
      "Averaged stats: loss: 1.0729 (1.0805)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_227_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.080%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:228]  [  0/431]  eta: 0:33:45  lr: 0.000172  loss: 1.1459 (1.1459)  time: 4.7002  data: 3.5143  max mem: 15925\n",
      "Train: [epoch:228]  [ 10/431]  eta: 0:09:37  lr: 0.000172  loss: 1.1276 (1.1532)  time: 1.3727  data: 0.3197  max mem: 15925\n",
      "Train: [epoch:228]  [ 20/431]  eta: 0:08:19  lr: 0.000172  loss: 1.1263 (1.1466)  time: 1.0406  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [ 30/431]  eta: 0:07:49  lr: 0.000172  loss: 1.0839 (1.1280)  time: 1.0615  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [ 40/431]  eta: 0:07:29  lr: 0.000172  loss: 1.0839 (1.1272)  time: 1.0809  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [ 50/431]  eta: 0:07:14  lr: 0.000172  loss: 1.1174 (1.1167)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [ 60/431]  eta: 0:07:00  lr: 0.000172  loss: 1.0610 (1.1079)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [ 70/431]  eta: 0:06:48  lr: 0.000172  loss: 1.0731 (1.1101)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [ 80/431]  eta: 0:06:36  lr: 0.000172  loss: 1.1575 (1.1161)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [ 90/431]  eta: 0:06:24  lr: 0.000172  loss: 1.1229 (1.1164)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [100/431]  eta: 0:06:12  lr: 0.000172  loss: 1.1072 (1.1121)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [110/431]  eta: 0:06:01  lr: 0.000172  loss: 1.0348 (1.1069)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [120/431]  eta: 0:05:49  lr: 0.000172  loss: 1.0528 (1.1063)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [130/431]  eta: 0:05:38  lr: 0.000172  loss: 1.0683 (1.1059)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [140/431]  eta: 0:05:26  lr: 0.000172  loss: 1.0307 (1.1035)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [150/431]  eta: 0:05:15  lr: 0.000172  loss: 1.0209 (1.0978)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [160/431]  eta: 0:05:03  lr: 0.000172  loss: 1.0209 (1.0975)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [170/431]  eta: 0:04:52  lr: 0.000172  loss: 1.0695 (1.0970)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [180/431]  eta: 0:04:40  lr: 0.000172  loss: 1.0747 (1.0997)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [190/431]  eta: 0:04:28  lr: 0.000172  loss: 1.1330 (1.1030)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [200/431]  eta: 0:04:17  lr: 0.000172  loss: 1.1216 (1.1060)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [210/431]  eta: 0:04:06  lr: 0.000172  loss: 1.1005 (1.1044)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [220/431]  eta: 0:03:55  lr: 0.000172  loss: 1.1107 (1.1076)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [230/431]  eta: 0:03:43  lr: 0.000172  loss: 1.1096 (1.1070)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [240/431]  eta: 0:03:32  lr: 0.000172  loss: 1.0628 (1.1053)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [250/431]  eta: 0:03:21  lr: 0.000172  loss: 1.0587 (1.1052)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [260/431]  eta: 0:03:10  lr: 0.000172  loss: 1.0448 (1.1038)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [270/431]  eta: 0:02:59  lr: 0.000172  loss: 1.0362 (1.1036)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [280/431]  eta: 0:02:48  lr: 0.000172  loss: 1.0212 (1.1011)  time: 1.1061  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:228]  [290/431]  eta: 0:02:36  lr: 0.000172  loss: 1.0372 (1.1023)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [300/431]  eta: 0:02:25  lr: 0.000172  loss: 1.1262 (1.1036)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [310/431]  eta: 0:02:14  lr: 0.000172  loss: 1.0975 (1.1041)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [320/431]  eta: 0:02:03  lr: 0.000172  loss: 1.0824 (1.1041)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [330/431]  eta: 0:01:52  lr: 0.000172  loss: 1.0959 (1.1051)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [340/431]  eta: 0:01:41  lr: 0.000172  loss: 1.0867 (1.1045)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [350/431]  eta: 0:01:29  lr: 0.000172  loss: 1.0916 (1.1076)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [360/431]  eta: 0:01:18  lr: 0.000172  loss: 1.0924 (1.1068)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [370/431]  eta: 0:01:07  lr: 0.000172  loss: 1.0354 (1.1059)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [380/431]  eta: 0:00:56  lr: 0.000172  loss: 1.0818 (1.1067)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [390/431]  eta: 0:00:45  lr: 0.000172  loss: 1.1227 (1.1067)  time: 1.0964  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:228]  [400/431]  eta: 0:00:34  lr: 0.000172  loss: 1.0845 (1.1066)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [410/431]  eta: 0:00:23  lr: 0.000172  loss: 1.1066 (1.1081)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:228]  [420/431]  eta: 0:00:12  lr: 0.000172  loss: 1.0827 (1.1072)  time: 1.0929  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:228]  [430/431]  eta: 0:00:01  lr: 0.000172  loss: 1.0576 (1.1082)  time: 1.1008  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:228] Total time: 0:07:58 (1.1096 s / it)\n",
      "Averaged stats: lr: 0.000172  loss: 1.0576 (1.1082)\n",
      "Valid: [epoch:228]  [ 0/14]  eta: 0:00:36  loss: 1.0953 (1.0953)  time: 2.6095  data: 2.4214  max mem: 15925\n",
      "Valid: [epoch:228]  [13/14]  eta: 0:00:00  loss: 1.0439 (1.0537)  time: 0.2776  data: 0.1731  max mem: 15925\n",
      "Valid: [epoch:228] Total time: 0:00:04 (0.2948 s / it)\n",
      "Averaged stats: loss: 1.0439 (1.0537)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_228_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.054%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:229]  [  0/431]  eta: 0:32:21  lr: 0.000172  loss: 0.9663 (0.9663)  time: 4.5043  data: 3.2363  max mem: 15925\n",
      "Train: [epoch:229]  [ 10/431]  eta: 0:09:31  lr: 0.000172  loss: 1.1279 (1.1703)  time: 1.3586  data: 0.2944  max mem: 15925\n",
      "Train: [epoch:229]  [ 20/431]  eta: 0:08:19  lr: 0.000172  loss: 1.1185 (1.1186)  time: 1.0507  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [ 30/431]  eta: 0:07:46  lr: 0.000172  loss: 1.0485 (1.0939)  time: 1.0572  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [ 40/431]  eta: 0:07:28  lr: 0.000172  loss: 1.0517 (1.0975)  time: 1.0760  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [ 50/431]  eta: 0:07:13  lr: 0.000172  loss: 1.0540 (1.0910)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [ 60/431]  eta: 0:07:00  lr: 0.000172  loss: 1.0862 (1.1032)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [ 70/431]  eta: 0:06:47  lr: 0.000172  loss: 1.0875 (1.1091)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [ 80/431]  eta: 0:06:34  lr: 0.000172  loss: 1.0498 (1.1035)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [ 90/431]  eta: 0:06:21  lr: 0.000172  loss: 1.0729 (1.1017)  time: 1.0868  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [100/431]  eta: 0:06:10  lr: 0.000172  loss: 1.0851 (1.0964)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [110/431]  eta: 0:05:58  lr: 0.000172  loss: 1.0348 (1.0946)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [120/431]  eta: 0:05:46  lr: 0.000172  loss: 1.0331 (1.0963)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [130/431]  eta: 0:05:35  lr: 0.000172  loss: 1.0665 (1.0956)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [140/431]  eta: 0:05:23  lr: 0.000172  loss: 1.0681 (1.0949)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [150/431]  eta: 0:05:11  lr: 0.000172  loss: 1.1250 (1.1001)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [160/431]  eta: 0:05:00  lr: 0.000172  loss: 1.1615 (1.1031)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [170/431]  eta: 0:04:49  lr: 0.000172  loss: 1.1016 (1.1005)  time: 1.0895  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [180/431]  eta: 0:04:37  lr: 0.000172  loss: 1.0310 (1.0984)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [190/431]  eta: 0:04:26  lr: 0.000172  loss: 1.0870 (1.1014)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [200/431]  eta: 0:04:15  lr: 0.000172  loss: 1.1401 (1.1034)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [210/431]  eta: 0:04:04  lr: 0.000172  loss: 1.0920 (1.0997)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [220/431]  eta: 0:03:53  lr: 0.000172  loss: 1.0200 (1.0979)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [230/431]  eta: 0:03:42  lr: 0.000172  loss: 1.0580 (1.0970)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [240/431]  eta: 0:03:31  lr: 0.000172  loss: 1.0837 (1.0994)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [250/431]  eta: 0:03:20  lr: 0.000172  loss: 1.1185 (1.1010)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [260/431]  eta: 0:03:09  lr: 0.000172  loss: 1.1185 (1.1019)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [270/431]  eta: 0:02:58  lr: 0.000172  loss: 1.1110 (1.1028)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [280/431]  eta: 0:02:47  lr: 0.000172  loss: 1.1008 (1.1031)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [290/431]  eta: 0:02:35  lr: 0.000172  loss: 1.0668 (1.1011)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [300/431]  eta: 0:02:24  lr: 0.000172  loss: 1.0537 (1.1029)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [310/431]  eta: 0:02:13  lr: 0.000172  loss: 1.1008 (1.1022)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [320/431]  eta: 0:02:02  lr: 0.000172  loss: 1.0943 (1.1020)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [330/431]  eta: 0:01:51  lr: 0.000172  loss: 1.1295 (1.1041)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [340/431]  eta: 0:01:40  lr: 0.000172  loss: 1.1546 (1.1055)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [350/431]  eta: 0:01:29  lr: 0.000172  loss: 1.1546 (1.1076)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [360/431]  eta: 0:01:18  lr: 0.000172  loss: 1.1373 (1.1089)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [370/431]  eta: 0:01:07  lr: 0.000172  loss: 1.0504 (1.1076)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [380/431]  eta: 0:00:56  lr: 0.000172  loss: 1.0348 (1.1059)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [390/431]  eta: 0:00:45  lr: 0.000172  loss: 1.0496 (1.1069)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [400/431]  eta: 0:00:34  lr: 0.000172  loss: 1.0580 (1.1057)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [410/431]  eta: 0:00:23  lr: 0.000172  loss: 1.0542 (1.1056)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:229]  [420/431]  eta: 0:00:12  lr: 0.000172  loss: 1.1031 (1.1061)  time: 1.0936  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:229]  [430/431]  eta: 0:00:01  lr: 0.000172  loss: 1.1031 (1.1064)  time: 1.1069  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:229] Total time: 0:07:55 (1.1038 s / it)\n",
      "Averaged stats: lr: 0.000172  loss: 1.1031 (1.1064)\n",
      "Valid: [epoch:229]  [ 0/14]  eta: 0:00:37  loss: 0.9928 (0.9928)  time: 2.7003  data: 2.5297  max mem: 15925\n",
      "Valid: [epoch:229]  [13/14]  eta: 0:00:00  loss: 1.0454 (1.0556)  time: 0.2794  data: 0.1808  max mem: 15925\n",
      "Valid: [epoch:229] Total time: 0:00:04 (0.2949 s / it)\n",
      "Averaged stats: loss: 1.0454 (1.0556)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_229_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.056%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:230]  [  0/431]  eta: 0:30:39  lr: 0.000171  loss: 1.1383 (1.1383)  time: 4.2672  data: 2.9635  max mem: 15925\n",
      "Train: [epoch:230]  [ 10/431]  eta: 0:09:24  lr: 0.000171  loss: 1.1194 (1.1494)  time: 1.3410  data: 0.2696  max mem: 15925\n",
      "Train: [epoch:230]  [ 20/431]  eta: 0:08:15  lr: 0.000171  loss: 1.0878 (1.1197)  time: 1.0519  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [ 30/431]  eta: 0:07:48  lr: 0.000171  loss: 1.0708 (1.1144)  time: 1.0739  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [ 40/431]  eta: 0:07:29  lr: 0.000171  loss: 1.0518 (1.1018)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [ 50/431]  eta: 0:07:15  lr: 0.000171  loss: 1.0561 (1.1019)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [ 60/431]  eta: 0:07:01  lr: 0.000171  loss: 1.0473 (1.0930)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [ 70/431]  eta: 0:06:48  lr: 0.000171  loss: 1.0720 (1.0995)  time: 1.1061  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:230]  [ 80/431]  eta: 0:06:36  lr: 0.000171  loss: 1.1076 (1.1062)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [ 90/431]  eta: 0:06:23  lr: 0.000171  loss: 1.0821 (1.1018)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [100/431]  eta: 0:06:10  lr: 0.000171  loss: 1.0757 (1.1024)  time: 1.0852  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [110/431]  eta: 0:05:58  lr: 0.000171  loss: 1.0945 (1.0992)  time: 1.0846  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [120/431]  eta: 0:05:47  lr: 0.000171  loss: 1.0267 (1.0955)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [130/431]  eta: 0:05:35  lr: 0.000171  loss: 1.0635 (1.0955)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [140/431]  eta: 0:05:23  lr: 0.000171  loss: 1.0847 (1.0964)  time: 1.0882  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [150/431]  eta: 0:05:12  lr: 0.000171  loss: 1.1043 (1.1002)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [160/431]  eta: 0:05:01  lr: 0.000171  loss: 1.1560 (1.1064)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [170/431]  eta: 0:04:50  lr: 0.000171  loss: 1.1559 (1.1068)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [180/431]  eta: 0:04:38  lr: 0.000171  loss: 1.0985 (1.1068)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [190/431]  eta: 0:04:27  lr: 0.000171  loss: 1.1076 (1.1062)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [200/431]  eta: 0:04:16  lr: 0.000171  loss: 1.0867 (1.1067)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [210/431]  eta: 0:04:04  lr: 0.000171  loss: 1.0658 (1.1048)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [220/431]  eta: 0:03:53  lr: 0.000171  loss: 1.0651 (1.1022)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [230/431]  eta: 0:03:42  lr: 0.000171  loss: 1.0421 (1.0999)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [240/431]  eta: 0:03:31  lr: 0.000171  loss: 1.0620 (1.1010)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [250/431]  eta: 0:03:20  lr: 0.000171  loss: 1.1545 (1.1028)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [260/431]  eta: 0:03:09  lr: 0.000171  loss: 1.1401 (1.1023)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [270/431]  eta: 0:02:57  lr: 0.000171  loss: 1.0936 (1.1025)  time: 1.0925  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [280/431]  eta: 0:02:46  lr: 0.000171  loss: 1.0912 (1.1026)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [290/431]  eta: 0:02:35  lr: 0.000171  loss: 1.0851 (1.1028)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [300/431]  eta: 0:02:24  lr: 0.000171  loss: 1.0972 (1.1047)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [310/431]  eta: 0:02:13  lr: 0.000171  loss: 1.1248 (1.1067)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [320/431]  eta: 0:02:02  lr: 0.000171  loss: 1.1128 (1.1064)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [330/431]  eta: 0:01:51  lr: 0.000171  loss: 1.1177 (1.1082)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [340/431]  eta: 0:01:40  lr: 0.000171  loss: 1.1452 (1.1092)  time: 1.0835  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [350/431]  eta: 0:01:29  lr: 0.000171  loss: 1.1260 (1.1108)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [360/431]  eta: 0:01:18  lr: 0.000171  loss: 1.0707 (1.1098)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [370/431]  eta: 0:01:07  lr: 0.000171  loss: 1.0963 (1.1106)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [380/431]  eta: 0:00:56  lr: 0.000171  loss: 1.1220 (1.1100)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [390/431]  eta: 0:00:45  lr: 0.000171  loss: 1.0624 (1.1093)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [400/431]  eta: 0:00:34  lr: 0.000171  loss: 1.0527 (1.1085)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:230]  [410/431]  eta: 0:00:23  lr: 0.000171  loss: 1.0527 (1.1074)  time: 1.0879  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:230]  [420/431]  eta: 0:00:12  lr: 0.000171  loss: 1.0858 (1.1076)  time: 1.1014  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:230]  [430/431]  eta: 0:00:01  lr: 0.000171  loss: 1.1005 (1.1069)  time: 1.1036  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:230] Total time: 0:07:55 (1.1027 s / it)\n",
      "Averaged stats: lr: 0.000171  loss: 1.1005 (1.1069)\n",
      "Valid: [epoch:230]  [ 0/14]  eta: 0:00:35  loss: 0.9821 (0.9821)  time: 2.5673  data: 2.4196  max mem: 15925\n",
      "Valid: [epoch:230]  [13/14]  eta: 0:00:00  loss: 1.0368 (1.0471)  time: 0.2719  data: 0.1729  max mem: 15925\n",
      "Valid: [epoch:230] Total time: 0:00:04 (0.2892 s / it)\n",
      "Averaged stats: loss: 1.0368 (1.0471)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_230_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:231]  [  0/431]  eta: 0:32:26  lr: 0.000171  loss: 1.0862 (1.0862)  time: 4.5167  data: 3.3579  max mem: 15925\n",
      "Train: [epoch:231]  [ 10/431]  eta: 0:09:31  lr: 0.000171  loss: 1.1994 (1.1983)  time: 1.3563  data: 0.3054  max mem: 15925\n",
      "Train: [epoch:231]  [ 20/431]  eta: 0:08:21  lr: 0.000171  loss: 1.1840 (1.1697)  time: 1.0542  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [ 30/431]  eta: 0:07:50  lr: 0.000171  loss: 1.0907 (1.1252)  time: 1.0715  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [ 40/431]  eta: 0:07:29  lr: 0.000171  loss: 1.0771 (1.1344)  time: 1.0749  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [ 50/431]  eta: 0:07:12  lr: 0.000171  loss: 1.1311 (1.1342)  time: 1.0769  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [ 60/431]  eta: 0:06:58  lr: 0.000171  loss: 1.1015 (1.1313)  time: 1.0875  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [ 70/431]  eta: 0:06:45  lr: 0.000171  loss: 1.1309 (1.1270)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [ 80/431]  eta: 0:06:33  lr: 0.000171  loss: 1.1049 (1.1278)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [ 90/431]  eta: 0:06:21  lr: 0.000171  loss: 1.0943 (1.1266)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [100/431]  eta: 0:06:09  lr: 0.000171  loss: 1.1009 (1.1257)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [110/431]  eta: 0:05:57  lr: 0.000171  loss: 1.0733 (1.1240)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [120/431]  eta: 0:05:46  lr: 0.000171  loss: 1.0865 (1.1260)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [130/431]  eta: 0:05:34  lr: 0.000171  loss: 1.0938 (1.1219)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [140/431]  eta: 0:05:23  lr: 0.000171  loss: 1.0500 (1.1176)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [150/431]  eta: 0:05:11  lr: 0.000171  loss: 1.0516 (1.1196)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [160/431]  eta: 0:05:00  lr: 0.000171  loss: 1.0766 (1.1201)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [170/431]  eta: 0:04:49  lr: 0.000171  loss: 1.0766 (1.1184)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [180/431]  eta: 0:04:38  lr: 0.000171  loss: 1.1045 (1.1190)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [190/431]  eta: 0:04:27  lr: 0.000171  loss: 1.1334 (1.1182)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [200/431]  eta: 0:04:16  lr: 0.000171  loss: 1.1329 (1.1166)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [210/431]  eta: 0:04:04  lr: 0.000171  loss: 1.0614 (1.1144)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [220/431]  eta: 0:03:53  lr: 0.000171  loss: 1.0283 (1.1118)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [230/431]  eta: 0:03:42  lr: 0.000171  loss: 1.1017 (1.1126)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [240/431]  eta: 0:03:31  lr: 0.000171  loss: 1.0992 (1.1110)  time: 1.0885  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:231]  [250/431]  eta: 0:03:20  lr: 0.000171  loss: 1.0611 (1.1083)  time: 1.0870  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [260/431]  eta: 0:03:08  lr: 0.000171  loss: 1.0539 (1.1089)  time: 1.0918  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [270/431]  eta: 0:02:57  lr: 0.000171  loss: 1.0916 (1.1104)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [280/431]  eta: 0:02:46  lr: 0.000171  loss: 1.0561 (1.1075)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [290/431]  eta: 0:02:35  lr: 0.000171  loss: 1.0142 (1.1064)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [300/431]  eta: 0:02:24  lr: 0.000171  loss: 1.0788 (1.1069)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [310/431]  eta: 0:02:13  lr: 0.000171  loss: 1.0852 (1.1076)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [320/431]  eta: 0:02:02  lr: 0.000171  loss: 1.0846 (1.1073)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [330/431]  eta: 0:01:51  lr: 0.000171  loss: 1.0983 (1.1074)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [340/431]  eta: 0:01:40  lr: 0.000171  loss: 1.0983 (1.1092)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [350/431]  eta: 0:01:29  lr: 0.000171  loss: 1.0882 (1.1103)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [360/431]  eta: 0:01:18  lr: 0.000171  loss: 1.0680 (1.1084)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [370/431]  eta: 0:01:07  lr: 0.000171  loss: 1.0487 (1.1079)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [380/431]  eta: 0:00:56  lr: 0.000171  loss: 1.0510 (1.1075)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [390/431]  eta: 0:00:45  lr: 0.000171  loss: 1.0674 (1.1074)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [400/431]  eta: 0:00:34  lr: 0.000171  loss: 1.0611 (1.1068)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [410/431]  eta: 0:00:23  lr: 0.000171  loss: 1.0951 (1.1071)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:231]  [420/431]  eta: 0:00:12  lr: 0.000171  loss: 1.1235 (1.1081)  time: 1.1019  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:231]  [430/431]  eta: 0:00:01  lr: 0.000171  loss: 1.1474 (1.1085)  time: 1.1062  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:231] Total time: 0:07:56 (1.1054 s / it)\n",
      "Averaged stats: lr: 0.000171  loss: 1.1474 (1.1085)\n",
      "Valid: [epoch:231]  [ 0/14]  eta: 0:00:36  loss: 1.0983 (1.0983)  time: 2.6131  data: 2.4263  max mem: 15925\n",
      "Valid: [epoch:231]  [13/14]  eta: 0:00:00  loss: 1.0476 (1.0558)  time: 0.2717  data: 0.1734  max mem: 15925\n",
      "Valid: [epoch:231] Total time: 0:00:04 (0.2882 s / it)\n",
      "Averaged stats: loss: 1.0476 (1.0558)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_231_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.056%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:232]  [  0/431]  eta: 0:30:45  lr: 0.000171  loss: 0.9596 (0.9596)  time: 4.2808  data: 3.0234  max mem: 15925\n",
      "Train: [epoch:232]  [ 10/431]  eta: 0:09:27  lr: 0.000171  loss: 1.1414 (1.1354)  time: 1.3487  data: 0.2751  max mem: 15925\n",
      "Train: [epoch:232]  [ 20/431]  eta: 0:08:17  lr: 0.000171  loss: 1.1217 (1.1308)  time: 1.0565  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [ 30/431]  eta: 0:07:50  lr: 0.000171  loss: 1.0962 (1.1269)  time: 1.0751  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [ 40/431]  eta: 0:07:32  lr: 0.000171  loss: 1.0598 (1.1203)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [ 50/431]  eta: 0:07:15  lr: 0.000171  loss: 1.0598 (1.1149)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [ 60/431]  eta: 0:07:03  lr: 0.000171  loss: 1.0859 (1.1181)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [ 70/431]  eta: 0:06:50  lr: 0.000171  loss: 1.0954 (1.1153)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [ 80/431]  eta: 0:06:37  lr: 0.000171  loss: 1.0887 (1.1133)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [ 90/431]  eta: 0:06:25  lr: 0.000171  loss: 1.0749 (1.1085)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [100/431]  eta: 0:06:13  lr: 0.000171  loss: 1.0164 (1.1063)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [110/431]  eta: 0:06:01  lr: 0.000171  loss: 1.0561 (1.1039)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [120/431]  eta: 0:05:49  lr: 0.000171  loss: 1.0444 (1.0985)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [130/431]  eta: 0:05:38  lr: 0.000171  loss: 1.0509 (1.0968)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [140/431]  eta: 0:05:27  lr: 0.000171  loss: 1.0858 (1.0985)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [150/431]  eta: 0:05:15  lr: 0.000171  loss: 1.0476 (1.0945)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [160/431]  eta: 0:05:04  lr: 0.000171  loss: 1.0427 (1.0966)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [170/431]  eta: 0:04:52  lr: 0.000171  loss: 1.0544 (1.0948)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [180/431]  eta: 0:04:41  lr: 0.000171  loss: 1.0544 (1.0941)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [190/431]  eta: 0:04:30  lr: 0.000171  loss: 1.0591 (1.0945)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [200/431]  eta: 0:04:18  lr: 0.000171  loss: 1.0414 (1.0939)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [210/431]  eta: 0:04:07  lr: 0.000171  loss: 1.1370 (1.0970)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [220/431]  eta: 0:03:56  lr: 0.000171  loss: 1.1370 (1.0979)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [230/431]  eta: 0:03:44  lr: 0.000171  loss: 1.1153 (1.1008)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [240/431]  eta: 0:03:33  lr: 0.000171  loss: 1.1472 (1.1020)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [250/431]  eta: 0:03:22  lr: 0.000171  loss: 1.0918 (1.1004)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [260/431]  eta: 0:03:11  lr: 0.000171  loss: 1.0581 (1.1027)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [270/431]  eta: 0:02:59  lr: 0.000171  loss: 1.0707 (1.1038)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [280/431]  eta: 0:02:48  lr: 0.000171  loss: 1.1017 (1.1058)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [290/431]  eta: 0:02:37  lr: 0.000171  loss: 1.1024 (1.1046)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [300/431]  eta: 0:02:26  lr: 0.000171  loss: 1.1013 (1.1039)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [310/431]  eta: 0:02:15  lr: 0.000171  loss: 1.0662 (1.1044)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [320/431]  eta: 0:02:03  lr: 0.000171  loss: 1.0843 (1.1042)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [330/431]  eta: 0:01:52  lr: 0.000171  loss: 1.1270 (1.1071)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [340/431]  eta: 0:01:41  lr: 0.000171  loss: 1.1079 (1.1069)  time: 1.1176  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:232]  [350/431]  eta: 0:01:30  lr: 0.000171  loss: 1.0604 (1.1078)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [360/431]  eta: 0:01:19  lr: 0.000171  loss: 1.1148 (1.1082)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [370/431]  eta: 0:01:08  lr: 0.000171  loss: 1.1148 (1.1076)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [380/431]  eta: 0:00:56  lr: 0.000171  loss: 1.0584 (1.1077)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [390/431]  eta: 0:00:45  lr: 0.000171  loss: 1.0750 (1.1070)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [400/431]  eta: 0:00:34  lr: 0.000171  loss: 1.0762 (1.1068)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:232]  [410/431]  eta: 0:00:23  lr: 0.000171  loss: 1.0963 (1.1074)  time: 1.1102  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:232]  [420/431]  eta: 0:00:12  lr: 0.000171  loss: 1.0590 (1.1067)  time: 1.1030  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:232]  [430/431]  eta: 0:00:01  lr: 0.000171  loss: 1.1117 (1.1074)  time: 1.1033  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:232] Total time: 0:08:00 (1.1155 s / it)\n",
      "Averaged stats: lr: 0.000171  loss: 1.1117 (1.1074)\n",
      "Valid: [epoch:232]  [ 0/14]  eta: 0:00:35  loss: 1.0870 (1.0870)  time: 2.5459  data: 2.4030  max mem: 15925\n",
      "Valid: [epoch:232]  [13/14]  eta: 0:00:00  loss: 1.0373 (1.0487)  time: 0.2710  data: 0.1717  max mem: 15925\n",
      "Valid: [epoch:232] Total time: 0:00:04 (0.2889 s / it)\n",
      "Averaged stats: loss: 1.0373 (1.0487)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_232_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:233]  [  0/431]  eta: 0:33:27  lr: 0.000171  loss: 1.0271 (1.0271)  time: 4.6580  data: 3.4572  max mem: 15925\n",
      "Train: [epoch:233]  [ 10/431]  eta: 0:09:40  lr: 0.000171  loss: 1.0526 (1.1419)  time: 1.3793  data: 0.3145  max mem: 15925\n",
      "Train: [epoch:233]  [ 20/431]  eta: 0:08:27  lr: 0.000171  loss: 1.1426 (1.1480)  time: 1.0630  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [ 30/431]  eta: 0:07:56  lr: 0.000171  loss: 1.1401 (1.1268)  time: 1.0813  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [ 40/431]  eta: 0:07:34  lr: 0.000171  loss: 1.0679 (1.1216)  time: 1.0889  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [ 50/431]  eta: 0:07:18  lr: 0.000171  loss: 1.0647 (1.1041)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [ 60/431]  eta: 0:07:05  lr: 0.000171  loss: 1.0100 (1.0911)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [ 70/431]  eta: 0:06:51  lr: 0.000171  loss: 1.0304 (1.0926)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [ 80/431]  eta: 0:06:38  lr: 0.000171  loss: 1.0410 (1.0950)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [ 90/431]  eta: 0:06:26  lr: 0.000171  loss: 1.0604 (1.0951)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [100/431]  eta: 0:06:14  lr: 0.000171  loss: 1.0989 (1.0996)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [110/431]  eta: 0:06:02  lr: 0.000171  loss: 1.0556 (1.0931)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [120/431]  eta: 0:05:50  lr: 0.000171  loss: 1.0276 (1.0935)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [130/431]  eta: 0:05:38  lr: 0.000171  loss: 1.1070 (1.0961)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [140/431]  eta: 0:05:26  lr: 0.000171  loss: 1.1276 (1.1015)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [150/431]  eta: 0:05:15  lr: 0.000171  loss: 1.1527 (1.1043)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [160/431]  eta: 0:05:03  lr: 0.000171  loss: 1.1463 (1.1034)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [170/431]  eta: 0:04:52  lr: 0.000171  loss: 1.0685 (1.1012)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [180/431]  eta: 0:04:41  lr: 0.000171  loss: 1.0883 (1.1036)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [190/431]  eta: 0:04:29  lr: 0.000171  loss: 1.0883 (1.1032)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [200/431]  eta: 0:04:18  lr: 0.000171  loss: 1.0899 (1.1025)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [210/431]  eta: 0:04:06  lr: 0.000171  loss: 1.0899 (1.0997)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [220/431]  eta: 0:03:55  lr: 0.000171  loss: 1.0888 (1.0998)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [230/431]  eta: 0:03:44  lr: 0.000171  loss: 1.0888 (1.1014)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [240/431]  eta: 0:03:33  lr: 0.000171  loss: 1.0939 (1.1019)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [250/431]  eta: 0:03:22  lr: 0.000171  loss: 1.0939 (1.1026)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [260/431]  eta: 0:03:10  lr: 0.000171  loss: 1.1373 (1.1031)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [270/431]  eta: 0:02:59  lr: 0.000171  loss: 1.0909 (1.1024)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [280/431]  eta: 0:02:48  lr: 0.000171  loss: 1.0412 (1.1014)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [290/431]  eta: 0:02:37  lr: 0.000171  loss: 1.0412 (1.1034)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [300/431]  eta: 0:02:26  lr: 0.000171  loss: 1.1333 (1.1051)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [310/431]  eta: 0:02:14  lr: 0.000171  loss: 1.1186 (1.1069)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [320/431]  eta: 0:02:03  lr: 0.000171  loss: 1.0887 (1.1083)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [330/431]  eta: 0:01:52  lr: 0.000171  loss: 1.1114 (1.1094)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [340/431]  eta: 0:01:41  lr: 0.000171  loss: 1.1164 (1.1097)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [350/431]  eta: 0:01:30  lr: 0.000171  loss: 1.1248 (1.1113)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [360/431]  eta: 0:01:19  lr: 0.000171  loss: 1.1079 (1.1100)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [370/431]  eta: 0:01:07  lr: 0.000171  loss: 1.0624 (1.1097)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [380/431]  eta: 0:00:56  lr: 0.000171  loss: 1.1049 (1.1100)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [390/431]  eta: 0:00:45  lr: 0.000171  loss: 1.1194 (1.1098)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [400/431]  eta: 0:00:34  lr: 0.000171  loss: 1.0874 (1.1097)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [410/431]  eta: 0:00:23  lr: 0.000171  loss: 1.0667 (1.1087)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:233]  [420/431]  eta: 0:00:12  lr: 0.000171  loss: 1.0536 (1.1083)  time: 1.1016  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:233]  [430/431]  eta: 0:00:01  lr: 0.000171  loss: 1.0536 (1.1071)  time: 1.1008  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:233] Total time: 0:07:59 (1.1117 s / it)\n",
      "Averaged stats: lr: 0.000171  loss: 1.0536 (1.1071)\n",
      "Valid: [epoch:233]  [ 0/14]  eta: 0:00:36  loss: 1.0901 (1.0901)  time: 2.5800  data: 2.4590  max mem: 15925\n",
      "Valid: [epoch:233]  [13/14]  eta: 0:00:00  loss: 1.0350 (1.0466)  time: 0.2841  data: 0.1757  max mem: 15925\n",
      "Valid: [epoch:233] Total time: 0:00:04 (0.3030 s / it)\n",
      "Averaged stats: loss: 1.0350 (1.0466)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_233_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:234]  [  0/431]  eta: 0:30:57  lr: 0.000170  loss: 1.1031 (1.1031)  time: 4.3105  data: 3.0251  max mem: 15925\n",
      "Train: [epoch:234]  [ 10/431]  eta: 0:09:29  lr: 0.000170  loss: 1.1031 (1.1157)  time: 1.3536  data: 0.2752  max mem: 15925\n",
      "Train: [epoch:234]  [ 20/431]  eta: 0:08:15  lr: 0.000170  loss: 1.0802 (1.1005)  time: 1.0500  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [ 30/431]  eta: 0:07:47  lr: 0.000170  loss: 1.1099 (1.1159)  time: 1.0630  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [ 40/431]  eta: 0:07:28  lr: 0.000170  loss: 1.1233 (1.1229)  time: 1.0882  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [ 50/431]  eta: 0:07:13  lr: 0.000170  loss: 1.1233 (1.1325)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [ 60/431]  eta: 0:07:00  lr: 0.000170  loss: 1.1513 (1.1368)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [ 70/431]  eta: 0:06:47  lr: 0.000170  loss: 1.0945 (1.1326)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [ 80/431]  eta: 0:06:35  lr: 0.000170  loss: 1.0670 (1.1268)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [ 90/431]  eta: 0:06:22  lr: 0.000170  loss: 1.0594 (1.1237)  time: 1.0994  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:234]  [100/431]  eta: 0:06:10  lr: 0.000170  loss: 1.0882 (1.1223)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [110/431]  eta: 0:05:59  lr: 0.000170  loss: 1.0839 (1.1214)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [120/431]  eta: 0:05:47  lr: 0.000170  loss: 1.0341 (1.1211)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [130/431]  eta: 0:05:36  lr: 0.000170  loss: 1.0359 (1.1151)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [140/431]  eta: 0:05:24  lr: 0.000170  loss: 1.0359 (1.1121)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [150/431]  eta: 0:05:13  lr: 0.000170  loss: 1.0499 (1.1122)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [160/431]  eta: 0:05:02  lr: 0.000170  loss: 1.0551 (1.1089)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [170/431]  eta: 0:04:50  lr: 0.000170  loss: 1.0655 (1.1099)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [180/431]  eta: 0:04:39  lr: 0.000170  loss: 1.0775 (1.1080)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [190/431]  eta: 0:04:27  lr: 0.000170  loss: 1.0298 (1.1072)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [200/431]  eta: 0:04:16  lr: 0.000170  loss: 1.0298 (1.1063)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [210/431]  eta: 0:04:05  lr: 0.000170  loss: 1.1277 (1.1090)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [220/431]  eta: 0:03:54  lr: 0.000170  loss: 1.1278 (1.1089)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [230/431]  eta: 0:03:43  lr: 0.000170  loss: 1.1021 (1.1104)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [240/431]  eta: 0:03:32  lr: 0.000170  loss: 1.1089 (1.1089)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [250/431]  eta: 0:03:20  lr: 0.000170  loss: 1.0422 (1.1080)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [260/431]  eta: 0:03:09  lr: 0.000170  loss: 1.0771 (1.1087)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [270/431]  eta: 0:02:58  lr: 0.000170  loss: 1.1166 (1.1095)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [280/431]  eta: 0:02:47  lr: 0.000170  loss: 1.0372 (1.1076)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [290/431]  eta: 0:02:36  lr: 0.000170  loss: 1.0372 (1.1071)  time: 1.1138  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:234]  [300/431]  eta: 0:02:25  lr: 0.000170  loss: 1.1209 (1.1088)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [310/431]  eta: 0:02:14  lr: 0.000170  loss: 1.0822 (1.1068)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [320/431]  eta: 0:02:03  lr: 0.000170  loss: 1.0350 (1.1059)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [330/431]  eta: 0:01:51  lr: 0.000170  loss: 1.0764 (1.1074)  time: 1.0881  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [340/431]  eta: 0:01:40  lr: 0.000170  loss: 1.1092 (1.1076)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [350/431]  eta: 0:01:29  lr: 0.000170  loss: 1.1092 (1.1070)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [360/431]  eta: 0:01:18  lr: 0.000170  loss: 1.0935 (1.1066)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [370/431]  eta: 0:01:07  lr: 0.000170  loss: 1.0715 (1.1058)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [380/431]  eta: 0:00:56  lr: 0.000170  loss: 1.0572 (1.1062)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [390/431]  eta: 0:00:45  lr: 0.000170  loss: 1.0578 (1.1066)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [400/431]  eta: 0:00:34  lr: 0.000170  loss: 1.0794 (1.1075)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [410/431]  eta: 0:00:23  lr: 0.000170  loss: 1.0809 (1.1072)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:234]  [420/431]  eta: 0:00:12  lr: 0.000170  loss: 1.0977 (1.1072)  time: 1.0994  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:234]  [430/431]  eta: 0:00:01  lr: 0.000170  loss: 1.1124 (1.1071)  time: 1.1069  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:234] Total time: 0:07:57 (1.1086 s / it)\n",
      "Averaged stats: lr: 0.000170  loss: 1.1124 (1.1071)\n",
      "Valid: [epoch:234]  [ 0/14]  eta: 0:00:34  loss: 1.0920 (1.0920)  time: 2.4945  data: 2.3224  max mem: 15925\n",
      "Valid: [epoch:234]  [13/14]  eta: 0:00:00  loss: 1.0415 (1.0533)  time: 0.2709  data: 0.1660  max mem: 15925\n",
      "Valid: [epoch:234] Total time: 0:00:04 (0.2888 s / it)\n",
      "Averaged stats: loss: 1.0415 (1.0533)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_234_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.053%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:235]  [  0/431]  eta: 0:34:45  lr: 0.000170  loss: 1.1252 (1.1252)  time: 4.8384  data: 3.6806  max mem: 15925\n",
      "Train: [epoch:235]  [ 10/431]  eta: 0:09:44  lr: 0.000170  loss: 1.1067 (1.1217)  time: 1.3876  data: 0.3348  max mem: 15925\n",
      "Train: [epoch:235]  [ 20/431]  eta: 0:08:22  lr: 0.000170  loss: 1.0916 (1.1054)  time: 1.0424  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [ 30/431]  eta: 0:07:50  lr: 0.000170  loss: 1.0794 (1.0971)  time: 1.0573  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [ 40/431]  eta: 0:07:29  lr: 0.000170  loss: 1.0584 (1.0898)  time: 1.0728  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [ 50/431]  eta: 0:07:13  lr: 0.000170  loss: 1.0806 (1.0963)  time: 1.0808  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [ 60/431]  eta: 0:06:59  lr: 0.000170  loss: 1.1256 (1.1054)  time: 1.0875  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [ 70/431]  eta: 0:06:45  lr: 0.000170  loss: 1.1106 (1.1040)  time: 1.0858  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [ 80/431]  eta: 0:06:33  lr: 0.000170  loss: 1.1260 (1.1106)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [ 90/431]  eta: 0:06:22  lr: 0.000170  loss: 1.1260 (1.1085)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [100/431]  eta: 0:06:10  lr: 0.000170  loss: 1.0078 (1.1079)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [110/431]  eta: 0:05:58  lr: 0.000170  loss: 1.0503 (1.1090)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [120/431]  eta: 0:05:47  lr: 0.000170  loss: 1.0693 (1.1087)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [130/431]  eta: 0:05:35  lr: 0.000170  loss: 1.0646 (1.1034)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [140/431]  eta: 0:05:24  lr: 0.000170  loss: 1.0856 (1.1041)  time: 1.1023  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:235]  [150/431]  eta: 0:05:12  lr: 0.000170  loss: 1.1164 (1.1060)  time: 1.0968  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:235]  [160/431]  eta: 0:05:01  lr: 0.000170  loss: 1.1164 (1.1078)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [170/431]  eta: 0:04:50  lr: 0.000170  loss: 1.1164 (1.1086)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [180/431]  eta: 0:04:39  lr: 0.000170  loss: 1.1106 (1.1099)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [190/431]  eta: 0:04:27  lr: 0.000170  loss: 1.1106 (1.1110)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [200/431]  eta: 0:04:16  lr: 0.000170  loss: 1.1404 (1.1141)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [210/431]  eta: 0:04:05  lr: 0.000170  loss: 1.1215 (1.1126)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [220/431]  eta: 0:03:54  lr: 0.000170  loss: 1.0351 (1.1092)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [230/431]  eta: 0:03:42  lr: 0.000170  loss: 1.0265 (1.1071)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [240/431]  eta: 0:03:31  lr: 0.000170  loss: 1.0432 (1.1083)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [250/431]  eta: 0:03:20  lr: 0.000170  loss: 1.0719 (1.1096)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [260/431]  eta: 0:03:09  lr: 0.000170  loss: 1.1196 (1.1110)  time: 1.0965  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:235]  [270/431]  eta: 0:02:58  lr: 0.000170  loss: 1.1196 (1.1125)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [280/431]  eta: 0:02:47  lr: 0.000170  loss: 1.0737 (1.1104)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [290/431]  eta: 0:02:36  lr: 0.000170  loss: 1.0730 (1.1105)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [300/431]  eta: 0:02:25  lr: 0.000170  loss: 1.1509 (1.1122)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [310/431]  eta: 0:02:13  lr: 0.000170  loss: 1.1509 (1.1123)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [320/431]  eta: 0:02:02  lr: 0.000170  loss: 1.1036 (1.1123)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [330/431]  eta: 0:01:51  lr: 0.000170  loss: 1.0945 (1.1115)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [340/431]  eta: 0:01:40  lr: 0.000170  loss: 1.0541 (1.1112)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [350/431]  eta: 0:01:29  lr: 0.000170  loss: 1.0972 (1.1111)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [360/431]  eta: 0:01:18  lr: 0.000170  loss: 1.0896 (1.1102)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [370/431]  eta: 0:01:07  lr: 0.000170  loss: 1.0896 (1.1105)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [380/431]  eta: 0:00:56  lr: 0.000170  loss: 1.1082 (1.1107)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [390/431]  eta: 0:00:45  lr: 0.000170  loss: 1.0537 (1.1091)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [400/431]  eta: 0:00:34  lr: 0.000170  loss: 1.0405 (1.1087)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [410/431]  eta: 0:00:23  lr: 0.000170  loss: 1.1034 (1.1079)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:235]  [420/431]  eta: 0:00:12  lr: 0.000170  loss: 1.0649 (1.1083)  time: 1.0951  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:235]  [430/431]  eta: 0:00:01  lr: 0.000170  loss: 1.0649 (1.1074)  time: 1.0880  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:235] Total time: 0:07:56 (1.1053 s / it)\n",
      "Averaged stats: lr: 0.000170  loss: 1.0649 (1.1074)\n",
      "Valid: [epoch:235]  [ 0/14]  eta: 0:00:35  loss: 1.0278 (1.0278)  time: 2.5569  data: 2.4269  max mem: 15925\n",
      "Valid: [epoch:235]  [13/14]  eta: 0:00:00  loss: 1.0336 (1.0467)  time: 0.2687  data: 0.1735  max mem: 15925\n",
      "Valid: [epoch:235] Total time: 0:00:04 (0.2866 s / it)\n",
      "Averaged stats: loss: 1.0336 (1.0467)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_235_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:236]  [  0/431]  eta: 0:36:18  lr: 0.000170  loss: 1.0806 (1.0806)  time: 5.0555  data: 3.9390  max mem: 15925\n",
      "Train: [epoch:236]  [ 10/431]  eta: 0:09:49  lr: 0.000170  loss: 1.1508 (1.1510)  time: 1.4010  data: 0.3583  max mem: 15925\n",
      "Train: [epoch:236]  [ 20/431]  eta: 0:08:26  lr: 0.000170  loss: 1.1004 (1.1106)  time: 1.0422  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [ 30/431]  eta: 0:07:53  lr: 0.000170  loss: 1.0574 (1.1137)  time: 1.0599  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:236]  [ 40/431]  eta: 0:07:32  lr: 0.000170  loss: 1.1129 (1.1136)  time: 1.0791  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [ 50/431]  eta: 0:07:16  lr: 0.000170  loss: 1.0918 (1.1128)  time: 1.0895  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [ 60/431]  eta: 0:07:00  lr: 0.000170  loss: 1.0970 (1.1209)  time: 1.0850  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [ 70/431]  eta: 0:06:48  lr: 0.000170  loss: 1.1057 (1.1265)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [ 80/431]  eta: 0:06:35  lr: 0.000170  loss: 1.0955 (1.1228)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [ 90/431]  eta: 0:06:23  lr: 0.000170  loss: 1.1003 (1.1243)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [100/431]  eta: 0:06:12  lr: 0.000170  loss: 1.1008 (1.1213)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [110/431]  eta: 0:05:59  lr: 0.000170  loss: 1.0231 (1.1135)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [120/431]  eta: 0:05:48  lr: 0.000170  loss: 1.0250 (1.1077)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [130/431]  eta: 0:05:36  lr: 0.000170  loss: 1.0756 (1.1074)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [140/431]  eta: 0:05:25  lr: 0.000170  loss: 1.0138 (1.1025)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [150/431]  eta: 0:05:13  lr: 0.000170  loss: 1.0456 (1.1017)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [160/431]  eta: 0:05:02  lr: 0.000170  loss: 1.0954 (1.1032)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [170/431]  eta: 0:04:51  lr: 0.000170  loss: 1.1026 (1.1037)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [180/431]  eta: 0:04:39  lr: 0.000170  loss: 1.1015 (1.1051)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [190/431]  eta: 0:04:28  lr: 0.000170  loss: 1.1578 (1.1085)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [200/431]  eta: 0:04:17  lr: 0.000170  loss: 1.1209 (1.1082)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [210/431]  eta: 0:04:05  lr: 0.000170  loss: 1.1041 (1.1128)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [220/431]  eta: 0:03:54  lr: 0.000170  loss: 1.1181 (1.1157)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [230/431]  eta: 0:03:43  lr: 0.000170  loss: 1.0950 (1.1151)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [240/431]  eta: 0:03:31  lr: 0.000170  loss: 1.0912 (1.1157)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [250/431]  eta: 0:03:20  lr: 0.000170  loss: 1.1286 (1.1154)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [260/431]  eta: 0:03:09  lr: 0.000170  loss: 1.0783 (1.1138)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [270/431]  eta: 0:02:58  lr: 0.000170  loss: 1.0862 (1.1137)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [280/431]  eta: 0:02:47  lr: 0.000170  loss: 1.1127 (1.1138)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [290/431]  eta: 0:02:36  lr: 0.000170  loss: 1.1185 (1.1138)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [300/431]  eta: 0:02:25  lr: 0.000170  loss: 1.1578 (1.1160)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [310/431]  eta: 0:02:14  lr: 0.000170  loss: 1.1455 (1.1155)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [320/431]  eta: 0:02:03  lr: 0.000170  loss: 1.1153 (1.1147)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [330/431]  eta: 0:01:52  lr: 0.000170  loss: 1.1209 (1.1161)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [340/431]  eta: 0:01:40  lr: 0.000170  loss: 1.1002 (1.1154)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [350/431]  eta: 0:01:29  lr: 0.000170  loss: 1.0554 (1.1142)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [360/431]  eta: 0:01:18  lr: 0.000170  loss: 1.0514 (1.1144)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [370/431]  eta: 0:01:07  lr: 0.000170  loss: 1.1092 (1.1138)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [380/431]  eta: 0:00:56  lr: 0.000170  loss: 1.1081 (1.1134)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [390/431]  eta: 0:00:45  lr: 0.000170  loss: 1.0181 (1.1110)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [400/431]  eta: 0:00:34  lr: 0.000170  loss: 1.0052 (1.1096)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:236]  [410/431]  eta: 0:00:23  lr: 0.000170  loss: 1.0904 (1.1103)  time: 1.1083  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:236]  [420/431]  eta: 0:00:12  lr: 0.000170  loss: 1.0791 (1.1084)  time: 1.1101  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:236]  [430/431]  eta: 0:00:01  lr: 0.000170  loss: 1.0356 (1.1082)  time: 1.1010  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:236] Total time: 0:07:58 (1.1095 s / it)\n",
      "Averaged stats: lr: 0.000170  loss: 1.0356 (1.1082)\n",
      "Valid: [epoch:236]  [ 0/14]  eta: 0:00:36  loss: 1.1373 (1.1373)  time: 2.5719  data: 2.4097  max mem: 15925\n",
      "Valid: [epoch:236]  [13/14]  eta: 0:00:00  loss: 1.0310 (1.0438)  time: 0.2693  data: 0.1722  max mem: 15925\n",
      "Valid: [epoch:236] Total time: 0:00:04 (0.2864 s / it)\n",
      "Averaged stats: loss: 1.0310 (1.0438)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_236_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:237]  [  0/431]  eta: 0:36:13  lr: 0.000170  loss: 1.3080 (1.3080)  time: 5.0441  data: 3.8963  max mem: 15925\n",
      "Train: [epoch:237]  [ 10/431]  eta: 0:09:51  lr: 0.000170  loss: 1.1094 (1.1219)  time: 1.4061  data: 0.3544  max mem: 15925\n",
      "Train: [epoch:237]  [ 20/431]  eta: 0:08:33  lr: 0.000170  loss: 1.1094 (1.1096)  time: 1.0607  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [ 30/431]  eta: 0:07:58  lr: 0.000170  loss: 1.0816 (1.0999)  time: 1.0774  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [ 40/431]  eta: 0:07:39  lr: 0.000170  loss: 1.0727 (1.1000)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [ 50/431]  eta: 0:07:21  lr: 0.000170  loss: 1.0228 (1.0945)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [ 60/431]  eta: 0:07:06  lr: 0.000170  loss: 1.0136 (1.0898)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [ 70/431]  eta: 0:06:53  lr: 0.000170  loss: 1.0847 (1.0914)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [ 80/431]  eta: 0:06:40  lr: 0.000170  loss: 1.0907 (1.0893)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [ 90/431]  eta: 0:06:28  lr: 0.000170  loss: 1.0299 (1.0877)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [100/431]  eta: 0:06:16  lr: 0.000170  loss: 1.0857 (1.0914)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [110/431]  eta: 0:06:03  lr: 0.000170  loss: 1.0597 (1.0871)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [120/431]  eta: 0:05:52  lr: 0.000170  loss: 1.0547 (1.0871)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [130/431]  eta: 0:05:40  lr: 0.000170  loss: 1.0991 (1.0902)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [140/431]  eta: 0:05:28  lr: 0.000170  loss: 1.1070 (1.0900)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [150/431]  eta: 0:05:16  lr: 0.000170  loss: 1.0810 (1.0903)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [160/431]  eta: 0:05:04  lr: 0.000170  loss: 1.0299 (1.0870)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [170/431]  eta: 0:04:53  lr: 0.000170  loss: 1.0301 (1.0878)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [180/431]  eta: 0:04:41  lr: 0.000170  loss: 1.1246 (1.0952)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [190/431]  eta: 0:04:29  lr: 0.000170  loss: 1.0979 (1.0971)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [200/431]  eta: 0:04:18  lr: 0.000170  loss: 1.0662 (1.0965)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [210/431]  eta: 0:04:07  lr: 0.000170  loss: 1.0662 (1.0977)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [220/431]  eta: 0:03:55  lr: 0.000170  loss: 1.0799 (1.0991)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [230/431]  eta: 0:03:44  lr: 0.000170  loss: 1.1120 (1.1006)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [240/431]  eta: 0:03:33  lr: 0.000170  loss: 1.1411 (1.1042)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [250/431]  eta: 0:03:22  lr: 0.000170  loss: 1.1281 (1.1038)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [260/431]  eta: 0:03:10  lr: 0.000170  loss: 1.0627 (1.1009)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [270/431]  eta: 0:02:59  lr: 0.000170  loss: 1.0633 (1.1033)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [280/431]  eta: 0:02:48  lr: 0.000170  loss: 1.0586 (1.1022)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [290/431]  eta: 0:02:37  lr: 0.000170  loss: 1.0570 (1.1029)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [300/431]  eta: 0:02:26  lr: 0.000170  loss: 1.0570 (1.1016)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [310/431]  eta: 0:02:14  lr: 0.000170  loss: 1.0692 (1.1026)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [320/431]  eta: 0:02:03  lr: 0.000170  loss: 1.1276 (1.1031)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [330/431]  eta: 0:01:52  lr: 0.000170  loss: 1.1040 (1.1033)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [340/431]  eta: 0:01:41  lr: 0.000170  loss: 1.1409 (1.1055)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [350/431]  eta: 0:01:30  lr: 0.000170  loss: 1.0819 (1.1036)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [360/431]  eta: 0:01:19  lr: 0.000170  loss: 1.0736 (1.1061)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [370/431]  eta: 0:01:07  lr: 0.000170  loss: 1.1134 (1.1059)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [380/431]  eta: 0:00:56  lr: 0.000170  loss: 1.0934 (1.1053)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [390/431]  eta: 0:00:45  lr: 0.000170  loss: 1.0872 (1.1048)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [400/431]  eta: 0:00:34  lr: 0.000170  loss: 1.0632 (1.1042)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [410/431]  eta: 0:00:23  lr: 0.000170  loss: 1.0726 (1.1050)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:237]  [420/431]  eta: 0:00:12  lr: 0.000170  loss: 1.1284 (1.1066)  time: 1.1121  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:237]  [430/431]  eta: 0:00:01  lr: 0.000170  loss: 1.0976 (1.1068)  time: 1.0991  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:237] Total time: 0:07:59 (1.1124 s / it)\n",
      "Averaged stats: lr: 0.000170  loss: 1.0976 (1.1068)\n",
      "Valid: [epoch:237]  [ 0/14]  eta: 0:00:36  loss: 1.0876 (1.0876)  time: 2.6007  data: 2.4812  max mem: 15925\n",
      "Valid: [epoch:237]  [13/14]  eta: 0:00:00  loss: 1.0404 (1.0503)  time: 0.2733  data: 0.1773  max mem: 15925\n",
      "Valid: [epoch:237] Total time: 0:00:04 (0.2892 s / it)\n",
      "Averaged stats: loss: 1.0404 (1.0503)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_237_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.044\n",
      "Best Epoch: 197.000\n",
      "Train: [epoch:238]  [  0/431]  eta: 0:34:54  lr: 0.000170  loss: 1.1826 (1.1826)  time: 4.8601  data: 3.6574  max mem: 15925\n",
      "Train: [epoch:238]  [ 10/431]  eta: 0:09:42  lr: 0.000170  loss: 1.1826 (1.2109)  time: 1.3835  data: 0.3327  max mem: 15925\n",
      "Train: [epoch:238]  [ 20/431]  eta: 0:08:29  lr: 0.000170  loss: 1.1671 (1.1878)  time: 1.0576  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [ 30/431]  eta: 0:07:56  lr: 0.000170  loss: 1.1151 (1.1691)  time: 1.0797  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [ 40/431]  eta: 0:07:35  lr: 0.000170  loss: 1.1037 (1.1522)  time: 1.0875  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [ 50/431]  eta: 0:07:18  lr: 0.000170  loss: 1.0312 (1.1278)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [ 60/431]  eta: 0:07:04  lr: 0.000170  loss: 1.0094 (1.1093)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [ 70/431]  eta: 0:06:50  lr: 0.000170  loss: 1.0215 (1.1145)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [ 80/431]  eta: 0:06:37  lr: 0.000170  loss: 1.0585 (1.1135)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [ 90/431]  eta: 0:06:24  lr: 0.000170  loss: 1.0771 (1.1142)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [100/431]  eta: 0:06:12  lr: 0.000170  loss: 1.1006 (1.1141)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [110/431]  eta: 0:06:00  lr: 0.000170  loss: 1.0707 (1.1118)  time: 1.0993  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:238]  [120/431]  eta: 0:05:48  lr: 0.000170  loss: 1.0226 (1.1069)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [130/431]  eta: 0:05:37  lr: 0.000170  loss: 1.0325 (1.1064)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [140/431]  eta: 0:05:25  lr: 0.000170  loss: 1.0470 (1.1022)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [150/431]  eta: 0:05:14  lr: 0.000170  loss: 1.0511 (1.1055)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [160/431]  eta: 0:05:02  lr: 0.000170  loss: 1.0669 (1.1043)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [170/431]  eta: 0:04:51  lr: 0.000170  loss: 1.0669 (1.1037)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [180/431]  eta: 0:04:40  lr: 0.000170  loss: 1.1338 (1.1062)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [190/431]  eta: 0:04:28  lr: 0.000170  loss: 1.0926 (1.1064)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [200/431]  eta: 0:04:17  lr: 0.000170  loss: 1.0926 (1.1082)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [210/431]  eta: 0:04:06  lr: 0.000170  loss: 1.1163 (1.1081)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [220/431]  eta: 0:03:55  lr: 0.000170  loss: 1.1035 (1.1082)  time: 1.1031  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:238]  [230/431]  eta: 0:03:43  lr: 0.000170  loss: 1.0964 (1.1073)  time: 1.1075  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:238]  [240/431]  eta: 0:03:32  lr: 0.000170  loss: 1.0492 (1.1057)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [250/431]  eta: 0:03:21  lr: 0.000170  loss: 1.0738 (1.1066)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [260/431]  eta: 0:03:10  lr: 0.000170  loss: 1.1012 (1.1055)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [270/431]  eta: 0:02:59  lr: 0.000170  loss: 1.0436 (1.1051)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [280/431]  eta: 0:02:47  lr: 0.000170  loss: 1.0385 (1.1028)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [290/431]  eta: 0:02:36  lr: 0.000170  loss: 1.1159 (1.1050)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [300/431]  eta: 0:02:25  lr: 0.000170  loss: 1.1227 (1.1059)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [310/431]  eta: 0:02:14  lr: 0.000170  loss: 1.0517 (1.1050)  time: 1.0866  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [320/431]  eta: 0:02:03  lr: 0.000170  loss: 1.0666 (1.1069)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [330/431]  eta: 0:01:52  lr: 0.000170  loss: 1.1072 (1.1067)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [340/431]  eta: 0:01:40  lr: 0.000170  loss: 1.0799 (1.1062)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [350/431]  eta: 0:01:29  lr: 0.000170  loss: 1.0827 (1.1069)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [360/431]  eta: 0:01:18  lr: 0.000170  loss: 1.1033 (1.1066)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [370/431]  eta: 0:01:07  lr: 0.000170  loss: 1.1148 (1.1082)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [380/431]  eta: 0:00:56  lr: 0.000170  loss: 1.1127 (1.1076)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [390/431]  eta: 0:00:45  lr: 0.000170  loss: 1.0996 (1.1082)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [400/431]  eta: 0:00:34  lr: 0.000170  loss: 1.0965 (1.1082)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [410/431]  eta: 0:00:23  lr: 0.000170  loss: 1.0616 (1.1077)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:238]  [420/431]  eta: 0:00:12  lr: 0.000170  loss: 1.0985 (1.1080)  time: 1.0964  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:238]  [430/431]  eta: 0:00:01  lr: 0.000170  loss: 1.1161 (1.1069)  time: 1.1034  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:238] Total time: 0:07:57 (1.1085 s / it)\n",
      "Averaged stats: lr: 0.000170  loss: 1.1161 (1.1069)\n",
      "Valid: [epoch:238]  [ 0/14]  eta: 0:00:35  loss: 1.0862 (1.0862)  time: 2.5627  data: 2.3607  max mem: 15925\n",
      "Valid: [epoch:238]  [13/14]  eta: 0:00:00  loss: 1.0293 (1.0425)  time: 0.2796  data: 0.1687  max mem: 15925\n",
      "Valid: [epoch:238] Total time: 0:00:04 (0.2948 s / it)\n",
      "Averaged stats: loss: 1.0293 (1.0425)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_238_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:239]  [  0/431]  eta: 0:35:03  lr: 0.000169  loss: 1.1269 (1.1269)  time: 4.8811  data: 3.7386  max mem: 15925\n",
      "Train: [epoch:239]  [ 10/431]  eta: 0:09:46  lr: 0.000169  loss: 1.0651 (1.0833)  time: 1.3941  data: 0.3401  max mem: 15925\n",
      "Train: [epoch:239]  [ 20/431]  eta: 0:08:27  lr: 0.000169  loss: 1.0726 (1.1072)  time: 1.0521  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [ 30/431]  eta: 0:07:55  lr: 0.000169  loss: 1.1117 (1.0981)  time: 1.0708  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [ 40/431]  eta: 0:07:35  lr: 0.000169  loss: 1.1112 (1.1087)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [ 50/431]  eta: 0:07:18  lr: 0.000169  loss: 1.0474 (1.0976)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [ 60/431]  eta: 0:07:05  lr: 0.000169  loss: 1.0233 (1.0875)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [ 70/431]  eta: 0:06:51  lr: 0.000169  loss: 1.0255 (1.0883)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [ 80/431]  eta: 0:06:38  lr: 0.000169  loss: 1.1082 (1.0961)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [ 90/431]  eta: 0:06:25  lr: 0.000169  loss: 1.1310 (1.0999)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [100/431]  eta: 0:06:13  lr: 0.000169  loss: 1.0831 (1.0974)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [110/431]  eta: 0:06:00  lr: 0.000169  loss: 1.0635 (1.1002)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [120/431]  eta: 0:05:48  lr: 0.000169  loss: 1.1491 (1.1027)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [130/431]  eta: 0:05:36  lr: 0.000169  loss: 1.1153 (1.1019)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [140/431]  eta: 0:05:25  lr: 0.000169  loss: 1.0375 (1.0983)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [150/431]  eta: 0:05:13  lr: 0.000169  loss: 1.0570 (1.1001)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [160/431]  eta: 0:05:02  lr: 0.000169  loss: 1.1287 (1.0993)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [170/431]  eta: 0:04:51  lr: 0.000169  loss: 1.0498 (1.0981)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [180/431]  eta: 0:04:39  lr: 0.000169  loss: 1.0423 (1.0959)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [190/431]  eta: 0:04:28  lr: 0.000169  loss: 1.0431 (1.0961)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [200/431]  eta: 0:04:17  lr: 0.000169  loss: 1.0457 (1.0957)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [210/431]  eta: 0:04:05  lr: 0.000169  loss: 1.1068 (1.0997)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [220/431]  eta: 0:03:54  lr: 0.000169  loss: 1.1866 (1.1033)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [230/431]  eta: 0:03:43  lr: 0.000169  loss: 1.0927 (1.1019)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [240/431]  eta: 0:03:32  lr: 0.000169  loss: 1.0823 (1.1025)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [250/431]  eta: 0:03:21  lr: 0.000169  loss: 1.1011 (1.1064)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [260/431]  eta: 0:03:09  lr: 0.000169  loss: 1.1317 (1.1079)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [270/431]  eta: 0:02:58  lr: 0.000169  loss: 1.1377 (1.1094)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [280/431]  eta: 0:02:47  lr: 0.000169  loss: 1.0757 (1.1077)  time: 1.0994  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:239]  [290/431]  eta: 0:02:36  lr: 0.000169  loss: 1.0518 (1.1066)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [300/431]  eta: 0:02:25  lr: 0.000169  loss: 1.0553 (1.1065)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [310/431]  eta: 0:02:14  lr: 0.000169  loss: 1.1099 (1.1072)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [320/431]  eta: 0:02:03  lr: 0.000169  loss: 1.0928 (1.1072)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [330/431]  eta: 0:01:51  lr: 0.000169  loss: 1.1126 (1.1076)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [340/431]  eta: 0:01:40  lr: 0.000169  loss: 1.0881 (1.1079)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [350/431]  eta: 0:01:29  lr: 0.000169  loss: 1.0988 (1.1092)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [360/431]  eta: 0:01:18  lr: 0.000169  loss: 1.1198 (1.1099)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [370/431]  eta: 0:01:07  lr: 0.000169  loss: 1.0962 (1.1092)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [380/431]  eta: 0:00:56  lr: 0.000169  loss: 1.0577 (1.1083)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [390/431]  eta: 0:00:45  lr: 0.000169  loss: 1.0775 (1.1085)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [400/431]  eta: 0:00:34  lr: 0.000169  loss: 1.0903 (1.1090)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [410/431]  eta: 0:00:23  lr: 0.000169  loss: 1.0844 (1.1079)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:239]  [420/431]  eta: 0:00:12  lr: 0.000169  loss: 1.0573 (1.1072)  time: 1.1024  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:239]  [430/431]  eta: 0:00:01  lr: 0.000169  loss: 1.0573 (1.1059)  time: 1.1025  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:239] Total time: 0:07:57 (1.1071 s / it)\n",
      "Averaged stats: lr: 0.000169  loss: 1.0573 (1.1059)\n",
      "Valid: [epoch:239]  [ 0/14]  eta: 0:00:36  loss: 1.0074 (1.0074)  time: 2.5908  data: 2.4394  max mem: 15925\n",
      "Valid: [epoch:239]  [13/14]  eta: 0:00:00  loss: 1.0293 (1.0430)  time: 0.2719  data: 0.1743  max mem: 15925\n",
      "Valid: [epoch:239] Total time: 0:00:04 (0.2924 s / it)\n",
      "Averaged stats: loss: 1.0293 (1.0430)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_239_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:240]  [  0/431]  eta: 0:35:42  lr: 0.000169  loss: 1.1241 (1.1241)  time: 4.9716  data: 3.5797  max mem: 15925\n",
      "Train: [epoch:240]  [ 10/431]  eta: 0:09:46  lr: 0.000169  loss: 1.1010 (1.1119)  time: 1.3938  data: 0.3256  max mem: 15925\n",
      "Train: [epoch:240]  [ 20/431]  eta: 0:08:23  lr: 0.000169  loss: 1.0697 (1.0868)  time: 1.0373  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [ 30/431]  eta: 0:07:52  lr: 0.000169  loss: 1.0726 (1.1069)  time: 1.0600  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [ 40/431]  eta: 0:07:31  lr: 0.000169  loss: 1.1098 (1.1147)  time: 1.0794  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [ 50/431]  eta: 0:07:15  lr: 0.000169  loss: 1.0722 (1.1117)  time: 1.0850  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [ 60/431]  eta: 0:07:01  lr: 0.000169  loss: 1.0722 (1.1089)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [ 70/431]  eta: 0:06:48  lr: 0.000169  loss: 1.0918 (1.1092)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [ 80/431]  eta: 0:06:35  lr: 0.000169  loss: 1.1188 (1.1191)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [ 90/431]  eta: 0:06:23  lr: 0.000169  loss: 1.1188 (1.1171)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [100/431]  eta: 0:06:11  lr: 0.000169  loss: 1.0947 (1.1151)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [110/431]  eta: 0:05:59  lr: 0.000169  loss: 1.1047 (1.1145)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [120/431]  eta: 0:05:48  lr: 0.000169  loss: 1.0527 (1.1137)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [130/431]  eta: 0:05:36  lr: 0.000169  loss: 1.0291 (1.1081)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [140/431]  eta: 0:05:25  lr: 0.000169  loss: 1.0291 (1.1071)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [150/431]  eta: 0:05:13  lr: 0.000169  loss: 1.0426 (1.1065)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [160/431]  eta: 0:05:02  lr: 0.000169  loss: 1.0564 (1.1062)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [170/431]  eta: 0:04:51  lr: 0.000169  loss: 1.0564 (1.1042)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [180/431]  eta: 0:04:40  lr: 0.000169  loss: 1.0528 (1.1068)  time: 1.1128  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:240]  [190/431]  eta: 0:04:29  lr: 0.000169  loss: 1.0802 (1.1057)  time: 1.1158  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:240]  [200/431]  eta: 0:04:17  lr: 0.000169  loss: 1.0802 (1.1068)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [210/431]  eta: 0:04:06  lr: 0.000169  loss: 1.1223 (1.1072)  time: 1.1041  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:240]  [220/431]  eta: 0:03:55  lr: 0.000169  loss: 1.0917 (1.1053)  time: 1.1032  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:240]  [230/431]  eta: 0:03:43  lr: 0.000169  loss: 1.0297 (1.1028)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [240/431]  eta: 0:03:32  lr: 0.000169  loss: 1.0666 (1.1034)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [250/431]  eta: 0:03:21  lr: 0.000169  loss: 1.1070 (1.1026)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [260/431]  eta: 0:03:10  lr: 0.000169  loss: 1.1180 (1.1041)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [270/431]  eta: 0:02:59  lr: 0.000169  loss: 1.1180 (1.1053)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [280/431]  eta: 0:02:48  lr: 0.000169  loss: 1.1012 (1.1066)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [290/431]  eta: 0:02:36  lr: 0.000169  loss: 1.0845 (1.1065)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [300/431]  eta: 0:02:25  lr: 0.000169  loss: 1.0779 (1.1062)  time: 1.1056  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:240]  [310/431]  eta: 0:02:14  lr: 0.000169  loss: 1.1184 (1.1071)  time: 1.1012  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:240]  [320/431]  eta: 0:02:03  lr: 0.000169  loss: 1.1225 (1.1067)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [330/431]  eta: 0:01:52  lr: 0.000169  loss: 1.1189 (1.1063)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [340/431]  eta: 0:01:41  lr: 0.000169  loss: 1.1263 (1.1086)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [350/431]  eta: 0:01:30  lr: 0.000169  loss: 1.0881 (1.1091)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [360/431]  eta: 0:01:18  lr: 0.000169  loss: 1.0881 (1.1085)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [370/431]  eta: 0:01:07  lr: 0.000169  loss: 1.0881 (1.1077)  time: 1.1302  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [380/431]  eta: 0:00:56  lr: 0.000169  loss: 1.0565 (1.1075)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [390/431]  eta: 0:00:45  lr: 0.000169  loss: 1.0565 (1.1069)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [400/431]  eta: 0:00:34  lr: 0.000169  loss: 1.0731 (1.1075)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [410/431]  eta: 0:00:23  lr: 0.000169  loss: 1.0847 (1.1071)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:240]  [420/431]  eta: 0:00:12  lr: 0.000169  loss: 1.1129 (1.1071)  time: 1.1047  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:240]  [430/431]  eta: 0:00:01  lr: 0.000169  loss: 1.1129 (1.1067)  time: 1.1097  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:240] Total time: 0:07:59 (1.1126 s / it)\n",
      "Averaged stats: lr: 0.000169  loss: 1.1129 (1.1067)\n",
      "Valid: [epoch:240]  [ 0/14]  eta: 0:00:36  loss: 0.9469 (0.9469)  time: 2.5808  data: 2.4318  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:240]  [13/14]  eta: 0:00:00  loss: 1.0473 (1.0576)  time: 0.2721  data: 0.1738  max mem: 15925\n",
      "Valid: [epoch:240] Total time: 0:00:04 (0.2895 s / it)\n",
      "Averaged stats: loss: 1.0473 (1.0576)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_240_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.058%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:241]  [  0/431]  eta: 0:33:50  lr: 0.000169  loss: 1.1280 (1.1280)  time: 4.7100  data: 3.4528  max mem: 15925\n",
      "Train: [epoch:241]  [ 10/431]  eta: 0:09:39  lr: 0.000169  loss: 1.0668 (1.1108)  time: 1.3775  data: 0.3141  max mem: 15925\n",
      "Train: [epoch:241]  [ 20/431]  eta: 0:08:24  lr: 0.000169  loss: 1.0903 (1.1221)  time: 1.0531  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [ 30/431]  eta: 0:07:53  lr: 0.000169  loss: 1.0903 (1.1073)  time: 1.0719  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [ 40/431]  eta: 0:07:33  lr: 0.000169  loss: 1.1259 (1.1156)  time: 1.0866  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [ 50/431]  eta: 0:07:17  lr: 0.000169  loss: 1.1259 (1.1080)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [ 60/431]  eta: 0:07:03  lr: 0.000169  loss: 1.0757 (1.1021)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [ 70/431]  eta: 0:06:50  lr: 0.000169  loss: 1.1046 (1.1095)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [ 80/431]  eta: 0:06:37  lr: 0.000169  loss: 1.1046 (1.1080)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [ 90/431]  eta: 0:06:25  lr: 0.000169  loss: 1.0664 (1.1060)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [100/431]  eta: 0:06:12  lr: 0.000169  loss: 1.1193 (1.1087)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [110/431]  eta: 0:06:01  lr: 0.000169  loss: 1.1194 (1.1047)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [120/431]  eta: 0:05:49  lr: 0.000169  loss: 1.0443 (1.1018)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [130/431]  eta: 0:05:38  lr: 0.000169  loss: 1.0114 (1.0966)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [140/431]  eta: 0:05:26  lr: 0.000169  loss: 0.9980 (1.0894)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [150/431]  eta: 0:05:15  lr: 0.000169  loss: 1.0212 (1.0903)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [160/431]  eta: 0:05:03  lr: 0.000169  loss: 1.0774 (1.0934)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [170/431]  eta: 0:04:51  lr: 0.000169  loss: 1.0660 (1.0919)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [180/431]  eta: 0:04:40  lr: 0.000169  loss: 1.1004 (1.0968)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [190/431]  eta: 0:04:29  lr: 0.000169  loss: 1.0841 (1.0960)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [200/431]  eta: 0:04:17  lr: 0.000169  loss: 1.0656 (1.0958)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [210/431]  eta: 0:04:06  lr: 0.000169  loss: 1.0798 (1.0970)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [220/431]  eta: 0:03:55  lr: 0.000169  loss: 1.0798 (1.0957)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [230/431]  eta: 0:03:43  lr: 0.000169  loss: 1.0970 (1.0955)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [240/431]  eta: 0:03:32  lr: 0.000169  loss: 1.1078 (1.0961)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [250/431]  eta: 0:03:21  lr: 0.000169  loss: 1.0874 (1.0964)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [260/431]  eta: 0:03:10  lr: 0.000169  loss: 1.0653 (1.0951)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [270/431]  eta: 0:02:58  lr: 0.000169  loss: 1.0680 (1.0971)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [280/431]  eta: 0:02:47  lr: 0.000169  loss: 1.1226 (1.0973)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [290/431]  eta: 0:02:36  lr: 0.000169  loss: 1.0854 (1.0966)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [300/431]  eta: 0:02:25  lr: 0.000169  loss: 1.1085 (1.0978)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [310/431]  eta: 0:02:14  lr: 0.000169  loss: 1.1343 (1.0981)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [320/431]  eta: 0:02:03  lr: 0.000169  loss: 1.1171 (1.0988)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [330/431]  eta: 0:01:52  lr: 0.000169  loss: 1.1171 (1.0994)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [340/431]  eta: 0:01:40  lr: 0.000169  loss: 1.1246 (1.1010)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [350/431]  eta: 0:01:29  lr: 0.000169  loss: 1.1018 (1.1020)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [360/431]  eta: 0:01:18  lr: 0.000169  loss: 1.0914 (1.1017)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [370/431]  eta: 0:01:07  lr: 0.000169  loss: 1.1163 (1.1037)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [380/431]  eta: 0:00:56  lr: 0.000169  loss: 1.1255 (1.1038)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [390/431]  eta: 0:00:45  lr: 0.000169  loss: 1.1218 (1.1052)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [400/431]  eta: 0:00:34  lr: 0.000169  loss: 1.1475 (1.1065)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:241]  [410/431]  eta: 0:00:23  lr: 0.000169  loss: 1.1475 (1.1075)  time: 1.0970  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:241]  [420/431]  eta: 0:00:12  lr: 0.000169  loss: 1.1046 (1.1074)  time: 1.1032  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:241]  [430/431]  eta: 0:00:01  lr: 0.000169  loss: 1.0825 (1.1068)  time: 1.1005  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:241] Total time: 0:07:57 (1.1089 s / it)\n",
      "Averaged stats: lr: 0.000169  loss: 1.0825 (1.1068)\n",
      "Valid: [epoch:241]  [ 0/14]  eta: 0:00:35  loss: 0.9561 (0.9561)  time: 2.5395  data: 2.3921  max mem: 15925\n",
      "Valid: [epoch:241]  [13/14]  eta: 0:00:00  loss: 1.0339 (1.0455)  time: 0.2691  data: 0.1710  max mem: 15925\n",
      "Valid: [epoch:241] Total time: 0:00:03 (0.2844 s / it)\n",
      "Averaged stats: loss: 1.0339 (1.0455)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_241_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:242]  [  0/431]  eta: 0:34:21  lr: 0.000169  loss: 1.3252 (1.3252)  time: 4.7840  data: 3.5735  max mem: 15925\n",
      "Train: [epoch:242]  [ 10/431]  eta: 0:09:39  lr: 0.000169  loss: 1.1764 (1.2042)  time: 1.3758  data: 0.3251  max mem: 15925\n",
      "Train: [epoch:242]  [ 20/431]  eta: 0:08:23  lr: 0.000169  loss: 1.1198 (1.1445)  time: 1.0478  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [ 30/431]  eta: 0:07:53  lr: 0.000169  loss: 1.0614 (1.1253)  time: 1.0730  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [ 40/431]  eta: 0:07:32  lr: 0.000169  loss: 1.0471 (1.1087)  time: 1.0836  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [ 50/431]  eta: 0:07:16  lr: 0.000169  loss: 1.0408 (1.1104)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [ 60/431]  eta: 0:07:01  lr: 0.000169  loss: 1.0585 (1.1045)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [ 70/431]  eta: 0:06:48  lr: 0.000169  loss: 1.0300 (1.0972)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [ 80/431]  eta: 0:06:36  lr: 0.000169  loss: 1.0303 (1.0963)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [ 90/431]  eta: 0:06:23  lr: 0.000169  loss: 1.0545 (1.0903)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [100/431]  eta: 0:06:11  lr: 0.000169  loss: 1.1007 (1.0947)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [110/431]  eta: 0:05:59  lr: 0.000169  loss: 1.0927 (1.0905)  time: 1.1016  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:242]  [120/431]  eta: 0:05:48  lr: 0.000169  loss: 1.0655 (1.0945)  time: 1.1004  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:242]  [130/431]  eta: 0:05:36  lr: 0.000169  loss: 1.0627 (1.0915)  time: 1.1038  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:242]  [140/431]  eta: 0:05:25  lr: 0.000169  loss: 1.0551 (1.0897)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [150/431]  eta: 0:05:13  lr: 0.000169  loss: 1.1063 (1.0936)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [160/431]  eta: 0:05:01  lr: 0.000169  loss: 1.0990 (1.0944)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [170/431]  eta: 0:04:50  lr: 0.000169  loss: 1.0764 (1.0931)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [180/431]  eta: 0:04:39  lr: 0.000169  loss: 1.0653 (1.0930)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [190/431]  eta: 0:04:28  lr: 0.000169  loss: 1.1121 (1.0967)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [200/431]  eta: 0:04:16  lr: 0.000169  loss: 1.1434 (1.0974)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [210/431]  eta: 0:04:05  lr: 0.000169  loss: 1.1536 (1.0990)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [220/431]  eta: 0:03:54  lr: 0.000169  loss: 1.0642 (1.0974)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [230/431]  eta: 0:03:43  lr: 0.000169  loss: 1.0705 (1.0980)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [240/431]  eta: 0:03:32  lr: 0.000169  loss: 1.1093 (1.0983)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [250/431]  eta: 0:03:21  lr: 0.000169  loss: 1.1083 (1.1007)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [260/431]  eta: 0:03:10  lr: 0.000169  loss: 1.0877 (1.1008)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [270/431]  eta: 0:02:59  lr: 0.000169  loss: 1.0877 (1.1024)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [280/431]  eta: 0:02:47  lr: 0.000169  loss: 1.0454 (1.1024)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [290/431]  eta: 0:02:36  lr: 0.000169  loss: 1.0733 (1.1013)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [300/431]  eta: 0:02:25  lr: 0.000169  loss: 1.0278 (1.0999)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [310/431]  eta: 0:02:14  lr: 0.000169  loss: 1.0651 (1.1001)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [320/431]  eta: 0:02:03  lr: 0.000169  loss: 1.0908 (1.1005)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [330/431]  eta: 0:01:52  lr: 0.000169  loss: 1.1020 (1.1023)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [340/431]  eta: 0:01:41  lr: 0.000169  loss: 1.1020 (1.1031)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [350/431]  eta: 0:01:30  lr: 0.000169  loss: 1.1119 (1.1050)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [360/431]  eta: 0:01:18  lr: 0.000169  loss: 1.1915 (1.1058)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [370/431]  eta: 0:01:07  lr: 0.000169  loss: 1.1403 (1.1063)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [380/431]  eta: 0:00:56  lr: 0.000169  loss: 1.0859 (1.1046)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [390/431]  eta: 0:00:45  lr: 0.000169  loss: 1.0660 (1.1045)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [400/431]  eta: 0:00:34  lr: 0.000169  loss: 1.1333 (1.1067)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [410/431]  eta: 0:00:23  lr: 0.000169  loss: 1.0891 (1.1059)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:242]  [420/431]  eta: 0:00:12  lr: 0.000169  loss: 1.0434 (1.1057)  time: 1.1099  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:242]  [430/431]  eta: 0:00:01  lr: 0.000169  loss: 1.0956 (1.1052)  time: 1.1064  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:242] Total time: 0:07:59 (1.1118 s / it)\n",
      "Averaged stats: lr: 0.000169  loss: 1.0956 (1.1052)\n",
      "Valid: [epoch:242]  [ 0/14]  eta: 0:00:36  loss: 1.0304 (1.0304)  time: 2.6035  data: 2.4606  max mem: 15925\n",
      "Valid: [epoch:242]  [13/14]  eta: 0:00:00  loss: 1.0304 (1.0441)  time: 0.2733  data: 0.1758  max mem: 15925\n",
      "Valid: [epoch:242] Total time: 0:00:04 (0.2885 s / it)\n",
      "Averaged stats: loss: 1.0304 (1.0441)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_242_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:243]  [  0/431]  eta: 0:35:50  lr: 0.000168  loss: 1.0636 (1.0636)  time: 4.9894  data: 3.8299  max mem: 15925\n",
      "Train: [epoch:243]  [ 10/431]  eta: 0:10:00  lr: 0.000168  loss: 1.1388 (1.1499)  time: 1.4252  data: 0.3484  max mem: 15925\n",
      "Train: [epoch:243]  [ 20/431]  eta: 0:08:33  lr: 0.000168  loss: 1.1290 (1.1402)  time: 1.0625  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [ 30/431]  eta: 0:08:00  lr: 0.000168  loss: 1.1108 (1.1271)  time: 1.0739  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [ 40/431]  eta: 0:07:38  lr: 0.000168  loss: 1.0603 (1.1225)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [ 50/431]  eta: 0:07:21  lr: 0.000168  loss: 1.0510 (1.1078)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [ 60/431]  eta: 0:07:06  lr: 0.000168  loss: 1.0160 (1.1040)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [ 70/431]  eta: 0:06:52  lr: 0.000168  loss: 1.0592 (1.1009)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [ 80/431]  eta: 0:06:39  lr: 0.000168  loss: 1.0523 (1.0972)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [ 90/431]  eta: 0:06:26  lr: 0.000168  loss: 1.0523 (1.0933)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [100/431]  eta: 0:06:14  lr: 0.000168  loss: 1.0611 (1.0984)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [110/431]  eta: 0:06:02  lr: 0.000168  loss: 1.0724 (1.0974)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [120/431]  eta: 0:05:50  lr: 0.000168  loss: 1.0696 (1.0936)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [130/431]  eta: 0:05:38  lr: 0.000168  loss: 1.0300 (1.0917)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [140/431]  eta: 0:05:26  lr: 0.000168  loss: 0.9946 (1.0886)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [150/431]  eta: 0:05:15  lr: 0.000168  loss: 1.0426 (1.0924)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [160/431]  eta: 0:05:04  lr: 0.000168  loss: 1.0863 (1.0919)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [170/431]  eta: 0:04:52  lr: 0.000168  loss: 1.0694 (1.0932)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [180/431]  eta: 0:04:41  lr: 0.000168  loss: 1.1144 (1.0972)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [190/431]  eta: 0:04:29  lr: 0.000168  loss: 1.0996 (1.0955)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [200/431]  eta: 0:04:18  lr: 0.000168  loss: 1.0774 (1.0961)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [210/431]  eta: 0:04:07  lr: 0.000168  loss: 1.0774 (1.0963)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [220/431]  eta: 0:03:55  lr: 0.000168  loss: 1.0994 (1.0989)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [230/431]  eta: 0:03:44  lr: 0.000168  loss: 1.1584 (1.1049)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [240/431]  eta: 0:03:33  lr: 0.000168  loss: 1.2051 (1.1060)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [250/431]  eta: 0:03:22  lr: 0.000168  loss: 1.0920 (1.1040)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [260/431]  eta: 0:03:10  lr: 0.000168  loss: 1.0565 (1.1038)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [270/431]  eta: 0:02:59  lr: 0.000168  loss: 1.0565 (1.1038)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [280/431]  eta: 0:02:48  lr: 0.000168  loss: 1.0464 (1.1022)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [290/431]  eta: 0:02:37  lr: 0.000168  loss: 1.0697 (1.1024)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [300/431]  eta: 0:02:25  lr: 0.000168  loss: 1.0724 (1.1018)  time: 1.1017  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:243]  [310/431]  eta: 0:02:14  lr: 0.000168  loss: 1.0528 (1.1008)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [320/431]  eta: 0:02:03  lr: 0.000168  loss: 1.0788 (1.1017)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [330/431]  eta: 0:01:52  lr: 0.000168  loss: 1.1022 (1.1044)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [340/431]  eta: 0:01:41  lr: 0.000168  loss: 1.0781 (1.1040)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [350/431]  eta: 0:01:30  lr: 0.000168  loss: 1.0667 (1.1046)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [360/431]  eta: 0:01:19  lr: 0.000168  loss: 1.0785 (1.1046)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [370/431]  eta: 0:01:07  lr: 0.000168  loss: 1.0040 (1.1016)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [380/431]  eta: 0:00:56  lr: 0.000168  loss: 1.0040 (1.1017)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [390/431]  eta: 0:00:45  lr: 0.000168  loss: 1.0367 (1.1001)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [400/431]  eta: 0:00:34  lr: 0.000168  loss: 1.0760 (1.1026)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [410/431]  eta: 0:00:23  lr: 0.000168  loss: 1.1334 (1.1028)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:243]  [420/431]  eta: 0:00:12  lr: 0.000168  loss: 1.0704 (1.1032)  time: 1.1073  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:243]  [430/431]  eta: 0:00:01  lr: 0.000168  loss: 1.1121 (1.1044)  time: 1.1051  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:243] Total time: 0:07:59 (1.1125 s / it)\n",
      "Averaged stats: lr: 0.000168  loss: 1.1121 (1.1044)\n",
      "Valid: [epoch:243]  [ 0/14]  eta: 0:00:34  loss: 1.1022 (1.1022)  time: 2.4539  data: 2.3020  max mem: 15925\n",
      "Valid: [epoch:243]  [13/14]  eta: 0:00:00  loss: 1.0388 (1.0501)  time: 0.2557  data: 0.1645  max mem: 15925\n",
      "Valid: [epoch:243] Total time: 0:00:03 (0.2707 s / it)\n",
      "Averaged stats: loss: 1.0388 (1.0501)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_243_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:244]  [  0/431]  eta: 0:35:43  lr: 0.000168  loss: 1.1244 (1.1244)  time: 4.9725  data: 3.5282  max mem: 15925\n",
      "Train: [epoch:244]  [ 10/431]  eta: 0:09:40  lr: 0.000168  loss: 1.1244 (1.1277)  time: 1.3784  data: 0.3209  max mem: 15925\n",
      "Train: [epoch:244]  [ 20/431]  eta: 0:08:23  lr: 0.000168  loss: 1.0997 (1.1235)  time: 1.0380  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [ 30/431]  eta: 0:07:53  lr: 0.000168  loss: 1.0774 (1.1048)  time: 1.0707  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [ 40/431]  eta: 0:07:32  lr: 0.000168  loss: 1.0416 (1.0962)  time: 1.0840  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [ 50/431]  eta: 0:07:16  lr: 0.000168  loss: 1.0416 (1.0920)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [ 60/431]  eta: 0:07:02  lr: 0.000168  loss: 1.0378 (1.0867)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [ 70/431]  eta: 0:06:48  lr: 0.000168  loss: 1.0265 (1.0824)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [ 80/431]  eta: 0:06:35  lr: 0.000168  loss: 1.0296 (1.0807)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [ 90/431]  eta: 0:06:23  lr: 0.000168  loss: 1.0445 (1.0851)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [100/431]  eta: 0:06:11  lr: 0.000168  loss: 1.0500 (1.0784)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [110/431]  eta: 0:05:59  lr: 0.000168  loss: 0.9954 (1.0769)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [120/431]  eta: 0:05:48  lr: 0.000168  loss: 1.0684 (1.0799)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [130/431]  eta: 0:05:36  lr: 0.000168  loss: 1.0684 (1.0783)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [140/431]  eta: 0:05:25  lr: 0.000168  loss: 1.0632 (1.0814)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [150/431]  eta: 0:05:14  lr: 0.000168  loss: 1.0733 (1.0811)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [160/431]  eta: 0:05:02  lr: 0.000168  loss: 1.0822 (1.0854)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [170/431]  eta: 0:04:51  lr: 0.000168  loss: 1.0560 (1.0859)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [180/431]  eta: 0:04:40  lr: 0.000168  loss: 1.0491 (1.0869)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [190/431]  eta: 0:04:29  lr: 0.000168  loss: 1.0752 (1.0890)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [200/431]  eta: 0:04:18  lr: 0.000168  loss: 1.1056 (1.0909)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [210/431]  eta: 0:04:06  lr: 0.000168  loss: 1.1129 (1.0921)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [220/431]  eta: 0:03:55  lr: 0.000168  loss: 1.1129 (1.0959)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [230/431]  eta: 0:03:44  lr: 0.000168  loss: 1.1051 (1.0951)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [240/431]  eta: 0:03:32  lr: 0.000168  loss: 1.0999 (1.0944)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [250/431]  eta: 0:03:21  lr: 0.000168  loss: 1.0999 (1.0940)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [260/431]  eta: 0:03:10  lr: 0.000168  loss: 1.0673 (1.0963)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [270/431]  eta: 0:02:59  lr: 0.000168  loss: 1.0880 (1.0979)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [280/431]  eta: 0:02:48  lr: 0.000168  loss: 1.1309 (1.0978)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [290/431]  eta: 0:02:36  lr: 0.000168  loss: 1.0711 (1.0977)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [300/431]  eta: 0:02:25  lr: 0.000168  loss: 1.0711 (1.0989)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [310/431]  eta: 0:02:14  lr: 0.000168  loss: 1.1151 (1.0997)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [320/431]  eta: 0:02:03  lr: 0.000168  loss: 1.1222 (1.1010)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [330/431]  eta: 0:01:52  lr: 0.000168  loss: 1.1513 (1.1023)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [340/431]  eta: 0:01:41  lr: 0.000168  loss: 1.1286 (1.1047)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [350/431]  eta: 0:01:30  lr: 0.000168  loss: 1.0616 (1.1063)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [360/431]  eta: 0:01:18  lr: 0.000168  loss: 1.0586 (1.1059)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [370/431]  eta: 0:01:07  lr: 0.000168  loss: 1.0690 (1.1048)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [380/431]  eta: 0:00:56  lr: 0.000168  loss: 1.0697 (1.1042)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [390/431]  eta: 0:00:45  lr: 0.000168  loss: 1.0880 (1.1051)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [400/431]  eta: 0:00:34  lr: 0.000168  loss: 1.1147 (1.1049)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [410/431]  eta: 0:00:23  lr: 0.000168  loss: 1.1287 (1.1057)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:244]  [420/431]  eta: 0:00:12  lr: 0.000168  loss: 1.1300 (1.1057)  time: 1.0975  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:244]  [430/431]  eta: 0:00:01  lr: 0.000168  loss: 1.1033 (1.1056)  time: 1.0990  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:244] Total time: 0:07:58 (1.1103 s / it)\n",
      "Averaged stats: lr: 0.000168  loss: 1.1033 (1.1056)\n",
      "Valid: [epoch:244]  [ 0/14]  eta: 0:00:35  loss: 1.0272 (1.0272)  time: 2.5445  data: 2.3890  max mem: 15925\n",
      "Valid: [epoch:244]  [13/14]  eta: 0:00:00  loss: 1.0352 (1.0456)  time: 0.2761  data: 0.1707  max mem: 15925\n",
      "Valid: [epoch:244] Total time: 0:00:04 (0.2913 s / it)\n",
      "Averaged stats: loss: 1.0352 (1.0456)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_244_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:245]  [  0/431]  eta: 0:32:00  lr: 0.000168  loss: 1.0565 (1.0565)  time: 4.4567  data: 3.1541  max mem: 15925\n",
      "Train: [epoch:245]  [ 10/431]  eta: 0:09:24  lr: 0.000168  loss: 1.1116 (1.1077)  time: 1.3417  data: 0.2869  max mem: 15925\n",
      "Train: [epoch:245]  [ 20/431]  eta: 0:08:12  lr: 0.000168  loss: 1.1390 (1.1319)  time: 1.0356  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [ 30/431]  eta: 0:07:42  lr: 0.000168  loss: 1.0954 (1.1053)  time: 1.0513  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:245]  [ 40/431]  eta: 0:07:25  lr: 0.000168  loss: 1.0342 (1.1017)  time: 1.0783  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [ 50/431]  eta: 0:07:11  lr: 0.000168  loss: 1.0832 (1.1014)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [ 60/431]  eta: 0:07:00  lr: 0.000168  loss: 1.0985 (1.1075)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [ 70/431]  eta: 0:06:47  lr: 0.000168  loss: 1.1241 (1.1061)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [ 80/431]  eta: 0:06:36  lr: 0.000168  loss: 1.0949 (1.1039)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [ 90/431]  eta: 0:06:24  lr: 0.000168  loss: 1.0718 (1.0995)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [100/431]  eta: 0:06:13  lr: 0.000168  loss: 1.0243 (1.0932)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [110/431]  eta: 0:06:01  lr: 0.000168  loss: 1.0118 (1.0929)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [120/431]  eta: 0:05:49  lr: 0.000168  loss: 1.0357 (1.0884)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [130/431]  eta: 0:05:38  lr: 0.000168  loss: 1.0537 (1.0934)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [140/431]  eta: 0:05:26  lr: 0.000168  loss: 1.1144 (1.0960)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [150/431]  eta: 0:05:15  lr: 0.000168  loss: 1.1047 (1.0956)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [160/431]  eta: 0:05:03  lr: 0.000168  loss: 1.1590 (1.1000)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [170/431]  eta: 0:04:52  lr: 0.000168  loss: 1.1118 (1.0995)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [180/431]  eta: 0:04:41  lr: 0.000168  loss: 1.0473 (1.0978)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [190/431]  eta: 0:04:29  lr: 0.000168  loss: 1.0631 (1.0976)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [200/431]  eta: 0:04:18  lr: 0.000168  loss: 1.0710 (1.0962)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [210/431]  eta: 0:04:06  lr: 0.000168  loss: 1.0726 (1.0966)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [220/431]  eta: 0:03:55  lr: 0.000168  loss: 1.1051 (1.0983)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [230/431]  eta: 0:03:44  lr: 0.000168  loss: 1.1573 (1.1008)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [240/431]  eta: 0:03:32  lr: 0.000168  loss: 1.0907 (1.0994)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [250/431]  eta: 0:03:21  lr: 0.000168  loss: 1.0674 (1.1008)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [260/431]  eta: 0:03:10  lr: 0.000168  loss: 1.1251 (1.1026)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [270/431]  eta: 0:02:59  lr: 0.000168  loss: 1.1291 (1.1042)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [280/431]  eta: 0:02:48  lr: 0.000168  loss: 1.0607 (1.1022)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [290/431]  eta: 0:02:36  lr: 0.000168  loss: 1.0381 (1.1019)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [300/431]  eta: 0:02:25  lr: 0.000168  loss: 1.0785 (1.1028)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [310/431]  eta: 0:02:14  lr: 0.000168  loss: 1.0989 (1.1031)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [320/431]  eta: 0:02:03  lr: 0.000168  loss: 1.0602 (1.1024)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [330/431]  eta: 0:01:52  lr: 0.000168  loss: 1.0516 (1.1031)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [340/431]  eta: 0:01:41  lr: 0.000168  loss: 1.1894 (1.1069)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [350/431]  eta: 0:01:30  lr: 0.000168  loss: 1.1501 (1.1076)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [360/431]  eta: 0:01:18  lr: 0.000168  loss: 1.0861 (1.1074)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [370/431]  eta: 0:01:07  lr: 0.000168  loss: 1.0831 (1.1063)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [380/431]  eta: 0:00:56  lr: 0.000168  loss: 1.1048 (1.1085)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [390/431]  eta: 0:00:45  lr: 0.000168  loss: 1.1267 (1.1081)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [400/431]  eta: 0:00:34  lr: 0.000168  loss: 1.0861 (1.1078)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:245]  [410/431]  eta: 0:00:23  lr: 0.000168  loss: 1.0746 (1.1076)  time: 1.0906  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:245]  [420/431]  eta: 0:00:12  lr: 0.000168  loss: 1.0673 (1.1073)  time: 1.0942  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:245]  [430/431]  eta: 0:00:01  lr: 0.000168  loss: 1.0587 (1.1065)  time: 1.1040  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:245] Total time: 0:07:58 (1.1105 s / it)\n",
      "Averaged stats: lr: 0.000168  loss: 1.0587 (1.1065)\n",
      "Valid: [epoch:245]  [ 0/14]  eta: 0:00:35  loss: 1.0979 (1.0979)  time: 2.5310  data: 2.3920  max mem: 15925\n",
      "Valid: [epoch:245]  [13/14]  eta: 0:00:00  loss: 1.0340 (1.0450)  time: 0.2691  data: 0.1709  max mem: 15925\n",
      "Valid: [epoch:245] Total time: 0:00:04 (0.2862 s / it)\n",
      "Averaged stats: loss: 1.0340 (1.0450)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_245_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:246]  [  0/431]  eta: 0:32:26  lr: 0.000168  loss: 1.2954 (1.2954)  time: 4.5172  data: 3.3509  max mem: 15925\n",
      "Train: [epoch:246]  [ 10/431]  eta: 0:09:33  lr: 0.000168  loss: 1.1082 (1.1654)  time: 1.3617  data: 0.3048  max mem: 15925\n",
      "Train: [epoch:246]  [ 20/431]  eta: 0:08:21  lr: 0.000168  loss: 1.1082 (1.1471)  time: 1.0560  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [ 30/431]  eta: 0:07:51  lr: 0.000168  loss: 1.1151 (1.1397)  time: 1.0729  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [ 40/431]  eta: 0:07:31  lr: 0.000168  loss: 1.0607 (1.1173)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [ 50/431]  eta: 0:07:16  lr: 0.000168  loss: 1.0452 (1.1124)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [ 60/431]  eta: 0:07:02  lr: 0.000168  loss: 1.0959 (1.1119)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [ 70/431]  eta: 0:06:49  lr: 0.000168  loss: 1.0662 (1.1134)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [ 80/431]  eta: 0:06:37  lr: 0.000168  loss: 1.0571 (1.1086)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [ 90/431]  eta: 0:06:24  lr: 0.000168  loss: 1.0486 (1.1050)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [100/431]  eta: 0:06:12  lr: 0.000168  loss: 1.0238 (1.1042)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [110/431]  eta: 0:06:01  lr: 0.000168  loss: 1.0503 (1.1007)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [120/431]  eta: 0:05:49  lr: 0.000168  loss: 1.0503 (1.0950)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [130/431]  eta: 0:05:37  lr: 0.000168  loss: 1.0567 (1.0916)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [140/431]  eta: 0:05:26  lr: 0.000168  loss: 1.0539 (1.0892)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [150/431]  eta: 0:05:14  lr: 0.000168  loss: 1.0539 (1.0898)  time: 1.1072  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:246]  [160/431]  eta: 0:05:03  lr: 0.000168  loss: 1.0964 (1.0914)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [170/431]  eta: 0:04:51  lr: 0.000168  loss: 1.0739 (1.0913)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [180/431]  eta: 0:04:40  lr: 0.000168  loss: 1.0519 (1.0946)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [190/431]  eta: 0:04:29  lr: 0.000168  loss: 1.0566 (1.0937)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [200/431]  eta: 0:04:18  lr: 0.000168  loss: 1.0615 (1.0924)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [210/431]  eta: 0:04:06  lr: 0.000168  loss: 1.0949 (1.0946)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [220/431]  eta: 0:03:55  lr: 0.000168  loss: 1.0949 (1.0956)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [230/431]  eta: 0:03:44  lr: 0.000168  loss: 1.0436 (1.0958)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [240/431]  eta: 0:03:33  lr: 0.000168  loss: 1.0576 (1.0956)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [250/431]  eta: 0:03:22  lr: 0.000168  loss: 1.0576 (1.0976)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [260/431]  eta: 0:03:10  lr: 0.000168  loss: 1.1195 (1.0977)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [270/431]  eta: 0:02:59  lr: 0.000168  loss: 1.1195 (1.1002)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [280/431]  eta: 0:02:48  lr: 0.000168  loss: 1.1050 (1.0994)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [290/431]  eta: 0:02:37  lr: 0.000168  loss: 1.0574 (1.0984)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [300/431]  eta: 0:02:26  lr: 0.000168  loss: 1.0705 (1.1009)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [310/431]  eta: 0:02:14  lr: 0.000168  loss: 1.1505 (1.1010)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [320/431]  eta: 0:02:03  lr: 0.000168  loss: 1.0678 (1.0995)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [330/431]  eta: 0:01:52  lr: 0.000168  loss: 1.0522 (1.0992)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [340/431]  eta: 0:01:41  lr: 0.000168  loss: 1.0760 (1.1004)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [350/431]  eta: 0:01:30  lr: 0.000168  loss: 1.1355 (1.1046)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [360/431]  eta: 0:01:19  lr: 0.000168  loss: 1.1355 (1.1045)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [370/431]  eta: 0:01:07  lr: 0.000168  loss: 1.0450 (1.1042)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [380/431]  eta: 0:00:56  lr: 0.000168  loss: 1.0478 (1.1049)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [390/431]  eta: 0:00:45  lr: 0.000168  loss: 1.1328 (1.1057)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [400/431]  eta: 0:00:34  lr: 0.000168  loss: 1.0701 (1.1061)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [410/431]  eta: 0:00:23  lr: 0.000168  loss: 1.0562 (1.1059)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:246]  [420/431]  eta: 0:00:12  lr: 0.000168  loss: 1.0714 (1.1062)  time: 1.1010  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:246]  [430/431]  eta: 0:00:01  lr: 0.000168  loss: 1.0910 (1.1067)  time: 1.1111  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:246] Total time: 0:08:00 (1.1138 s / it)\n",
      "Averaged stats: lr: 0.000168  loss: 1.0910 (1.1067)\n",
      "Valid: [epoch:246]  [ 0/14]  eta: 0:00:36  loss: 0.9385 (0.9385)  time: 2.6064  data: 2.4620  max mem: 15925\n",
      "Valid: [epoch:246]  [13/14]  eta: 0:00:00  loss: 1.0388 (1.0490)  time: 0.2751  data: 0.1760  max mem: 15925\n",
      "Valid: [epoch:246] Total time: 0:00:04 (0.2913 s / it)\n",
      "Averaged stats: loss: 1.0388 (1.0490)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_246_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:247]  [  0/431]  eta: 0:32:41  lr: 0.000168  loss: 1.0587 (1.0587)  time: 4.5505  data: 3.2442  max mem: 15925\n",
      "Train: [epoch:247]  [ 10/431]  eta: 0:09:41  lr: 0.000168  loss: 1.1762 (1.1707)  time: 1.3808  data: 0.2952  max mem: 15925\n",
      "Train: [epoch:247]  [ 20/431]  eta: 0:08:22  lr: 0.000168  loss: 1.1543 (1.1458)  time: 1.0571  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [ 30/431]  eta: 0:07:52  lr: 0.000168  loss: 1.0484 (1.1138)  time: 1.0670  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [ 40/431]  eta: 0:07:31  lr: 0.000168  loss: 1.0245 (1.1012)  time: 1.0834  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [ 50/431]  eta: 0:07:14  lr: 0.000168  loss: 1.0758 (1.1111)  time: 1.0824  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [ 60/431]  eta: 0:07:00  lr: 0.000168  loss: 1.0863 (1.1031)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [ 70/431]  eta: 0:06:48  lr: 0.000168  loss: 1.0741 (1.1048)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [ 80/431]  eta: 0:06:35  lr: 0.000168  loss: 1.1085 (1.1096)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [ 90/431]  eta: 0:06:23  lr: 0.000168  loss: 1.0798 (1.1021)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [100/431]  eta: 0:06:11  lr: 0.000168  loss: 1.0893 (1.1070)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [110/431]  eta: 0:06:00  lr: 0.000168  loss: 1.0882 (1.0999)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [120/431]  eta: 0:05:48  lr: 0.000168  loss: 0.9978 (1.0950)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [130/431]  eta: 0:05:37  lr: 0.000168  loss: 1.0395 (1.0942)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [140/431]  eta: 0:05:25  lr: 0.000168  loss: 1.0594 (1.0931)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [150/431]  eta: 0:05:14  lr: 0.000168  loss: 1.0271 (1.0910)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [160/431]  eta: 0:05:03  lr: 0.000168  loss: 1.0281 (1.0900)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [170/431]  eta: 0:04:51  lr: 0.000168  loss: 1.0484 (1.0919)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [180/431]  eta: 0:04:40  lr: 0.000168  loss: 1.1177 (1.0952)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [190/431]  eta: 0:04:29  lr: 0.000168  loss: 1.1277 (1.0985)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [200/431]  eta: 0:04:17  lr: 0.000168  loss: 1.0772 (1.1001)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [210/431]  eta: 0:04:06  lr: 0.000168  loss: 1.0518 (1.0976)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [220/431]  eta: 0:03:55  lr: 0.000168  loss: 1.0700 (1.1011)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [230/431]  eta: 0:03:43  lr: 0.000168  loss: 1.0800 (1.1012)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [240/431]  eta: 0:03:32  lr: 0.000168  loss: 1.0473 (1.1018)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [250/431]  eta: 0:03:21  lr: 0.000168  loss: 1.0668 (1.1017)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [260/431]  eta: 0:03:10  lr: 0.000168  loss: 1.0840 (1.1028)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [270/431]  eta: 0:02:59  lr: 0.000168  loss: 1.0899 (1.1047)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [280/431]  eta: 0:02:48  lr: 0.000168  loss: 1.0652 (1.1025)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [290/431]  eta: 0:02:36  lr: 0.000168  loss: 1.0708 (1.1043)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [300/431]  eta: 0:02:25  lr: 0.000168  loss: 1.1313 (1.1070)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [310/431]  eta: 0:02:14  lr: 0.000168  loss: 1.0956 (1.1059)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [320/431]  eta: 0:02:03  lr: 0.000168  loss: 1.0704 (1.1056)  time: 1.0977  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:247]  [330/431]  eta: 0:01:52  lr: 0.000168  loss: 1.1143 (1.1072)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [340/431]  eta: 0:01:41  lr: 0.000168  loss: 1.1334 (1.1084)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [350/431]  eta: 0:01:29  lr: 0.000168  loss: 1.0687 (1.1069)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [360/431]  eta: 0:01:18  lr: 0.000168  loss: 1.0604 (1.1060)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [370/431]  eta: 0:01:07  lr: 0.000168  loss: 1.0870 (1.1051)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [380/431]  eta: 0:00:56  lr: 0.000168  loss: 1.1005 (1.1063)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [390/431]  eta: 0:00:45  lr: 0.000168  loss: 1.0853 (1.1050)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [400/431]  eta: 0:00:34  lr: 0.000168  loss: 1.0554 (1.1059)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [410/431]  eta: 0:00:23  lr: 0.000168  loss: 1.0792 (1.1054)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:247]  [420/431]  eta: 0:00:12  lr: 0.000168  loss: 1.0558 (1.1042)  time: 1.1020  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:247]  [430/431]  eta: 0:00:01  lr: 0.000168  loss: 1.0622 (1.1056)  time: 1.1054  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:247] Total time: 0:07:58 (1.1101 s / it)\n",
      "Averaged stats: lr: 0.000168  loss: 1.0622 (1.1056)\n",
      "Valid: [epoch:247]  [ 0/14]  eta: 0:00:35  loss: 0.9565 (0.9565)  time: 2.5034  data: 2.3571  max mem: 15925\n",
      "Valid: [epoch:247]  [13/14]  eta: 0:00:00  loss: 1.0349 (1.0463)  time: 0.2576  data: 0.1684  max mem: 15925\n",
      "Valid: [epoch:247] Total time: 0:00:03 (0.2735 s / it)\n",
      "Averaged stats: loss: 1.0349 (1.0463)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_247_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:248]  [  0/431]  eta: 0:33:12  lr: 0.000167  loss: 1.1119 (1.1119)  time: 4.6229  data: 3.4548  max mem: 15925\n",
      "Train: [epoch:248]  [ 10/431]  eta: 0:09:35  lr: 0.000167  loss: 1.1525 (1.1475)  time: 1.3664  data: 0.3143  max mem: 15925\n",
      "Train: [epoch:248]  [ 20/431]  eta: 0:08:21  lr: 0.000167  loss: 1.0815 (1.1219)  time: 1.0489  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [ 30/431]  eta: 0:07:49  lr: 0.000167  loss: 1.0431 (1.1087)  time: 1.0635  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:248]  [ 40/431]  eta: 0:07:30  lr: 0.000167  loss: 1.0511 (1.1031)  time: 1.0825  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [ 50/431]  eta: 0:07:13  lr: 0.000167  loss: 1.0628 (1.0968)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [ 60/431]  eta: 0:07:00  lr: 0.000167  loss: 1.0653 (1.0953)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [ 70/431]  eta: 0:06:47  lr: 0.000167  loss: 1.0795 (1.0955)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [ 80/431]  eta: 0:06:34  lr: 0.000167  loss: 1.1092 (1.1036)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [ 90/431]  eta: 0:06:23  lr: 0.000167  loss: 1.1489 (1.1081)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [100/431]  eta: 0:06:11  lr: 0.000167  loss: 1.0523 (1.1061)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [110/431]  eta: 0:05:59  lr: 0.000167  loss: 1.0523 (1.1016)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [120/431]  eta: 0:05:48  lr: 0.000167  loss: 1.0630 (1.0980)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [130/431]  eta: 0:05:36  lr: 0.000167  loss: 1.0472 (1.0948)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [140/431]  eta: 0:05:25  lr: 0.000167  loss: 1.0497 (1.0947)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [150/431]  eta: 0:05:13  lr: 0.000167  loss: 1.0449 (1.0911)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [160/431]  eta: 0:05:02  lr: 0.000167  loss: 1.0298 (1.0912)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [170/431]  eta: 0:04:51  lr: 0.000167  loss: 1.0975 (1.0935)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [180/431]  eta: 0:04:40  lr: 0.000167  loss: 1.0962 (1.0927)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [190/431]  eta: 0:04:28  lr: 0.000167  loss: 1.1111 (1.0947)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [200/431]  eta: 0:04:17  lr: 0.000167  loss: 1.1322 (1.0954)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [210/431]  eta: 0:04:06  lr: 0.000167  loss: 1.0897 (1.0980)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [220/431]  eta: 0:03:55  lr: 0.000167  loss: 1.1285 (1.1031)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [230/431]  eta: 0:03:44  lr: 0.000167  loss: 1.1099 (1.1025)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [240/431]  eta: 0:03:33  lr: 0.000167  loss: 1.0809 (1.1028)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [250/431]  eta: 0:03:21  lr: 0.000167  loss: 1.1275 (1.1020)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [260/431]  eta: 0:03:10  lr: 0.000167  loss: 1.0808 (1.1017)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [270/431]  eta: 0:02:59  lr: 0.000167  loss: 1.1034 (1.1030)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [280/431]  eta: 0:02:48  lr: 0.000167  loss: 1.1300 (1.1032)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [290/431]  eta: 0:02:37  lr: 0.000167  loss: 1.0847 (1.1035)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [300/431]  eta: 0:02:25  lr: 0.000167  loss: 1.0526 (1.1011)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [310/431]  eta: 0:02:14  lr: 0.000167  loss: 1.0278 (1.1007)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [320/431]  eta: 0:02:03  lr: 0.000167  loss: 1.0413 (1.0987)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [330/431]  eta: 0:01:52  lr: 0.000167  loss: 1.0470 (1.0990)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [340/431]  eta: 0:01:41  lr: 0.000167  loss: 1.1073 (1.0996)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [350/431]  eta: 0:01:30  lr: 0.000167  loss: 1.1488 (1.1005)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [360/431]  eta: 0:01:19  lr: 0.000167  loss: 1.0970 (1.1001)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [370/431]  eta: 0:01:07  lr: 0.000167  loss: 1.0542 (1.0998)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [380/431]  eta: 0:00:56  lr: 0.000167  loss: 1.1056 (1.1006)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [390/431]  eta: 0:00:45  lr: 0.000167  loss: 1.1281 (1.1022)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [400/431]  eta: 0:00:34  lr: 0.000167  loss: 1.1723 (1.1043)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [410/431]  eta: 0:00:23  lr: 0.000167  loss: 1.1217 (1.1053)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:248]  [420/431]  eta: 0:00:12  lr: 0.000167  loss: 1.0583 (1.1045)  time: 1.1060  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:248]  [430/431]  eta: 0:00:01  lr: 0.000167  loss: 1.1064 (1.1055)  time: 1.1087  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:248] Total time: 0:08:00 (1.1141 s / it)\n",
      "Averaged stats: lr: 0.000167  loss: 1.1064 (1.1055)\n",
      "Valid: [epoch:248]  [ 0/14]  eta: 0:00:35  loss: 1.0993 (1.0993)  time: 2.5483  data: 2.3702  max mem: 15925\n",
      "Valid: [epoch:248]  [13/14]  eta: 0:00:00  loss: 1.0393 (1.0486)  time: 0.2696  data: 0.1694  max mem: 15925\n",
      "Valid: [epoch:248] Total time: 0:00:04 (0.2858 s / it)\n",
      "Averaged stats: loss: 1.0393 (1.0486)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_248_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:249]  [  0/431]  eta: 0:37:11  lr: 0.000167  loss: 1.0476 (1.0476)  time: 5.1771  data: 3.9924  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:249]  [ 10/431]  eta: 0:09:55  lr: 0.000167  loss: 1.0610 (1.1343)  time: 1.4139  data: 0.3631  max mem: 15925\n",
      "Train: [epoch:249]  [ 20/431]  eta: 0:08:32  lr: 0.000167  loss: 1.0890 (1.1663)  time: 1.0493  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [ 30/431]  eta: 0:07:57  lr: 0.000167  loss: 1.1263 (1.1437)  time: 1.0665  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [ 40/431]  eta: 0:07:35  lr: 0.000167  loss: 1.1186 (1.1276)  time: 1.0801  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [ 50/431]  eta: 0:07:19  lr: 0.000167  loss: 1.0829 (1.1257)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [ 60/431]  eta: 0:07:05  lr: 0.000167  loss: 1.0599 (1.1231)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [ 70/431]  eta: 0:06:52  lr: 0.000167  loss: 1.1056 (1.1279)  time: 1.1157  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:249]  [ 80/431]  eta: 0:06:39  lr: 0.000167  loss: 1.1109 (1.1233)  time: 1.1073  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:249]  [ 90/431]  eta: 0:06:26  lr: 0.000167  loss: 1.1109 (1.1227)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [100/431]  eta: 0:06:14  lr: 0.000167  loss: 1.0718 (1.1183)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [110/431]  eta: 0:06:03  lr: 0.000167  loss: 1.0577 (1.1124)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [120/431]  eta: 0:05:50  lr: 0.000167  loss: 1.0524 (1.1100)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [130/431]  eta: 0:05:39  lr: 0.000167  loss: 1.0691 (1.1082)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [140/431]  eta: 0:05:27  lr: 0.000167  loss: 1.0932 (1.1087)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [150/431]  eta: 0:05:15  lr: 0.000167  loss: 1.1142 (1.1105)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [160/431]  eta: 0:05:04  lr: 0.000167  loss: 1.1172 (1.1105)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [170/431]  eta: 0:04:52  lr: 0.000167  loss: 1.0195 (1.1060)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [180/431]  eta: 0:04:41  lr: 0.000167  loss: 1.0360 (1.1050)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [190/431]  eta: 0:04:29  lr: 0.000167  loss: 1.0758 (1.1040)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [200/431]  eta: 0:04:18  lr: 0.000167  loss: 1.0730 (1.1049)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [210/431]  eta: 0:04:07  lr: 0.000167  loss: 1.0562 (1.1043)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [220/431]  eta: 0:03:55  lr: 0.000167  loss: 1.0487 (1.1028)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [230/431]  eta: 0:03:44  lr: 0.000167  loss: 1.0491 (1.1024)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [240/431]  eta: 0:03:33  lr: 0.000167  loss: 1.0519 (1.1022)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [250/431]  eta: 0:03:22  lr: 0.000167  loss: 1.0686 (1.1009)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [260/431]  eta: 0:03:10  lr: 0.000167  loss: 1.0686 (1.0995)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [270/431]  eta: 0:02:59  lr: 0.000167  loss: 1.0994 (1.1021)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [280/431]  eta: 0:02:48  lr: 0.000167  loss: 1.1469 (1.1048)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [290/431]  eta: 0:02:37  lr: 0.000167  loss: 1.0638 (1.1027)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [300/431]  eta: 0:02:26  lr: 0.000167  loss: 1.0532 (1.1028)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [310/431]  eta: 0:02:14  lr: 0.000167  loss: 1.0671 (1.1024)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [320/431]  eta: 0:02:03  lr: 0.000167  loss: 1.0540 (1.1016)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [330/431]  eta: 0:01:52  lr: 0.000167  loss: 1.0649 (1.1038)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [340/431]  eta: 0:01:41  lr: 0.000167  loss: 1.1348 (1.1039)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [350/431]  eta: 0:01:30  lr: 0.000167  loss: 1.1157 (1.1044)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [360/431]  eta: 0:01:19  lr: 0.000167  loss: 1.1373 (1.1050)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [370/431]  eta: 0:01:07  lr: 0.000167  loss: 1.1032 (1.1057)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [380/431]  eta: 0:00:56  lr: 0.000167  loss: 1.0604 (1.1058)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [390/431]  eta: 0:00:45  lr: 0.000167  loss: 1.0646 (1.1045)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [400/431]  eta: 0:00:34  lr: 0.000167  loss: 1.0646 (1.1048)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [410/431]  eta: 0:00:23  lr: 0.000167  loss: 1.1147 (1.1052)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:249]  [420/431]  eta: 0:00:12  lr: 0.000167  loss: 1.1201 (1.1062)  time: 1.1039  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:249]  [430/431]  eta: 0:00:01  lr: 0.000167  loss: 1.1174 (1.1055)  time: 1.1079  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:249] Total time: 0:07:59 (1.1122 s / it)\n",
      "Averaged stats: lr: 0.000167  loss: 1.1174 (1.1055)\n",
      "Valid: [epoch:249]  [ 0/14]  eta: 0:00:36  loss: 0.9968 (0.9968)  time: 2.5894  data: 2.4508  max mem: 15925\n",
      "Valid: [epoch:249]  [13/14]  eta: 0:00:00  loss: 1.0527 (1.0585)  time: 0.2791  data: 0.1751  max mem: 15925\n",
      "Valid: [epoch:249] Total time: 0:00:04 (0.2939 s / it)\n",
      "Averaged stats: loss: 1.0527 (1.0585)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_249_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.059%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:250]  [  0/431]  eta: 0:31:06  lr: 0.000167  loss: 1.1067 (1.1067)  time: 4.3314  data: 3.0217  max mem: 15925\n",
      "Train: [epoch:250]  [ 10/431]  eta: 0:09:26  lr: 0.000167  loss: 1.1336 (1.1930)  time: 1.3446  data: 0.2749  max mem: 15925\n",
      "Train: [epoch:250]  [ 20/431]  eta: 0:08:15  lr: 0.000167  loss: 1.1288 (1.1894)  time: 1.0503  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [ 30/431]  eta: 0:07:45  lr: 0.000167  loss: 1.0506 (1.1561)  time: 1.0589  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [ 40/431]  eta: 0:07:26  lr: 0.000167  loss: 1.0813 (1.1481)  time: 1.0735  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [ 50/431]  eta: 0:07:11  lr: 0.000167  loss: 1.1028 (1.1475)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [ 60/431]  eta: 0:06:59  lr: 0.000167  loss: 1.1209 (1.1422)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [ 70/431]  eta: 0:06:46  lr: 0.000167  loss: 1.0866 (1.1325)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [ 80/431]  eta: 0:06:34  lr: 0.000167  loss: 1.0652 (1.1267)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [ 90/431]  eta: 0:06:22  lr: 0.000167  loss: 1.0583 (1.1241)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [100/431]  eta: 0:06:10  lr: 0.000167  loss: 1.0371 (1.1134)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [110/431]  eta: 0:05:59  lr: 0.000167  loss: 1.0752 (1.1131)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [120/431]  eta: 0:05:48  lr: 0.000167  loss: 1.0846 (1.1120)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [130/431]  eta: 0:05:36  lr: 0.000167  loss: 1.0964 (1.1130)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [140/431]  eta: 0:05:24  lr: 0.000167  loss: 1.0645 (1.1074)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [150/431]  eta: 0:05:13  lr: 0.000167  loss: 1.0645 (1.1094)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [160/431]  eta: 0:05:02  lr: 0.000167  loss: 1.1004 (1.1070)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [170/431]  eta: 0:04:50  lr: 0.000167  loss: 1.0550 (1.1040)  time: 1.1121  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:250]  [180/431]  eta: 0:04:39  lr: 0.000167  loss: 1.0763 (1.1052)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [190/431]  eta: 0:04:28  lr: 0.000167  loss: 1.1226 (1.1073)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [200/431]  eta: 0:04:17  lr: 0.000167  loss: 1.1214 (1.1086)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [210/431]  eta: 0:04:06  lr: 0.000167  loss: 1.1137 (1.1112)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [220/431]  eta: 0:03:54  lr: 0.000167  loss: 1.0905 (1.1117)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [230/431]  eta: 0:03:43  lr: 0.000167  loss: 1.0677 (1.1099)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [240/431]  eta: 0:03:32  lr: 0.000167  loss: 1.1254 (1.1152)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [250/431]  eta: 0:03:21  lr: 0.000167  loss: 1.1877 (1.1146)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [260/431]  eta: 0:03:10  lr: 0.000167  loss: 1.0597 (1.1111)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [270/431]  eta: 0:02:59  lr: 0.000167  loss: 1.0597 (1.1100)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [280/431]  eta: 0:02:48  lr: 0.000167  loss: 1.1127 (1.1108)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [290/431]  eta: 0:02:37  lr: 0.000167  loss: 1.1127 (1.1111)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [300/431]  eta: 0:02:25  lr: 0.000167  loss: 1.0933 (1.1122)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [310/431]  eta: 0:02:14  lr: 0.000167  loss: 1.0896 (1.1113)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [320/431]  eta: 0:02:03  lr: 0.000167  loss: 1.0823 (1.1107)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [330/431]  eta: 0:01:52  lr: 0.000167  loss: 1.0834 (1.1121)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [340/431]  eta: 0:01:41  lr: 0.000167  loss: 1.0794 (1.1105)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [350/431]  eta: 0:01:30  lr: 0.000167  loss: 1.0649 (1.1107)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [360/431]  eta: 0:01:19  lr: 0.000167  loss: 1.0758 (1.1107)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [370/431]  eta: 0:01:07  lr: 0.000167  loss: 1.0947 (1.1097)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [380/431]  eta: 0:00:56  lr: 0.000167  loss: 1.0493 (1.1085)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [390/431]  eta: 0:00:45  lr: 0.000167  loss: 1.0570 (1.1079)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [400/431]  eta: 0:00:34  lr: 0.000167  loss: 1.0442 (1.1062)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [410/431]  eta: 0:00:23  lr: 0.000167  loss: 1.0528 (1.1065)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:250]  [420/431]  eta: 0:00:12  lr: 0.000167  loss: 1.0876 (1.1059)  time: 1.1057  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:250]  [430/431]  eta: 0:00:01  lr: 0.000167  loss: 1.0727 (1.1061)  time: 1.1109  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:250] Total time: 0:07:59 (1.1131 s / it)\n",
      "Averaged stats: lr: 0.000167  loss: 1.0727 (1.1061)\n",
      "Valid: [epoch:250]  [ 0/14]  eta: 0:00:36  loss: 0.9299 (0.9299)  time: 2.5757  data: 2.3953  max mem: 15925\n",
      "Valid: [epoch:250]  [13/14]  eta: 0:00:00  loss: 1.0309 (1.0427)  time: 0.2752  data: 0.1712  max mem: 15925\n",
      "Valid: [epoch:250] Total time: 0:00:04 (0.2935 s / it)\n",
      "Averaged stats: loss: 1.0309 (1.0427)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_250_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:251]  [  0/431]  eta: 0:29:49  lr: 0.000167  loss: 1.4959 (1.4959)  time: 4.1523  data: 2.9094  max mem: 15925\n",
      "Train: [epoch:251]  [ 10/431]  eta: 0:09:21  lr: 0.000167  loss: 1.1919 (1.2008)  time: 1.3348  data: 0.2647  max mem: 15925\n",
      "Train: [epoch:251]  [ 20/431]  eta: 0:08:17  lr: 0.000167  loss: 1.1112 (1.1467)  time: 1.0630  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [ 30/431]  eta: 0:07:46  lr: 0.000167  loss: 1.1211 (1.1519)  time: 1.0706  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [ 40/431]  eta: 0:07:28  lr: 0.000167  loss: 1.1004 (1.1364)  time: 1.0818  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:251]  [ 50/431]  eta: 0:07:13  lr: 0.000167  loss: 1.0199 (1.1182)  time: 1.0952  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:251]  [ 60/431]  eta: 0:07:00  lr: 0.000167  loss: 1.0199 (1.1264)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [ 70/431]  eta: 0:06:49  lr: 0.000167  loss: 1.0497 (1.1168)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [ 80/431]  eta: 0:06:36  lr: 0.000167  loss: 1.0885 (1.1215)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [ 90/431]  eta: 0:06:23  lr: 0.000167  loss: 1.0642 (1.1152)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [100/431]  eta: 0:06:11  lr: 0.000167  loss: 0.9945 (1.1044)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [110/431]  eta: 0:05:59  lr: 0.000167  loss: 0.9930 (1.0982)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [120/431]  eta: 0:05:47  lr: 0.000167  loss: 1.0305 (1.0978)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [130/431]  eta: 0:05:36  lr: 0.000167  loss: 1.0291 (1.0938)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [140/431]  eta: 0:05:25  lr: 0.000167  loss: 1.0627 (1.0987)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [150/431]  eta: 0:05:13  lr: 0.000167  loss: 1.1023 (1.0974)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [160/431]  eta: 0:05:02  lr: 0.000167  loss: 1.0442 (1.0960)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [170/431]  eta: 0:04:51  lr: 0.000167  loss: 1.0212 (1.0910)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [180/431]  eta: 0:04:39  lr: 0.000167  loss: 1.0445 (1.0928)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [190/431]  eta: 0:04:28  lr: 0.000167  loss: 1.1129 (1.0965)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [200/431]  eta: 0:04:17  lr: 0.000167  loss: 1.1302 (1.1016)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [210/431]  eta: 0:04:06  lr: 0.000167  loss: 1.1437 (1.1003)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [220/431]  eta: 0:03:54  lr: 0.000167  loss: 1.0247 (1.0988)  time: 1.1083  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:251]  [230/431]  eta: 0:03:43  lr: 0.000167  loss: 1.0247 (1.1014)  time: 1.1091  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:251]  [240/431]  eta: 0:03:32  lr: 0.000167  loss: 1.1270 (1.1037)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [250/431]  eta: 0:03:21  lr: 0.000167  loss: 1.1270 (1.1061)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [260/431]  eta: 0:03:10  lr: 0.000167  loss: 1.1219 (1.1066)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [270/431]  eta: 0:02:59  lr: 0.000167  loss: 1.1216 (1.1074)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [280/431]  eta: 0:02:48  lr: 0.000167  loss: 1.1094 (1.1075)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [290/431]  eta: 0:02:36  lr: 0.000167  loss: 1.0718 (1.1057)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [300/431]  eta: 0:02:25  lr: 0.000167  loss: 1.0727 (1.1056)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [310/431]  eta: 0:02:14  lr: 0.000167  loss: 1.1165 (1.1054)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [320/431]  eta: 0:02:03  lr: 0.000167  loss: 1.1040 (1.1048)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [330/431]  eta: 0:01:52  lr: 0.000167  loss: 1.0502 (1.1055)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [340/431]  eta: 0:01:41  lr: 0.000167  loss: 1.1160 (1.1070)  time: 1.0995  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:251]  [350/431]  eta: 0:01:30  lr: 0.000167  loss: 1.0665 (1.1071)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [360/431]  eta: 0:01:18  lr: 0.000167  loss: 1.0660 (1.1066)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [370/431]  eta: 0:01:07  lr: 0.000167  loss: 1.0665 (1.1064)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [380/431]  eta: 0:00:56  lr: 0.000167  loss: 1.0732 (1.1070)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [390/431]  eta: 0:00:45  lr: 0.000167  loss: 1.0697 (1.1049)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [400/431]  eta: 0:00:34  lr: 0.000167  loss: 1.0763 (1.1052)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:251]  [410/431]  eta: 0:00:23  lr: 0.000167  loss: 1.1121 (1.1060)  time: 1.1018  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:251]  [420/431]  eta: 0:00:12  lr: 0.000167  loss: 1.0666 (1.1048)  time: 1.1060  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:251]  [430/431]  eta: 0:00:01  lr: 0.000167  loss: 1.0481 (1.1044)  time: 1.1020  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:251] Total time: 0:07:58 (1.1103 s / it)\n",
      "Averaged stats: lr: 0.000167  loss: 1.0481 (1.1044)\n",
      "Valid: [epoch:251]  [ 0/14]  eta: 0:00:35  loss: 0.9312 (0.9312)  time: 2.5305  data: 2.3694  max mem: 15925\n",
      "Valid: [epoch:251]  [13/14]  eta: 0:00:00  loss: 1.0326 (1.0430)  time: 0.2669  data: 0.1693  max mem: 15925\n",
      "Valid: [epoch:251] Total time: 0:00:03 (0.2841 s / it)\n",
      "Averaged stats: loss: 1.0326 (1.0430)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_251_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:252]  [  0/431]  eta: 0:32:13  lr: 0.000166  loss: 1.0519 (1.0519)  time: 4.4854  data: 3.3415  max mem: 15925\n",
      "Train: [epoch:252]  [ 10/431]  eta: 0:09:35  lr: 0.000166  loss: 1.1103 (1.1220)  time: 1.3669  data: 0.3040  max mem: 15925\n",
      "Train: [epoch:252]  [ 20/431]  eta: 0:08:24  lr: 0.000166  loss: 1.0965 (1.1138)  time: 1.0656  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [ 30/431]  eta: 0:07:55  lr: 0.000166  loss: 1.0730 (1.0928)  time: 1.0881  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [ 40/431]  eta: 0:07:34  lr: 0.000166  loss: 1.0208 (1.0952)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [ 50/431]  eta: 0:07:17  lr: 0.000166  loss: 1.0920 (1.0978)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [ 60/431]  eta: 0:07:03  lr: 0.000166  loss: 1.0793 (1.0938)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [ 70/431]  eta: 0:06:49  lr: 0.000166  loss: 1.1028 (1.1029)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [ 80/431]  eta: 0:06:36  lr: 0.000166  loss: 1.1028 (1.1012)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [ 90/431]  eta: 0:06:24  lr: 0.000166  loss: 1.0402 (1.0983)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [100/431]  eta: 0:06:12  lr: 0.000166  loss: 1.0402 (1.1026)  time: 1.1030  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:252]  [110/431]  eta: 0:06:00  lr: 0.000166  loss: 1.0682 (1.1029)  time: 1.1037  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:252]  [120/431]  eta: 0:05:49  lr: 0.000166  loss: 1.0458 (1.1023)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [130/431]  eta: 0:05:37  lr: 0.000166  loss: 1.0591 (1.0989)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [140/431]  eta: 0:05:25  lr: 0.000166  loss: 1.0401 (1.0962)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [150/431]  eta: 0:05:14  lr: 0.000166  loss: 1.0358 (1.0972)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [160/431]  eta: 0:05:03  lr: 0.000166  loss: 1.0912 (1.0956)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [170/431]  eta: 0:04:51  lr: 0.000166  loss: 1.0430 (1.0949)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [180/431]  eta: 0:04:40  lr: 0.000166  loss: 1.0751 (1.0948)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [190/431]  eta: 0:04:29  lr: 0.000166  loss: 1.1077 (1.0969)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [200/431]  eta: 0:04:17  lr: 0.000166  loss: 1.1083 (1.0976)  time: 1.1085  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:252]  [210/431]  eta: 0:04:06  lr: 0.000166  loss: 1.1398 (1.1019)  time: 1.1143  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:252]  [220/431]  eta: 0:03:55  lr: 0.000166  loss: 1.1332 (1.1041)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [230/431]  eta: 0:03:44  lr: 0.000166  loss: 1.1061 (1.1051)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [240/431]  eta: 0:03:32  lr: 0.000166  loss: 1.0722 (1.1057)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [250/431]  eta: 0:03:21  lr: 0.000166  loss: 1.1039 (1.1069)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [260/431]  eta: 0:03:10  lr: 0.000166  loss: 1.1230 (1.1064)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [270/431]  eta: 0:02:59  lr: 0.000166  loss: 1.1020 (1.1070)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [280/431]  eta: 0:02:48  lr: 0.000166  loss: 1.1160 (1.1064)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [290/431]  eta: 0:02:37  lr: 0.000166  loss: 1.0703 (1.1046)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [300/431]  eta: 0:02:25  lr: 0.000166  loss: 1.0901 (1.1069)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [310/431]  eta: 0:02:14  lr: 0.000166  loss: 1.0889 (1.1052)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [320/431]  eta: 0:02:03  lr: 0.000166  loss: 1.0855 (1.1082)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [330/431]  eta: 0:01:52  lr: 0.000166  loss: 1.0990 (1.1077)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [340/431]  eta: 0:01:41  lr: 0.000166  loss: 1.0652 (1.1072)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [350/431]  eta: 0:01:30  lr: 0.000166  loss: 1.0799 (1.1074)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [360/431]  eta: 0:01:18  lr: 0.000166  loss: 1.1197 (1.1063)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [370/431]  eta: 0:01:07  lr: 0.000166  loss: 1.0848 (1.1065)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [380/431]  eta: 0:00:56  lr: 0.000166  loss: 1.0514 (1.1048)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [390/431]  eta: 0:00:45  lr: 0.000166  loss: 1.0514 (1.1055)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [400/431]  eta: 0:00:34  lr: 0.000166  loss: 1.1057 (1.1051)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [410/431]  eta: 0:00:23  lr: 0.000166  loss: 1.0537 (1.1057)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:252]  [420/431]  eta: 0:00:12  lr: 0.000166  loss: 1.1141 (1.1065)  time: 1.0979  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:252]  [430/431]  eta: 0:00:01  lr: 0.000166  loss: 1.0944 (1.1061)  time: 1.1076  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:252] Total time: 0:07:59 (1.1119 s / it)\n",
      "Averaged stats: lr: 0.000166  loss: 1.0944 (1.1061)\n",
      "Valid: [epoch:252]  [ 0/14]  eta: 0:00:36  loss: 0.9815 (0.9815)  time: 2.6402  data: 2.4542  max mem: 15925\n",
      "Valid: [epoch:252]  [13/14]  eta: 0:00:00  loss: 1.0349 (1.0485)  time: 0.3029  data: 0.1754  max mem: 15925\n",
      "Valid: [epoch:252] Total time: 0:00:04 (0.3187 s / it)\n",
      "Averaged stats: loss: 1.0349 (1.0485)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_252_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.048%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:253]  [  0/431]  eta: 0:31:13  lr: 0.000166  loss: 1.2949 (1.2949)  time: 4.3476  data: 3.1522  max mem: 15925\n",
      "Train: [epoch:253]  [ 10/431]  eta: 0:09:25  lr: 0.000166  loss: 1.1201 (1.1315)  time: 1.3440  data: 0.2869  max mem: 15925\n",
      "Train: [epoch:253]  [ 20/431]  eta: 0:08:20  lr: 0.000166  loss: 1.0847 (1.1417)  time: 1.0605  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:253]  [ 30/431]  eta: 0:07:52  lr: 0.000166  loss: 1.0847 (1.1214)  time: 1.0872  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [ 40/431]  eta: 0:07:34  lr: 0.000166  loss: 1.0551 (1.1076)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [ 50/431]  eta: 0:07:17  lr: 0.000166  loss: 1.0220 (1.1009)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [ 60/431]  eta: 0:07:05  lr: 0.000166  loss: 1.0220 (1.0972)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [ 70/431]  eta: 0:06:51  lr: 0.000166  loss: 1.1089 (1.1024)  time: 1.1198  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [ 80/431]  eta: 0:06:39  lr: 0.000166  loss: 1.1445 (1.1163)  time: 1.1164  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [ 90/431]  eta: 0:06:27  lr: 0.000166  loss: 1.1467 (1.1170)  time: 1.1253  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [100/431]  eta: 0:06:16  lr: 0.000166  loss: 0.9888 (1.1082)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [110/431]  eta: 0:06:04  lr: 0.000166  loss: 1.0352 (1.1062)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [120/431]  eta: 0:05:52  lr: 0.000166  loss: 1.0454 (1.1006)  time: 1.1275  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [130/431]  eta: 0:05:42  lr: 0.000166  loss: 1.0433 (1.1007)  time: 1.1367  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [140/431]  eta: 0:05:30  lr: 0.000166  loss: 1.0795 (1.1001)  time: 1.1346  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [150/431]  eta: 0:05:18  lr: 0.000166  loss: 1.0577 (1.0995)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [160/431]  eta: 0:05:06  lr: 0.000166  loss: 1.0520 (1.0992)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [170/431]  eta: 0:04:55  lr: 0.000166  loss: 1.0709 (1.0997)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [180/431]  eta: 0:04:43  lr: 0.000166  loss: 1.0782 (1.0992)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [190/431]  eta: 0:04:31  lr: 0.000166  loss: 1.0623 (1.1016)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [200/431]  eta: 0:04:20  lr: 0.000166  loss: 1.0968 (1.1020)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [210/431]  eta: 0:04:08  lr: 0.000166  loss: 1.1294 (1.1018)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [220/431]  eta: 0:03:57  lr: 0.000166  loss: 1.1324 (1.1021)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [230/431]  eta: 0:03:46  lr: 0.000166  loss: 1.1175 (1.1027)  time: 1.1064  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [240/431]  eta: 0:03:34  lr: 0.000166  loss: 1.0963 (1.1023)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [250/431]  eta: 0:03:23  lr: 0.000166  loss: 1.0822 (1.1020)  time: 1.1116  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [260/431]  eta: 0:03:11  lr: 0.000166  loss: 1.0822 (1.1029)  time: 1.1138  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [270/431]  eta: 0:03:00  lr: 0.000166  loss: 1.1089 (1.1036)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [280/431]  eta: 0:02:49  lr: 0.000166  loss: 1.0609 (1.1028)  time: 1.1188  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [290/431]  eta: 0:02:38  lr: 0.000166  loss: 1.0595 (1.1029)  time: 1.1118  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [300/431]  eta: 0:02:27  lr: 0.000166  loss: 1.0946 (1.1043)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [310/431]  eta: 0:02:15  lr: 0.000166  loss: 1.0989 (1.1063)  time: 1.1296  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [320/431]  eta: 0:02:04  lr: 0.000166  loss: 1.1108 (1.1076)  time: 1.1314  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [330/431]  eta: 0:01:53  lr: 0.000166  loss: 1.1108 (1.1072)  time: 1.1315  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [340/431]  eta: 0:01:42  lr: 0.000166  loss: 1.0670 (1.1072)  time: 1.1143  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [350/431]  eta: 0:01:30  lr: 0.000166  loss: 1.1087 (1.1082)  time: 1.1102  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [360/431]  eta: 0:01:19  lr: 0.000166  loss: 1.0665 (1.1060)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [370/431]  eta: 0:01:08  lr: 0.000166  loss: 1.0273 (1.1052)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [380/431]  eta: 0:00:57  lr: 0.000166  loss: 1.0885 (1.1057)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [390/431]  eta: 0:00:46  lr: 0.000166  loss: 1.0859 (1.1046)  time: 1.1307  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:253]  [400/431]  eta: 0:00:34  lr: 0.000166  loss: 1.0365 (1.1040)  time: 1.1268  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [410/431]  eta: 0:00:23  lr: 0.000166  loss: 1.0563 (1.1036)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [420/431]  eta: 0:00:12  lr: 0.000166  loss: 1.0921 (1.1049)  time: 1.1267  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253]  [430/431]  eta: 0:00:01  lr: 0.000166  loss: 1.0754 (1.1040)  time: 1.1293  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:253] Total time: 0:08:04 (1.1235 s / it)\n",
      "Averaged stats: lr: 0.000166  loss: 1.0754 (1.1040)\n",
      "Valid: [epoch:253]  [ 0/14]  eta: 0:00:35  loss: 0.9626 (0.9626)  time: 2.5481  data: 2.3627  max mem: 15925\n",
      "Valid: [epoch:253]  [13/14]  eta: 0:00:00  loss: 1.0410 (1.0485)  time: 0.2983  data: 0.1689  max mem: 15925\n",
      "Valid: [epoch:253] Total time: 0:00:04 (0.3157 s / it)\n",
      "Averaged stats: loss: 1.0410 (1.0485)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_253_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:254]  [  0/431]  eta: 0:32:36  lr: 0.000166  loss: 1.3259 (1.3259)  time: 4.5393  data: 3.2079  max mem: 15925\n",
      "Train: [epoch:254]  [ 10/431]  eta: 0:09:44  lr: 0.000166  loss: 1.1889 (1.1846)  time: 1.3884  data: 0.2919  max mem: 15925\n",
      "Train: [epoch:254]  [ 20/431]  eta: 0:08:30  lr: 0.000166  loss: 1.1889 (1.1764)  time: 1.0782  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [ 30/431]  eta: 0:08:00  lr: 0.000166  loss: 1.1412 (1.1534)  time: 1.0940  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [ 40/431]  eta: 0:07:39  lr: 0.000166  loss: 1.0965 (1.1402)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [ 50/431]  eta: 0:07:21  lr: 0.000166  loss: 1.0805 (1.1385)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [ 60/431]  eta: 0:07:07  lr: 0.000166  loss: 1.0805 (1.1261)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [ 70/431]  eta: 0:06:53  lr: 0.000166  loss: 1.0782 (1.1209)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [ 80/431]  eta: 0:06:40  lr: 0.000166  loss: 1.0843 (1.1193)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [ 90/431]  eta: 0:06:27  lr: 0.000166  loss: 1.0560 (1.1126)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [100/431]  eta: 0:06:15  lr: 0.000166  loss: 1.0351 (1.1123)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [110/431]  eta: 0:06:03  lr: 0.000166  loss: 1.1266 (1.1186)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [120/431]  eta: 0:05:51  lr: 0.000166  loss: 1.1131 (1.1142)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [130/431]  eta: 0:05:39  lr: 0.000166  loss: 1.0029 (1.1058)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [140/431]  eta: 0:05:28  lr: 0.000166  loss: 1.0194 (1.1049)  time: 1.1272  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [150/431]  eta: 0:05:16  lr: 0.000166  loss: 1.0785 (1.1056)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [160/431]  eta: 0:05:05  lr: 0.000166  loss: 1.0710 (1.1029)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [170/431]  eta: 0:04:54  lr: 0.000166  loss: 1.0355 (1.1008)  time: 1.1359  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [180/431]  eta: 0:04:43  lr: 0.000166  loss: 1.0355 (1.0971)  time: 1.1405  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [190/431]  eta: 0:04:32  lr: 0.000166  loss: 1.0643 (1.0989)  time: 1.1339  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:254]  [200/431]  eta: 0:04:20  lr: 0.000166  loss: 1.0776 (1.0985)  time: 1.1286  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [210/431]  eta: 0:04:09  lr: 0.000166  loss: 1.0718 (1.0976)  time: 1.1326  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [220/431]  eta: 0:03:58  lr: 0.000166  loss: 1.0803 (1.0993)  time: 1.1347  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [230/431]  eta: 0:03:47  lr: 0.000166  loss: 1.0957 (1.0980)  time: 1.1366  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [240/431]  eta: 0:03:35  lr: 0.000166  loss: 1.0867 (1.1007)  time: 1.1358  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [250/431]  eta: 0:03:24  lr: 0.000166  loss: 1.0612 (1.0992)  time: 1.1221  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [260/431]  eta: 0:03:13  lr: 0.000166  loss: 1.0543 (1.0980)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [270/431]  eta: 0:03:01  lr: 0.000166  loss: 1.1066 (1.1003)  time: 1.1373  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [280/431]  eta: 0:02:50  lr: 0.000166  loss: 1.1362 (1.1009)  time: 1.1439  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [290/431]  eta: 0:02:39  lr: 0.000166  loss: 1.1018 (1.1005)  time: 1.1281  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [300/431]  eta: 0:02:28  lr: 0.000166  loss: 1.0567 (1.1021)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [310/431]  eta: 0:02:16  lr: 0.000166  loss: 1.0564 (1.1012)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [320/431]  eta: 0:02:05  lr: 0.000166  loss: 1.0559 (1.1008)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [330/431]  eta: 0:01:54  lr: 0.000166  loss: 1.0599 (1.1017)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [340/431]  eta: 0:01:42  lr: 0.000166  loss: 1.0981 (1.1011)  time: 1.1142  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [350/431]  eta: 0:01:31  lr: 0.000166  loss: 1.0590 (1.0997)  time: 1.1219  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [360/431]  eta: 0:01:20  lr: 0.000166  loss: 1.0288 (1.0998)  time: 1.1241  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [370/431]  eta: 0:01:08  lr: 0.000166  loss: 1.0880 (1.1013)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [380/431]  eta: 0:00:57  lr: 0.000166  loss: 1.1073 (1.1018)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [390/431]  eta: 0:00:46  lr: 0.000166  loss: 1.0695 (1.1020)  time: 1.1253  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [400/431]  eta: 0:00:34  lr: 0.000166  loss: 1.0587 (1.1018)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:254]  [410/431]  eta: 0:00:23  lr: 0.000166  loss: 1.0842 (1.1029)  time: 1.1412  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [420/431]  eta: 0:00:12  lr: 0.000166  loss: 1.1088 (1.1030)  time: 1.1415  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254]  [430/431]  eta: 0:00:01  lr: 0.000166  loss: 1.0762 (1.1033)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:254] Total time: 0:08:06 (1.1287 s / it)\n",
      "Averaged stats: lr: 0.000166  loss: 1.0762 (1.1033)\n",
      "Valid: [epoch:254]  [ 0/14]  eta: 0:00:36  loss: 0.9386 (0.9386)  time: 2.6167  data: 2.4938  max mem: 15925\n",
      "Valid: [epoch:254]  [13/14]  eta: 0:00:00  loss: 1.0404 (1.0500)  time: 0.2825  data: 0.1782  max mem: 15925\n",
      "Valid: [epoch:254] Total time: 0:00:04 (0.3023 s / it)\n",
      "Averaged stats: loss: 1.0404 (1.0500)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_254_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:255]  [  0/431]  eta: 0:36:55  lr: 0.000166  loss: 0.9632 (0.9632)  time: 5.1401  data: 3.7605  max mem: 15925\n",
      "Train: [epoch:255]  [ 10/431]  eta: 0:10:03  lr: 0.000166  loss: 1.0610 (1.1336)  time: 1.4331  data: 0.3421  max mem: 15925\n",
      "Train: [epoch:255]  [ 20/431]  eta: 0:08:43  lr: 0.000166  loss: 1.1439 (1.1441)  time: 1.0796  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:255]  [ 30/431]  eta: 0:08:08  lr: 0.000166  loss: 1.0436 (1.1206)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [ 40/431]  eta: 0:07:44  lr: 0.000166  loss: 1.0937 (1.1272)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [ 50/431]  eta: 0:07:25  lr: 0.000166  loss: 1.0937 (1.1224)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [ 60/431]  eta: 0:07:09  lr: 0.000166  loss: 1.0724 (1.1234)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [ 70/431]  eta: 0:06:55  lr: 0.000166  loss: 1.0604 (1.1155)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [ 80/431]  eta: 0:06:41  lr: 0.000166  loss: 1.0701 (1.1187)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [ 90/431]  eta: 0:06:29  lr: 0.000166  loss: 1.0679 (1.1111)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [100/431]  eta: 0:06:15  lr: 0.000166  loss: 1.0616 (1.1043)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [110/431]  eta: 0:06:04  lr: 0.000166  loss: 1.0675 (1.1011)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [120/431]  eta: 0:05:51  lr: 0.000166  loss: 1.0319 (1.0991)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [130/431]  eta: 0:05:40  lr: 0.000166  loss: 1.0242 (1.0982)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [140/431]  eta: 0:05:29  lr: 0.000166  loss: 1.0724 (1.0993)  time: 1.1291  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [150/431]  eta: 0:05:17  lr: 0.000166  loss: 1.0824 (1.0999)  time: 1.1297  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:255]  [160/431]  eta: 0:05:05  lr: 0.000166  loss: 1.0762 (1.0996)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [170/431]  eta: 0:04:54  lr: 0.000166  loss: 1.0869 (1.0997)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [180/431]  eta: 0:04:42  lr: 0.000166  loss: 1.1077 (1.1011)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [190/431]  eta: 0:04:31  lr: 0.000166  loss: 1.1422 (1.1061)  time: 1.1157  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:255]  [200/431]  eta: 0:04:19  lr: 0.000166  loss: 1.1051 (1.1065)  time: 1.1131  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:255]  [210/431]  eta: 0:04:08  lr: 0.000166  loss: 1.0500 (1.1046)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [220/431]  eta: 0:03:57  lr: 0.000166  loss: 1.0390 (1.1031)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [230/431]  eta: 0:03:45  lr: 0.000166  loss: 1.0858 (1.1053)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [240/431]  eta: 0:03:34  lr: 0.000166  loss: 1.0858 (1.1042)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [250/431]  eta: 0:03:23  lr: 0.000166  loss: 1.0699 (1.1068)  time: 1.1175  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:255]  [260/431]  eta: 0:03:11  lr: 0.000166  loss: 1.1227 (1.1068)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [270/431]  eta: 0:03:00  lr: 0.000166  loss: 1.1244 (1.1076)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [280/431]  eta: 0:02:49  lr: 0.000166  loss: 1.0974 (1.1070)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [290/431]  eta: 0:02:38  lr: 0.000166  loss: 1.0727 (1.1067)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [300/431]  eta: 0:02:26  lr: 0.000166  loss: 1.0727 (1.1079)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [310/431]  eta: 0:02:15  lr: 0.000166  loss: 1.0618 (1.1062)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [320/431]  eta: 0:02:04  lr: 0.000166  loss: 1.0618 (1.1071)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [330/431]  eta: 0:01:53  lr: 0.000166  loss: 1.0692 (1.1067)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [340/431]  eta: 0:01:41  lr: 0.000166  loss: 1.1026 (1.1075)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [350/431]  eta: 0:01:30  lr: 0.000166  loss: 1.1026 (1.1066)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [360/431]  eta: 0:01:19  lr: 0.000166  loss: 1.0789 (1.1072)  time: 1.1110  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:255]  [370/431]  eta: 0:01:08  lr: 0.000166  loss: 1.0789 (1.1066)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [380/431]  eta: 0:00:57  lr: 0.000166  loss: 1.0785 (1.1072)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [390/431]  eta: 0:00:45  lr: 0.000166  loss: 1.0639 (1.1064)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [400/431]  eta: 0:00:34  lr: 0.000166  loss: 1.0911 (1.1068)  time: 1.1288  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:255]  [410/431]  eta: 0:00:23  lr: 0.000166  loss: 1.0593 (1.1049)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [420/431]  eta: 0:00:12  lr: 0.000166  loss: 1.0681 (1.1066)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255]  [430/431]  eta: 0:00:01  lr: 0.000166  loss: 1.1028 (1.1067)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:255] Total time: 0:08:02 (1.1201 s / it)\n",
      "Averaged stats: lr: 0.000166  loss: 1.1028 (1.1067)\n",
      "Valid: [epoch:255]  [ 0/14]  eta: 0:00:36  loss: 1.1384 (1.1384)  time: 2.6363  data: 2.4531  max mem: 15925\n",
      "Valid: [epoch:255]  [13/14]  eta: 0:00:00  loss: 1.0397 (1.0497)  time: 0.2925  data: 0.1753  max mem: 15925\n",
      "Valid: [epoch:255] Total time: 0:00:04 (0.3078 s / it)\n",
      "Averaged stats: loss: 1.0397 (1.0497)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_255_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:256]  [  0/431]  eta: 0:35:55  lr: 0.000166  loss: 1.0407 (1.0407)  time: 5.0020  data: 3.8613  max mem: 15925\n",
      "Train: [epoch:256]  [ 10/431]  eta: 0:09:49  lr: 0.000166  loss: 1.0935 (1.1117)  time: 1.4011  data: 0.3512  max mem: 15925\n",
      "Train: [epoch:256]  [ 20/431]  eta: 0:08:32  lr: 0.000166  loss: 1.0904 (1.1184)  time: 1.0581  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [ 30/431]  eta: 0:07:56  lr: 0.000166  loss: 1.0747 (1.1003)  time: 1.0707  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [ 40/431]  eta: 0:07:35  lr: 0.000166  loss: 1.1062 (1.1088)  time: 1.0785  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [ 50/431]  eta: 0:07:19  lr: 0.000166  loss: 1.0934 (1.1006)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [ 60/431]  eta: 0:07:05  lr: 0.000166  loss: 1.0482 (1.0929)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [ 70/431]  eta: 0:06:51  lr: 0.000166  loss: 1.0100 (1.0880)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [ 80/431]  eta: 0:06:38  lr: 0.000166  loss: 1.0748 (1.0944)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [ 90/431]  eta: 0:06:27  lr: 0.000166  loss: 1.1400 (1.0963)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [100/431]  eta: 0:06:15  lr: 0.000166  loss: 1.1106 (1.0980)  time: 1.1379  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [110/431]  eta: 0:06:04  lr: 0.000166  loss: 1.0924 (1.0991)  time: 1.1345  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [120/431]  eta: 0:05:52  lr: 0.000166  loss: 1.1029 (1.0990)  time: 1.1303  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [130/431]  eta: 0:05:41  lr: 0.000166  loss: 1.0761 (1.0999)  time: 1.1283  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [140/431]  eta: 0:05:30  lr: 0.000166  loss: 1.0593 (1.1014)  time: 1.1327  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [150/431]  eta: 0:05:18  lr: 0.000166  loss: 1.1490 (1.1048)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [160/431]  eta: 0:05:06  lr: 0.000166  loss: 1.1183 (1.1046)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [170/431]  eta: 0:04:55  lr: 0.000166  loss: 1.0839 (1.1030)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [180/431]  eta: 0:04:43  lr: 0.000166  loss: 1.0870 (1.1049)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [190/431]  eta: 0:04:31  lr: 0.000166  loss: 1.1383 (1.1105)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [200/431]  eta: 0:04:20  lr: 0.000166  loss: 1.1213 (1.1089)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [210/431]  eta: 0:04:08  lr: 0.000166  loss: 1.0297 (1.1082)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [220/431]  eta: 0:03:57  lr: 0.000166  loss: 1.0733 (1.1063)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [230/431]  eta: 0:03:46  lr: 0.000166  loss: 1.0846 (1.1070)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [240/431]  eta: 0:03:34  lr: 0.000166  loss: 1.0761 (1.1058)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [250/431]  eta: 0:03:23  lr: 0.000166  loss: 1.0598 (1.1046)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [260/431]  eta: 0:03:12  lr: 0.000166  loss: 1.0586 (1.1045)  time: 1.1229  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:256]  [270/431]  eta: 0:03:00  lr: 0.000166  loss: 1.0905 (1.1042)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [280/431]  eta: 0:02:49  lr: 0.000166  loss: 1.0942 (1.1039)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [290/431]  eta: 0:02:38  lr: 0.000166  loss: 1.0877 (1.1046)  time: 1.1212  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:256]  [300/431]  eta: 0:02:27  lr: 0.000166  loss: 1.0979 (1.1048)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [310/431]  eta: 0:02:15  lr: 0.000166  loss: 1.0979 (1.1050)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [320/431]  eta: 0:02:04  lr: 0.000166  loss: 1.0658 (1.1035)  time: 1.0794  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [330/431]  eta: 0:01:52  lr: 0.000166  loss: 1.0748 (1.1039)  time: 1.0769  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [340/431]  eta: 0:01:41  lr: 0.000166  loss: 1.1109 (1.1045)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [350/431]  eta: 0:01:30  lr: 0.000166  loss: 1.1041 (1.1040)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [360/431]  eta: 0:01:19  lr: 0.000166  loss: 1.1050 (1.1053)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [370/431]  eta: 0:01:08  lr: 0.000166  loss: 1.1050 (1.1045)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [380/431]  eta: 0:00:57  lr: 0.000166  loss: 1.0740 (1.1044)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [390/431]  eta: 0:00:45  lr: 0.000166  loss: 1.0740 (1.1036)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [400/431]  eta: 0:00:34  lr: 0.000166  loss: 1.0926 (1.1030)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [410/431]  eta: 0:00:23  lr: 0.000166  loss: 1.0825 (1.1027)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:256]  [420/431]  eta: 0:00:12  lr: 0.000166  loss: 1.0747 (1.1029)  time: 1.1085  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:256]  [430/431]  eta: 0:00:01  lr: 0.000166  loss: 1.0736 (1.1030)  time: 1.0878  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:256] Total time: 0:08:01 (1.1176 s / it)\n",
      "Averaged stats: lr: 0.000166  loss: 1.0736 (1.1030)\n",
      "Valid: [epoch:256]  [ 0/14]  eta: 0:00:34  loss: 0.9468 (0.9468)  time: 2.4624  data: 2.2984  max mem: 15925\n",
      "Valid: [epoch:256]  [13/14]  eta: 0:00:00  loss: 1.0497 (1.0568)  time: 0.2646  data: 0.1643  max mem: 15925\n",
      "Valid: [epoch:256] Total time: 0:00:03 (0.2790 s / it)\n",
      "Averaged stats: loss: 1.0497 (1.0568)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_256_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.057%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:257]  [  0/431]  eta: 0:32:19  lr: 0.000165  loss: 1.1746 (1.1746)  time: 4.5004  data: 3.2947  max mem: 15925\n",
      "Train: [epoch:257]  [ 10/431]  eta: 0:09:30  lr: 0.000165  loss: 1.0715 (1.1323)  time: 1.3545  data: 0.2997  max mem: 15925\n",
      "Train: [epoch:257]  [ 20/431]  eta: 0:08:21  lr: 0.000165  loss: 1.0474 (1.1004)  time: 1.0554  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [ 30/431]  eta: 0:07:47  lr: 0.000165  loss: 1.0419 (1.1005)  time: 1.0620  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [ 40/431]  eta: 0:07:26  lr: 0.000165  loss: 1.0958 (1.1108)  time: 1.0612  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:257]  [ 50/431]  eta: 0:07:10  lr: 0.000165  loss: 1.0958 (1.1018)  time: 1.0767  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [ 60/431]  eta: 0:06:56  lr: 0.000165  loss: 1.0535 (1.1003)  time: 1.0824  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [ 70/431]  eta: 0:06:44  lr: 0.000165  loss: 1.0556 (1.0970)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [ 80/431]  eta: 0:06:33  lr: 0.000165  loss: 1.0587 (1.1028)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [ 90/431]  eta: 0:06:21  lr: 0.000165  loss: 1.0725 (1.1027)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [100/431]  eta: 0:06:09  lr: 0.000165  loss: 1.0391 (1.0967)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [110/431]  eta: 0:05:58  lr: 0.000165  loss: 1.0222 (1.0961)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [120/431]  eta: 0:05:47  lr: 0.000165  loss: 1.0421 (1.0958)  time: 1.1280  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [130/431]  eta: 0:05:37  lr: 0.000165  loss: 1.0634 (1.0965)  time: 1.1323  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [140/431]  eta: 0:05:25  lr: 0.000165  loss: 1.0592 (1.0976)  time: 1.1280  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [150/431]  eta: 0:05:14  lr: 0.000165  loss: 1.0762 (1.1005)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [160/431]  eta: 0:05:03  lr: 0.000165  loss: 1.0732 (1.0988)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [170/431]  eta: 0:04:51  lr: 0.000165  loss: 1.0551 (1.0969)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [180/431]  eta: 0:04:39  lr: 0.000165  loss: 1.0526 (1.0989)  time: 1.0834  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [190/431]  eta: 0:04:28  lr: 0.000165  loss: 1.1251 (1.1034)  time: 1.0851  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [200/431]  eta: 0:04:17  lr: 0.000165  loss: 1.1374 (1.1053)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [210/431]  eta: 0:04:05  lr: 0.000165  loss: 1.0900 (1.1061)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [220/431]  eta: 0:03:55  lr: 0.000165  loss: 1.0535 (1.1033)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [230/431]  eta: 0:03:43  lr: 0.000165  loss: 1.0417 (1.1015)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [240/431]  eta: 0:03:32  lr: 0.000165  loss: 1.0914 (1.1042)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [250/431]  eta: 0:03:21  lr: 0.000165  loss: 1.0918 (1.1044)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [260/431]  eta: 0:03:10  lr: 0.000165  loss: 1.0747 (1.1061)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [270/431]  eta: 0:02:59  lr: 0.000165  loss: 1.1130 (1.1081)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [280/431]  eta: 0:02:48  lr: 0.000165  loss: 1.1130 (1.1074)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [290/431]  eta: 0:02:36  lr: 0.000165  loss: 1.1111 (1.1086)  time: 1.1141  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [300/431]  eta: 0:02:25  lr: 0.000165  loss: 1.1056 (1.1081)  time: 1.1097  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [310/431]  eta: 0:02:14  lr: 0.000165  loss: 1.0391 (1.1076)  time: 1.1102  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [320/431]  eta: 0:02:03  lr: 0.000165  loss: 1.0651 (1.1063)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [330/431]  eta: 0:01:52  lr: 0.000165  loss: 1.0249 (1.1042)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [340/431]  eta: 0:01:41  lr: 0.000165  loss: 1.0139 (1.1031)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [350/431]  eta: 0:01:30  lr: 0.000165  loss: 1.0425 (1.1025)  time: 1.1229  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [360/431]  eta: 0:01:19  lr: 0.000165  loss: 1.1073 (1.1041)  time: 1.1221  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:257]  [370/431]  eta: 0:01:07  lr: 0.000165  loss: 1.1284 (1.1038)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [380/431]  eta: 0:00:56  lr: 0.000165  loss: 1.0737 (1.1037)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [390/431]  eta: 0:00:45  lr: 0.000165  loss: 1.1129 (1.1056)  time: 1.0890  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [400/431]  eta: 0:00:34  lr: 0.000165  loss: 1.1397 (1.1073)  time: 1.0806  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [410/431]  eta: 0:00:23  lr: 0.000165  loss: 1.0896 (1.1068)  time: 1.0845  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257]  [420/431]  eta: 0:00:12  lr: 0.000165  loss: 1.0811 (1.1069)  time: 1.1046  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:257]  [430/431]  eta: 0:00:01  lr: 0.000165  loss: 1.1061 (1.1071)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:257] Total time: 0:07:59 (1.1115 s / it)\n",
      "Averaged stats: lr: 0.000165  loss: 1.1061 (1.1071)\n",
      "Valid: [epoch:257]  [ 0/14]  eta: 0:00:37  loss: 1.0407 (1.0407)  time: 2.6632  data: 2.5230  max mem: 15925\n",
      "Valid: [epoch:257]  [13/14]  eta: 0:00:00  loss: 1.0407 (1.0495)  time: 0.2859  data: 0.1803  max mem: 15925\n",
      "Valid: [epoch:257] Total time: 0:00:04 (0.3036 s / it)\n",
      "Averaged stats: loss: 1.0407 (1.0495)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_257_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:258]  [  0/431]  eta: 0:36:33  lr: 0.000165  loss: 1.0705 (1.0705)  time: 5.0903  data: 3.8715  max mem: 15925\n",
      "Train: [epoch:258]  [ 10/431]  eta: 0:09:55  lr: 0.000165  loss: 1.1544 (1.1795)  time: 1.4141  data: 0.3522  max mem: 15925\n",
      "Train: [epoch:258]  [ 20/431]  eta: 0:08:33  lr: 0.000165  loss: 1.1383 (1.1532)  time: 1.0580  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:258]  [ 30/431]  eta: 0:07:54  lr: 0.000165  loss: 1.1048 (1.1463)  time: 1.0554  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:258]  [ 40/431]  eta: 0:07:31  lr: 0.000165  loss: 1.0791 (1.1264)  time: 1.0559  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [ 50/431]  eta: 0:07:16  lr: 0.000165  loss: 1.0273 (1.1091)  time: 1.0850  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [ 60/431]  eta: 0:07:02  lr: 0.000165  loss: 1.0583 (1.1095)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [ 70/431]  eta: 0:06:47  lr: 0.000165  loss: 1.0851 (1.1115)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [ 80/431]  eta: 0:06:35  lr: 0.000165  loss: 1.1106 (1.1111)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [ 90/431]  eta: 0:06:23  lr: 0.000165  loss: 1.0932 (1.1089)  time: 1.1077  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:258]  [100/431]  eta: 0:06:12  lr: 0.000165  loss: 1.0932 (1.1104)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [110/431]  eta: 0:06:00  lr: 0.000165  loss: 1.0636 (1.1076)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [120/431]  eta: 0:05:49  lr: 0.000165  loss: 1.0282 (1.1076)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [130/431]  eta: 0:05:37  lr: 0.000165  loss: 1.0620 (1.1051)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [140/431]  eta: 0:05:26  lr: 0.000165  loss: 1.0628 (1.1037)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [150/431]  eta: 0:05:14  lr: 0.000165  loss: 1.0963 (1.1043)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [160/431]  eta: 0:05:03  lr: 0.000165  loss: 1.0689 (1.1036)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [170/431]  eta: 0:04:52  lr: 0.000165  loss: 1.0689 (1.1051)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [180/431]  eta: 0:04:40  lr: 0.000165  loss: 1.0524 (1.1023)  time: 1.1194  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:258]  [190/431]  eta: 0:04:29  lr: 0.000165  loss: 1.0966 (1.1052)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [200/431]  eta: 0:04:18  lr: 0.000165  loss: 1.1034 (1.1050)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [210/431]  eta: 0:04:07  lr: 0.000165  loss: 1.0457 (1.1043)  time: 1.1153  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:258]  [220/431]  eta: 0:03:55  lr: 0.000165  loss: 1.0750 (1.1025)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [230/431]  eta: 0:03:44  lr: 0.000165  loss: 1.1036 (1.1068)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [240/431]  eta: 0:03:33  lr: 0.000165  loss: 1.1145 (1.1049)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [250/431]  eta: 0:03:22  lr: 0.000165  loss: 1.0765 (1.1073)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [260/431]  eta: 0:03:10  lr: 0.000165  loss: 1.1490 (1.1093)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [270/431]  eta: 0:02:59  lr: 0.000165  loss: 1.1685 (1.1112)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [280/431]  eta: 0:02:48  lr: 0.000165  loss: 1.0753 (1.1108)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [290/431]  eta: 0:02:37  lr: 0.000165  loss: 1.0798 (1.1103)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [300/431]  eta: 0:02:26  lr: 0.000165  loss: 1.0770 (1.1103)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [310/431]  eta: 0:02:15  lr: 0.000165  loss: 1.0255 (1.1070)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [320/431]  eta: 0:02:03  lr: 0.000165  loss: 1.0488 (1.1070)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [330/431]  eta: 0:01:52  lr: 0.000165  loss: 1.0751 (1.1073)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [340/431]  eta: 0:01:41  lr: 0.000165  loss: 1.0751 (1.1060)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [350/431]  eta: 0:01:30  lr: 0.000165  loss: 1.0573 (1.1055)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [360/431]  eta: 0:01:19  lr: 0.000165  loss: 1.1099 (1.1076)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [370/431]  eta: 0:01:07  lr: 0.000165  loss: 1.1099 (1.1066)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [380/431]  eta: 0:00:56  lr: 0.000165  loss: 1.1075 (1.1074)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [390/431]  eta: 0:00:45  lr: 0.000165  loss: 1.1241 (1.1075)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [400/431]  eta: 0:00:34  lr: 0.000165  loss: 1.0612 (1.1064)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [410/431]  eta: 0:00:23  lr: 0.000165  loss: 1.0612 (1.1056)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:258]  [420/431]  eta: 0:00:12  lr: 0.000165  loss: 1.0338 (1.1041)  time: 1.0986  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:258]  [430/431]  eta: 0:00:01  lr: 0.000165  loss: 1.0300 (1.1035)  time: 1.0945  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:258] Total time: 0:07:59 (1.1123 s / it)\n",
      "Averaged stats: lr: 0.000165  loss: 1.0300 (1.1035)\n",
      "Valid: [epoch:258]  [ 0/14]  eta: 0:00:37  loss: 1.0068 (1.0068)  time: 2.6659  data: 2.4963  max mem: 15925\n",
      "Valid: [epoch:258]  [13/14]  eta: 0:00:00  loss: 1.0361 (1.0443)  time: 0.2809  data: 0.1784  max mem: 15925\n",
      "Valid: [epoch:258] Total time: 0:00:04 (0.2975 s / it)\n",
      "Averaged stats: loss: 1.0361 (1.0443)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_258_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:259]  [  0/431]  eta: 0:35:24  lr: 0.000165  loss: 1.0939 (1.0939)  time: 4.9282  data: 3.7512  max mem: 15925\n",
      "Train: [epoch:259]  [ 10/431]  eta: 0:09:42  lr: 0.000165  loss: 1.0982 (1.1126)  time: 1.3844  data: 0.3412  max mem: 15925\n",
      "Train: [epoch:259]  [ 20/431]  eta: 0:08:27  lr: 0.000165  loss: 1.0982 (1.0969)  time: 1.0501  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [ 30/431]  eta: 0:07:51  lr: 0.000165  loss: 1.1008 (1.1084)  time: 1.0598  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [ 40/431]  eta: 0:07:29  lr: 0.000165  loss: 1.0737 (1.1062)  time: 1.0616  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [ 50/431]  eta: 0:07:12  lr: 0.000165  loss: 1.0549 (1.1056)  time: 1.0736  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [ 60/431]  eta: 0:06:59  lr: 0.000165  loss: 1.0916 (1.0995)  time: 1.0875  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [ 70/431]  eta: 0:06:45  lr: 0.000165  loss: 1.0844 (1.1056)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [ 80/431]  eta: 0:06:33  lr: 0.000165  loss: 1.0844 (1.1095)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [ 90/431]  eta: 0:06:21  lr: 0.000165  loss: 1.0393 (1.1019)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [100/431]  eta: 0:06:08  lr: 0.000165  loss: 1.0386 (1.0988)  time: 1.0842  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [110/431]  eta: 0:05:56  lr: 0.000165  loss: 1.0825 (1.1005)  time: 1.0705  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [120/431]  eta: 0:05:45  lr: 0.000165  loss: 1.0922 (1.0998)  time: 1.0822  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [130/431]  eta: 0:05:33  lr: 0.000165  loss: 1.0847 (1.0997)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [140/431]  eta: 0:05:21  lr: 0.000165  loss: 1.0847 (1.0987)  time: 1.0831  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [150/431]  eta: 0:05:10  lr: 0.000165  loss: 1.0824 (1.1000)  time: 1.0801  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [160/431]  eta: 0:04:59  lr: 0.000165  loss: 1.1261 (1.1026)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [170/431]  eta: 0:04:48  lr: 0.000165  loss: 1.0661 (1.1002)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [180/431]  eta: 0:04:36  lr: 0.000165  loss: 1.0661 (1.1011)  time: 1.0785  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [190/431]  eta: 0:04:25  lr: 0.000165  loss: 1.1062 (1.1039)  time: 1.0701  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [200/431]  eta: 0:04:13  lr: 0.000165  loss: 1.1226 (1.1050)  time: 1.0743  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [210/431]  eta: 0:04:02  lr: 0.000165  loss: 1.0843 (1.1044)  time: 1.0775  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [220/431]  eta: 0:03:51  lr: 0.000165  loss: 1.0592 (1.1032)  time: 1.0738  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [230/431]  eta: 0:03:40  lr: 0.000165  loss: 1.0615 (1.1050)  time: 1.0794  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [240/431]  eta: 0:03:29  lr: 0.000165  loss: 1.1633 (1.1063)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [250/431]  eta: 0:03:18  lr: 0.000165  loss: 1.0963 (1.1056)  time: 1.0825  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [260/431]  eta: 0:03:07  lr: 0.000165  loss: 1.0960 (1.1049)  time: 1.0806  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [270/431]  eta: 0:02:56  lr: 0.000165  loss: 1.0923 (1.1060)  time: 1.0799  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [280/431]  eta: 0:02:45  lr: 0.000165  loss: 1.0794 (1.1055)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [290/431]  eta: 0:02:34  lr: 0.000165  loss: 1.0385 (1.1026)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [300/431]  eta: 0:02:23  lr: 0.000165  loss: 1.0571 (1.1050)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [310/431]  eta: 0:02:12  lr: 0.000165  loss: 1.1201 (1.1047)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [320/431]  eta: 0:02:01  lr: 0.000165  loss: 1.0917 (1.1049)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [330/431]  eta: 0:01:50  lr: 0.000165  loss: 1.0738 (1.1048)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [340/431]  eta: 0:01:39  lr: 0.000165  loss: 1.0470 (1.1049)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [350/431]  eta: 0:01:28  lr: 0.000165  loss: 1.0974 (1.1062)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [360/431]  eta: 0:01:17  lr: 0.000165  loss: 1.1023 (1.1056)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [370/431]  eta: 0:01:06  lr: 0.000165  loss: 1.0410 (1.1058)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [380/431]  eta: 0:00:56  lr: 0.000165  loss: 1.0560 (1.1049)  time: 1.1012  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:259]  [390/431]  eta: 0:00:45  lr: 0.000165  loss: 1.0172 (1.1031)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [400/431]  eta: 0:00:34  lr: 0.000165  loss: 1.0307 (1.1033)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [410/431]  eta: 0:00:23  lr: 0.000165  loss: 1.0983 (1.1039)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:259]  [420/431]  eta: 0:00:12  lr: 0.000165  loss: 1.0987 (1.1046)  time: 1.0988  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:259]  [430/431]  eta: 0:00:01  lr: 0.000165  loss: 1.1092 (1.1050)  time: 1.1074  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:259] Total time: 0:07:54 (1.1003 s / it)\n",
      "Averaged stats: lr: 0.000165  loss: 1.1092 (1.1050)\n",
      "Valid: [epoch:259]  [ 0/14]  eta: 0:00:36  loss: 1.1376 (1.1376)  time: 2.5814  data: 2.4332  max mem: 15925\n",
      "Valid: [epoch:259]  [13/14]  eta: 0:00:00  loss: 1.0411 (1.0498)  time: 0.2801  data: 0.1739  max mem: 15925\n",
      "Valid: [epoch:259] Total time: 0:00:04 (0.2974 s / it)\n",
      "Averaged stats: loss: 1.0411 (1.0498)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_259_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:260]  [  0/431]  eta: 0:30:19  lr: 0.000165  loss: 1.1980 (1.1980)  time: 4.2206  data: 2.8758  max mem: 15925\n",
      "Train: [epoch:260]  [ 10/431]  eta: 0:09:21  lr: 0.000165  loss: 1.1604 (1.1716)  time: 1.3327  data: 0.2616  max mem: 15925\n",
      "Train: [epoch:260]  [ 20/431]  eta: 0:08:15  lr: 0.000165  loss: 1.0907 (1.1351)  time: 1.0538  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [ 30/431]  eta: 0:07:46  lr: 0.000165  loss: 1.0907 (1.1426)  time: 1.0720  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [ 40/431]  eta: 0:07:29  lr: 0.000165  loss: 1.1157 (1.1329)  time: 1.0901  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [ 50/431]  eta: 0:07:13  lr: 0.000165  loss: 1.0618 (1.1187)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [ 60/431]  eta: 0:07:00  lr: 0.000165  loss: 1.0595 (1.1088)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [ 70/431]  eta: 0:06:46  lr: 0.000165  loss: 1.0811 (1.1075)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [ 80/431]  eta: 0:06:34  lr: 0.000165  loss: 1.0750 (1.1055)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [ 90/431]  eta: 0:06:22  lr: 0.000165  loss: 1.0477 (1.1021)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [100/431]  eta: 0:06:10  lr: 0.000165  loss: 1.0379 (1.1007)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [110/431]  eta: 0:05:58  lr: 0.000165  loss: 1.0379 (1.0986)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [120/431]  eta: 0:05:47  lr: 0.000165  loss: 1.0232 (1.0928)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [130/431]  eta: 0:05:35  lr: 0.000165  loss: 1.0445 (1.0911)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [140/431]  eta: 0:05:24  lr: 0.000165  loss: 1.1307 (1.0928)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [150/431]  eta: 0:05:12  lr: 0.000165  loss: 1.0804 (1.0932)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [160/431]  eta: 0:05:01  lr: 0.000165  loss: 1.0706 (1.0932)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [170/431]  eta: 0:04:50  lr: 0.000165  loss: 1.0706 (1.0916)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [180/431]  eta: 0:04:39  lr: 0.000165  loss: 1.0388 (1.0920)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [190/431]  eta: 0:04:28  lr: 0.000165  loss: 1.0666 (1.0925)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [200/431]  eta: 0:04:17  lr: 0.000165  loss: 1.0767 (1.0937)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [210/431]  eta: 0:04:06  lr: 0.000165  loss: 1.0687 (1.0924)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [220/431]  eta: 0:03:55  lr: 0.000165  loss: 1.0357 (1.0897)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [230/431]  eta: 0:03:43  lr: 0.000165  loss: 1.0580 (1.0916)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [240/431]  eta: 0:03:32  lr: 0.000165  loss: 1.0794 (1.0915)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [250/431]  eta: 0:03:21  lr: 0.000165  loss: 1.1302 (1.0962)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [260/431]  eta: 0:03:10  lr: 0.000165  loss: 1.0928 (1.0954)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [270/431]  eta: 0:02:59  lr: 0.000165  loss: 1.0928 (1.0997)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [280/431]  eta: 0:02:48  lr: 0.000165  loss: 1.1747 (1.1000)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [290/431]  eta: 0:02:37  lr: 0.000165  loss: 1.0752 (1.0992)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [300/431]  eta: 0:02:25  lr: 0.000165  loss: 1.0498 (1.1004)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [310/431]  eta: 0:02:14  lr: 0.000165  loss: 1.0763 (1.1013)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [320/431]  eta: 0:02:03  lr: 0.000165  loss: 1.0420 (1.1007)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [330/431]  eta: 0:01:52  lr: 0.000165  loss: 1.0944 (1.1017)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [340/431]  eta: 0:01:41  lr: 0.000165  loss: 1.1300 (1.1030)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [350/431]  eta: 0:01:30  lr: 0.000165  loss: 1.1410 (1.1042)  time: 1.1125  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:260]  [360/431]  eta: 0:01:19  lr: 0.000165  loss: 1.1646 (1.1065)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [370/431]  eta: 0:01:07  lr: 0.000165  loss: 1.1107 (1.1062)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [380/431]  eta: 0:00:56  lr: 0.000165  loss: 1.0285 (1.1059)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [390/431]  eta: 0:00:45  lr: 0.000165  loss: 1.0721 (1.1058)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [400/431]  eta: 0:00:34  lr: 0.000165  loss: 1.0356 (1.1045)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [410/431]  eta: 0:00:23  lr: 0.000165  loss: 1.0855 (1.1060)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:260]  [420/431]  eta: 0:00:12  lr: 0.000165  loss: 1.0855 (1.1050)  time: 1.1160  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:260]  [430/431]  eta: 0:00:01  lr: 0.000165  loss: 1.0620 (1.1042)  time: 1.1210  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:260] Total time: 0:08:00 (1.1155 s / it)\n",
      "Averaged stats: lr: 0.000165  loss: 1.0620 (1.1042)\n",
      "Valid: [epoch:260]  [ 0/14]  eta: 0:00:37  loss: 1.0853 (1.0853)  time: 2.6831  data: 2.5133  max mem: 15925\n",
      "Valid: [epoch:260]  [13/14]  eta: 0:00:00  loss: 1.0338 (1.0430)  time: 0.3178  data: 0.1796  max mem: 15925\n",
      "Valid: [epoch:260] Total time: 0:00:04 (0.3337 s / it)\n",
      "Averaged stats: loss: 1.0338 (1.0430)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_260_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:261]  [  0/431]  eta: 0:34:00  lr: 0.000164  loss: 1.0647 (1.0647)  time: 4.7334  data: 3.4480  max mem: 15925\n",
      "Train: [epoch:261]  [ 10/431]  eta: 0:09:45  lr: 0.000164  loss: 1.1116 (1.1208)  time: 1.3905  data: 0.3137  max mem: 15925\n",
      "Train: [epoch:261]  [ 20/431]  eta: 0:08:29  lr: 0.000164  loss: 1.1019 (1.1361)  time: 1.0639  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [ 30/431]  eta: 0:07:56  lr: 0.000164  loss: 1.1013 (1.1367)  time: 1.0787  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [ 40/431]  eta: 0:07:34  lr: 0.000164  loss: 1.0713 (1.1218)  time: 1.0822  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [ 50/431]  eta: 0:07:19  lr: 0.000164  loss: 1.0616 (1.1110)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [ 60/431]  eta: 0:07:04  lr: 0.000164  loss: 1.0937 (1.1171)  time: 1.1071  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:261]  [ 70/431]  eta: 0:06:51  lr: 0.000164  loss: 1.0824 (1.1125)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [ 80/431]  eta: 0:06:39  lr: 0.000164  loss: 1.0895 (1.1117)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [ 90/431]  eta: 0:06:27  lr: 0.000164  loss: 1.0895 (1.1080)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [100/431]  eta: 0:06:16  lr: 0.000164  loss: 1.0509 (1.1079)  time: 1.1323  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [110/431]  eta: 0:06:05  lr: 0.000164  loss: 1.0509 (1.1050)  time: 1.1451  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:261]  [120/431]  eta: 0:05:54  lr: 0.000164  loss: 1.0836 (1.1061)  time: 1.1407  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:261]  [130/431]  eta: 0:05:42  lr: 0.000164  loss: 1.0995 (1.1038)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:261]  [140/431]  eta: 0:05:30  lr: 0.000164  loss: 1.0919 (1.1024)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [150/431]  eta: 0:05:18  lr: 0.000164  loss: 1.0571 (1.1022)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [160/431]  eta: 0:05:07  lr: 0.000164  loss: 1.0837 (1.1026)  time: 1.1274  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [170/431]  eta: 0:04:56  lr: 0.000164  loss: 1.1399 (1.1081)  time: 1.1313  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [180/431]  eta: 0:04:44  lr: 0.000164  loss: 1.1399 (1.1085)  time: 1.1323  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:261]  [190/431]  eta: 0:04:33  lr: 0.000164  loss: 1.0920 (1.1102)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [200/431]  eta: 0:04:21  lr: 0.000164  loss: 1.1673 (1.1147)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [210/431]  eta: 0:04:10  lr: 0.000164  loss: 1.1239 (1.1122)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [220/431]  eta: 0:03:58  lr: 0.000164  loss: 1.0408 (1.1117)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [230/431]  eta: 0:03:47  lr: 0.000164  loss: 1.0850 (1.1122)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [240/431]  eta: 0:03:35  lr: 0.000164  loss: 1.0996 (1.1114)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [250/431]  eta: 0:03:24  lr: 0.000164  loss: 1.0885 (1.1125)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [260/431]  eta: 0:03:12  lr: 0.000164  loss: 1.1115 (1.1146)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [270/431]  eta: 0:03:01  lr: 0.000164  loss: 1.0914 (1.1123)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [280/431]  eta: 0:02:50  lr: 0.000164  loss: 1.0784 (1.1120)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [290/431]  eta: 0:02:38  lr: 0.000164  loss: 1.0784 (1.1090)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [300/431]  eta: 0:02:27  lr: 0.000164  loss: 1.0824 (1.1111)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [310/431]  eta: 0:02:16  lr: 0.000164  loss: 1.1139 (1.1105)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [320/431]  eta: 0:02:04  lr: 0.000164  loss: 1.0758 (1.1102)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [330/431]  eta: 0:01:53  lr: 0.000164  loss: 1.0750 (1.1100)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [340/431]  eta: 0:01:42  lr: 0.000164  loss: 1.1003 (1.1106)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [350/431]  eta: 0:01:30  lr: 0.000164  loss: 1.0388 (1.1090)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [360/431]  eta: 0:01:19  lr: 0.000164  loss: 1.0121 (1.1074)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [370/431]  eta: 0:01:08  lr: 0.000164  loss: 1.1415 (1.1097)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [380/431]  eta: 0:00:57  lr: 0.000164  loss: 1.1232 (1.1092)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [390/431]  eta: 0:00:46  lr: 0.000164  loss: 1.0210 (1.1072)  time: 1.1136  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:261]  [400/431]  eta: 0:00:34  lr: 0.000164  loss: 1.0242 (1.1063)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [410/431]  eta: 0:00:23  lr: 0.000164  loss: 1.0553 (1.1058)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:261]  [420/431]  eta: 0:00:12  lr: 0.000164  loss: 1.0495 (1.1055)  time: 1.1016  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:261]  [430/431]  eta: 0:00:01  lr: 0.000164  loss: 1.0495 (1.1060)  time: 1.1066  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:261] Total time: 0:08:03 (1.1218 s / it)\n",
      "Averaged stats: lr: 0.000164  loss: 1.0495 (1.1060)\n",
      "Valid: [epoch:261]  [ 0/14]  eta: 0:00:34  loss: 1.0917 (1.0917)  time: 2.4776  data: 2.3083  max mem: 15925\n",
      "Valid: [epoch:261]  [13/14]  eta: 0:00:00  loss: 1.0357 (1.0469)  time: 0.2618  data: 0.1650  max mem: 15925\n",
      "Valid: [epoch:261] Total time: 0:00:03 (0.2759 s / it)\n",
      "Averaged stats: loss: 1.0357 (1.0469)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_261_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:262]  [  0/431]  eta: 0:36:53  lr: 0.000164  loss: 1.0601 (1.0601)  time: 5.1348  data: 3.8225  max mem: 15925\n",
      "Train: [epoch:262]  [ 10/431]  eta: 0:09:54  lr: 0.000164  loss: 1.1230 (1.1223)  time: 1.4112  data: 0.3477  max mem: 15925\n",
      "Train: [epoch:262]  [ 20/431]  eta: 0:08:34  lr: 0.000164  loss: 1.0706 (1.0966)  time: 1.0569  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [ 30/431]  eta: 0:08:04  lr: 0.000164  loss: 1.0490 (1.0809)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [ 40/431]  eta: 0:07:42  lr: 0.000164  loss: 1.0147 (1.0829)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [ 50/431]  eta: 0:07:23  lr: 0.000164  loss: 1.0991 (1.1007)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [ 60/431]  eta: 0:07:08  lr: 0.000164  loss: 1.0299 (1.0900)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [ 70/431]  eta: 0:06:55  lr: 0.000164  loss: 1.0634 (1.0997)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [ 80/431]  eta: 0:06:42  lr: 0.000164  loss: 1.1267 (1.0984)  time: 1.1260  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [ 90/431]  eta: 0:06:29  lr: 0.000164  loss: 1.0944 (1.0985)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [100/431]  eta: 0:06:16  lr: 0.000164  loss: 1.0575 (1.0942)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [110/431]  eta: 0:06:05  lr: 0.000164  loss: 1.0575 (1.0917)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [120/431]  eta: 0:05:53  lr: 0.000164  loss: 1.0568 (1.0924)  time: 1.1290  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:262]  [130/431]  eta: 0:05:41  lr: 0.000164  loss: 1.0978 (1.0934)  time: 1.1192  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:262]  [140/431]  eta: 0:05:29  lr: 0.000164  loss: 1.0978 (1.0942)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [150/431]  eta: 0:05:18  lr: 0.000164  loss: 1.0730 (1.0954)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [160/431]  eta: 0:05:07  lr: 0.000164  loss: 1.0667 (1.0929)  time: 1.1326  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [170/431]  eta: 0:04:55  lr: 0.000164  loss: 1.0869 (1.0958)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [180/431]  eta: 0:04:43  lr: 0.000164  loss: 1.1412 (1.0990)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [190/431]  eta: 0:04:32  lr: 0.000164  loss: 1.1223 (1.0998)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [200/431]  eta: 0:04:20  lr: 0.000164  loss: 1.0650 (1.0973)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [210/431]  eta: 0:04:09  lr: 0.000164  loss: 1.0573 (1.0972)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [220/431]  eta: 0:03:57  lr: 0.000164  loss: 1.0513 (1.0962)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [230/431]  eta: 0:03:46  lr: 0.000164  loss: 1.0358 (1.0944)  time: 1.1033  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:262]  [240/431]  eta: 0:03:34  lr: 0.000164  loss: 1.0737 (1.0943)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [250/431]  eta: 0:03:23  lr: 0.000164  loss: 1.1293 (1.0990)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [260/431]  eta: 0:03:11  lr: 0.000164  loss: 1.1851 (1.1001)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [270/431]  eta: 0:03:00  lr: 0.000164  loss: 1.1122 (1.1011)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [280/431]  eta: 0:02:49  lr: 0.000164  loss: 1.0956 (1.0998)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [290/431]  eta: 0:02:38  lr: 0.000164  loss: 1.0889 (1.1005)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [300/431]  eta: 0:02:26  lr: 0.000164  loss: 1.0998 (1.0999)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [310/431]  eta: 0:02:15  lr: 0.000164  loss: 1.0630 (1.0984)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [320/431]  eta: 0:02:04  lr: 0.000164  loss: 1.0705 (1.0990)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [330/431]  eta: 0:01:53  lr: 0.000164  loss: 1.1153 (1.0987)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [340/431]  eta: 0:01:41  lr: 0.000164  loss: 1.0715 (1.1004)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [350/431]  eta: 0:01:30  lr: 0.000164  loss: 1.0728 (1.0998)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [360/431]  eta: 0:01:19  lr: 0.000164  loss: 1.0789 (1.0999)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [370/431]  eta: 0:01:08  lr: 0.000164  loss: 1.0614 (1.0999)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [380/431]  eta: 0:00:57  lr: 0.000164  loss: 1.0269 (1.0993)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [390/431]  eta: 0:00:45  lr: 0.000164  loss: 1.0789 (1.0992)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [400/431]  eta: 0:00:34  lr: 0.000164  loss: 1.1334 (1.1012)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [410/431]  eta: 0:00:23  lr: 0.000164  loss: 1.1353 (1.1042)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:262]  [420/431]  eta: 0:00:12  lr: 0.000164  loss: 1.1197 (1.1042)  time: 1.1012  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:262]  [430/431]  eta: 0:00:01  lr: 0.000164  loss: 1.1197 (1.1038)  time: 1.1111  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:262] Total time: 0:08:01 (1.1176 s / it)\n",
      "Averaged stats: lr: 0.000164  loss: 1.1197 (1.1038)\n",
      "Valid: [epoch:262]  [ 0/14]  eta: 0:00:35  loss: 1.1345 (1.1345)  time: 2.5625  data: 2.4097  max mem: 15925\n",
      "Valid: [epoch:262]  [13/14]  eta: 0:00:00  loss: 1.0418 (1.0489)  time: 0.2920  data: 0.1722  max mem: 15925\n",
      "Valid: [epoch:262] Total time: 0:00:04 (0.3077 s / it)\n",
      "Averaged stats: loss: 1.0418 (1.0489)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_262_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:263]  [  0/431]  eta: 0:34:51  lr: 0.000164  loss: 1.2558 (1.2558)  time: 4.8530  data: 3.5701  max mem: 15925\n",
      "Train: [epoch:263]  [ 10/431]  eta: 0:09:49  lr: 0.000164  loss: 1.1109 (1.1352)  time: 1.4012  data: 0.3248  max mem: 15925\n",
      "Train: [epoch:263]  [ 20/431]  eta: 0:08:31  lr: 0.000164  loss: 1.0818 (1.1299)  time: 1.0643  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:263]  [ 30/431]  eta: 0:07:55  lr: 0.000164  loss: 1.0268 (1.1129)  time: 1.0684  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [ 40/431]  eta: 0:07:34  lr: 0.000164  loss: 1.0405 (1.1006)  time: 1.0777  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [ 50/431]  eta: 0:07:19  lr: 0.000164  loss: 1.0722 (1.1090)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [ 60/431]  eta: 0:07:04  lr: 0.000164  loss: 1.1167 (1.1096)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [ 70/431]  eta: 0:06:51  lr: 0.000164  loss: 1.1333 (1.1222)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [ 80/431]  eta: 0:06:38  lr: 0.000164  loss: 1.1232 (1.1250)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [ 90/431]  eta: 0:06:26  lr: 0.000164  loss: 1.0897 (1.1235)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [100/431]  eta: 0:06:14  lr: 0.000164  loss: 1.0637 (1.1147)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [110/431]  eta: 0:06:02  lr: 0.000164  loss: 1.0326 (1.1075)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [120/431]  eta: 0:05:50  lr: 0.000164  loss: 1.0229 (1.1033)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [130/431]  eta: 0:05:38  lr: 0.000164  loss: 1.0342 (1.1024)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [140/431]  eta: 0:05:27  lr: 0.000164  loss: 1.0443 (1.1011)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [150/431]  eta: 0:05:15  lr: 0.000164  loss: 1.0869 (1.1016)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [160/431]  eta: 0:05:04  lr: 0.000164  loss: 1.0840 (1.0980)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [170/431]  eta: 0:04:52  lr: 0.000164  loss: 1.0295 (1.0962)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [180/431]  eta: 0:04:41  lr: 0.000164  loss: 1.0472 (1.0972)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [190/431]  eta: 0:04:30  lr: 0.000164  loss: 1.0712 (1.0948)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [200/431]  eta: 0:04:18  lr: 0.000164  loss: 1.0800 (1.0942)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [210/431]  eta: 0:04:07  lr: 0.000164  loss: 1.0987 (1.0943)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [220/431]  eta: 0:03:56  lr: 0.000164  loss: 1.0715 (1.0949)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [230/431]  eta: 0:03:44  lr: 0.000164  loss: 1.0814 (1.0952)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [240/431]  eta: 0:03:33  lr: 0.000164  loss: 1.0724 (1.0941)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [250/431]  eta: 0:03:22  lr: 0.000164  loss: 1.0823 (1.0961)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [260/431]  eta: 0:03:11  lr: 0.000164  loss: 1.1171 (1.0972)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [270/431]  eta: 0:02:59  lr: 0.000164  loss: 1.1138 (1.0977)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [280/431]  eta: 0:02:48  lr: 0.000164  loss: 1.0606 (1.0981)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [290/431]  eta: 0:02:37  lr: 0.000164  loss: 1.0486 (1.0968)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [300/431]  eta: 0:02:26  lr: 0.000164  loss: 1.0739 (1.0971)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [310/431]  eta: 0:02:14  lr: 0.000164  loss: 1.0777 (1.0959)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [320/431]  eta: 0:02:03  lr: 0.000164  loss: 1.0735 (1.0951)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [330/431]  eta: 0:01:52  lr: 0.000164  loss: 1.0951 (1.0969)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [340/431]  eta: 0:01:41  lr: 0.000164  loss: 1.1332 (1.0981)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [350/431]  eta: 0:01:30  lr: 0.000164  loss: 1.1315 (1.0991)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [360/431]  eta: 0:01:19  lr: 0.000164  loss: 1.1317 (1.1007)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [370/431]  eta: 0:01:07  lr: 0.000164  loss: 1.1120 (1.1000)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [380/431]  eta: 0:00:56  lr: 0.000164  loss: 1.1049 (1.1012)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [390/431]  eta: 0:00:45  lr: 0.000164  loss: 1.1089 (1.1025)  time: 1.1201  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:263]  [400/431]  eta: 0:00:34  lr: 0.000164  loss: 1.1541 (1.1039)  time: 1.1081  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:263]  [410/431]  eta: 0:00:23  lr: 0.000164  loss: 1.1123 (1.1039)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [420/431]  eta: 0:00:12  lr: 0.000164  loss: 1.0904 (1.1045)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263]  [430/431]  eta: 0:00:01  lr: 0.000164  loss: 1.1136 (1.1049)  time: 1.1237  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:263] Total time: 0:08:00 (1.1152 s / it)\n",
      "Averaged stats: lr: 0.000164  loss: 1.1136 (1.1049)\n",
      "Valid: [epoch:263]  [ 0/14]  eta: 0:00:35  loss: 1.0954 (1.0954)  time: 2.5600  data: 2.3992  max mem: 15925\n",
      "Valid: [epoch:263]  [13/14]  eta: 0:00:00  loss: 1.0335 (1.0436)  time: 0.2856  data: 0.1715  max mem: 15925\n",
      "Valid: [epoch:263] Total time: 0:00:04 (0.3018 s / it)\n",
      "Averaged stats: loss: 1.0335 (1.0436)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_263_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:264]  [  0/431]  eta: 0:33:10  lr: 0.000164  loss: 1.1364 (1.1364)  time: 4.6192  data: 3.4579  max mem: 15925\n",
      "Train: [epoch:264]  [ 10/431]  eta: 0:09:51  lr: 0.000164  loss: 1.0828 (1.1366)  time: 1.4042  data: 0.3146  max mem: 15925\n",
      "Train: [epoch:264]  [ 20/431]  eta: 0:08:37  lr: 0.000164  loss: 1.0938 (1.1408)  time: 1.0914  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [ 30/431]  eta: 0:08:04  lr: 0.000164  loss: 1.1181 (1.1531)  time: 1.1011  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [ 40/431]  eta: 0:07:44  lr: 0.000164  loss: 1.1183 (1.1397)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [ 50/431]  eta: 0:07:26  lr: 0.000164  loss: 1.0849 (1.1359)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [ 60/431]  eta: 0:07:13  lr: 0.000164  loss: 1.0472 (1.1223)  time: 1.1292  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [ 70/431]  eta: 0:06:58  lr: 0.000164  loss: 1.0481 (1.1154)  time: 1.1271  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [ 80/431]  eta: 0:06:45  lr: 0.000164  loss: 1.0659 (1.1147)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [ 90/431]  eta: 0:06:31  lr: 0.000164  loss: 1.0964 (1.1161)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [100/431]  eta: 0:06:19  lr: 0.000164  loss: 1.1375 (1.1150)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [110/431]  eta: 0:06:06  lr: 0.000164  loss: 1.1123 (1.1115)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [120/431]  eta: 0:05:53  lr: 0.000164  loss: 1.1072 (1.1138)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [130/431]  eta: 0:05:42  lr: 0.000164  loss: 1.0336 (1.1073)  time: 1.1273  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [140/431]  eta: 0:05:31  lr: 0.000164  loss: 1.0290 (1.1026)  time: 1.1367  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [150/431]  eta: 0:05:19  lr: 0.000164  loss: 1.0273 (1.0984)  time: 1.1316  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [160/431]  eta: 0:05:07  lr: 0.000164  loss: 1.0661 (1.0988)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [170/431]  eta: 0:04:56  lr: 0.000164  loss: 1.0661 (1.0966)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [180/431]  eta: 0:04:44  lr: 0.000164  loss: 1.0454 (1.0967)  time: 1.1131  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [190/431]  eta: 0:04:33  lr: 0.000164  loss: 1.1001 (1.0968)  time: 1.1214  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [200/431]  eta: 0:04:21  lr: 0.000164  loss: 1.1021 (1.0976)  time: 1.1209  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [210/431]  eta: 0:04:10  lr: 0.000164  loss: 1.0918 (1.0968)  time: 1.1212  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [220/431]  eta: 0:03:58  lr: 0.000164  loss: 1.0136 (1.0948)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [230/431]  eta: 0:03:47  lr: 0.000164  loss: 1.0471 (1.0952)  time: 1.1199  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [240/431]  eta: 0:03:36  lr: 0.000164  loss: 1.1197 (1.0996)  time: 1.1393  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [250/431]  eta: 0:03:24  lr: 0.000164  loss: 1.1630 (1.0998)  time: 1.1428  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [260/431]  eta: 0:03:13  lr: 0.000164  loss: 1.0996 (1.0999)  time: 1.1365  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [270/431]  eta: 0:03:02  lr: 0.000164  loss: 1.1135 (1.1022)  time: 1.1425  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [280/431]  eta: 0:02:50  lr: 0.000164  loss: 1.0975 (1.1019)  time: 1.1238  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [290/431]  eta: 0:02:39  lr: 0.000164  loss: 1.0546 (1.0998)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [300/431]  eta: 0:02:28  lr: 0.000164  loss: 1.0686 (1.1004)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [310/431]  eta: 0:02:16  lr: 0.000164  loss: 1.1002 (1.1000)  time: 1.1011  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:264]  [320/431]  eta: 0:02:05  lr: 0.000164  loss: 1.0597 (1.0997)  time: 1.0971  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:264]  [330/431]  eta: 0:01:53  lr: 0.000164  loss: 1.0864 (1.1012)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [340/431]  eta: 0:01:42  lr: 0.000164  loss: 1.1540 (1.1022)  time: 1.1107  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [350/431]  eta: 0:01:31  lr: 0.000164  loss: 1.1157 (1.1021)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [360/431]  eta: 0:01:19  lr: 0.000164  loss: 1.0768 (1.1022)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [370/431]  eta: 0:01:08  lr: 0.000164  loss: 1.0972 (1.1034)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [380/431]  eta: 0:00:57  lr: 0.000164  loss: 1.0927 (1.1036)  time: 1.1325  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [390/431]  eta: 0:00:46  lr: 0.000164  loss: 1.0945 (1.1041)  time: 1.1243  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [400/431]  eta: 0:00:34  lr: 0.000164  loss: 1.1452 (1.1054)  time: 1.1292  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:264]  [410/431]  eta: 0:00:23  lr: 0.000164  loss: 1.0946 (1.1051)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [420/431]  eta: 0:00:12  lr: 0.000164  loss: 1.0946 (1.1057)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264]  [430/431]  eta: 0:00:01  lr: 0.000164  loss: 1.0662 (1.1046)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:264] Total time: 0:08:05 (1.1266 s / it)\n",
      "Averaged stats: lr: 0.000164  loss: 1.0662 (1.1046)\n",
      "Valid: [epoch:264]  [ 0/14]  eta: 0:00:36  loss: 0.9874 (0.9874)  time: 2.5815  data: 2.4348  max mem: 15925\n",
      "Valid: [epoch:264]  [13/14]  eta: 0:00:00  loss: 1.0430 (1.0508)  time: 0.2973  data: 0.1740  max mem: 15925\n",
      "Valid: [epoch:264] Total time: 0:00:04 (0.3132 s / it)\n",
      "Averaged stats: loss: 1.0430 (1.0508)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_264_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.051%\n",
      "Min loss: 1.043\n",
      "Best Epoch: 238.000\n",
      "Train: [epoch:265]  [  0/431]  eta: 0:32:49  lr: 0.000164  loss: 1.1341 (1.1341)  time: 4.5703  data: 3.3281  max mem: 15925\n",
      "Train: [epoch:265]  [ 10/431]  eta: 0:09:54  lr: 0.000164  loss: 1.1319 (1.1433)  time: 1.4119  data: 0.3028  max mem: 15925\n",
      "Train: [epoch:265]  [ 20/431]  eta: 0:08:37  lr: 0.000164  loss: 1.1258 (1.1457)  time: 1.0933  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:265]  [ 30/431]  eta: 0:08:04  lr: 0.000164  loss: 1.0637 (1.1276)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [ 40/431]  eta: 0:07:39  lr: 0.000164  loss: 1.0575 (1.1255)  time: 1.0889  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [ 50/431]  eta: 0:07:22  lr: 0.000164  loss: 1.0439 (1.1107)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [ 60/431]  eta: 0:07:06  lr: 0.000164  loss: 1.0323 (1.0976)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [ 70/431]  eta: 0:06:53  lr: 0.000164  loss: 1.0635 (1.1097)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [ 80/431]  eta: 0:06:41  lr: 0.000164  loss: 1.1530 (1.1176)  time: 1.1196  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:265]  [ 90/431]  eta: 0:06:29  lr: 0.000164  loss: 1.1211 (1.1192)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [100/431]  eta: 0:06:16  lr: 0.000164  loss: 1.0848 (1.1162)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [110/431]  eta: 0:06:04  lr: 0.000164  loss: 1.0353 (1.1104)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [120/431]  eta: 0:05:52  lr: 0.000164  loss: 1.0198 (1.1085)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [130/431]  eta: 0:05:40  lr: 0.000164  loss: 1.0081 (1.1035)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [140/431]  eta: 0:05:28  lr: 0.000164  loss: 1.0652 (1.1053)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [150/431]  eta: 0:05:17  lr: 0.000164  loss: 1.0023 (1.0982)  time: 1.1284  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [160/431]  eta: 0:05:05  lr: 0.000164  loss: 1.0027 (1.0988)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [170/431]  eta: 0:04:54  lr: 0.000164  loss: 1.1386 (1.1025)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [180/431]  eta: 0:04:43  lr: 0.000164  loss: 1.1542 (1.1036)  time: 1.1241  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:265]  [190/431]  eta: 0:04:31  lr: 0.000164  loss: 1.1115 (1.1024)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:265]  [200/431]  eta: 0:04:20  lr: 0.000164  loss: 1.0528 (1.1008)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [210/431]  eta: 0:04:08  lr: 0.000164  loss: 1.0504 (1.0998)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [220/431]  eta: 0:03:57  lr: 0.000164  loss: 1.0895 (1.0991)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [230/431]  eta: 0:03:46  lr: 0.000164  loss: 1.0895 (1.1000)  time: 1.1255  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:265]  [240/431]  eta: 0:03:35  lr: 0.000164  loss: 1.0762 (1.1012)  time: 1.1198  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [250/431]  eta: 0:03:23  lr: 0.000164  loss: 1.0762 (1.1022)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [260/431]  eta: 0:03:12  lr: 0.000164  loss: 1.0603 (1.1034)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [270/431]  eta: 0:03:00  lr: 0.000164  loss: 1.0613 (1.1030)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [280/431]  eta: 0:02:49  lr: 0.000164  loss: 1.0813 (1.1035)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [290/431]  eta: 0:02:38  lr: 0.000164  loss: 1.1031 (1.1048)  time: 1.1215  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:265]  [300/431]  eta: 0:02:27  lr: 0.000164  loss: 1.1217 (1.1050)  time: 1.1147  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:265]  [310/431]  eta: 0:02:15  lr: 0.000164  loss: 1.1392 (1.1071)  time: 1.1245  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [320/431]  eta: 0:02:04  lr: 0.000164  loss: 1.1627 (1.1080)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [330/431]  eta: 0:01:53  lr: 0.000164  loss: 1.0911 (1.1080)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [340/431]  eta: 0:01:42  lr: 0.000164  loss: 1.0906 (1.1087)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [350/431]  eta: 0:01:30  lr: 0.000164  loss: 1.1079 (1.1091)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [360/431]  eta: 0:01:19  lr: 0.000164  loss: 1.1114 (1.1093)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [370/431]  eta: 0:01:08  lr: 0.000164  loss: 1.1106 (1.1095)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [380/431]  eta: 0:00:57  lr: 0.000164  loss: 1.1009 (1.1090)  time: 1.1323  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [390/431]  eta: 0:00:46  lr: 0.000164  loss: 1.0658 (1.1079)  time: 1.1367  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:265]  [400/431]  eta: 0:00:34  lr: 0.000164  loss: 1.0485 (1.1080)  time: 1.1231  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:265]  [410/431]  eta: 0:00:23  lr: 0.000164  loss: 1.0763 (1.1078)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [420/431]  eta: 0:00:12  lr: 0.000164  loss: 1.0871 (1.1073)  time: 1.1284  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265]  [430/431]  eta: 0:00:01  lr: 0.000164  loss: 1.0412 (1.1050)  time: 1.1404  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:265] Total time: 0:08:04 (1.1249 s / it)\n",
      "Averaged stats: lr: 0.000164  loss: 1.0412 (1.1050)\n",
      "Valid: [epoch:265]  [ 0/14]  eta: 0:00:35  loss: 1.0825 (1.0825)  time: 2.5677  data: 2.3979  max mem: 15925\n",
      "Valid: [epoch:265]  [13/14]  eta: 0:00:00  loss: 1.0339 (1.0418)  time: 0.2766  data: 0.1714  max mem: 15925\n",
      "Valid: [epoch:265] Total time: 0:00:04 (0.2920 s / it)\n",
      "Averaged stats: loss: 1.0339 (1.0418)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_265_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:266]  [  0/431]  eta: 0:34:55  lr: 0.000163  loss: 1.2518 (1.2518)  time: 4.8610  data: 3.4556  max mem: 15925\n",
      "Train: [epoch:266]  [ 10/431]  eta: 0:09:48  lr: 0.000163  loss: 1.0387 (1.1071)  time: 1.3968  data: 0.3144  max mem: 15925\n",
      "Train: [epoch:266]  [ 20/431]  eta: 0:08:27  lr: 0.000163  loss: 1.0971 (1.1163)  time: 1.0533  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [ 30/431]  eta: 0:07:58  lr: 0.000163  loss: 1.0267 (1.0796)  time: 1.0827  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [ 40/431]  eta: 0:07:39  lr: 0.000163  loss: 0.9794 (1.0628)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [ 50/431]  eta: 0:07:23  lr: 0.000163  loss: 1.0239 (1.0591)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [ 60/431]  eta: 0:07:07  lr: 0.000163  loss: 1.0527 (1.0671)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [ 70/431]  eta: 0:06:54  lr: 0.000163  loss: 1.1155 (1.0738)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [ 80/431]  eta: 0:06:41  lr: 0.000163  loss: 1.0687 (1.0718)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [ 90/431]  eta: 0:06:29  lr: 0.000163  loss: 1.0268 (1.0712)  time: 1.1243  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [100/431]  eta: 0:06:17  lr: 0.000163  loss: 1.0492 (1.0726)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [110/431]  eta: 0:06:05  lr: 0.000163  loss: 1.0492 (1.0717)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [120/431]  eta: 0:05:54  lr: 0.000163  loss: 1.0461 (1.0749)  time: 1.1404  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [130/431]  eta: 0:05:43  lr: 0.000163  loss: 1.0649 (1.0767)  time: 1.1506  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [140/431]  eta: 0:05:31  lr: 0.000163  loss: 1.0558 (1.0757)  time: 1.1314  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [150/431]  eta: 0:05:19  lr: 0.000163  loss: 1.0749 (1.0789)  time: 1.1258  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [160/431]  eta: 0:05:08  lr: 0.000163  loss: 1.0769 (1.0793)  time: 1.1285  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [170/431]  eta: 0:04:56  lr: 0.000163  loss: 1.0526 (1.0790)  time: 1.1222  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [180/431]  eta: 0:04:45  lr: 0.000163  loss: 1.0756 (1.0825)  time: 1.1274  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [190/431]  eta: 0:04:33  lr: 0.000163  loss: 1.1112 (1.0832)  time: 1.1312  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [200/431]  eta: 0:04:22  lr: 0.000163  loss: 1.0861 (1.0865)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [210/431]  eta: 0:04:10  lr: 0.000163  loss: 1.0861 (1.0883)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [220/431]  eta: 0:03:58  lr: 0.000163  loss: 1.0475 (1.0876)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [230/431]  eta: 0:03:47  lr: 0.000163  loss: 1.1225 (1.0917)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [240/431]  eta: 0:03:35  lr: 0.000163  loss: 1.1227 (1.0914)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [250/431]  eta: 0:03:24  lr: 0.000163  loss: 1.0756 (1.0917)  time: 1.1097  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:266]  [260/431]  eta: 0:03:13  lr: 0.000163  loss: 1.0756 (1.0928)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [270/431]  eta: 0:03:01  lr: 0.000163  loss: 1.1224 (1.0959)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [280/431]  eta: 0:02:50  lr: 0.000163  loss: 1.1085 (1.0951)  time: 1.1193  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [290/431]  eta: 0:02:39  lr: 0.000163  loss: 1.0917 (1.0955)  time: 1.1422  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [300/431]  eta: 0:02:28  lr: 0.000163  loss: 1.1089 (1.0963)  time: 1.1542  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [310/431]  eta: 0:02:16  lr: 0.000163  loss: 1.0811 (1.0947)  time: 1.1491  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [320/431]  eta: 0:02:05  lr: 0.000163  loss: 1.0387 (1.0927)  time: 1.1404  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [330/431]  eta: 0:01:54  lr: 0.000163  loss: 1.0461 (1.0923)  time: 1.1340  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [340/431]  eta: 0:01:42  lr: 0.000163  loss: 1.0734 (1.0948)  time: 1.1315  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [350/431]  eta: 0:01:31  lr: 0.000163  loss: 1.1398 (1.0966)  time: 1.1253  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:266]  [360/431]  eta: 0:01:20  lr: 0.000163  loss: 1.1237 (1.0988)  time: 1.1318  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [370/431]  eta: 0:01:08  lr: 0.000163  loss: 1.0815 (1.0989)  time: 1.1277  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [380/431]  eta: 0:00:57  lr: 0.000163  loss: 1.0815 (1.0991)  time: 1.1277  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [390/431]  eta: 0:00:46  lr: 0.000163  loss: 1.1005 (1.0991)  time: 1.1297  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [400/431]  eta: 0:00:35  lr: 0.000163  loss: 1.1076 (1.1004)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [410/431]  eta: 0:00:23  lr: 0.000163  loss: 1.1331 (1.1019)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [420/431]  eta: 0:00:12  lr: 0.000163  loss: 1.1344 (1.1025)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:266]  [430/431]  eta: 0:00:01  lr: 0.000163  loss: 1.1344 (1.1038)  time: 1.1096  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:266] Total time: 0:08:06 (1.1288 s / it)\n",
      "Averaged stats: lr: 0.000163  loss: 1.1344 (1.1038)\n",
      "Valid: [epoch:266]  [ 0/14]  eta: 0:00:37  loss: 0.9626 (0.9626)  time: 2.6500  data: 2.4480  max mem: 15925\n",
      "Valid: [epoch:266]  [13/14]  eta: 0:00:00  loss: 1.0420 (1.0501)  time: 0.2837  data: 0.1750  max mem: 15925\n",
      "Valid: [epoch:266] Total time: 0:00:04 (0.3014 s / it)\n",
      "Averaged stats: loss: 1.0420 (1.0501)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_266_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:267]  [  0/431]  eta: 0:35:40  lr: 0.000163  loss: 1.1386 (1.1386)  time: 4.9658  data: 3.7865  max mem: 15925\n",
      "Train: [epoch:267]  [ 10/431]  eta: 0:09:53  lr: 0.000163  loss: 1.1601 (1.1800)  time: 1.4089  data: 0.3445  max mem: 15925\n",
      "Train: [epoch:267]  [ 20/431]  eta: 0:08:35  lr: 0.000163  loss: 1.1130 (1.1097)  time: 1.0690  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:267]  [ 30/431]  eta: 0:08:05  lr: 0.000163  loss: 1.0564 (1.1088)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [ 40/431]  eta: 0:07:43  lr: 0.000163  loss: 1.0697 (1.1100)  time: 1.1119  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:267]  [ 50/431]  eta: 0:07:26  lr: 0.000163  loss: 1.1161 (1.1130)  time: 1.1140  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:267]  [ 60/431]  eta: 0:07:12  lr: 0.000163  loss: 1.0570 (1.1029)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [ 70/431]  eta: 0:06:59  lr: 0.000163  loss: 1.0792 (1.1128)  time: 1.1319  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:267]  [ 80/431]  eta: 0:06:45  lr: 0.000163  loss: 1.0913 (1.1112)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:267]  [ 90/431]  eta: 0:06:32  lr: 0.000163  loss: 1.0719 (1.1073)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [100/431]  eta: 0:06:20  lr: 0.000163  loss: 1.0985 (1.1096)  time: 1.1205  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [110/431]  eta: 0:06:08  lr: 0.000163  loss: 1.1585 (1.1109)  time: 1.1259  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [120/431]  eta: 0:05:56  lr: 0.000163  loss: 1.0862 (1.1077)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [130/431]  eta: 0:05:44  lr: 0.000163  loss: 1.0862 (1.1102)  time: 1.1273  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [140/431]  eta: 0:05:32  lr: 0.000163  loss: 1.0814 (1.1054)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [150/431]  eta: 0:05:20  lr: 0.000163  loss: 1.0395 (1.1033)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [160/431]  eta: 0:05:08  lr: 0.000163  loss: 1.0546 (1.1036)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [170/431]  eta: 0:04:56  lr: 0.000163  loss: 1.0404 (1.1017)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [180/431]  eta: 0:04:44  lr: 0.000163  loss: 1.0650 (1.1019)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [190/431]  eta: 0:04:32  lr: 0.000163  loss: 1.0824 (1.1036)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [200/431]  eta: 0:04:21  lr: 0.000163  loss: 1.1257 (1.1028)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [210/431]  eta: 0:04:09  lr: 0.000163  loss: 1.1239 (1.1058)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [220/431]  eta: 0:03:58  lr: 0.000163  loss: 1.1239 (1.1064)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [230/431]  eta: 0:03:47  lr: 0.000163  loss: 1.0667 (1.1047)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [240/431]  eta: 0:03:35  lr: 0.000163  loss: 1.0612 (1.1045)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [250/431]  eta: 0:03:24  lr: 0.000163  loss: 1.0912 (1.1051)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [260/431]  eta: 0:03:12  lr: 0.000163  loss: 1.0912 (1.1060)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [270/431]  eta: 0:03:01  lr: 0.000163  loss: 1.0497 (1.1023)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [280/431]  eta: 0:02:50  lr: 0.000163  loss: 1.0286 (1.1029)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:267]  [290/431]  eta: 0:02:38  lr: 0.000163  loss: 1.0954 (1.1030)  time: 1.1250  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:267]  [300/431]  eta: 0:02:27  lr: 0.000163  loss: 1.0826 (1.1031)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [310/431]  eta: 0:02:16  lr: 0.000163  loss: 1.0743 (1.1031)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [320/431]  eta: 0:02:04  lr: 0.000163  loss: 1.0743 (1.1030)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [330/431]  eta: 0:01:53  lr: 0.000163  loss: 1.0918 (1.1040)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [340/431]  eta: 0:01:42  lr: 0.000163  loss: 1.1181 (1.1035)  time: 1.1203  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:267]  [350/431]  eta: 0:01:31  lr: 0.000163  loss: 1.1297 (1.1057)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [360/431]  eta: 0:01:19  lr: 0.000163  loss: 1.0767 (1.1042)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [370/431]  eta: 0:01:08  lr: 0.000163  loss: 1.0767 (1.1050)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [380/431]  eta: 0:00:57  lr: 0.000163  loss: 1.0772 (1.1043)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [390/431]  eta: 0:00:46  lr: 0.000163  loss: 1.0336 (1.1029)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [400/431]  eta: 0:00:34  lr: 0.000163  loss: 1.0381 (1.1026)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [410/431]  eta: 0:00:23  lr: 0.000163  loss: 1.0381 (1.1017)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267]  [420/431]  eta: 0:00:12  lr: 0.000163  loss: 1.0637 (1.1020)  time: 1.1259  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:267]  [430/431]  eta: 0:00:01  lr: 0.000163  loss: 1.0940 (1.1038)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:267] Total time: 0:08:04 (1.1240 s / it)\n",
      "Averaged stats: lr: 0.000163  loss: 1.0940 (1.1038)\n",
      "Valid: [epoch:267]  [ 0/14]  eta: 0:00:36  loss: 1.1433 (1.1433)  time: 2.6278  data: 2.4580  max mem: 15925\n",
      "Valid: [epoch:267]  [13/14]  eta: 0:00:00  loss: 1.0467 (1.0550)  time: 0.2909  data: 0.1757  max mem: 15925\n",
      "Valid: [epoch:267] Total time: 0:00:04 (0.3073 s / it)\n",
      "Averaged stats: loss: 1.0467 (1.0550)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_267_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.055%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:268]  [  0/431]  eta: 0:32:36  lr: 0.000163  loss: 1.1728 (1.1728)  time: 4.5394  data: 3.3006  max mem: 15925\n",
      "Train: [epoch:268]  [ 10/431]  eta: 0:09:41  lr: 0.000163  loss: 1.1223 (1.1456)  time: 1.3817  data: 0.3003  max mem: 15925\n",
      "Train: [epoch:268]  [ 20/431]  eta: 0:08:28  lr: 0.000163  loss: 1.1087 (1.1211)  time: 1.0734  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [ 30/431]  eta: 0:07:58  lr: 0.000163  loss: 1.0635 (1.1082)  time: 1.0880  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [ 40/431]  eta: 0:07:37  lr: 0.000163  loss: 1.0703 (1.1095)  time: 1.0986  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [ 50/431]  eta: 0:07:22  lr: 0.000163  loss: 1.1577 (1.1174)  time: 1.1148  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [ 60/431]  eta: 0:07:08  lr: 0.000163  loss: 1.0855 (1.1144)  time: 1.1278  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [ 70/431]  eta: 0:06:56  lr: 0.000163  loss: 1.0946 (1.1197)  time: 1.1352  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [ 80/431]  eta: 0:06:43  lr: 0.000163  loss: 1.0510 (1.1136)  time: 1.1317  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [ 90/431]  eta: 0:06:31  lr: 0.000163  loss: 1.0297 (1.1113)  time: 1.1209  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [100/431]  eta: 0:06:18  lr: 0.000163  loss: 1.0371 (1.1100)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [110/431]  eta: 0:06:05  lr: 0.000163  loss: 1.0856 (1.1095)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [120/431]  eta: 0:05:53  lr: 0.000163  loss: 1.0313 (1.1038)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [130/431]  eta: 0:05:41  lr: 0.000163  loss: 1.0341 (1.1008)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [140/431]  eta: 0:05:29  lr: 0.000163  loss: 1.0429 (1.0975)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [150/431]  eta: 0:05:18  lr: 0.000163  loss: 1.0371 (1.0962)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [160/431]  eta: 0:05:06  lr: 0.000163  loss: 1.0772 (1.0960)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [170/431]  eta: 0:04:55  lr: 0.000163  loss: 1.1002 (1.0951)  time: 1.1191  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [180/431]  eta: 0:04:43  lr: 0.000163  loss: 1.0489 (1.0947)  time: 1.1216  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [190/431]  eta: 0:04:32  lr: 0.000163  loss: 1.0653 (1.0970)  time: 1.1340  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [200/431]  eta: 0:04:20  lr: 0.000163  loss: 1.1506 (1.1000)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [210/431]  eta: 0:04:09  lr: 0.000163  loss: 1.0704 (1.0982)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [220/431]  eta: 0:03:58  lr: 0.000163  loss: 1.0534 (1.0963)  time: 1.1155  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:268]  [230/431]  eta: 0:03:46  lr: 0.000163  loss: 1.0920 (1.1005)  time: 1.1203  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:268]  [240/431]  eta: 0:03:35  lr: 0.000163  loss: 1.1379 (1.0992)  time: 1.1192  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [250/431]  eta: 0:03:24  lr: 0.000163  loss: 1.0390 (1.1026)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [260/431]  eta: 0:03:12  lr: 0.000163  loss: 1.1462 (1.1048)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [270/431]  eta: 0:03:01  lr: 0.000163  loss: 1.1462 (1.1063)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [280/431]  eta: 0:02:49  lr: 0.000163  loss: 1.0788 (1.1048)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [290/431]  eta: 0:02:38  lr: 0.000163  loss: 1.0280 (1.1025)  time: 1.1044  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [300/431]  eta: 0:02:27  lr: 0.000163  loss: 1.0200 (1.1021)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [310/431]  eta: 0:02:15  lr: 0.000163  loss: 1.0339 (1.1004)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [320/431]  eta: 0:02:04  lr: 0.000163  loss: 1.0451 (1.1007)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [330/431]  eta: 0:01:53  lr: 0.000163  loss: 1.1063 (1.1011)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [340/431]  eta: 0:01:42  lr: 0.000163  loss: 1.0878 (1.1017)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [350/431]  eta: 0:01:30  lr: 0.000163  loss: 1.1050 (1.1033)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [360/431]  eta: 0:01:19  lr: 0.000163  loss: 1.1366 (1.1035)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [370/431]  eta: 0:01:08  lr: 0.000163  loss: 1.1158 (1.1034)  time: 1.1206  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:268]  [380/431]  eta: 0:00:57  lr: 0.000163  loss: 1.1234 (1.1050)  time: 1.1203  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:268]  [390/431]  eta: 0:00:45  lr: 0.000163  loss: 1.1964 (1.1071)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [400/431]  eta: 0:00:34  lr: 0.000163  loss: 1.1558 (1.1076)  time: 1.1234  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:268]  [410/431]  eta: 0:00:23  lr: 0.000163  loss: 1.1016 (1.1077)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [420/431]  eta: 0:00:12  lr: 0.000163  loss: 1.0745 (1.1066)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268]  [430/431]  eta: 0:00:01  lr: 0.000163  loss: 1.0499 (1.1055)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:268] Total time: 0:08:02 (1.1202 s / it)\n",
      "Averaged stats: lr: 0.000163  loss: 1.0499 (1.1055)\n",
      "Valid: [epoch:268]  [ 0/14]  eta: 0:00:35  loss: 1.0955 (1.0955)  time: 2.5124  data: 2.3578  max mem: 15925\n",
      "Valid: [epoch:268]  [13/14]  eta: 0:00:00  loss: 1.0334 (1.0438)  time: 0.2831  data: 0.1685  max mem: 15925\n",
      "Valid: [epoch:268] Total time: 0:00:04 (0.3008 s / it)\n",
      "Averaged stats: loss: 1.0334 (1.0438)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_268_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:269]  [  0/431]  eta: 0:33:27  lr: 0.000163  loss: 1.1381 (1.1381)  time: 4.6574  data: 3.0989  max mem: 15925\n",
      "Train: [epoch:269]  [ 10/431]  eta: 0:09:37  lr: 0.000163  loss: 1.1269 (1.1337)  time: 1.3707  data: 0.2820  max mem: 15925\n",
      "Train: [epoch:269]  [ 20/431]  eta: 0:08:30  lr: 0.000163  loss: 1.0980 (1.1457)  time: 1.0710  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [ 30/431]  eta: 0:07:58  lr: 0.000163  loss: 1.1058 (1.1367)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [ 40/431]  eta: 0:07:39  lr: 0.000163  loss: 1.1092 (1.1272)  time: 1.1066  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:269]  [ 50/431]  eta: 0:07:22  lr: 0.000163  loss: 1.0639 (1.1076)  time: 1.1152  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:269]  [ 60/431]  eta: 0:07:08  lr: 0.000163  loss: 1.0278 (1.0999)  time: 1.1119  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:269]  [ 70/431]  eta: 0:06:55  lr: 0.000163  loss: 1.0656 (1.1000)  time: 1.1241  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:269]  [ 80/431]  eta: 0:06:43  lr: 0.000163  loss: 1.0789 (1.1096)  time: 1.1290  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [ 90/431]  eta: 0:06:31  lr: 0.000163  loss: 1.0654 (1.1054)  time: 1.1323  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [100/431]  eta: 0:06:19  lr: 0.000163  loss: 1.0665 (1.1069)  time: 1.1333  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:269]  [110/431]  eta: 0:06:07  lr: 0.000163  loss: 1.0806 (1.1043)  time: 1.1336  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:269]  [120/431]  eta: 0:05:55  lr: 0.000163  loss: 1.1023 (1.1080)  time: 1.1233  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:269]  [130/431]  eta: 0:05:43  lr: 0.000163  loss: 1.0942 (1.1057)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [140/431]  eta: 0:05:31  lr: 0.000163  loss: 1.0492 (1.1027)  time: 1.1334  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [150/431]  eta: 0:05:20  lr: 0.000163  loss: 1.0646 (1.1022)  time: 1.1316  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [160/431]  eta: 0:05:08  lr: 0.000163  loss: 1.1304 (1.1043)  time: 1.1323  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [170/431]  eta: 0:04:56  lr: 0.000163  loss: 1.1188 (1.1024)  time: 1.1227  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [180/431]  eta: 0:04:45  lr: 0.000163  loss: 1.0696 (1.1030)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [190/431]  eta: 0:04:34  lr: 0.000163  loss: 1.0941 (1.1017)  time: 1.1362  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:269]  [200/431]  eta: 0:04:22  lr: 0.000163  loss: 1.0941 (1.1032)  time: 1.1478  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [210/431]  eta: 0:04:11  lr: 0.000163  loss: 1.1315 (1.1056)  time: 1.1415  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [220/431]  eta: 0:03:59  lr: 0.000163  loss: 1.1304 (1.1058)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [230/431]  eta: 0:03:48  lr: 0.000163  loss: 1.0808 (1.1041)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [240/431]  eta: 0:03:36  lr: 0.000163  loss: 1.0529 (1.1054)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [250/431]  eta: 0:03:25  lr: 0.000163  loss: 1.1151 (1.1049)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [260/431]  eta: 0:03:14  lr: 0.000163  loss: 1.1143 (1.1065)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [270/431]  eta: 0:03:02  lr: 0.000163  loss: 1.0903 (1.1056)  time: 1.1321  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [280/431]  eta: 0:02:51  lr: 0.000163  loss: 1.0277 (1.1033)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [290/431]  eta: 0:02:39  lr: 0.000163  loss: 1.0474 (1.1025)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [300/431]  eta: 0:02:28  lr: 0.000163  loss: 1.0658 (1.1020)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [310/431]  eta: 0:02:16  lr: 0.000163  loss: 1.0293 (1.1004)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [320/431]  eta: 0:02:05  lr: 0.000163  loss: 1.0555 (1.1010)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [330/431]  eta: 0:01:54  lr: 0.000163  loss: 1.0952 (1.1019)  time: 1.1179  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [340/431]  eta: 0:01:42  lr: 0.000163  loss: 1.1167 (1.1031)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [350/431]  eta: 0:01:31  lr: 0.000163  loss: 1.0995 (1.1040)  time: 1.1346  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [360/431]  eta: 0:01:20  lr: 0.000163  loss: 1.0731 (1.1031)  time: 1.1403  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:269]  [370/431]  eta: 0:01:09  lr: 0.000163  loss: 1.0895 (1.1034)  time: 1.1330  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [380/431]  eta: 0:00:57  lr: 0.000163  loss: 1.1000 (1.1033)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [390/431]  eta: 0:00:46  lr: 0.000163  loss: 1.0460 (1.1020)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [400/431]  eta: 0:00:35  lr: 0.000163  loss: 1.0382 (1.1013)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [410/431]  eta: 0:00:23  lr: 0.000163  loss: 1.0777 (1.1021)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:269]  [420/431]  eta: 0:00:12  lr: 0.000163  loss: 1.1286 (1.1028)  time: 1.1147  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:269]  [430/431]  eta: 0:00:01  lr: 0.000163  loss: 1.0882 (1.1019)  time: 1.1107  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:269] Total time: 0:08:06 (1.1298 s / it)\n",
      "Averaged stats: lr: 0.000163  loss: 1.0882 (1.1019)\n",
      "Valid: [epoch:269]  [ 0/14]  eta: 0:00:37  loss: 1.1326 (1.1326)  time: 2.6707  data: 2.4878  max mem: 15925\n",
      "Valid: [epoch:269]  [13/14]  eta: 0:00:00  loss: 1.0354 (1.0445)  time: 0.2826  data: 0.1778  max mem: 15925\n",
      "Valid: [epoch:269] Total time: 0:00:04 (0.3000 s / it)\n",
      "Averaged stats: loss: 1.0354 (1.0445)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_269_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:270]  [  0/431]  eta: 0:36:20  lr: 0.000162  loss: 1.1340 (1.1340)  time: 5.0582  data: 3.8622  max mem: 15925\n",
      "Train: [epoch:270]  [ 10/431]  eta: 0:09:52  lr: 0.000162  loss: 1.1207 (1.1252)  time: 1.4081  data: 0.3513  max mem: 15925\n",
      "Train: [epoch:270]  [ 20/431]  eta: 0:08:32  lr: 0.000162  loss: 1.1204 (1.1133)  time: 1.0552  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [ 30/431]  eta: 0:07:56  lr: 0.000162  loss: 1.0313 (1.0972)  time: 1.0678  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [ 40/431]  eta: 0:07:35  lr: 0.000162  loss: 1.0141 (1.0926)  time: 1.0820  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [ 50/431]  eta: 0:07:18  lr: 0.000162  loss: 1.0113 (1.0828)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [ 60/431]  eta: 0:07:04  lr: 0.000162  loss: 1.0551 (1.0847)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [ 70/431]  eta: 0:06:52  lr: 0.000162  loss: 1.0528 (1.0817)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [ 80/431]  eta: 0:06:40  lr: 0.000162  loss: 1.0528 (1.0825)  time: 1.1273  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [ 90/431]  eta: 0:06:27  lr: 0.000162  loss: 1.0586 (1.0867)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [100/431]  eta: 0:06:15  lr: 0.000162  loss: 1.0714 (1.0924)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [110/431]  eta: 0:06:03  lr: 0.000162  loss: 1.0833 (1.0891)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [120/431]  eta: 0:05:52  lr: 0.000162  loss: 1.0833 (1.0907)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [130/431]  eta: 0:05:40  lr: 0.000162  loss: 1.0833 (1.0917)  time: 1.1219  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [140/431]  eta: 0:05:28  lr: 0.000162  loss: 1.0865 (1.0912)  time: 1.1186  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [150/431]  eta: 0:05:17  lr: 0.000162  loss: 1.0093 (1.0871)  time: 1.1204  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [160/431]  eta: 0:05:05  lr: 0.000162  loss: 1.0296 (1.0862)  time: 1.1201  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [170/431]  eta: 0:04:54  lr: 0.000162  loss: 1.0536 (1.0852)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [180/431]  eta: 0:04:42  lr: 0.000162  loss: 1.0616 (1.0900)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [190/431]  eta: 0:04:31  lr: 0.000162  loss: 1.1386 (1.0914)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [200/431]  eta: 0:04:20  lr: 0.000162  loss: 1.0978 (1.0904)  time: 1.1245  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [210/431]  eta: 0:04:08  lr: 0.000162  loss: 1.0390 (1.0894)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [220/431]  eta: 0:03:57  lr: 0.000162  loss: 1.0390 (1.0902)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [230/431]  eta: 0:03:46  lr: 0.000162  loss: 1.0461 (1.0899)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [240/431]  eta: 0:03:34  lr: 0.000162  loss: 1.0778 (1.0925)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [250/431]  eta: 0:03:23  lr: 0.000162  loss: 1.0922 (1.0926)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [260/431]  eta: 0:03:11  lr: 0.000162  loss: 1.0859 (1.0927)  time: 1.1095  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:270]  [270/431]  eta: 0:03:00  lr: 0.000162  loss: 1.0287 (1.0899)  time: 1.1071  data: 0.0004  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:270]  [280/431]  eta: 0:02:49  lr: 0.000162  loss: 1.0287 (1.0909)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [290/431]  eta: 0:02:38  lr: 0.000162  loss: 1.0634 (1.0913)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [300/431]  eta: 0:02:26  lr: 0.000162  loss: 1.0787 (1.0926)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [310/431]  eta: 0:02:15  lr: 0.000162  loss: 1.1040 (1.0934)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [320/431]  eta: 0:02:04  lr: 0.000162  loss: 1.1040 (1.0948)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [330/431]  eta: 0:01:53  lr: 0.000162  loss: 1.1105 (1.0964)  time: 1.1170  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [340/431]  eta: 0:01:41  lr: 0.000162  loss: 1.1177 (1.0983)  time: 1.1243  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [350/431]  eta: 0:01:30  lr: 0.000162  loss: 1.1448 (1.1006)  time: 1.1277  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [360/431]  eta: 0:01:19  lr: 0.000162  loss: 1.1271 (1.1010)  time: 1.1345  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [370/431]  eta: 0:01:08  lr: 0.000162  loss: 1.1025 (1.1019)  time: 1.1315  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [380/431]  eta: 0:00:57  lr: 0.000162  loss: 1.0759 (1.1014)  time: 1.1326  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [390/431]  eta: 0:00:45  lr: 0.000162  loss: 1.1112 (1.1032)  time: 1.1258  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [400/431]  eta: 0:00:34  lr: 0.000162  loss: 1.1450 (1.1036)  time: 1.1291  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:270]  [410/431]  eta: 0:00:23  lr: 0.000162  loss: 1.0850 (1.1038)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [420/431]  eta: 0:00:12  lr: 0.000162  loss: 1.0737 (1.1039)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:270]  [430/431]  eta: 0:00:01  lr: 0.000162  loss: 1.0471 (1.1037)  time: 1.1143  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:270] Total time: 0:08:03 (1.1220 s / it)\n",
      "Averaged stats: lr: 0.000162  loss: 1.0471 (1.1037)\n",
      "Valid: [epoch:270]  [ 0/14]  eta: 0:00:35  loss: 1.0952 (1.0952)  time: 2.5059  data: 2.3688  max mem: 15925\n",
      "Valid: [epoch:270]  [13/14]  eta: 0:00:00  loss: 1.0367 (1.0456)  time: 0.2830  data: 0.1693  max mem: 15925\n",
      "Valid: [epoch:270] Total time: 0:00:04 (0.2992 s / it)\n",
      "Averaged stats: loss: 1.0367 (1.0456)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_270_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:271]  [  0/431]  eta: 0:33:48  lr: 0.000162  loss: 0.9816 (0.9816)  time: 4.7054  data: 3.5095  max mem: 15925\n",
      "Train: [epoch:271]  [ 10/431]  eta: 0:09:41  lr: 0.000162  loss: 1.1098 (1.1231)  time: 1.3807  data: 0.3193  max mem: 15925\n",
      "Train: [epoch:271]  [ 20/431]  eta: 0:08:30  lr: 0.000162  loss: 1.1098 (1.1294)  time: 1.0684  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:271]  [ 30/431]  eta: 0:07:57  lr: 0.000162  loss: 1.0572 (1.0974)  time: 1.0849  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [ 40/431]  eta: 0:07:35  lr: 0.000162  loss: 1.0398 (1.0995)  time: 1.0855  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [ 50/431]  eta: 0:07:19  lr: 0.000162  loss: 1.0513 (1.0973)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [ 60/431]  eta: 0:07:05  lr: 0.000162  loss: 1.0487 (1.0909)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [ 70/431]  eta: 0:06:51  lr: 0.000162  loss: 1.0808 (1.0950)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [ 80/431]  eta: 0:06:39  lr: 0.000162  loss: 1.0808 (1.0948)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [ 90/431]  eta: 0:06:27  lr: 0.000162  loss: 1.0587 (1.0984)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [100/431]  eta: 0:06:15  lr: 0.000162  loss: 1.0654 (1.0988)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [110/431]  eta: 0:06:03  lr: 0.000162  loss: 1.0654 (1.0977)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [120/431]  eta: 0:05:52  lr: 0.000162  loss: 1.0785 (1.0978)  time: 1.1271  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:271]  [130/431]  eta: 0:05:40  lr: 0.000162  loss: 1.0570 (1.0946)  time: 1.1284  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:271]  [140/431]  eta: 0:05:28  lr: 0.000162  loss: 1.0438 (1.0978)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [150/431]  eta: 0:05:17  lr: 0.000162  loss: 1.0629 (1.0960)  time: 1.1297  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [160/431]  eta: 0:05:06  lr: 0.000162  loss: 1.0671 (1.0978)  time: 1.1324  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [170/431]  eta: 0:04:54  lr: 0.000162  loss: 1.1066 (1.0963)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [180/431]  eta: 0:04:43  lr: 0.000162  loss: 1.1024 (1.0980)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [190/431]  eta: 0:04:31  lr: 0.000162  loss: 1.0662 (1.0966)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [200/431]  eta: 0:04:20  lr: 0.000162  loss: 1.0371 (1.0959)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [210/431]  eta: 0:04:08  lr: 0.000162  loss: 1.0371 (1.0950)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [220/431]  eta: 0:03:57  lr: 0.000162  loss: 1.0479 (1.0956)  time: 1.1108  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:271]  [230/431]  eta: 0:03:46  lr: 0.000162  loss: 1.1468 (1.0984)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:271]  [240/431]  eta: 0:03:34  lr: 0.000162  loss: 1.1683 (1.1000)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [250/431]  eta: 0:03:23  lr: 0.000162  loss: 1.1026 (1.1006)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [260/431]  eta: 0:03:11  lr: 0.000162  loss: 1.0638 (1.0988)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [270/431]  eta: 0:03:00  lr: 0.000162  loss: 1.0684 (1.1005)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [280/431]  eta: 0:02:49  lr: 0.000162  loss: 1.1286 (1.1011)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [290/431]  eta: 0:02:38  lr: 0.000162  loss: 1.1304 (1.1035)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [300/431]  eta: 0:02:26  lr: 0.000162  loss: 1.1756 (1.1051)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [310/431]  eta: 0:02:15  lr: 0.000162  loss: 1.1335 (1.1057)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [320/431]  eta: 0:02:04  lr: 0.000162  loss: 1.0617 (1.1036)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [330/431]  eta: 0:01:53  lr: 0.000162  loss: 1.0351 (1.1050)  time: 1.1196  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:271]  [340/431]  eta: 0:01:41  lr: 0.000162  loss: 1.1483 (1.1072)  time: 1.1064  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:271]  [350/431]  eta: 0:01:30  lr: 0.000162  loss: 1.1008 (1.1066)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [360/431]  eta: 0:01:19  lr: 0.000162  loss: 1.0873 (1.1071)  time: 1.1154  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:271]  [370/431]  eta: 0:01:08  lr: 0.000162  loss: 1.1054 (1.1077)  time: 1.1089  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:271]  [380/431]  eta: 0:00:57  lr: 0.000162  loss: 1.0595 (1.1059)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [390/431]  eta: 0:00:45  lr: 0.000162  loss: 1.0374 (1.1064)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [400/431]  eta: 0:00:34  lr: 0.000162  loss: 1.0967 (1.1079)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [410/431]  eta: 0:00:23  lr: 0.000162  loss: 1.0754 (1.1065)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [420/431]  eta: 0:00:12  lr: 0.000162  loss: 1.0659 (1.1061)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271]  [430/431]  eta: 0:00:01  lr: 0.000162  loss: 1.0572 (1.1052)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:271] Total time: 0:08:02 (1.1188 s / it)\n",
      "Averaged stats: lr: 0.000162  loss: 1.0572 (1.1052)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:271]  [ 0/14]  eta: 0:00:36  loss: 1.0994 (1.0994)  time: 2.5886  data: 2.4386  max mem: 15925\n",
      "Valid: [epoch:271]  [13/14]  eta: 0:00:00  loss: 1.0402 (1.0487)  time: 0.2748  data: 0.1743  max mem: 15925\n",
      "Valid: [epoch:271] Total time: 0:00:04 (0.2911 s / it)\n",
      "Averaged stats: loss: 1.0402 (1.0487)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_271_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:272]  [  0/431]  eta: 0:31:18  lr: 0.000162  loss: 1.1381 (1.1381)  time: 4.3573  data: 3.1502  max mem: 15925\n",
      "Train: [epoch:272]  [ 10/431]  eta: 0:09:30  lr: 0.000162  loss: 1.0678 (1.1111)  time: 1.3556  data: 0.2867  max mem: 15925\n",
      "Train: [epoch:272]  [ 20/431]  eta: 0:08:19  lr: 0.000162  loss: 1.0678 (1.0986)  time: 1.0590  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:272]  [ 30/431]  eta: 0:07:50  lr: 0.000162  loss: 1.0884 (1.0964)  time: 1.0749  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [ 40/431]  eta: 0:07:33  lr: 0.000162  loss: 1.0921 (1.0952)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [ 50/431]  eta: 0:07:17  lr: 0.000162  loss: 1.0730 (1.0891)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [ 60/431]  eta: 0:07:04  lr: 0.000162  loss: 1.0619 (1.0886)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [ 70/431]  eta: 0:06:52  lr: 0.000162  loss: 1.1519 (1.0954)  time: 1.1297  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [ 80/431]  eta: 0:06:41  lr: 0.000162  loss: 1.0954 (1.0911)  time: 1.1414  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:272]  [ 90/431]  eta: 0:06:30  lr: 0.000162  loss: 1.0954 (1.0993)  time: 1.1526  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [100/431]  eta: 0:06:18  lr: 0.000162  loss: 1.0771 (1.0936)  time: 1.1447  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [110/431]  eta: 0:06:06  lr: 0.000162  loss: 1.0360 (1.0912)  time: 1.1341  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [120/431]  eta: 0:05:55  lr: 0.000162  loss: 1.0649 (1.0893)  time: 1.1331  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [130/431]  eta: 0:05:43  lr: 0.000162  loss: 1.0649 (1.0862)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [140/431]  eta: 0:05:31  lr: 0.000162  loss: 1.0770 (1.0852)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [150/431]  eta: 0:05:19  lr: 0.000162  loss: 1.0664 (1.0857)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [160/431]  eta: 0:05:07  lr: 0.000162  loss: 1.0538 (1.0903)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [170/431]  eta: 0:04:56  lr: 0.000162  loss: 1.0726 (1.0895)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [180/431]  eta: 0:04:45  lr: 0.000162  loss: 1.0778 (1.0911)  time: 1.1331  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [190/431]  eta: 0:04:33  lr: 0.000162  loss: 1.0778 (1.0916)  time: 1.1315  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [200/431]  eta: 0:04:22  lr: 0.000162  loss: 1.0560 (1.0920)  time: 1.1292  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [210/431]  eta: 0:04:10  lr: 0.000162  loss: 1.0241 (1.0908)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [220/431]  eta: 0:03:59  lr: 0.000162  loss: 1.0232 (1.0892)  time: 1.1180  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [230/431]  eta: 0:03:47  lr: 0.000162  loss: 1.0427 (1.0902)  time: 1.1180  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:272]  [240/431]  eta: 0:03:36  lr: 0.000162  loss: 1.0477 (1.0895)  time: 1.1239  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:272]  [250/431]  eta: 0:03:25  lr: 0.000162  loss: 1.0438 (1.0917)  time: 1.1372  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [260/431]  eta: 0:03:13  lr: 0.000162  loss: 1.1585 (1.0938)  time: 1.1410  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [270/431]  eta: 0:03:02  lr: 0.000162  loss: 1.1235 (1.0951)  time: 1.1378  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [280/431]  eta: 0:02:51  lr: 0.000162  loss: 1.1235 (1.0975)  time: 1.1352  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:272]  [290/431]  eta: 0:02:39  lr: 0.000162  loss: 1.1321 (1.0990)  time: 1.1269  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:272]  [300/431]  eta: 0:02:28  lr: 0.000162  loss: 1.1804 (1.1035)  time: 1.1229  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [310/431]  eta: 0:02:17  lr: 0.000162  loss: 1.1229 (1.1027)  time: 1.1233  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:272]  [320/431]  eta: 0:02:05  lr: 0.000162  loss: 1.0279 (1.1019)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [330/431]  eta: 0:01:54  lr: 0.000162  loss: 1.0995 (1.1027)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [340/431]  eta: 0:01:42  lr: 0.000162  loss: 1.1033 (1.1041)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [350/431]  eta: 0:01:31  lr: 0.000162  loss: 1.1165 (1.1041)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [360/431]  eta: 0:01:20  lr: 0.000162  loss: 1.0848 (1.1040)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [370/431]  eta: 0:01:08  lr: 0.000162  loss: 1.0511 (1.1038)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [380/431]  eta: 0:00:57  lr: 0.000162  loss: 1.1014 (1.1054)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [390/431]  eta: 0:00:46  lr: 0.000162  loss: 1.1325 (1.1051)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [400/431]  eta: 0:00:35  lr: 0.000162  loss: 1.0700 (1.1044)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:272]  [410/431]  eta: 0:00:23  lr: 0.000162  loss: 1.0700 (1.1050)  time: 1.1220  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [420/431]  eta: 0:00:12  lr: 0.000162  loss: 1.0786 (1.1045)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272]  [430/431]  eta: 0:00:01  lr: 0.000162  loss: 1.0560 (1.1033)  time: 1.1288  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:272] Total time: 0:08:06 (1.1298 s / it)\n",
      "Averaged stats: lr: 0.000162  loss: 1.0560 (1.1033)\n",
      "Valid: [epoch:272]  [ 0/14]  eta: 0:00:36  loss: 1.0170 (1.0170)  time: 2.6084  data: 2.4554  max mem: 15925\n",
      "Valid: [epoch:272]  [13/14]  eta: 0:00:00  loss: 1.0414 (1.0487)  time: 0.2739  data: 0.1755  max mem: 15925\n",
      "Valid: [epoch:272] Total time: 0:00:04 (0.2918 s / it)\n",
      "Averaged stats: loss: 1.0414 (1.0487)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_272_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:273]  [  0/431]  eta: 0:32:50  lr: 0.000162  loss: 0.9629 (0.9629)  time: 4.5713  data: 3.3844  max mem: 15925\n",
      "Train: [epoch:273]  [ 10/431]  eta: 0:09:44  lr: 0.000162  loss: 1.0765 (1.0445)  time: 1.3894  data: 0.3079  max mem: 15925\n",
      "Train: [epoch:273]  [ 20/431]  eta: 0:08:32  lr: 0.000162  loss: 1.0890 (1.0839)  time: 1.0812  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:273]  [ 30/431]  eta: 0:08:02  lr: 0.000162  loss: 1.1034 (1.0856)  time: 1.1003  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:273]  [ 40/431]  eta: 0:07:41  lr: 0.000162  loss: 1.0756 (1.0984)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [ 50/431]  eta: 0:07:24  lr: 0.000162  loss: 1.1211 (1.1103)  time: 1.1095  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:273]  [ 60/431]  eta: 0:07:09  lr: 0.000162  loss: 1.0616 (1.0985)  time: 1.1163  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:273]  [ 70/431]  eta: 0:06:56  lr: 0.000162  loss: 1.0488 (1.0988)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:273]  [ 80/431]  eta: 0:06:42  lr: 0.000162  loss: 1.1251 (1.1020)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [ 90/431]  eta: 0:06:29  lr: 0.000162  loss: 1.0993 (1.0968)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [100/431]  eta: 0:06:17  lr: 0.000162  loss: 1.0561 (1.0965)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [110/431]  eta: 0:06:05  lr: 0.000162  loss: 1.0502 (1.0944)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [120/431]  eta: 0:05:54  lr: 0.000162  loss: 1.0606 (1.0937)  time: 1.1290  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:273]  [130/431]  eta: 0:05:42  lr: 0.000162  loss: 1.0606 (1.0929)  time: 1.1196  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:273]  [140/431]  eta: 0:05:30  lr: 0.000162  loss: 1.0389 (1.0891)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [150/431]  eta: 0:05:18  lr: 0.000162  loss: 1.0387 (1.0908)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [160/431]  eta: 0:05:06  lr: 0.000162  loss: 1.0953 (1.0929)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [170/431]  eta: 0:04:55  lr: 0.000162  loss: 1.1185 (1.0921)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [180/431]  eta: 0:04:43  lr: 0.000162  loss: 1.1220 (1.0950)  time: 1.1123  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:273]  [190/431]  eta: 0:04:31  lr: 0.000162  loss: 1.1220 (1.0954)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [200/431]  eta: 0:04:20  lr: 0.000162  loss: 1.1024 (1.0963)  time: 1.0886  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [210/431]  eta: 0:04:08  lr: 0.000162  loss: 1.0737 (1.0964)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [220/431]  eta: 0:03:57  lr: 0.000162  loss: 1.1434 (1.0986)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [230/431]  eta: 0:03:45  lr: 0.000162  loss: 1.1604 (1.1005)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [240/431]  eta: 0:03:34  lr: 0.000162  loss: 1.1256 (1.1001)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [250/431]  eta: 0:03:22  lr: 0.000162  loss: 1.0812 (1.1013)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [260/431]  eta: 0:03:11  lr: 0.000162  loss: 1.0812 (1.0988)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [270/431]  eta: 0:03:00  lr: 0.000162  loss: 1.0464 (1.0991)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [280/431]  eta: 0:02:48  lr: 0.000162  loss: 1.0493 (1.0983)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [290/431]  eta: 0:02:37  lr: 0.000162  loss: 1.0413 (1.0980)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [300/431]  eta: 0:02:26  lr: 0.000162  loss: 1.0771 (1.1002)  time: 1.0905  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [310/431]  eta: 0:02:14  lr: 0.000162  loss: 1.0771 (1.0992)  time: 1.0816  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [320/431]  eta: 0:02:03  lr: 0.000162  loss: 1.0587 (1.0996)  time: 1.0845  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [330/431]  eta: 0:01:52  lr: 0.000162  loss: 1.1343 (1.1015)  time: 1.0788  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [340/431]  eta: 0:01:41  lr: 0.000162  loss: 1.1523 (1.1036)  time: 1.0760  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [350/431]  eta: 0:01:29  lr: 0.000162  loss: 1.1192 (1.1041)  time: 1.0740  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [360/431]  eta: 0:01:18  lr: 0.000162  loss: 1.0622 (1.1028)  time: 1.0787  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [370/431]  eta: 0:01:07  lr: 0.000162  loss: 1.0503 (1.1025)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [380/431]  eta: 0:00:56  lr: 0.000162  loss: 1.0974 (1.1032)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [390/431]  eta: 0:00:45  lr: 0.000162  loss: 1.0835 (1.1025)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [400/431]  eta: 0:00:34  lr: 0.000162  loss: 1.1339 (1.1038)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [410/431]  eta: 0:00:23  lr: 0.000162  loss: 1.1431 (1.1043)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [420/431]  eta: 0:00:12  lr: 0.000162  loss: 1.0802 (1.1041)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273]  [430/431]  eta: 0:00:01  lr: 0.000162  loss: 1.1194 (1.1042)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:273] Total time: 0:07:58 (1.1113 s / it)\n",
      "Averaged stats: lr: 0.000162  loss: 1.1194 (1.1042)\n",
      "Valid: [epoch:273]  [ 0/14]  eta: 0:00:37  loss: 1.0900 (1.0900)  time: 2.6676  data: 2.4646  max mem: 15925\n",
      "Valid: [epoch:273]  [13/14]  eta: 0:00:00  loss: 1.0385 (1.0463)  time: 0.2975  data: 0.1761  max mem: 15925\n",
      "Valid: [epoch:273] Total time: 0:00:04 (0.3146 s / it)\n",
      "Averaged stats: loss: 1.0385 (1.0463)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_273_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:274]  [  0/431]  eta: 0:34:20  lr: 0.000162  loss: 1.0471 (1.0471)  time: 4.7800  data: 3.6010  max mem: 15925\n",
      "Train: [epoch:274]  [ 10/431]  eta: 0:09:48  lr: 0.000162  loss: 1.0944 (1.1519)  time: 1.3971  data: 0.3276  max mem: 15925\n",
      "Train: [epoch:274]  [ 20/431]  eta: 0:08:32  lr: 0.000162  loss: 1.0877 (1.1456)  time: 1.0711  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [ 30/431]  eta: 0:07:59  lr: 0.000162  loss: 1.0711 (1.1363)  time: 1.0846  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [ 40/431]  eta: 0:07:37  lr: 0.000162  loss: 1.0665 (1.1327)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [ 50/431]  eta: 0:07:20  lr: 0.000162  loss: 1.0632 (1.1328)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [ 60/431]  eta: 0:07:06  lr: 0.000162  loss: 1.0744 (1.1183)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [ 70/431]  eta: 0:06:53  lr: 0.000162  loss: 1.0744 (1.1159)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [ 80/431]  eta: 0:06:40  lr: 0.000162  loss: 1.0774 (1.1168)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [ 90/431]  eta: 0:06:27  lr: 0.000162  loss: 1.1084 (1.1164)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [100/431]  eta: 0:06:15  lr: 0.000162  loss: 1.1084 (1.1174)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [110/431]  eta: 0:06:03  lr: 0.000162  loss: 1.1398 (1.1196)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [120/431]  eta: 0:05:52  lr: 0.000162  loss: 1.0679 (1.1149)  time: 1.1320  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [130/431]  eta: 0:05:41  lr: 0.000162  loss: 1.0583 (1.1143)  time: 1.1417  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [140/431]  eta: 0:05:29  lr: 0.000162  loss: 1.0852 (1.1137)  time: 1.1307  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [150/431]  eta: 0:05:18  lr: 0.000162  loss: 1.0633 (1.1080)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [160/431]  eta: 0:05:06  lr: 0.000162  loss: 1.0633 (1.1092)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [170/431]  eta: 0:04:54  lr: 0.000162  loss: 1.0786 (1.1077)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [180/431]  eta: 0:04:43  lr: 0.000162  loss: 1.0154 (1.1024)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [190/431]  eta: 0:04:31  lr: 0.000162  loss: 1.0097 (1.1016)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [200/431]  eta: 0:04:20  lr: 0.000162  loss: 1.0116 (1.0972)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [210/431]  eta: 0:04:08  lr: 0.000162  loss: 1.0275 (1.0966)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [220/431]  eta: 0:03:57  lr: 0.000162  loss: 1.1013 (1.0972)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [230/431]  eta: 0:03:46  lr: 0.000162  loss: 1.1212 (1.0985)  time: 1.1144  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:274]  [240/431]  eta: 0:03:34  lr: 0.000162  loss: 1.1763 (1.1011)  time: 1.1135  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:274]  [250/431]  eta: 0:03:23  lr: 0.000162  loss: 1.1375 (1.1007)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [260/431]  eta: 0:03:11  lr: 0.000162  loss: 1.0687 (1.1015)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [270/431]  eta: 0:03:00  lr: 0.000162  loss: 1.0687 (1.1006)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [280/431]  eta: 0:02:49  lr: 0.000162  loss: 1.0669 (1.0996)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [290/431]  eta: 0:02:37  lr: 0.000162  loss: 1.0687 (1.0998)  time: 1.1102  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:274]  [300/431]  eta: 0:02:26  lr: 0.000162  loss: 1.0826 (1.1002)  time: 1.1195  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [310/431]  eta: 0:02:15  lr: 0.000162  loss: 1.0798 (1.0994)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [320/431]  eta: 0:02:04  lr: 0.000162  loss: 1.0997 (1.1012)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [330/431]  eta: 0:01:53  lr: 0.000162  loss: 1.1297 (1.1021)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [340/431]  eta: 0:01:41  lr: 0.000162  loss: 1.0978 (1.1030)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [350/431]  eta: 0:01:30  lr: 0.000162  loss: 1.1006 (1.1055)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [360/431]  eta: 0:01:19  lr: 0.000162  loss: 1.0953 (1.1038)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [370/431]  eta: 0:01:08  lr: 0.000162  loss: 1.0295 (1.1043)  time: 1.1376  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [380/431]  eta: 0:00:57  lr: 0.000162  loss: 1.0671 (1.1041)  time: 1.1287  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [390/431]  eta: 0:00:45  lr: 0.000162  loss: 1.1079 (1.1037)  time: 1.1183  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:274]  [400/431]  eta: 0:00:34  lr: 0.000162  loss: 1.0736 (1.1036)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [410/431]  eta: 0:00:23  lr: 0.000162  loss: 1.0736 (1.1050)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [420/431]  eta: 0:00:12  lr: 0.000162  loss: 1.1469 (1.1068)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274]  [430/431]  eta: 0:00:01  lr: 0.000162  loss: 1.0606 (1.1046)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:274] Total time: 0:08:02 (1.1197 s / it)\n",
      "Averaged stats: lr: 0.000162  loss: 1.0606 (1.1046)\n",
      "Valid: [epoch:274]  [ 0/14]  eta: 0:00:37  loss: 1.0869 (1.0869)  time: 2.6773  data: 2.5507  max mem: 15925\n",
      "Valid: [epoch:274]  [13/14]  eta: 0:00:00  loss: 1.0321 (1.0423)  time: 0.2946  data: 0.1823  max mem: 15925\n",
      "Valid: [epoch:274] Total time: 0:00:04 (0.3130 s / it)\n",
      "Averaged stats: loss: 1.0321 (1.0423)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_274_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:275]  [  0/431]  eta: 0:29:44  lr: 0.000161  loss: 1.0604 (1.0604)  time: 4.1409  data: 2.9390  max mem: 15925\n",
      "Train: [epoch:275]  [ 10/431]  eta: 0:09:17  lr: 0.000161  loss: 1.1094 (1.1383)  time: 1.3248  data: 0.2674  max mem: 15925\n",
      "Train: [epoch:275]  [ 20/431]  eta: 0:08:13  lr: 0.000161  loss: 1.1094 (1.1514)  time: 1.0526  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:275]  [ 30/431]  eta: 0:07:44  lr: 0.000161  loss: 1.0920 (1.1325)  time: 1.0687  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [ 40/431]  eta: 0:07:29  lr: 0.000161  loss: 1.0884 (1.1112)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [ 50/431]  eta: 0:07:12  lr: 0.000161  loss: 1.0303 (1.1056)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [ 60/431]  eta: 0:07:00  lr: 0.000161  loss: 1.0281 (1.0923)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [ 70/431]  eta: 0:06:47  lr: 0.000161  loss: 1.0281 (1.0850)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [ 80/431]  eta: 0:06:36  lr: 0.000161  loss: 1.0659 (1.0898)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [ 90/431]  eta: 0:06:24  lr: 0.000161  loss: 1.0843 (1.0893)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [100/431]  eta: 0:06:12  lr: 0.000161  loss: 1.0676 (1.0872)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [110/431]  eta: 0:06:01  lr: 0.000161  loss: 1.0933 (1.0897)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [120/431]  eta: 0:05:50  lr: 0.000161  loss: 1.0933 (1.0885)  time: 1.1282  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [130/431]  eta: 0:05:39  lr: 0.000161  loss: 1.0391 (1.0848)  time: 1.1255  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:275]  [140/431]  eta: 0:05:27  lr: 0.000161  loss: 1.0406 (1.0856)  time: 1.1289  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [150/431]  eta: 0:05:16  lr: 0.000161  loss: 1.1038 (1.0858)  time: 1.1271  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:275]  [160/431]  eta: 0:05:05  lr: 0.000161  loss: 1.1122 (1.0888)  time: 1.1259  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:275]  [170/431]  eta: 0:04:54  lr: 0.000161  loss: 1.0677 (1.0862)  time: 1.1325  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [180/431]  eta: 0:04:42  lr: 0.000161  loss: 1.0677 (1.0907)  time: 1.1244  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:275]  [190/431]  eta: 0:04:31  lr: 0.000161  loss: 1.0835 (1.0899)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [200/431]  eta: 0:04:19  lr: 0.000161  loss: 1.1406 (1.0927)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [210/431]  eta: 0:04:08  lr: 0.000161  loss: 1.0679 (1.0901)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [220/431]  eta: 0:03:56  lr: 0.000161  loss: 1.0674 (1.0936)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [230/431]  eta: 0:03:45  lr: 0.000161  loss: 1.1414 (1.0954)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [240/431]  eta: 0:03:34  lr: 0.000161  loss: 1.1411 (1.0991)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [250/431]  eta: 0:03:23  lr: 0.000161  loss: 1.0561 (1.0965)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [260/431]  eta: 0:03:11  lr: 0.000161  loss: 1.0217 (1.0938)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [270/431]  eta: 0:03:00  lr: 0.000161  loss: 1.0384 (1.0944)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [280/431]  eta: 0:02:49  lr: 0.000161  loss: 1.1207 (1.0958)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [290/431]  eta: 0:02:38  lr: 0.000161  loss: 1.1042 (1.0962)  time: 1.1164  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:275]  [300/431]  eta: 0:02:26  lr: 0.000161  loss: 1.0660 (1.0968)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [310/431]  eta: 0:02:15  lr: 0.000161  loss: 1.1031 (1.0970)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [320/431]  eta: 0:02:04  lr: 0.000161  loss: 1.1073 (1.0970)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [330/431]  eta: 0:01:52  lr: 0.000161  loss: 1.1360 (1.1003)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [340/431]  eta: 0:01:41  lr: 0.000161  loss: 1.1220 (1.1012)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [350/431]  eta: 0:01:30  lr: 0.000161  loss: 1.1220 (1.1029)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [360/431]  eta: 0:01:19  lr: 0.000161  loss: 1.1178 (1.1031)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [370/431]  eta: 0:01:08  lr: 0.000161  loss: 1.0791 (1.1034)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [380/431]  eta: 0:00:57  lr: 0.000161  loss: 1.0693 (1.1024)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [390/431]  eta: 0:00:45  lr: 0.000161  loss: 1.0693 (1.1020)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [400/431]  eta: 0:00:34  lr: 0.000161  loss: 1.0768 (1.1030)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [410/431]  eta: 0:00:23  lr: 0.000161  loss: 1.0675 (1.1029)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:275]  [420/431]  eta: 0:00:12  lr: 0.000161  loss: 1.0442 (1.1031)  time: 1.1029  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:275]  [430/431]  eta: 0:00:01  lr: 0.000161  loss: 1.0592 (1.1031)  time: 1.1025  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:275] Total time: 0:08:01 (1.1171 s / it)\n",
      "Averaged stats: lr: 0.000161  loss: 1.0592 (1.1031)\n",
      "Valid: [epoch:275]  [ 0/14]  eta: 0:00:35  loss: 1.0800 (1.0800)  time: 2.5057  data: 2.3489  max mem: 15925\n",
      "Valid: [epoch:275]  [13/14]  eta: 0:00:00  loss: 1.0343 (1.0434)  time: 0.2708  data: 0.1679  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:275] Total time: 0:00:04 (0.2872 s / it)\n",
      "Averaged stats: loss: 1.0343 (1.0434)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_275_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:276]  [  0/431]  eta: 0:35:48  lr: 0.000161  loss: 1.5138 (1.5138)  time: 4.9839  data: 3.7265  max mem: 15925\n",
      "Train: [epoch:276]  [ 10/431]  eta: 0:10:02  lr: 0.000161  loss: 1.1021 (1.1314)  time: 1.4313  data: 0.3390  max mem: 15925\n",
      "Train: [epoch:276]  [ 20/431]  eta: 0:08:33  lr: 0.000161  loss: 1.1021 (1.1473)  time: 1.0634  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [ 30/431]  eta: 0:08:02  lr: 0.000161  loss: 1.1252 (1.1431)  time: 1.0768  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [ 40/431]  eta: 0:07:39  lr: 0.000161  loss: 1.0865 (1.1279)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [ 50/431]  eta: 0:07:21  lr: 0.000161  loss: 1.0864 (1.1244)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [ 60/431]  eta: 0:07:07  lr: 0.000161  loss: 1.0598 (1.1081)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [ 70/431]  eta: 0:06:53  lr: 0.000161  loss: 1.0548 (1.1072)  time: 1.1153  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:276]  [ 80/431]  eta: 0:06:40  lr: 0.000161  loss: 1.0669 (1.1082)  time: 1.1107  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:276]  [ 90/431]  eta: 0:06:27  lr: 0.000161  loss: 1.0435 (1.0995)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [100/431]  eta: 0:06:15  lr: 0.000161  loss: 1.0712 (1.1078)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [110/431]  eta: 0:06:04  lr: 0.000161  loss: 1.1198 (1.1071)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [120/431]  eta: 0:05:52  lr: 0.000161  loss: 1.0735 (1.1059)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [130/431]  eta: 0:05:40  lr: 0.000161  loss: 1.0726 (1.1082)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [140/431]  eta: 0:05:28  lr: 0.000161  loss: 1.0605 (1.1050)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [150/431]  eta: 0:05:17  lr: 0.000161  loss: 1.0320 (1.1030)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [160/431]  eta: 0:05:05  lr: 0.000161  loss: 1.0705 (1.1014)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [170/431]  eta: 0:04:53  lr: 0.000161  loss: 1.0759 (1.0999)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [180/431]  eta: 0:04:42  lr: 0.000161  loss: 1.0532 (1.0996)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [190/431]  eta: 0:04:31  lr: 0.000161  loss: 1.0532 (1.0996)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [200/431]  eta: 0:04:19  lr: 0.000161  loss: 1.0961 (1.1014)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [210/431]  eta: 0:04:08  lr: 0.000161  loss: 1.0961 (1.1023)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [220/431]  eta: 0:03:57  lr: 0.000161  loss: 1.0499 (1.0993)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [230/431]  eta: 0:03:45  lr: 0.000161  loss: 1.0499 (1.0979)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [240/431]  eta: 0:03:34  lr: 0.000161  loss: 1.0647 (1.0986)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [250/431]  eta: 0:03:23  lr: 0.000161  loss: 1.0654 (1.0994)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [260/431]  eta: 0:03:11  lr: 0.000161  loss: 1.0891 (1.0999)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [270/431]  eta: 0:03:00  lr: 0.000161  loss: 1.1184 (1.1012)  time: 1.1187  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:276]  [280/431]  eta: 0:02:49  lr: 0.000161  loss: 1.1184 (1.1015)  time: 1.1200  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:276]  [290/431]  eta: 0:02:38  lr: 0.000161  loss: 1.0861 (1.1008)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [300/431]  eta: 0:02:26  lr: 0.000161  loss: 1.0694 (1.1003)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [310/431]  eta: 0:02:15  lr: 0.000161  loss: 1.0913 (1.1003)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [320/431]  eta: 0:02:04  lr: 0.000161  loss: 1.0798 (1.1009)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [330/431]  eta: 0:01:53  lr: 0.000161  loss: 1.1585 (1.1047)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [340/431]  eta: 0:01:42  lr: 0.000161  loss: 1.1527 (1.1047)  time: 1.1425  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [350/431]  eta: 0:01:30  lr: 0.000161  loss: 1.1346 (1.1058)  time: 1.1410  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:276]  [360/431]  eta: 0:01:19  lr: 0.000161  loss: 1.0484 (1.1042)  time: 1.1278  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [370/431]  eta: 0:01:08  lr: 0.000161  loss: 0.9975 (1.1033)  time: 1.1336  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [380/431]  eta: 0:00:57  lr: 0.000161  loss: 1.0755 (1.1042)  time: 1.1263  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [390/431]  eta: 0:00:46  lr: 0.000161  loss: 1.0682 (1.1027)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [400/431]  eta: 0:00:34  lr: 0.000161  loss: 1.0586 (1.1029)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [410/431]  eta: 0:00:23  lr: 0.000161  loss: 1.1353 (1.1031)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:276]  [420/431]  eta: 0:00:12  lr: 0.000161  loss: 1.1353 (1.1043)  time: 1.1103  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:276]  [430/431]  eta: 0:00:01  lr: 0.000161  loss: 1.0893 (1.1033)  time: 1.1198  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:276] Total time: 0:08:03 (1.1225 s / it)\n",
      "Averaged stats: lr: 0.000161  loss: 1.0893 (1.1033)\n",
      "Valid: [epoch:276]  [ 0/14]  eta: 0:00:36  loss: 1.0832 (1.0832)  time: 2.6392  data: 2.4429  max mem: 15925\n",
      "Valid: [epoch:276]  [13/14]  eta: 0:00:00  loss: 1.0408 (1.0477)  time: 0.2862  data: 0.1746  max mem: 15925\n",
      "Valid: [epoch:276] Total time: 0:00:04 (0.3026 s / it)\n",
      "Averaged stats: loss: 1.0408 (1.0477)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_276_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.048%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:277]  [  0/431]  eta: 0:33:35  lr: 0.000161  loss: 1.2004 (1.2004)  time: 4.6765  data: 3.3187  max mem: 15925\n",
      "Train: [epoch:277]  [ 10/431]  eta: 0:09:34  lr: 0.000161  loss: 1.1027 (1.1388)  time: 1.3637  data: 0.3020  max mem: 15925\n",
      "Train: [epoch:277]  [ 20/431]  eta: 0:08:25  lr: 0.000161  loss: 1.0888 (1.1227)  time: 1.0574  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [ 30/431]  eta: 0:07:55  lr: 0.000161  loss: 1.0556 (1.1171)  time: 1.0894  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [ 40/431]  eta: 0:07:37  lr: 0.000161  loss: 1.0473 (1.1152)  time: 1.1092  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [ 50/431]  eta: 0:07:20  lr: 0.000161  loss: 1.0473 (1.1064)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [ 60/431]  eta: 0:07:05  lr: 0.000161  loss: 1.0463 (1.0956)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [ 70/431]  eta: 0:06:51  lr: 0.000161  loss: 1.0706 (1.0976)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [ 80/431]  eta: 0:06:39  lr: 0.000161  loss: 1.1327 (1.1027)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [ 90/431]  eta: 0:06:28  lr: 0.000161  loss: 1.1200 (1.1032)  time: 1.1286  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [100/431]  eta: 0:06:16  lr: 0.000161  loss: 1.0105 (1.0987)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [110/431]  eta: 0:06:04  lr: 0.000161  loss: 1.0413 (1.0947)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [120/431]  eta: 0:05:52  lr: 0.000161  loss: 1.0691 (1.0929)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [130/431]  eta: 0:05:40  lr: 0.000161  loss: 1.1042 (1.0960)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [140/431]  eta: 0:05:28  lr: 0.000161  loss: 1.1016 (1.0944)  time: 1.1105  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:277]  [150/431]  eta: 0:05:17  lr: 0.000161  loss: 1.0590 (1.0930)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [160/431]  eta: 0:05:05  lr: 0.000161  loss: 1.0690 (1.0957)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [170/431]  eta: 0:04:54  lr: 0.000161  loss: 1.1241 (1.0963)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [180/431]  eta: 0:04:42  lr: 0.000161  loss: 1.1488 (1.1037)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [190/431]  eta: 0:04:31  lr: 0.000161  loss: 1.1689 (1.1041)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [200/431]  eta: 0:04:19  lr: 0.000161  loss: 1.0759 (1.1033)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [210/431]  eta: 0:04:08  lr: 0.000161  loss: 1.0537 (1.1008)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [220/431]  eta: 0:03:57  lr: 0.000161  loss: 1.0511 (1.1002)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [230/431]  eta: 0:03:45  lr: 0.000161  loss: 1.0842 (1.1004)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [240/431]  eta: 0:03:34  lr: 0.000161  loss: 1.1171 (1.1026)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [250/431]  eta: 0:03:23  lr: 0.000161  loss: 1.1339 (1.1038)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [260/431]  eta: 0:03:12  lr: 0.000161  loss: 1.1095 (1.1054)  time: 1.1171  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [270/431]  eta: 0:03:00  lr: 0.000161  loss: 1.0811 (1.1037)  time: 1.1300  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [280/431]  eta: 0:02:49  lr: 0.000161  loss: 1.0592 (1.1048)  time: 1.1371  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [290/431]  eta: 0:02:38  lr: 0.000161  loss: 1.0592 (1.1036)  time: 1.1299  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [300/431]  eta: 0:02:27  lr: 0.000161  loss: 1.0959 (1.1061)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [310/431]  eta: 0:02:15  lr: 0.000161  loss: 1.0905 (1.1059)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [320/431]  eta: 0:02:04  lr: 0.000161  loss: 1.0791 (1.1055)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [330/431]  eta: 0:01:53  lr: 0.000161  loss: 1.1206 (1.1075)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [340/431]  eta: 0:01:42  lr: 0.000161  loss: 1.0923 (1.1080)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [350/431]  eta: 0:01:30  lr: 0.000161  loss: 1.0414 (1.1086)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [360/431]  eta: 0:01:19  lr: 0.000161  loss: 1.0525 (1.1081)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [370/431]  eta: 0:01:08  lr: 0.000161  loss: 1.0488 (1.1068)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [380/431]  eta: 0:00:57  lr: 0.000161  loss: 1.0546 (1.1064)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [390/431]  eta: 0:00:45  lr: 0.000161  loss: 1.1106 (1.1069)  time: 1.1137  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [400/431]  eta: 0:00:34  lr: 0.000161  loss: 1.1174 (1.1062)  time: 1.1232  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:277]  [410/431]  eta: 0:00:23  lr: 0.000161  loss: 1.0118 (1.1053)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [420/431]  eta: 0:00:12  lr: 0.000161  loss: 1.0822 (1.1046)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277]  [430/431]  eta: 0:00:01  lr: 0.000161  loss: 1.0474 (1.1039)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:277] Total time: 0:08:02 (1.1202 s / it)\n",
      "Averaged stats: lr: 0.000161  loss: 1.0474 (1.1039)\n",
      "Valid: [epoch:277]  [ 0/14]  eta: 0:00:36  loss: 1.0047 (1.0047)  time: 2.6277  data: 2.4546  max mem: 15925\n",
      "Valid: [epoch:277]  [13/14]  eta: 0:00:00  loss: 1.0333 (1.0431)  time: 0.2932  data: 0.1754  max mem: 15925\n",
      "Valid: [epoch:277] Total time: 0:00:04 (0.3115 s / it)\n",
      "Averaged stats: loss: 1.0333 (1.0431)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_277_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:278]  [  0/431]  eta: 0:36:25  lr: 0.000161  loss: 1.0074 (1.0074)  time: 5.0697  data: 3.9350  max mem: 15925\n",
      "Train: [epoch:278]  [ 10/431]  eta: 0:09:59  lr: 0.000161  loss: 1.2478 (1.2163)  time: 1.4243  data: 0.3579  max mem: 15925\n",
      "Train: [epoch:278]  [ 20/431]  eta: 0:08:40  lr: 0.000161  loss: 1.1258 (1.1490)  time: 1.0754  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [ 30/431]  eta: 0:08:08  lr: 0.000161  loss: 1.0545 (1.1480)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [ 40/431]  eta: 0:07:45  lr: 0.000161  loss: 1.0916 (1.1334)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [ 50/431]  eta: 0:07:27  lr: 0.000161  loss: 1.0970 (1.1306)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [ 60/431]  eta: 0:07:11  lr: 0.000161  loss: 1.0903 (1.1201)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [ 70/431]  eta: 0:06:56  lr: 0.000161  loss: 1.0823 (1.1195)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [ 80/431]  eta: 0:06:43  lr: 0.000161  loss: 1.0949 (1.1200)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [ 90/431]  eta: 0:06:31  lr: 0.000161  loss: 1.0705 (1.1175)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [100/431]  eta: 0:06:17  lr: 0.000161  loss: 1.0823 (1.1138)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [110/431]  eta: 0:06:05  lr: 0.000161  loss: 1.0373 (1.1064)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [120/431]  eta: 0:05:54  lr: 0.000161  loss: 1.0089 (1.1046)  time: 1.1231  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [130/431]  eta: 0:05:42  lr: 0.000161  loss: 1.0134 (1.1019)  time: 1.1193  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [140/431]  eta: 0:05:30  lr: 0.000161  loss: 1.0778 (1.1008)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [150/431]  eta: 0:05:19  lr: 0.000161  loss: 1.0802 (1.1021)  time: 1.1266  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [160/431]  eta: 0:05:07  lr: 0.000161  loss: 1.0802 (1.1045)  time: 1.1318  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [170/431]  eta: 0:04:56  lr: 0.000161  loss: 1.1012 (1.1039)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [180/431]  eta: 0:04:45  lr: 0.000161  loss: 1.1012 (1.1052)  time: 1.1407  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [190/431]  eta: 0:04:33  lr: 0.000161  loss: 1.0931 (1.1065)  time: 1.1356  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [200/431]  eta: 0:04:22  lr: 0.000161  loss: 1.0832 (1.1059)  time: 1.1326  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [210/431]  eta: 0:04:10  lr: 0.000161  loss: 1.0465 (1.1056)  time: 1.1345  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [220/431]  eta: 0:03:59  lr: 0.000161  loss: 1.0824 (1.1068)  time: 1.1149  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [230/431]  eta: 0:03:47  lr: 0.000161  loss: 1.1018 (1.1073)  time: 1.1184  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [240/431]  eta: 0:03:36  lr: 0.000161  loss: 1.1018 (1.1090)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [250/431]  eta: 0:03:24  lr: 0.000161  loss: 1.1603 (1.1117)  time: 1.1219  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [260/431]  eta: 0:03:13  lr: 0.000161  loss: 1.1484 (1.1126)  time: 1.1182  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [270/431]  eta: 0:03:02  lr: 0.000161  loss: 1.0574 (1.1137)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [280/431]  eta: 0:02:50  lr: 0.000161  loss: 1.0480 (1.1135)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [290/431]  eta: 0:02:39  lr: 0.000161  loss: 1.0006 (1.1108)  time: 1.1262  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [300/431]  eta: 0:02:28  lr: 0.000161  loss: 1.0578 (1.1117)  time: 1.1345  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [310/431]  eta: 0:02:16  lr: 0.000161  loss: 1.0467 (1.1086)  time: 1.1276  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:278]  [320/431]  eta: 0:02:05  lr: 0.000161  loss: 1.0093 (1.1057)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [330/431]  eta: 0:01:54  lr: 0.000161  loss: 1.0344 (1.1060)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [340/431]  eta: 0:01:42  lr: 0.000161  loss: 1.1048 (1.1060)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [350/431]  eta: 0:01:31  lr: 0.000161  loss: 1.0842 (1.1051)  time: 1.1210  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [360/431]  eta: 0:01:20  lr: 0.000161  loss: 1.0624 (1.1040)  time: 1.1237  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:278]  [370/431]  eta: 0:01:08  lr: 0.000161  loss: 1.0889 (1.1050)  time: 1.1278  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:278]  [380/431]  eta: 0:00:57  lr: 0.000161  loss: 1.0974 (1.1044)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [390/431]  eta: 0:00:46  lr: 0.000161  loss: 1.0699 (1.1042)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [400/431]  eta: 0:00:34  lr: 0.000161  loss: 1.1287 (1.1061)  time: 1.1261  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:278]  [410/431]  eta: 0:00:23  lr: 0.000161  loss: 1.1134 (1.1052)  time: 1.1306  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [420/431]  eta: 0:00:12  lr: 0.000161  loss: 1.0531 (1.1051)  time: 1.1371  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278]  [430/431]  eta: 0:00:01  lr: 0.000161  loss: 1.0581 (1.1044)  time: 1.1354  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:278] Total time: 0:08:07 (1.1300 s / it)\n",
      "Averaged stats: lr: 0.000161  loss: 1.0581 (1.1044)\n",
      "Valid: [epoch:278]  [ 0/14]  eta: 0:00:35  loss: 1.0073 (1.0073)  time: 2.5190  data: 2.3625  max mem: 15925\n",
      "Valid: [epoch:278]  [13/14]  eta: 0:00:00  loss: 1.0371 (1.0458)  time: 0.2736  data: 0.1689  max mem: 15925\n",
      "Valid: [epoch:278] Total time: 0:00:04 (0.2902 s / it)\n",
      "Averaged stats: loss: 1.0371 (1.0458)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_278_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n",
      "Train: [epoch:279]  [  0/431]  eta: 0:32:54  lr: 0.000160  loss: 1.0983 (1.0983)  time: 4.5822  data: 3.2378  max mem: 15925\n",
      "Train: [epoch:279]  [ 10/431]  eta: 0:09:36  lr: 0.000160  loss: 1.1388 (1.1467)  time: 1.3682  data: 0.2946  max mem: 15925\n",
      "Train: [epoch:279]  [ 20/431]  eta: 0:08:23  lr: 0.000160  loss: 1.1294 (1.1345)  time: 1.0575  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:279]  [ 30/431]  eta: 0:07:52  lr: 0.000160  loss: 1.0716 (1.1066)  time: 1.0756  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [ 40/431]  eta: 0:07:33  lr: 0.000160  loss: 1.0430 (1.1020)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [ 50/431]  eta: 0:07:17  lr: 0.000160  loss: 1.0770 (1.0980)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [ 60/431]  eta: 0:07:02  lr: 0.000160  loss: 1.0938 (1.1001)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [ 70/431]  eta: 0:06:49  lr: 0.000160  loss: 1.1070 (1.1126)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [ 80/431]  eta: 0:06:37  lr: 0.000160  loss: 1.1248 (1.1125)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [ 90/431]  eta: 0:06:25  lr: 0.000160  loss: 1.0839 (1.1090)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [100/431]  eta: 0:06:12  lr: 0.000160  loss: 1.0839 (1.1060)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [110/431]  eta: 0:06:01  lr: 0.000160  loss: 1.0747 (1.1046)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [120/431]  eta: 0:05:50  lr: 0.000160  loss: 1.0747 (1.1023)  time: 1.1209  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:279]  [130/431]  eta: 0:05:39  lr: 0.000160  loss: 1.0601 (1.0984)  time: 1.1309  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:279]  [140/431]  eta: 0:05:27  lr: 0.000160  loss: 1.0639 (1.1004)  time: 1.1309  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [150/431]  eta: 0:05:16  lr: 0.000160  loss: 1.0432 (1.0987)  time: 1.1295  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:279]  [160/431]  eta: 0:05:05  lr: 0.000160  loss: 1.0679 (1.0990)  time: 1.1347  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [170/431]  eta: 0:04:54  lr: 0.000160  loss: 1.0954 (1.1007)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [180/431]  eta: 0:04:42  lr: 0.000160  loss: 1.1006 (1.1010)  time: 1.1212  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:279]  [190/431]  eta: 0:04:31  lr: 0.000160  loss: 1.1027 (1.1029)  time: 1.1310  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:279]  [200/431]  eta: 0:04:20  lr: 0.000160  loss: 1.0934 (1.1004)  time: 1.1279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [210/431]  eta: 0:04:08  lr: 0.000160  loss: 1.0305 (1.0988)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [220/431]  eta: 0:03:57  lr: 0.000160  loss: 1.0494 (1.0991)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [230/431]  eta: 0:03:45  lr: 0.000160  loss: 1.0583 (1.0975)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [240/431]  eta: 0:03:34  lr: 0.000160  loss: 1.0583 (1.0993)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [250/431]  eta: 0:03:23  lr: 0.000160  loss: 1.0495 (1.0982)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [260/431]  eta: 0:03:11  lr: 0.000160  loss: 1.0626 (1.0991)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [270/431]  eta: 0:03:00  lr: 0.000160  loss: 1.0804 (1.0993)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [280/431]  eta: 0:02:49  lr: 0.000160  loss: 1.0849 (1.1005)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [290/431]  eta: 0:02:37  lr: 0.000160  loss: 1.0965 (1.1018)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [300/431]  eta: 0:02:26  lr: 0.000160  loss: 1.1504 (1.1045)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [310/431]  eta: 0:02:15  lr: 0.000160  loss: 1.1056 (1.1034)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [320/431]  eta: 0:02:04  lr: 0.000160  loss: 1.0956 (1.1043)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [330/431]  eta: 0:01:53  lr: 0.000160  loss: 1.1048 (1.1042)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [340/431]  eta: 0:01:41  lr: 0.000160  loss: 1.1206 (1.1046)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [350/431]  eta: 0:01:30  lr: 0.000160  loss: 1.1208 (1.1052)  time: 1.1326  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [360/431]  eta: 0:01:19  lr: 0.000160  loss: 1.0997 (1.1057)  time: 1.1373  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [370/431]  eta: 0:01:08  lr: 0.000160  loss: 1.0336 (1.1054)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [380/431]  eta: 0:00:57  lr: 0.000160  loss: 1.0772 (1.1054)  time: 1.1171  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:279]  [390/431]  eta: 0:00:45  lr: 0.000160  loss: 1.1076 (1.1058)  time: 1.1220  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:279]  [400/431]  eta: 0:00:34  lr: 0.000160  loss: 1.0541 (1.1041)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [410/431]  eta: 0:00:23  lr: 0.000160  loss: 1.0432 (1.1037)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [420/431]  eta: 0:00:12  lr: 0.000160  loss: 1.0917 (1.1032)  time: 1.1186  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:279]  [430/431]  eta: 0:00:01  lr: 0.000160  loss: 1.0687 (1.1031)  time: 1.1051  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:279] Total time: 0:08:03 (1.1208 s / it)\n",
      "Averaged stats: lr: 0.000160  loss: 1.0687 (1.1031)\n",
      "Valid: [epoch:279]  [ 0/14]  eta: 0:00:34  loss: 1.0348 (1.0348)  time: 2.4542  data: 2.2837  max mem: 15925\n",
      "Valid: [epoch:279]  [13/14]  eta: 0:00:00  loss: 1.0446 (1.0505)  time: 0.2580  data: 0.1632  max mem: 15925\n",
      "Valid: [epoch:279] Total time: 0:00:03 (0.2751 s / it)\n",
      "Averaged stats: loss: 1.0446 (1.0505)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_279_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.042\n",
      "Best Epoch: 265.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:280]  [  0/431]  eta: 0:35:18  lr: 0.000160  loss: 1.0986 (1.0986)  time: 4.9149  data: 3.8172  max mem: 15925\n",
      "Train: [epoch:280]  [ 10/431]  eta: 0:09:54  lr: 0.000160  loss: 1.0986 (1.1032)  time: 1.4130  data: 0.3473  max mem: 15925\n",
      "Train: [epoch:280]  [ 20/431]  eta: 0:08:34  lr: 0.000160  loss: 1.0772 (1.0742)  time: 1.0680  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [ 30/431]  eta: 0:08:00  lr: 0.000160  loss: 1.0078 (1.0545)  time: 1.0803  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [ 40/431]  eta: 0:07:39  lr: 0.000160  loss: 1.0145 (1.0595)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [ 50/431]  eta: 0:07:23  lr: 0.000160  loss: 1.1082 (1.0737)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [ 60/431]  eta: 0:07:08  lr: 0.000160  loss: 1.0980 (1.0726)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [ 70/431]  eta: 0:06:56  lr: 0.000160  loss: 1.0980 (1.0854)  time: 1.1245  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [ 80/431]  eta: 0:06:43  lr: 0.000160  loss: 1.1173 (1.0924)  time: 1.1300  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [ 90/431]  eta: 0:06:31  lr: 0.000160  loss: 1.1071 (1.0908)  time: 1.1257  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [100/431]  eta: 0:06:19  lr: 0.000160  loss: 1.0904 (1.0908)  time: 1.1301  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [110/431]  eta: 0:06:07  lr: 0.000160  loss: 1.0768 (1.0927)  time: 1.1412  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [120/431]  eta: 0:05:55  lr: 0.000160  loss: 1.0930 (1.0952)  time: 1.1380  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:280]  [130/431]  eta: 0:05:44  lr: 0.000160  loss: 1.0372 (1.0931)  time: 1.1289  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:280]  [140/431]  eta: 0:05:32  lr: 0.000160  loss: 1.0609 (1.0972)  time: 1.1424  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [150/431]  eta: 0:05:21  lr: 0.000160  loss: 1.1228 (1.0966)  time: 1.1369  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [160/431]  eta: 0:05:09  lr: 0.000160  loss: 1.1228 (1.1001)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [170/431]  eta: 0:04:57  lr: 0.000160  loss: 1.0469 (1.0954)  time: 1.1279  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [180/431]  eta: 0:04:45  lr: 0.000160  loss: 1.0259 (1.0936)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [190/431]  eta: 0:04:34  lr: 0.000160  loss: 1.0999 (1.0973)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [200/431]  eta: 0:04:22  lr: 0.000160  loss: 1.1171 (1.0984)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [210/431]  eta: 0:04:10  lr: 0.000160  loss: 1.0921 (1.0999)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [220/431]  eta: 0:03:59  lr: 0.000160  loss: 1.0674 (1.0997)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [230/431]  eta: 0:03:47  lr: 0.000160  loss: 1.0764 (1.1024)  time: 1.1230  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [240/431]  eta: 0:03:36  lr: 0.000160  loss: 1.1249 (1.1024)  time: 1.1248  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [250/431]  eta: 0:03:24  lr: 0.000160  loss: 1.1120 (1.1018)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [260/431]  eta: 0:03:13  lr: 0.000160  loss: 1.0409 (1.1014)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [270/431]  eta: 0:03:02  lr: 0.000160  loss: 1.0835 (1.1009)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [280/431]  eta: 0:02:50  lr: 0.000160  loss: 1.0746 (1.1008)  time: 1.1270  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [290/431]  eta: 0:02:39  lr: 0.000160  loss: 1.0746 (1.1016)  time: 1.1298  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [300/431]  eta: 0:02:28  lr: 0.000160  loss: 1.0873 (1.1039)  time: 1.1354  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [310/431]  eta: 0:02:16  lr: 0.000160  loss: 1.1312 (1.1057)  time: 1.1326  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:280]  [320/431]  eta: 0:02:05  lr: 0.000160  loss: 1.1134 (1.1052)  time: 1.1214  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [330/431]  eta: 0:01:54  lr: 0.000160  loss: 1.0715 (1.1042)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [340/431]  eta: 0:01:42  lr: 0.000160  loss: 1.0721 (1.1043)  time: 1.1188  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:280]  [350/431]  eta: 0:01:31  lr: 0.000160  loss: 1.1158 (1.1045)  time: 1.1281  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:280]  [360/431]  eta: 0:01:20  lr: 0.000160  loss: 1.0502 (1.1035)  time: 1.1348  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:280]  [370/431]  eta: 0:01:08  lr: 0.000160  loss: 1.0430 (1.1048)  time: 1.1247  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [380/431]  eta: 0:00:57  lr: 0.000160  loss: 1.1246 (1.1051)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [390/431]  eta: 0:00:46  lr: 0.000160  loss: 1.0341 (1.1039)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [400/431]  eta: 0:00:34  lr: 0.000160  loss: 1.0532 (1.1036)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [410/431]  eta: 0:00:23  lr: 0.000160  loss: 1.0308 (1.1016)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [420/431]  eta: 0:00:12  lr: 0.000160  loss: 1.0275 (1.1025)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280]  [430/431]  eta: 0:00:01  lr: 0.000160  loss: 1.0718 (1.1018)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:280] Total time: 0:08:06 (1.1288 s / it)\n",
      "Averaged stats: lr: 0.000160  loss: 1.0718 (1.1018)\n",
      "Valid: [epoch:280]  [ 0/14]  eta: 0:00:36  loss: 0.9722 (0.9722)  time: 2.6036  data: 2.4488  max mem: 15925\n",
      "Valid: [epoch:280]  [13/14]  eta: 0:00:00  loss: 1.0264 (1.0399)  time: 0.2969  data: 0.1771  max mem: 15925\n",
      "Valid: [epoch:280] Total time: 0:00:04 (0.3142 s / it)\n",
      "Averaged stats: loss: 1.0264 (1.0399)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_280_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.040%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:281]  [  0/431]  eta: 0:36:00  lr: 0.000160  loss: 1.2809 (1.2809)  time: 5.0126  data: 3.6040  max mem: 15925\n",
      "Train: [epoch:281]  [ 10/431]  eta: 0:09:51  lr: 0.000160  loss: 1.1249 (1.1312)  time: 1.4050  data: 0.3279  max mem: 15925\n",
      "Train: [epoch:281]  [ 20/431]  eta: 0:08:29  lr: 0.000160  loss: 1.1249 (1.1303)  time: 1.0507  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [ 30/431]  eta: 0:07:59  lr: 0.000160  loss: 1.0659 (1.1052)  time: 1.0798  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [ 40/431]  eta: 0:07:36  lr: 0.000160  loss: 1.0059 (1.0978)  time: 1.0918  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [ 50/431]  eta: 0:07:20  lr: 0.000160  loss: 1.0973 (1.0938)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [ 60/431]  eta: 0:07:06  lr: 0.000160  loss: 1.1190 (1.0951)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [ 70/431]  eta: 0:06:53  lr: 0.000160  loss: 1.1142 (1.0973)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [ 80/431]  eta: 0:06:40  lr: 0.000160  loss: 1.0706 (1.0950)  time: 1.1204  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [ 90/431]  eta: 0:06:28  lr: 0.000160  loss: 1.0195 (1.0924)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [100/431]  eta: 0:06:16  lr: 0.000160  loss: 1.0486 (1.0933)  time: 1.1236  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [110/431]  eta: 0:06:04  lr: 0.000160  loss: 1.1011 (1.0971)  time: 1.1228  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [120/431]  eta: 0:05:53  lr: 0.000160  loss: 1.0923 (1.0942)  time: 1.1255  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [130/431]  eta: 0:05:41  lr: 0.000160  loss: 1.0453 (1.0986)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [140/431]  eta: 0:05:29  lr: 0.000160  loss: 1.0591 (1.0953)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [150/431]  eta: 0:05:17  lr: 0.000160  loss: 1.0757 (1.0962)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [160/431]  eta: 0:05:06  lr: 0.000160  loss: 1.0954 (1.0964)  time: 1.1214  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:281]  [170/431]  eta: 0:04:55  lr: 0.000160  loss: 1.0592 (1.0947)  time: 1.1369  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [180/431]  eta: 0:04:43  lr: 0.000160  loss: 1.0592 (1.0949)  time: 1.1268  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [190/431]  eta: 0:04:32  lr: 0.000160  loss: 1.0779 (1.0950)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [200/431]  eta: 0:04:20  lr: 0.000160  loss: 1.0725 (1.0952)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [210/431]  eta: 0:04:09  lr: 0.000160  loss: 1.0704 (1.0958)  time: 1.1232  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [220/431]  eta: 0:03:58  lr: 0.000160  loss: 1.0780 (1.0945)  time: 1.1370  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [230/431]  eta: 0:03:47  lr: 0.000160  loss: 1.0865 (1.0945)  time: 1.1267  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [240/431]  eta: 0:03:35  lr: 0.000160  loss: 1.0634 (1.0937)  time: 1.1241  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [250/431]  eta: 0:03:24  lr: 0.000160  loss: 1.0632 (1.0950)  time: 1.1211  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [260/431]  eta: 0:03:13  lr: 0.000160  loss: 1.1508 (1.0977)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [270/431]  eta: 0:03:01  lr: 0.000160  loss: 1.1071 (1.0979)  time: 1.1386  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [280/431]  eta: 0:02:50  lr: 0.000160  loss: 1.0648 (1.0968)  time: 1.1363  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [290/431]  eta: 0:02:39  lr: 0.000160  loss: 1.0709 (1.0972)  time: 1.1356  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [300/431]  eta: 0:02:27  lr: 0.000160  loss: 1.0559 (1.0960)  time: 1.1261  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [310/431]  eta: 0:02:16  lr: 0.000160  loss: 1.0503 (1.0966)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [320/431]  eta: 0:02:05  lr: 0.000160  loss: 1.1084 (1.0967)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [330/431]  eta: 0:01:53  lr: 0.000160  loss: 1.1173 (1.0981)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [340/431]  eta: 0:01:42  lr: 0.000160  loss: 1.1286 (1.0998)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [350/431]  eta: 0:01:31  lr: 0.000160  loss: 1.1136 (1.1004)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [360/431]  eta: 0:01:19  lr: 0.000160  loss: 1.0981 (1.1002)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [370/431]  eta: 0:01:08  lr: 0.000160  loss: 1.0733 (1.0993)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [380/431]  eta: 0:00:57  lr: 0.000160  loss: 1.1010 (1.1000)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [390/431]  eta: 0:00:46  lr: 0.000160  loss: 1.1077 (1.0993)  time: 1.1288  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:281]  [400/431]  eta: 0:00:34  lr: 0.000160  loss: 1.1038 (1.0998)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [410/431]  eta: 0:00:23  lr: 0.000160  loss: 1.1275 (1.1007)  time: 1.1309  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281]  [420/431]  eta: 0:00:12  lr: 0.000160  loss: 1.0947 (1.1006)  time: 1.1148  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:281]  [430/431]  eta: 0:00:01  lr: 0.000160  loss: 1.0855 (1.1010)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:281] Total time: 0:08:05 (1.1262 s / it)\n",
      "Averaged stats: lr: 0.000160  loss: 1.0855 (1.1010)\n",
      "Valid: [epoch:281]  [ 0/14]  eta: 0:00:36  loss: 1.0856 (1.0856)  time: 2.5893  data: 2.4679  max mem: 15925\n",
      "Valid: [epoch:281]  [13/14]  eta: 0:00:00  loss: 1.0331 (1.0428)  time: 0.3069  data: 0.1764  max mem: 15925\n",
      "Valid: [epoch:281] Total time: 0:00:04 (0.3248 s / it)\n",
      "Averaged stats: loss: 1.0331 (1.0428)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_281_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:282]  [  0/431]  eta: 0:33:35  lr: 0.000160  loss: 1.4143 (1.4143)  time: 4.6772  data: 3.4833  max mem: 15925\n",
      "Train: [epoch:282]  [ 10/431]  eta: 0:09:42  lr: 0.000160  loss: 1.1131 (1.1466)  time: 1.3827  data: 0.3169  max mem: 15925\n",
      "Train: [epoch:282]  [ 20/431]  eta: 0:08:30  lr: 0.000160  loss: 1.0774 (1.1210)  time: 1.0704  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [ 30/431]  eta: 0:07:57  lr: 0.000160  loss: 1.0774 (1.1184)  time: 1.0833  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [ 40/431]  eta: 0:07:36  lr: 0.000160  loss: 1.0560 (1.1030)  time: 1.0876  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [ 50/431]  eta: 0:07:21  lr: 0.000160  loss: 1.0662 (1.1031)  time: 1.1125  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [ 60/431]  eta: 0:07:07  lr: 0.000160  loss: 1.0805 (1.1067)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [ 70/431]  eta: 0:06:53  lr: 0.000160  loss: 1.0797 (1.1046)  time: 1.1100  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [ 80/431]  eta: 0:06:40  lr: 0.000160  loss: 1.0484 (1.1011)  time: 1.1108  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [ 90/431]  eta: 0:06:27  lr: 0.000160  loss: 1.0339 (1.0937)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [100/431]  eta: 0:06:14  lr: 0.000160  loss: 1.0678 (1.0991)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [110/431]  eta: 0:06:04  lr: 0.000160  loss: 1.0439 (1.0925)  time: 1.1272  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [120/431]  eta: 0:05:52  lr: 0.000160  loss: 1.0232 (1.0905)  time: 1.1322  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [130/431]  eta: 0:05:40  lr: 0.000160  loss: 1.0475 (1.0895)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [140/431]  eta: 0:05:28  lr: 0.000160  loss: 1.0655 (1.0911)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [150/431]  eta: 0:05:16  lr: 0.000160  loss: 1.0958 (1.0937)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [160/431]  eta: 0:05:05  lr: 0.000160  loss: 1.1057 (1.0972)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [170/431]  eta: 0:04:53  lr: 0.000160  loss: 1.0891 (1.0951)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [180/431]  eta: 0:04:42  lr: 0.000160  loss: 1.1490 (1.1008)  time: 1.1159  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [190/431]  eta: 0:04:30  lr: 0.000160  loss: 1.1585 (1.1050)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [200/431]  eta: 0:04:19  lr: 0.000160  loss: 1.1389 (1.1036)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [210/431]  eta: 0:04:08  lr: 0.000160  loss: 1.0811 (1.1057)  time: 1.1207  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [220/431]  eta: 0:03:57  lr: 0.000160  loss: 1.0997 (1.1045)  time: 1.1269  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [230/431]  eta: 0:03:45  lr: 0.000160  loss: 1.1066 (1.1065)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [240/431]  eta: 0:03:34  lr: 0.000160  loss: 1.1077 (1.1054)  time: 1.1145  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [250/431]  eta: 0:03:23  lr: 0.000160  loss: 1.0479 (1.1052)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [260/431]  eta: 0:03:11  lr: 0.000160  loss: 1.0926 (1.1071)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [270/431]  eta: 0:03:00  lr: 0.000160  loss: 1.0996 (1.1061)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [280/431]  eta: 0:02:49  lr: 0.000160  loss: 1.0996 (1.1070)  time: 1.1113  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [290/431]  eta: 0:02:38  lr: 0.000160  loss: 1.0840 (1.1082)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [300/431]  eta: 0:02:26  lr: 0.000160  loss: 1.0414 (1.1089)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [310/431]  eta: 0:02:15  lr: 0.000160  loss: 0.9943 (1.1062)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [320/431]  eta: 0:02:04  lr: 0.000160  loss: 1.0330 (1.1053)  time: 1.1235  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [330/431]  eta: 0:01:53  lr: 0.000160  loss: 1.0546 (1.1048)  time: 1.1254  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:282]  [340/431]  eta: 0:01:41  lr: 0.000160  loss: 1.0679 (1.1048)  time: 1.1179  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [350/431]  eta: 0:01:30  lr: 0.000160  loss: 1.0808 (1.1042)  time: 1.1149  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [360/431]  eta: 0:01:19  lr: 0.000160  loss: 1.0795 (1.1038)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [370/431]  eta: 0:01:08  lr: 0.000160  loss: 1.0868 (1.1045)  time: 1.1227  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [380/431]  eta: 0:00:57  lr: 0.000160  loss: 1.0558 (1.1025)  time: 1.1243  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [390/431]  eta: 0:00:45  lr: 0.000160  loss: 1.0414 (1.1030)  time: 1.1240  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:282]  [400/431]  eta: 0:00:34  lr: 0.000160  loss: 1.0580 (1.1024)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [410/431]  eta: 0:00:23  lr: 0.000160  loss: 1.0271 (1.1008)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [420/431]  eta: 0:00:12  lr: 0.000160  loss: 1.0590 (1.1022)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282]  [430/431]  eta: 0:00:01  lr: 0.000160  loss: 1.1254 (1.1020)  time: 1.1215  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:282] Total time: 0:08:03 (1.1212 s / it)\n",
      "Averaged stats: lr: 0.000160  loss: 1.1254 (1.1020)\n",
      "Valid: [epoch:282]  [ 0/14]  eta: 0:00:35  loss: 1.0887 (1.0887)  time: 2.5385  data: 2.3678  max mem: 15925\n",
      "Valid: [epoch:282]  [13/14]  eta: 0:00:00  loss: 1.0352 (1.0444)  time: 0.2776  data: 0.1692  max mem: 15925\n",
      "Valid: [epoch:282] Total time: 0:00:04 (0.2952 s / it)\n",
      "Averaged stats: loss: 1.0352 (1.0444)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_282_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:283]  [  0/431]  eta: 0:34:55  lr: 0.000160  loss: 1.1273 (1.1273)  time: 4.8629  data: 3.4853  max mem: 15925\n",
      "Train: [epoch:283]  [ 10/431]  eta: 0:09:39  lr: 0.000160  loss: 1.2058 (1.2404)  time: 1.3764  data: 0.3173  max mem: 15925\n",
      "Train: [epoch:283]  [ 20/431]  eta: 0:08:24  lr: 0.000160  loss: 1.1789 (1.2030)  time: 1.0460  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:283]  [ 30/431]  eta: 0:07:54  lr: 0.000160  loss: 1.0401 (1.1488)  time: 1.0770  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [ 40/431]  eta: 0:07:33  lr: 0.000160  loss: 1.0359 (1.1342)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [ 50/431]  eta: 0:07:17  lr: 0.000160  loss: 1.1047 (1.1319)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [ 60/431]  eta: 0:07:04  lr: 0.000160  loss: 1.0893 (1.1294)  time: 1.1115  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:283]  [ 70/431]  eta: 0:06:52  lr: 0.000160  loss: 1.0756 (1.1252)  time: 1.1253  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:283]  [ 80/431]  eta: 0:06:39  lr: 0.000160  loss: 1.0866 (1.1215)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [ 90/431]  eta: 0:06:27  lr: 0.000160  loss: 1.0959 (1.1261)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [100/431]  eta: 0:06:15  lr: 0.000160  loss: 1.0611 (1.1169)  time: 1.1132  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:283]  [110/431]  eta: 0:06:03  lr: 0.000160  loss: 1.0383 (1.1112)  time: 1.1171  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:283]  [120/431]  eta: 0:05:51  lr: 0.000160  loss: 1.0139 (1.1099)  time: 1.1223  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [130/431]  eta: 0:05:40  lr: 0.000160  loss: 1.0718 (1.1087)  time: 1.1253  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:283]  [140/431]  eta: 0:05:28  lr: 0.000160  loss: 1.0927 (1.1078)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [150/431]  eta: 0:05:17  lr: 0.000160  loss: 1.0927 (1.1083)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [160/431]  eta: 0:05:05  lr: 0.000160  loss: 1.0404 (1.1049)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [170/431]  eta: 0:04:54  lr: 0.000160  loss: 1.0404 (1.1059)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [180/431]  eta: 0:04:43  lr: 0.000160  loss: 1.0791 (1.1061)  time: 1.1267  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [190/431]  eta: 0:04:31  lr: 0.000160  loss: 1.0635 (1.1031)  time: 1.1235  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [200/431]  eta: 0:04:20  lr: 0.000160  loss: 1.0740 (1.1022)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [210/431]  eta: 0:04:08  lr: 0.000160  loss: 1.0635 (1.1000)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [220/431]  eta: 0:03:57  lr: 0.000160  loss: 1.0522 (1.0979)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [230/431]  eta: 0:03:46  lr: 0.000160  loss: 1.0753 (1.1003)  time: 1.1157  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:283]  [240/431]  eta: 0:03:34  lr: 0.000160  loss: 1.0969 (1.1004)  time: 1.1097  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:283]  [250/431]  eta: 0:03:23  lr: 0.000160  loss: 1.0853 (1.1006)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [260/431]  eta: 0:03:12  lr: 0.000160  loss: 1.0853 (1.1005)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [270/431]  eta: 0:03:00  lr: 0.000160  loss: 1.0936 (1.1013)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [280/431]  eta: 0:02:49  lr: 0.000160  loss: 1.1148 (1.1012)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [290/431]  eta: 0:02:38  lr: 0.000160  loss: 1.1114 (1.1019)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [300/431]  eta: 0:02:26  lr: 0.000160  loss: 1.1011 (1.1012)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [310/431]  eta: 0:02:15  lr: 0.000160  loss: 1.0703 (1.1010)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [320/431]  eta: 0:02:04  lr: 0.000160  loss: 1.0703 (1.1023)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [330/431]  eta: 0:01:53  lr: 0.000160  loss: 1.1073 (1.1041)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [340/431]  eta: 0:01:41  lr: 0.000160  loss: 1.1009 (1.1050)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [350/431]  eta: 0:01:30  lr: 0.000160  loss: 1.0822 (1.1046)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [360/431]  eta: 0:01:19  lr: 0.000160  loss: 1.0710 (1.1048)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [370/431]  eta: 0:01:08  lr: 0.000160  loss: 1.0724 (1.1047)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [380/431]  eta: 0:00:56  lr: 0.000160  loss: 1.0580 (1.1042)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [390/431]  eta: 0:00:45  lr: 0.000160  loss: 1.0659 (1.1039)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [400/431]  eta: 0:00:34  lr: 0.000160  loss: 1.1100 (1.1060)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:283]  [410/431]  eta: 0:00:23  lr: 0.000160  loss: 1.1588 (1.1069)  time: 1.1125  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:283]  [420/431]  eta: 0:00:12  lr: 0.000160  loss: 1.0822 (1.1056)  time: 1.1158  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:283]  [430/431]  eta: 0:00:01  lr: 0.000160  loss: 1.0674 (1.1056)  time: 1.1052  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:283] Total time: 0:08:01 (1.1166 s / it)\n",
      "Averaged stats: lr: 0.000160  loss: 1.0674 (1.1056)\n",
      "Valid: [epoch:283]  [ 0/14]  eta: 0:00:35  loss: 0.9296 (0.9296)  time: 2.5298  data: 2.3478  max mem: 15925\n",
      "Valid: [epoch:283]  [13/14]  eta: 0:00:00  loss: 1.0309 (1.0421)  time: 0.2585  data: 0.1678  max mem: 15925\n",
      "Valid: [epoch:283] Total time: 0:00:03 (0.2732 s / it)\n",
      "Averaged stats: loss: 1.0309 (1.0421)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_283_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:284]  [  0/431]  eta: 0:31:52  lr: 0.000159  loss: 1.0360 (1.0360)  time: 4.4365  data: 3.2806  max mem: 15925\n",
      "Train: [epoch:284]  [ 10/431]  eta: 0:09:27  lr: 0.000159  loss: 1.1400 (1.1607)  time: 1.3473  data: 0.2984  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:284]  [ 20/431]  eta: 0:08:17  lr: 0.000159  loss: 1.1074 (1.1234)  time: 1.0487  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [ 30/431]  eta: 0:07:48  lr: 0.000159  loss: 1.0441 (1.1081)  time: 1.0693  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [ 40/431]  eta: 0:07:29  lr: 0.000159  loss: 1.0439 (1.0863)  time: 1.0857  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [ 50/431]  eta: 0:07:12  lr: 0.000159  loss: 1.0439 (1.0860)  time: 1.0868  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [ 60/431]  eta: 0:06:59  lr: 0.000159  loss: 1.0749 (1.0900)  time: 1.0893  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [ 70/431]  eta: 0:06:46  lr: 0.000159  loss: 1.0846 (1.0911)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [ 80/431]  eta: 0:06:34  lr: 0.000159  loss: 1.0499 (1.0901)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [ 90/431]  eta: 0:06:21  lr: 0.000159  loss: 1.0452 (1.0900)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [100/431]  eta: 0:06:09  lr: 0.000159  loss: 1.0453 (1.0885)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [110/431]  eta: 0:05:58  lr: 0.000159  loss: 1.0422 (1.0878)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [120/431]  eta: 0:05:47  lr: 0.000159  loss: 1.0730 (1.0884)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [130/431]  eta: 0:05:35  lr: 0.000159  loss: 1.0730 (1.0900)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [140/431]  eta: 0:05:24  lr: 0.000159  loss: 1.0572 (1.0942)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [150/431]  eta: 0:05:13  lr: 0.000159  loss: 1.0794 (1.0930)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [160/431]  eta: 0:05:01  lr: 0.000159  loss: 1.0794 (1.0925)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [170/431]  eta: 0:04:50  lr: 0.000159  loss: 1.0840 (1.0906)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [180/431]  eta: 0:04:39  lr: 0.000159  loss: 1.0455 (1.0917)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [190/431]  eta: 0:04:28  lr: 0.000159  loss: 1.0934 (1.0943)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [200/431]  eta: 0:04:17  lr: 0.000159  loss: 1.1287 (1.0969)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [210/431]  eta: 0:04:05  lr: 0.000159  loss: 1.1591 (1.0992)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [220/431]  eta: 0:03:54  lr: 0.000159  loss: 1.1633 (1.1019)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [230/431]  eta: 0:03:43  lr: 0.000159  loss: 1.0929 (1.1002)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [240/431]  eta: 0:03:32  lr: 0.000159  loss: 1.0166 (1.0986)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [250/431]  eta: 0:03:21  lr: 0.000159  loss: 1.0794 (1.1014)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [260/431]  eta: 0:03:09  lr: 0.000159  loss: 1.1210 (1.1023)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [270/431]  eta: 0:02:58  lr: 0.000159  loss: 1.0897 (1.1016)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [280/431]  eta: 0:02:47  lr: 0.000159  loss: 1.0850 (1.1014)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [290/431]  eta: 0:02:36  lr: 0.000159  loss: 1.0900 (1.1018)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [300/431]  eta: 0:02:25  lr: 0.000159  loss: 1.1464 (1.1044)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [310/431]  eta: 0:02:14  lr: 0.000159  loss: 1.1000 (1.1027)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [320/431]  eta: 0:02:03  lr: 0.000159  loss: 1.0709 (1.1030)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [330/431]  eta: 0:01:51  lr: 0.000159  loss: 1.0766 (1.1036)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [340/431]  eta: 0:01:40  lr: 0.000159  loss: 1.0766 (1.1031)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [350/431]  eta: 0:01:29  lr: 0.000159  loss: 1.0792 (1.1041)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [360/431]  eta: 0:01:18  lr: 0.000159  loss: 1.0926 (1.1039)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [370/431]  eta: 0:01:07  lr: 0.000159  loss: 1.1195 (1.1053)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [380/431]  eta: 0:00:56  lr: 0.000159  loss: 1.0569 (1.1048)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:284]  [390/431]  eta: 0:00:45  lr: 0.000159  loss: 1.0492 (1.1048)  time: 1.1130  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:284]  [400/431]  eta: 0:00:34  lr: 0.000159  loss: 1.1006 (1.1047)  time: 1.1106  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:284]  [410/431]  eta: 0:00:23  lr: 0.000159  loss: 1.0935 (1.1046)  time: 1.1091  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:284]  [420/431]  eta: 0:00:12  lr: 0.000159  loss: 1.0443 (1.1022)  time: 1.1075  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:284]  [430/431]  eta: 0:00:01  lr: 0.000159  loss: 1.0317 (1.1014)  time: 1.0977  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:284] Total time: 0:07:57 (1.1088 s / it)\n",
      "Averaged stats: lr: 0.000159  loss: 1.0317 (1.1014)\n",
      "Valid: [epoch:284]  [ 0/14]  eta: 0:00:35  loss: 1.0868 (1.0868)  time: 2.5558  data: 2.4060  max mem: 15925\n",
      "Valid: [epoch:284]  [13/14]  eta: 0:00:00  loss: 1.0310 (1.0424)  time: 0.2667  data: 0.1719  max mem: 15925\n",
      "Valid: [epoch:284] Total time: 0:00:04 (0.2860 s / it)\n",
      "Averaged stats: loss: 1.0310 (1.0424)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_284_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:285]  [  0/431]  eta: 0:33:08  lr: 0.000159  loss: 1.1714 (1.1714)  time: 4.6148  data: 3.4397  max mem: 15925\n",
      "Train: [epoch:285]  [ 10/431]  eta: 0:09:40  lr: 0.000159  loss: 1.1839 (1.2005)  time: 1.3780  data: 0.3129  max mem: 15925\n",
      "Train: [epoch:285]  [ 20/431]  eta: 0:08:25  lr: 0.000159  loss: 1.1451 (1.1565)  time: 1.0604  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [ 30/431]  eta: 0:07:53  lr: 0.000159  loss: 1.1417 (1.1454)  time: 1.0712  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [ 40/431]  eta: 0:07:32  lr: 0.000159  loss: 1.0702 (1.1288)  time: 1.0803  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [ 50/431]  eta: 0:07:16  lr: 0.000159  loss: 0.9830 (1.1013)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [ 60/431]  eta: 0:07:02  lr: 0.000159  loss: 1.0572 (1.1075)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [ 70/431]  eta: 0:06:49  lr: 0.000159  loss: 1.0910 (1.1093)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [ 80/431]  eta: 0:06:36  lr: 0.000159  loss: 1.1054 (1.1112)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [ 90/431]  eta: 0:06:24  lr: 0.000159  loss: 1.0615 (1.1067)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [100/431]  eta: 0:06:12  lr: 0.000159  loss: 1.0224 (1.1000)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [110/431]  eta: 0:06:00  lr: 0.000159  loss: 1.0432 (1.1040)  time: 1.1032  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:285]  [120/431]  eta: 0:05:48  lr: 0.000159  loss: 1.1548 (1.1046)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [130/431]  eta: 0:05:37  lr: 0.000159  loss: 1.0661 (1.0993)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [140/431]  eta: 0:05:26  lr: 0.000159  loss: 1.0507 (1.1008)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [150/431]  eta: 0:05:14  lr: 0.000159  loss: 1.0839 (1.1010)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [160/431]  eta: 0:05:02  lr: 0.000159  loss: 1.0853 (1.0987)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [170/431]  eta: 0:04:51  lr: 0.000159  loss: 1.0871 (1.0971)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [180/431]  eta: 0:04:40  lr: 0.000159  loss: 1.1075 (1.0997)  time: 1.1072  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:285]  [190/431]  eta: 0:04:28  lr: 0.000159  loss: 1.1164 (1.1010)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [200/431]  eta: 0:04:17  lr: 0.000159  loss: 1.0901 (1.1002)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [210/431]  eta: 0:04:06  lr: 0.000159  loss: 1.0569 (1.0995)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [220/431]  eta: 0:03:55  lr: 0.000159  loss: 1.0449 (1.0956)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [230/431]  eta: 0:03:43  lr: 0.000159  loss: 1.0184 (1.0946)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [240/431]  eta: 0:03:32  lr: 0.000159  loss: 1.0663 (1.0958)  time: 1.0873  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [250/431]  eta: 0:03:21  lr: 0.000159  loss: 1.0755 (1.0948)  time: 1.0928  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:285]  [260/431]  eta: 0:03:10  lr: 0.000159  loss: 1.0755 (1.0959)  time: 1.1044  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:285]  [270/431]  eta: 0:02:58  lr: 0.000159  loss: 1.1208 (1.0975)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [280/431]  eta: 0:02:47  lr: 0.000159  loss: 1.0586 (1.0973)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [290/431]  eta: 0:02:36  lr: 0.000159  loss: 1.0586 (1.0962)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [300/431]  eta: 0:02:25  lr: 0.000159  loss: 1.0974 (1.0978)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [310/431]  eta: 0:02:14  lr: 0.000159  loss: 1.1577 (1.1011)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [320/431]  eta: 0:02:03  lr: 0.000159  loss: 1.0818 (1.0991)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [330/431]  eta: 0:01:52  lr: 0.000159  loss: 1.0818 (1.1025)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [340/431]  eta: 0:01:40  lr: 0.000159  loss: 1.1561 (1.1051)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [350/431]  eta: 0:01:29  lr: 0.000159  loss: 1.0979 (1.1037)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [360/431]  eta: 0:01:18  lr: 0.000159  loss: 1.0449 (1.1023)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [370/431]  eta: 0:01:07  lr: 0.000159  loss: 1.0467 (1.1023)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [380/431]  eta: 0:00:56  lr: 0.000159  loss: 1.0943 (1.1035)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [390/431]  eta: 0:00:45  lr: 0.000159  loss: 1.1500 (1.1049)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [400/431]  eta: 0:00:34  lr: 0.000159  loss: 1.1092 (1.1040)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:285]  [410/431]  eta: 0:00:23  lr: 0.000159  loss: 1.0539 (1.1038)  time: 1.1016  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:285]  [420/431]  eta: 0:00:12  lr: 0.000159  loss: 1.0457 (1.1031)  time: 1.1051  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:285]  [430/431]  eta: 0:00:01  lr: 0.000159  loss: 1.0777 (1.1035)  time: 1.1024  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:285] Total time: 0:07:57 (1.1090 s / it)\n",
      "Averaged stats: lr: 0.000159  loss: 1.0777 (1.1035)\n",
      "Valid: [epoch:285]  [ 0/14]  eta: 0:00:36  loss: 1.1392 (1.1392)  time: 2.5896  data: 2.4962  max mem: 15925\n",
      "Valid: [epoch:285]  [13/14]  eta: 0:00:00  loss: 1.0482 (1.0542)  time: 0.2618  data: 0.1784  max mem: 15925\n",
      "Valid: [epoch:285] Total time: 0:00:03 (0.2767 s / it)\n",
      "Averaged stats: loss: 1.0482 (1.0542)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_285_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.054%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:286]  [  0/431]  eta: 0:31:37  lr: 0.000159  loss: 1.2284 (1.2284)  time: 4.4037  data: 3.1428  max mem: 15925\n",
      "Train: [epoch:286]  [ 10/431]  eta: 0:09:26  lr: 0.000159  loss: 1.2201 (1.2010)  time: 1.3463  data: 0.2859  max mem: 15925\n",
      "Train: [epoch:286]  [ 20/431]  eta: 0:08:18  lr: 0.000159  loss: 1.1911 (1.1893)  time: 1.0527  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [ 30/431]  eta: 0:07:48  lr: 0.000159  loss: 1.1460 (1.1654)  time: 1.0721  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [ 40/431]  eta: 0:07:28  lr: 0.000159  loss: 1.0628 (1.1356)  time: 1.0801  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [ 50/431]  eta: 0:07:14  lr: 0.000159  loss: 1.0527 (1.1280)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [ 60/431]  eta: 0:07:00  lr: 0.000159  loss: 1.0424 (1.1183)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [ 70/431]  eta: 0:06:49  lr: 0.000159  loss: 1.0366 (1.1136)  time: 1.1174  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [ 80/431]  eta: 0:06:37  lr: 0.000159  loss: 1.1041 (1.1143)  time: 1.1226  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [ 90/431]  eta: 0:06:25  lr: 0.000159  loss: 1.0911 (1.1115)  time: 1.1224  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [100/431]  eta: 0:06:14  lr: 0.000159  loss: 1.0332 (1.1061)  time: 1.1249  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [110/431]  eta: 0:06:02  lr: 0.000159  loss: 1.0881 (1.1076)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [120/431]  eta: 0:05:50  lr: 0.000159  loss: 1.0881 (1.1067)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [130/431]  eta: 0:05:38  lr: 0.000159  loss: 1.0558 (1.1030)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [140/431]  eta: 0:05:27  lr: 0.000159  loss: 1.0558 (1.1037)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [150/431]  eta: 0:05:15  lr: 0.000159  loss: 1.0914 (1.1037)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [160/431]  eta: 0:05:04  lr: 0.000159  loss: 1.0713 (1.1013)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [170/431]  eta: 0:04:52  lr: 0.000159  loss: 1.0710 (1.1018)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [180/431]  eta: 0:04:41  lr: 0.000159  loss: 1.0984 (1.1034)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [190/431]  eta: 0:04:30  lr: 0.000159  loss: 1.0598 (1.1022)  time: 1.1242  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [200/431]  eta: 0:04:19  lr: 0.000159  loss: 1.0635 (1.1045)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [210/431]  eta: 0:04:07  lr: 0.000159  loss: 1.0698 (1.1042)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [220/431]  eta: 0:03:56  lr: 0.000159  loss: 1.0690 (1.1061)  time: 1.1251  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [230/431]  eta: 0:03:45  lr: 0.000159  loss: 1.0709 (1.1044)  time: 1.1280  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [240/431]  eta: 0:03:34  lr: 0.000159  loss: 1.0394 (1.1017)  time: 1.1328  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [250/431]  eta: 0:03:23  lr: 0.000159  loss: 1.0277 (1.1012)  time: 1.1265  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [260/431]  eta: 0:03:11  lr: 0.000159  loss: 1.0825 (1.1022)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [270/431]  eta: 0:03:00  lr: 0.000159  loss: 1.0753 (1.1005)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [280/431]  eta: 0:02:49  lr: 0.000159  loss: 1.0579 (1.0994)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [290/431]  eta: 0:02:38  lr: 0.000159  loss: 1.0714 (1.0982)  time: 1.1110  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [300/431]  eta: 0:02:27  lr: 0.000159  loss: 1.0714 (1.0995)  time: 1.1257  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [310/431]  eta: 0:02:15  lr: 0.000159  loss: 1.0747 (1.0990)  time: 1.1303  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [320/431]  eta: 0:02:04  lr: 0.000159  loss: 1.0814 (1.0993)  time: 1.1163  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [330/431]  eta: 0:01:53  lr: 0.000159  loss: 1.1189 (1.1011)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [340/431]  eta: 0:01:42  lr: 0.000159  loss: 1.1140 (1.1017)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [350/431]  eta: 0:01:30  lr: 0.000159  loss: 1.0624 (1.1027)  time: 1.1176  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:286]  [360/431]  eta: 0:01:19  lr: 0.000159  loss: 1.0624 (1.1030)  time: 1.1272  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [370/431]  eta: 0:01:08  lr: 0.000159  loss: 1.0788 (1.1031)  time: 1.1288  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [380/431]  eta: 0:00:57  lr: 0.000159  loss: 1.1115 (1.1040)  time: 1.1224  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [390/431]  eta: 0:00:45  lr: 0.000159  loss: 1.1160 (1.1037)  time: 1.1223  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [400/431]  eta: 0:00:34  lr: 0.000159  loss: 1.0651 (1.1034)  time: 1.1170  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:286]  [410/431]  eta: 0:00:23  lr: 0.000159  loss: 1.0706 (1.1035)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [420/431]  eta: 0:00:12  lr: 0.000159  loss: 1.1059 (1.1037)  time: 1.1253  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286]  [430/431]  eta: 0:00:01  lr: 0.000159  loss: 1.1059 (1.1034)  time: 1.1240  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:286] Total time: 0:08:03 (1.1223 s / it)\n",
      "Averaged stats: lr: 0.000159  loss: 1.1059 (1.1034)\n",
      "Valid: [epoch:286]  [ 0/14]  eta: 0:00:37  loss: 0.9539 (0.9539)  time: 2.6834  data: 2.5300  max mem: 15925\n",
      "Valid: [epoch:286]  [13/14]  eta: 0:00:00  loss: 1.0323 (1.0418)  time: 0.2757  data: 0.1808  max mem: 15925\n",
      "Valid: [epoch:286] Total time: 0:00:04 (0.2925 s / it)\n",
      "Averaged stats: loss: 1.0323 (1.0418)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_286_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:287]  [  0/431]  eta: 0:32:35  lr: 0.000159  loss: 1.1517 (1.1517)  time: 4.5364  data: 3.3513  max mem: 15925\n",
      "Train: [epoch:287]  [ 10/431]  eta: 0:09:42  lr: 0.000159  loss: 1.1504 (1.1282)  time: 1.3839  data: 0.3052  max mem: 15925\n",
      "Train: [epoch:287]  [ 20/431]  eta: 0:08:30  lr: 0.000159  loss: 1.1186 (1.1270)  time: 1.0766  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:287]  [ 30/431]  eta: 0:08:01  lr: 0.000159  loss: 1.1190 (1.1349)  time: 1.1001  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [ 40/431]  eta: 0:07:39  lr: 0.000159  loss: 1.1052 (1.1240)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [ 50/431]  eta: 0:07:23  lr: 0.000159  loss: 1.0579 (1.1101)  time: 1.1076  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [ 60/431]  eta: 0:07:08  lr: 0.000159  loss: 1.0315 (1.1019)  time: 1.1107  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [ 70/431]  eta: 0:06:54  lr: 0.000159  loss: 1.0444 (1.0963)  time: 1.1115  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [ 80/431]  eta: 0:06:41  lr: 0.000159  loss: 1.1172 (1.1014)  time: 1.1112  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [ 90/431]  eta: 0:06:28  lr: 0.000159  loss: 1.1153 (1.1012)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [100/431]  eta: 0:06:16  lr: 0.000159  loss: 1.0340 (1.0985)  time: 1.1069  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:287]  [110/431]  eta: 0:06:04  lr: 0.000159  loss: 1.0340 (1.0940)  time: 1.1199  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:287]  [120/431]  eta: 0:05:52  lr: 0.000159  loss: 1.0718 (1.0946)  time: 1.1144  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [130/431]  eta: 0:05:40  lr: 0.000159  loss: 1.0988 (1.0951)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [140/431]  eta: 0:05:28  lr: 0.000159  loss: 1.0949 (1.0933)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [150/431]  eta: 0:05:17  lr: 0.000159  loss: 1.0392 (1.0930)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [160/431]  eta: 0:05:05  lr: 0.000159  loss: 1.0570 (1.0923)  time: 1.1185  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [170/431]  eta: 0:04:54  lr: 0.000159  loss: 1.0658 (1.0917)  time: 1.1171  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [180/431]  eta: 0:04:42  lr: 0.000159  loss: 1.0879 (1.0938)  time: 1.1221  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [190/431]  eta: 0:04:31  lr: 0.000159  loss: 1.0935 (1.0942)  time: 1.1286  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [200/431]  eta: 0:04:20  lr: 0.000159  loss: 1.1242 (1.0976)  time: 1.1271  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [210/431]  eta: 0:04:09  lr: 0.000159  loss: 1.1304 (1.0986)  time: 1.1291  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [220/431]  eta: 0:03:58  lr: 0.000159  loss: 1.0823 (1.0978)  time: 1.1348  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [230/431]  eta: 0:03:46  lr: 0.000159  loss: 1.0292 (1.0979)  time: 1.1264  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [240/431]  eta: 0:03:35  lr: 0.000159  loss: 1.1369 (1.1001)  time: 1.1170  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [250/431]  eta: 0:03:23  lr: 0.000159  loss: 1.1050 (1.0985)  time: 1.1209  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [260/431]  eta: 0:03:12  lr: 0.000159  loss: 1.0669 (1.0973)  time: 1.1258  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [270/431]  eta: 0:03:01  lr: 0.000159  loss: 1.0722 (1.0970)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [280/431]  eta: 0:02:50  lr: 0.000159  loss: 1.0722 (1.0969)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [290/431]  eta: 0:02:38  lr: 0.000159  loss: 1.0775 (1.0994)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [300/431]  eta: 0:02:27  lr: 0.000159  loss: 1.1227 (1.1005)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [310/431]  eta: 0:02:16  lr: 0.000159  loss: 1.0886 (1.1005)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [320/431]  eta: 0:02:04  lr: 0.000159  loss: 1.0778 (1.1001)  time: 1.1341  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [330/431]  eta: 0:01:53  lr: 0.000159  loss: 1.0839 (1.1012)  time: 1.1241  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [340/431]  eta: 0:01:42  lr: 0.000159  loss: 1.1005 (1.1015)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [350/431]  eta: 0:01:31  lr: 0.000159  loss: 1.1223 (1.1028)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [360/431]  eta: 0:01:19  lr: 0.000159  loss: 1.1160 (1.1021)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [370/431]  eta: 0:01:08  lr: 0.000159  loss: 1.0543 (1.1026)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [380/431]  eta: 0:00:57  lr: 0.000159  loss: 1.0577 (1.1021)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [390/431]  eta: 0:00:46  lr: 0.000159  loss: 1.0439 (1.1023)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [400/431]  eta: 0:00:34  lr: 0.000159  loss: 1.1181 (1.1035)  time: 1.1200  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:287]  [410/431]  eta: 0:00:23  lr: 0.000159  loss: 1.1057 (1.1025)  time: 1.1109  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:287]  [420/431]  eta: 0:00:12  lr: 0.000159  loss: 1.0311 (1.1020)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287]  [430/431]  eta: 0:00:01  lr: 0.000159  loss: 1.1008 (1.1023)  time: 1.1187  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:287] Total time: 0:08:04 (1.1231 s / it)\n",
      "Averaged stats: lr: 0.000159  loss: 1.1008 (1.1023)\n",
      "Valid: [epoch:287]  [ 0/14]  eta: 0:00:35  loss: 1.0059 (1.0059)  time: 2.5328  data: 2.3813  max mem: 15925\n",
      "Valid: [epoch:287]  [13/14]  eta: 0:00:00  loss: 1.0350 (1.0470)  time: 0.2858  data: 0.1702  max mem: 15925\n",
      "Valid: [epoch:287] Total time: 0:00:04 (0.3009 s / it)\n",
      "Averaged stats: loss: 1.0350 (1.0470)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_287_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:288]  [  0/431]  eta: 0:31:43  lr: 0.000158  loss: 1.1919 (1.1919)  time: 4.4162  data: 3.2286  max mem: 15925\n",
      "Train: [epoch:288]  [ 10/431]  eta: 0:09:34  lr: 0.000158  loss: 1.2379 (1.2419)  time: 1.3637  data: 0.2937  max mem: 15925\n",
      "Train: [epoch:288]  [ 20/431]  eta: 0:08:22  lr: 0.000158  loss: 1.1814 (1.2152)  time: 1.0626  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [ 30/431]  eta: 0:07:50  lr: 0.000158  loss: 1.0782 (1.1601)  time: 1.0681  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:288]  [ 40/431]  eta: 0:07:31  lr: 0.000158  loss: 1.0724 (1.1554)  time: 1.0832  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [ 50/431]  eta: 0:07:15  lr: 0.000158  loss: 1.1125 (1.1460)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [ 60/431]  eta: 0:07:01  lr: 0.000158  loss: 1.0730 (1.1390)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [ 70/431]  eta: 0:06:48  lr: 0.000158  loss: 1.0700 (1.1299)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [ 80/431]  eta: 0:06:36  lr: 0.000158  loss: 1.0865 (1.1306)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [ 90/431]  eta: 0:06:23  lr: 0.000158  loss: 1.0738 (1.1331)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [100/431]  eta: 0:06:11  lr: 0.000158  loss: 1.0994 (1.1341)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [110/431]  eta: 0:06:00  lr: 0.000158  loss: 1.1075 (1.1308)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [120/431]  eta: 0:05:48  lr: 0.000158  loss: 1.1075 (1.1295)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [130/431]  eta: 0:05:37  lr: 0.000158  loss: 1.0808 (1.1255)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [140/431]  eta: 0:05:25  lr: 0.000158  loss: 1.0878 (1.1264)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [150/431]  eta: 0:05:14  lr: 0.000158  loss: 1.1101 (1.1263)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [160/431]  eta: 0:05:02  lr: 0.000158  loss: 1.0971 (1.1264)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [170/431]  eta: 0:04:51  lr: 0.000158  loss: 1.0901 (1.1251)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [180/431]  eta: 0:04:40  lr: 0.000158  loss: 1.0865 (1.1231)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [190/431]  eta: 0:04:28  lr: 0.000158  loss: 1.0865 (1.1221)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [200/431]  eta: 0:04:17  lr: 0.000158  loss: 1.1149 (1.1221)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [210/431]  eta: 0:04:06  lr: 0.000158  loss: 1.0854 (1.1187)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [220/431]  eta: 0:03:55  lr: 0.000158  loss: 1.0603 (1.1172)  time: 1.1194  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [230/431]  eta: 0:03:44  lr: 0.000158  loss: 1.0379 (1.1146)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [240/431]  eta: 0:03:33  lr: 0.000158  loss: 1.0716 (1.1156)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [250/431]  eta: 0:03:21  lr: 0.000158  loss: 1.1072 (1.1145)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [260/431]  eta: 0:03:10  lr: 0.000158  loss: 1.0724 (1.1141)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [270/431]  eta: 0:02:59  lr: 0.000158  loss: 1.0543 (1.1127)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [280/431]  eta: 0:02:48  lr: 0.000158  loss: 1.0431 (1.1102)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [290/431]  eta: 0:02:37  lr: 0.000158  loss: 1.0510 (1.1087)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [300/431]  eta: 0:02:25  lr: 0.000158  loss: 1.0643 (1.1099)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [310/431]  eta: 0:02:14  lr: 0.000158  loss: 1.0785 (1.1082)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [320/431]  eta: 0:02:03  lr: 0.000158  loss: 1.0405 (1.1068)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [330/431]  eta: 0:01:52  lr: 0.000158  loss: 1.0639 (1.1072)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [340/431]  eta: 0:01:41  lr: 0.000158  loss: 1.0836 (1.1073)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [350/431]  eta: 0:01:30  lr: 0.000158  loss: 1.0726 (1.1067)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [360/431]  eta: 0:01:19  lr: 0.000158  loss: 1.0430 (1.1060)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [370/431]  eta: 0:01:07  lr: 0.000158  loss: 1.0382 (1.1048)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [380/431]  eta: 0:00:56  lr: 0.000158  loss: 1.0711 (1.1054)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [390/431]  eta: 0:00:45  lr: 0.000158  loss: 1.0711 (1.1045)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [400/431]  eta: 0:00:34  lr: 0.000158  loss: 1.1025 (1.1059)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:288]  [410/431]  eta: 0:00:23  lr: 0.000158  loss: 1.1124 (1.1058)  time: 1.1018  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:288]  [420/431]  eta: 0:00:12  lr: 0.000158  loss: 1.0751 (1.1061)  time: 1.1151  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:288]  [430/431]  eta: 0:00:01  lr: 0.000158  loss: 1.0522 (1.1047)  time: 1.1110  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:288] Total time: 0:07:59 (1.1132 s / it)\n",
      "Averaged stats: lr: 0.000158  loss: 1.0522 (1.1047)\n",
      "Valid: [epoch:288]  [ 0/14]  eta: 0:00:30  loss: 1.0103 (1.0103)  time: 2.2083  data: 2.0213  max mem: 15925\n",
      "Valid: [epoch:288]  [13/14]  eta: 0:00:00  loss: 1.0329 (1.0431)  time: 0.2377  data: 0.1445  max mem: 15925\n",
      "Valid: [epoch:288] Total time: 0:00:03 (0.2526 s / it)\n",
      "Averaged stats: loss: 1.0329 (1.0431)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_288_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:289]  [  0/431]  eta: 0:35:03  lr: 0.000158  loss: 1.3035 (1.3035)  time: 4.8814  data: 3.6717  max mem: 15925\n",
      "Train: [epoch:289]  [ 10/431]  eta: 0:09:44  lr: 0.000158  loss: 1.0709 (1.1224)  time: 1.3895  data: 0.3340  max mem: 15925\n",
      "Train: [epoch:289]  [ 20/431]  eta: 0:08:24  lr: 0.000158  loss: 1.0709 (1.1043)  time: 1.0450  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [ 30/431]  eta: 0:07:53  lr: 0.000158  loss: 1.0310 (1.0836)  time: 1.0670  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [ 40/431]  eta: 0:07:33  lr: 0.000158  loss: 1.0274 (1.0691)  time: 1.0903  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [ 50/431]  eta: 0:07:17  lr: 0.000158  loss: 1.0383 (1.0689)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [ 60/431]  eta: 0:07:04  lr: 0.000158  loss: 1.1007 (1.0763)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [ 70/431]  eta: 0:06:50  lr: 0.000158  loss: 1.1230 (1.0860)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [ 80/431]  eta: 0:06:37  lr: 0.000158  loss: 1.0744 (1.0924)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [ 90/431]  eta: 0:06:25  lr: 0.000158  loss: 1.0622 (1.0878)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [100/431]  eta: 0:06:13  lr: 0.000158  loss: 1.0527 (1.0879)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [110/431]  eta: 0:06:01  lr: 0.000158  loss: 1.1329 (1.0920)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [120/431]  eta: 0:05:50  lr: 0.000158  loss: 1.1016 (1.0895)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [130/431]  eta: 0:05:38  lr: 0.000158  loss: 1.0379 (1.0857)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [140/431]  eta: 0:05:27  lr: 0.000158  loss: 1.0454 (1.0862)  time: 1.1130  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [150/431]  eta: 0:05:15  lr: 0.000158  loss: 1.1272 (1.0875)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [160/431]  eta: 0:05:04  lr: 0.000158  loss: 1.1349 (1.0923)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [170/431]  eta: 0:04:52  lr: 0.000158  loss: 1.1163 (1.0897)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [180/431]  eta: 0:04:41  lr: 0.000158  loss: 1.1039 (1.0926)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [190/431]  eta: 0:04:29  lr: 0.000158  loss: 1.1021 (1.0914)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [200/431]  eta: 0:04:18  lr: 0.000158  loss: 1.0290 (1.0881)  time: 1.1085  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:289]  [210/431]  eta: 0:04:07  lr: 0.000158  loss: 1.0543 (1.0887)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [220/431]  eta: 0:03:55  lr: 0.000158  loss: 1.0803 (1.0896)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [230/431]  eta: 0:03:44  lr: 0.000158  loss: 1.0588 (1.0875)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [240/431]  eta: 0:03:33  lr: 0.000158  loss: 1.0588 (1.0886)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [250/431]  eta: 0:03:21  lr: 0.000158  loss: 1.1009 (1.0884)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [260/431]  eta: 0:03:10  lr: 0.000158  loss: 1.0865 (1.0884)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [270/431]  eta: 0:02:59  lr: 0.000158  loss: 1.0939 (1.0898)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [280/431]  eta: 0:02:48  lr: 0.000158  loss: 1.1245 (1.0916)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [290/431]  eta: 0:02:37  lr: 0.000158  loss: 1.1092 (1.0939)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [300/431]  eta: 0:02:25  lr: 0.000158  loss: 1.1129 (1.0958)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [310/431]  eta: 0:02:14  lr: 0.000158  loss: 1.0889 (1.0957)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [320/431]  eta: 0:02:03  lr: 0.000158  loss: 1.0889 (1.0973)  time: 1.1084  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:289]  [330/431]  eta: 0:01:52  lr: 0.000158  loss: 1.1509 (1.1006)  time: 1.1056  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:289]  [340/431]  eta: 0:01:41  lr: 0.000158  loss: 1.1577 (1.1030)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [350/431]  eta: 0:01:30  lr: 0.000158  loss: 1.1204 (1.1027)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [360/431]  eta: 0:01:19  lr: 0.000158  loss: 1.1074 (1.1042)  time: 1.1167  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [370/431]  eta: 0:01:07  lr: 0.000158  loss: 1.0387 (1.1018)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [380/431]  eta: 0:00:56  lr: 0.000158  loss: 1.0350 (1.1028)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [390/431]  eta: 0:00:45  lr: 0.000158  loss: 1.1287 (1.1032)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [400/431]  eta: 0:00:34  lr: 0.000158  loss: 1.0742 (1.1031)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:289]  [410/431]  eta: 0:00:23  lr: 0.000158  loss: 1.0674 (1.1037)  time: 1.0998  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:289]  [420/431]  eta: 0:00:12  lr: 0.000158  loss: 1.0777 (1.1035)  time: 1.1059  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:289]  [430/431]  eta: 0:00:01  lr: 0.000158  loss: 1.1056 (1.1034)  time: 1.1072  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:289] Total time: 0:07:59 (1.1127 s / it)\n",
      "Averaged stats: lr: 0.000158  loss: 1.1056 (1.1034)\n",
      "Valid: [epoch:289]  [ 0/14]  eta: 0:00:35  loss: 1.0958 (1.0958)  time: 2.5500  data: 2.3876  max mem: 15925\n",
      "Valid: [epoch:289]  [13/14]  eta: 0:00:00  loss: 1.0463 (1.0529)  time: 0.2876  data: 0.1707  max mem: 15925\n",
      "Valid: [epoch:289] Total time: 0:00:04 (0.3076 s / it)\n",
      "Averaged stats: loss: 1.0463 (1.0529)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_289_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.053%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:290]  [  0/431]  eta: 0:34:38  lr: 0.000158  loss: 1.0283 (1.0283)  time: 4.8230  data: 3.6132  max mem: 15925\n",
      "Train: [epoch:290]  [ 10/431]  eta: 0:09:37  lr: 0.000158  loss: 1.1588 (1.1312)  time: 1.3721  data: 0.3287  max mem: 15925\n",
      "Train: [epoch:290]  [ 20/431]  eta: 0:08:23  lr: 0.000158  loss: 1.1588 (1.1434)  time: 1.0449  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [ 30/431]  eta: 0:07:51  lr: 0.000158  loss: 1.1218 (1.1325)  time: 1.0664  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [ 40/431]  eta: 0:07:31  lr: 0.000158  loss: 1.1042 (1.1290)  time: 1.0826  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [ 50/431]  eta: 0:07:15  lr: 0.000158  loss: 1.0862 (1.1150)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [ 60/431]  eta: 0:07:01  lr: 0.000158  loss: 1.0400 (1.1083)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [ 70/431]  eta: 0:06:48  lr: 0.000158  loss: 1.0811 (1.1093)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [ 80/431]  eta: 0:06:36  lr: 0.000158  loss: 1.0777 (1.1123)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [ 90/431]  eta: 0:06:24  lr: 0.000158  loss: 1.0748 (1.1113)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [100/431]  eta: 0:06:11  lr: 0.000158  loss: 1.0706 (1.1101)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [110/431]  eta: 0:06:00  lr: 0.000158  loss: 1.0706 (1.1085)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [120/431]  eta: 0:05:48  lr: 0.000158  loss: 1.0658 (1.1073)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [130/431]  eta: 0:05:36  lr: 0.000158  loss: 1.0559 (1.1097)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [140/431]  eta: 0:05:25  lr: 0.000158  loss: 1.0514 (1.1045)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [150/431]  eta: 0:05:14  lr: 0.000158  loss: 1.0577 (1.1067)  time: 1.1098  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:290]  [160/431]  eta: 0:05:02  lr: 0.000158  loss: 1.0844 (1.1044)  time: 1.1008  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:290]  [170/431]  eta: 0:04:51  lr: 0.000158  loss: 1.0013 (1.0992)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [180/431]  eta: 0:04:40  lr: 0.000158  loss: 1.0770 (1.1012)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [190/431]  eta: 0:04:28  lr: 0.000158  loss: 1.1351 (1.1050)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [200/431]  eta: 0:04:17  lr: 0.000158  loss: 1.1127 (1.1057)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [210/431]  eta: 0:04:06  lr: 0.000158  loss: 1.0832 (1.1039)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [220/431]  eta: 0:03:54  lr: 0.000158  loss: 1.0832 (1.1046)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [230/431]  eta: 0:03:43  lr: 0.000158  loss: 1.0891 (1.1037)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [240/431]  eta: 0:03:32  lr: 0.000158  loss: 1.0728 (1.1045)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [250/431]  eta: 0:03:21  lr: 0.000158  loss: 1.0728 (1.1054)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [260/431]  eta: 0:03:10  lr: 0.000158  loss: 1.0544 (1.1034)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [270/431]  eta: 0:02:59  lr: 0.000158  loss: 1.0616 (1.1046)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [280/431]  eta: 0:02:47  lr: 0.000158  loss: 1.0957 (1.1038)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [290/431]  eta: 0:02:36  lr: 0.000158  loss: 1.0273 (1.1015)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [300/431]  eta: 0:02:25  lr: 0.000158  loss: 1.0428 (1.1027)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [310/431]  eta: 0:02:14  lr: 0.000158  loss: 1.0693 (1.1020)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [320/431]  eta: 0:02:03  lr: 0.000158  loss: 1.0703 (1.1024)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [330/431]  eta: 0:01:52  lr: 0.000158  loss: 1.1010 (1.1025)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [340/431]  eta: 0:01:41  lr: 0.000158  loss: 1.1010 (1.1044)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [350/431]  eta: 0:01:29  lr: 0.000158  loss: 1.1356 (1.1061)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [360/431]  eta: 0:01:18  lr: 0.000158  loss: 1.1029 (1.1057)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [370/431]  eta: 0:01:07  lr: 0.000158  loss: 1.0820 (1.1060)  time: 1.1116  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:290]  [380/431]  eta: 0:00:56  lr: 0.000158  loss: 1.0547 (1.1049)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [390/431]  eta: 0:00:45  lr: 0.000158  loss: 1.0837 (1.1048)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [400/431]  eta: 0:00:34  lr: 0.000158  loss: 1.1160 (1.1044)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:290]  [410/431]  eta: 0:00:23  lr: 0.000158  loss: 1.0478 (1.1043)  time: 1.0989  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:290]  [420/431]  eta: 0:00:12  lr: 0.000158  loss: 1.0906 (1.1043)  time: 1.0935  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:290]  [430/431]  eta: 0:00:01  lr: 0.000158  loss: 1.0896 (1.1034)  time: 1.0991  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:290] Total time: 0:07:58 (1.1104 s / it)\n",
      "Averaged stats: lr: 0.000158  loss: 1.0896 (1.1034)\n",
      "Valid: [epoch:290]  [ 0/14]  eta: 0:00:35  loss: 0.9306 (0.9306)  time: 2.5526  data: 2.3616  max mem: 15925\n",
      "Valid: [epoch:290]  [13/14]  eta: 0:00:00  loss: 1.0323 (1.0420)  time: 0.2632  data: 0.1688  max mem: 15925\n",
      "Valid: [epoch:290] Total time: 0:00:03 (0.2820 s / it)\n",
      "Averaged stats: loss: 1.0323 (1.0420)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_290_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:291]  [  0/431]  eta: 0:31:06  lr: 0.000158  loss: 1.1369 (1.1369)  time: 4.3305  data: 3.0837  max mem: 15925\n",
      "Train: [epoch:291]  [ 10/431]  eta: 0:09:22  lr: 0.000158  loss: 1.1286 (1.1421)  time: 1.3373  data: 0.2805  max mem: 15925\n",
      "Train: [epoch:291]  [ 20/431]  eta: 0:08:14  lr: 0.000158  loss: 1.1131 (1.1423)  time: 1.0479  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [ 30/431]  eta: 0:07:46  lr: 0.000158  loss: 1.0297 (1.1048)  time: 1.0661  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [ 40/431]  eta: 0:07:27  lr: 0.000158  loss: 1.0297 (1.0951)  time: 1.0828  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [ 50/431]  eta: 0:07:11  lr: 0.000158  loss: 1.0462 (1.0883)  time: 1.0878  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [ 60/431]  eta: 0:06:58  lr: 0.000158  loss: 1.0345 (1.0858)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [ 70/431]  eta: 0:06:45  lr: 0.000158  loss: 1.0575 (1.0856)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [ 80/431]  eta: 0:06:33  lr: 0.000158  loss: 1.0722 (1.0826)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [ 90/431]  eta: 0:06:21  lr: 0.000158  loss: 1.0722 (1.0888)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [100/431]  eta: 0:06:10  lr: 0.000158  loss: 1.0606 (1.0905)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [110/431]  eta: 0:05:58  lr: 0.000158  loss: 1.0539 (1.0893)  time: 1.1026  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:291]  [120/431]  eta: 0:05:46  lr: 0.000158  loss: 1.0316 (1.0880)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [130/431]  eta: 0:05:35  lr: 0.000158  loss: 1.0583 (1.0887)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [140/431]  eta: 0:05:24  lr: 0.000158  loss: 1.0377 (1.0871)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [150/431]  eta: 0:05:12  lr: 0.000158  loss: 1.0867 (1.0911)  time: 1.1011  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:291]  [160/431]  eta: 0:05:01  lr: 0.000158  loss: 1.0867 (1.0894)  time: 1.1017  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:291]  [170/431]  eta: 0:04:50  lr: 0.000158  loss: 1.0319 (1.0872)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [180/431]  eta: 0:04:39  lr: 0.000158  loss: 1.0150 (1.0849)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [190/431]  eta: 0:04:27  lr: 0.000158  loss: 1.0474 (1.0852)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [200/431]  eta: 0:04:16  lr: 0.000158  loss: 1.0643 (1.0881)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [210/431]  eta: 0:04:05  lr: 0.000158  loss: 1.0578 (1.0877)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [220/431]  eta: 0:03:54  lr: 0.000158  loss: 1.0536 (1.0869)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [230/431]  eta: 0:03:43  lr: 0.000158  loss: 1.0795 (1.0880)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [240/431]  eta: 0:03:32  lr: 0.000158  loss: 1.0902 (1.0882)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [250/431]  eta: 0:03:20  lr: 0.000158  loss: 1.1352 (1.0906)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [260/431]  eta: 0:03:09  lr: 0.000158  loss: 1.1333 (1.0909)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [270/431]  eta: 0:02:58  lr: 0.000158  loss: 1.1402 (1.0953)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [280/431]  eta: 0:02:47  lr: 0.000158  loss: 1.0933 (1.0938)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [290/431]  eta: 0:02:36  lr: 0.000158  loss: 1.0352 (1.0941)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [300/431]  eta: 0:02:25  lr: 0.000158  loss: 1.0682 (1.0960)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [310/431]  eta: 0:02:14  lr: 0.000158  loss: 1.0539 (1.0951)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [320/431]  eta: 0:02:03  lr: 0.000158  loss: 1.0676 (1.0979)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [330/431]  eta: 0:01:52  lr: 0.000158  loss: 1.1099 (1.0979)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [340/431]  eta: 0:01:40  lr: 0.000158  loss: 1.0790 (1.0985)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [350/431]  eta: 0:01:29  lr: 0.000158  loss: 1.0788 (1.0993)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [360/431]  eta: 0:01:18  lr: 0.000158  loss: 1.0899 (1.1007)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [370/431]  eta: 0:01:07  lr: 0.000158  loss: 1.1378 (1.1016)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [380/431]  eta: 0:00:56  lr: 0.000158  loss: 1.0757 (1.1002)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [390/431]  eta: 0:00:45  lr: 0.000158  loss: 1.0503 (1.1010)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [400/431]  eta: 0:00:34  lr: 0.000158  loss: 1.1273 (1.1014)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [410/431]  eta: 0:00:23  lr: 0.000158  loss: 1.1098 (1.1013)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:291]  [420/431]  eta: 0:00:12  lr: 0.000158  loss: 1.1405 (1.1022)  time: 1.1059  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:291]  [430/431]  eta: 0:00:01  lr: 0.000158  loss: 1.0654 (1.1028)  time: 1.1071  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:291] Total time: 0:07:58 (1.1093 s / it)\n",
      "Averaged stats: lr: 0.000158  loss: 1.0654 (1.1028)\n",
      "Valid: [epoch:291]  [ 0/14]  eta: 0:00:37  loss: 1.0983 (1.0983)  time: 2.6495  data: 2.5164  max mem: 15925\n",
      "Valid: [epoch:291]  [13/14]  eta: 0:00:00  loss: 1.0346 (1.0455)  time: 0.2828  data: 0.1798  max mem: 15925\n",
      "Valid: [epoch:291] Total time: 0:00:04 (0.3001 s / it)\n",
      "Averaged stats: loss: 1.0346 (1.0455)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_291_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.040\n",
      "Best Epoch: 280.000\n",
      "Train: [epoch:292]  [  0/431]  eta: 0:31:20  lr: 0.000158  loss: 1.1990 (1.1990)  time: 4.3623  data: 3.1565  max mem: 15925\n",
      "Train: [epoch:292]  [ 10/431]  eta: 0:09:26  lr: 0.000158  loss: 1.1440 (1.1675)  time: 1.3466  data: 0.2872  max mem: 15925\n",
      "Train: [epoch:292]  [ 20/431]  eta: 0:08:15  lr: 0.000158  loss: 1.0737 (1.1113)  time: 1.0468  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [ 30/431]  eta: 0:07:47  lr: 0.000158  loss: 1.0706 (1.1164)  time: 1.0665  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [ 40/431]  eta: 0:07:27  lr: 0.000158  loss: 1.0712 (1.1026)  time: 1.0806  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [ 50/431]  eta: 0:07:12  lr: 0.000158  loss: 1.0517 (1.0975)  time: 1.0851  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:292]  [ 60/431]  eta: 0:06:59  lr: 0.000158  loss: 1.0291 (1.0852)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [ 70/431]  eta: 0:06:46  lr: 0.000158  loss: 1.0430 (1.0951)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [ 80/431]  eta: 0:06:34  lr: 0.000158  loss: 1.1368 (1.1089)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [ 90/431]  eta: 0:06:22  lr: 0.000158  loss: 1.1341 (1.1082)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [100/431]  eta: 0:06:10  lr: 0.000158  loss: 1.1109 (1.1146)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [110/431]  eta: 0:05:59  lr: 0.000158  loss: 1.1348 (1.1159)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [120/431]  eta: 0:05:47  lr: 0.000158  loss: 1.1099 (1.1195)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [130/431]  eta: 0:05:36  lr: 0.000158  loss: 1.1089 (1.1208)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [140/431]  eta: 0:05:24  lr: 0.000158  loss: 1.1089 (1.1220)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [150/431]  eta: 0:05:13  lr: 0.000158  loss: 1.0952 (1.1232)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [160/431]  eta: 0:05:01  lr: 0.000158  loss: 1.0555 (1.1182)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [170/431]  eta: 0:04:50  lr: 0.000158  loss: 1.0385 (1.1151)  time: 1.1151  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:292]  [180/431]  eta: 0:04:39  lr: 0.000158  loss: 1.0925 (1.1175)  time: 1.1104  data: 0.0005  max mem: 15925\n",
      "Train: [epoch:292]  [190/431]  eta: 0:04:28  lr: 0.000158  loss: 1.1063 (1.1166)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [200/431]  eta: 0:04:17  lr: 0.000158  loss: 1.0610 (1.1124)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [210/431]  eta: 0:04:05  lr: 0.000158  loss: 1.0661 (1.1122)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [220/431]  eta: 0:03:54  lr: 0.000158  loss: 1.0968 (1.1112)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [230/431]  eta: 0:03:43  lr: 0.000158  loss: 1.1163 (1.1113)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [240/431]  eta: 0:03:32  lr: 0.000158  loss: 1.0745 (1.1091)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [250/431]  eta: 0:03:21  lr: 0.000158  loss: 1.0530 (1.1079)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [260/431]  eta: 0:03:10  lr: 0.000158  loss: 1.0698 (1.1069)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [270/431]  eta: 0:02:58  lr: 0.000158  loss: 1.0979 (1.1064)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [280/431]  eta: 0:02:47  lr: 0.000158  loss: 1.1004 (1.1069)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [290/431]  eta: 0:02:36  lr: 0.000158  loss: 1.0860 (1.1065)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [300/431]  eta: 0:02:25  lr: 0.000158  loss: 1.1036 (1.1069)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [310/431]  eta: 0:02:14  lr: 0.000158  loss: 1.1027 (1.1061)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [320/431]  eta: 0:02:03  lr: 0.000158  loss: 1.0531 (1.1044)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [330/431]  eta: 0:01:52  lr: 0.000158  loss: 1.0531 (1.1039)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [340/431]  eta: 0:01:40  lr: 0.000158  loss: 1.1332 (1.1051)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [350/431]  eta: 0:01:29  lr: 0.000158  loss: 1.1449 (1.1075)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [360/431]  eta: 0:01:18  lr: 0.000158  loss: 1.0747 (1.1059)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [370/431]  eta: 0:01:07  lr: 0.000158  loss: 1.0618 (1.1065)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [380/431]  eta: 0:00:56  lr: 0.000158  loss: 1.0950 (1.1071)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [390/431]  eta: 0:00:45  lr: 0.000158  loss: 1.0577 (1.1069)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [400/431]  eta: 0:00:34  lr: 0.000158  loss: 1.0466 (1.1058)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:292]  [410/431]  eta: 0:00:23  lr: 0.000158  loss: 1.0524 (1.1060)  time: 1.1053  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:292]  [420/431]  eta: 0:00:12  lr: 0.000158  loss: 1.0489 (1.1040)  time: 1.1055  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:292]  [430/431]  eta: 0:00:01  lr: 0.000158  loss: 1.0078 (1.1028)  time: 1.1093  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:292] Total time: 0:07:58 (1.1091 s / it)\n",
      "Averaged stats: lr: 0.000158  loss: 1.0078 (1.1028)\n",
      "Valid: [epoch:292]  [ 0/14]  eta: 0:00:35  loss: 1.0875 (1.0875)  time: 2.5706  data: 2.4174  max mem: 15925\n",
      "Valid: [epoch:292]  [13/14]  eta: 0:00:00  loss: 1.0279 (1.0380)  time: 0.2615  data: 0.1728  max mem: 15925\n",
      "Valid: [epoch:292] Total time: 0:00:03 (0.2792 s / it)\n",
      "Averaged stats: loss: 1.0279 (1.0380)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_292_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.038%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:293]  [  0/431]  eta: 0:34:18  lr: 0.000157  loss: 1.2193 (1.2193)  time: 4.7769  data: 3.4903  max mem: 15925\n",
      "Train: [epoch:293]  [ 10/431]  eta: 0:09:33  lr: 0.000157  loss: 1.1608 (1.1539)  time: 1.3632  data: 0.3174  max mem: 15925\n",
      "Train: [epoch:293]  [ 20/431]  eta: 0:08:22  lr: 0.000157  loss: 1.0938 (1.1194)  time: 1.0462  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [ 30/431]  eta: 0:07:51  lr: 0.000157  loss: 1.0719 (1.1230)  time: 1.0724  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [ 40/431]  eta: 0:07:31  lr: 0.000157  loss: 1.1309 (1.1328)  time: 1.0807  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [ 50/431]  eta: 0:07:14  lr: 0.000157  loss: 1.1055 (1.1308)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [ 60/431]  eta: 0:07:00  lr: 0.000157  loss: 1.0610 (1.1222)  time: 1.0881  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [ 70/431]  eta: 0:06:47  lr: 0.000157  loss: 1.0478 (1.1168)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [ 80/431]  eta: 0:06:35  lr: 0.000157  loss: 1.0440 (1.1119)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [ 90/431]  eta: 0:06:23  lr: 0.000157  loss: 1.0440 (1.1073)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [100/431]  eta: 0:06:11  lr: 0.000157  loss: 1.0795 (1.1093)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [110/431]  eta: 0:05:59  lr: 0.000157  loss: 1.0789 (1.1073)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [120/431]  eta: 0:05:48  lr: 0.000157  loss: 1.0909 (1.1063)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [130/431]  eta: 0:05:36  lr: 0.000157  loss: 1.0943 (1.1095)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [140/431]  eta: 0:05:25  lr: 0.000157  loss: 1.0628 (1.1053)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [150/431]  eta: 0:05:14  lr: 0.000157  loss: 1.0421 (1.1024)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [160/431]  eta: 0:05:02  lr: 0.000157  loss: 1.0815 (1.1060)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [170/431]  eta: 0:04:51  lr: 0.000157  loss: 1.1227 (1.1073)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [180/431]  eta: 0:04:40  lr: 0.000157  loss: 1.1227 (1.1090)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [190/431]  eta: 0:04:28  lr: 0.000157  loss: 1.0795 (1.1068)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [200/431]  eta: 0:04:17  lr: 0.000157  loss: 1.0441 (1.1058)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [210/431]  eta: 0:04:06  lr: 0.000157  loss: 1.0737 (1.1054)  time: 1.1101  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:293]  [220/431]  eta: 0:03:55  lr: 0.000157  loss: 1.0979 (1.1059)  time: 1.1077  data: 0.0003  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:293]  [230/431]  eta: 0:03:43  lr: 0.000157  loss: 1.0979 (1.1068)  time: 1.1026  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:293]  [240/431]  eta: 0:03:32  lr: 0.000157  loss: 1.1172 (1.1084)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [250/431]  eta: 0:03:21  lr: 0.000157  loss: 1.1192 (1.1082)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [260/431]  eta: 0:03:10  lr: 0.000157  loss: 1.1006 (1.1068)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [270/431]  eta: 0:02:59  lr: 0.000157  loss: 1.0644 (1.1068)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [280/431]  eta: 0:02:47  lr: 0.000157  loss: 1.0438 (1.1065)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [290/431]  eta: 0:02:36  lr: 0.000157  loss: 1.0600 (1.1055)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [300/431]  eta: 0:02:25  lr: 0.000157  loss: 1.0839 (1.1062)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [310/431]  eta: 0:02:14  lr: 0.000157  loss: 1.0849 (1.1055)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [320/431]  eta: 0:02:03  lr: 0.000157  loss: 1.0316 (1.1046)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [330/431]  eta: 0:01:52  lr: 0.000157  loss: 1.0357 (1.1031)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [340/431]  eta: 0:01:41  lr: 0.000157  loss: 1.1057 (1.1068)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [350/431]  eta: 0:01:29  lr: 0.000157  loss: 1.1137 (1.1063)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [360/431]  eta: 0:01:18  lr: 0.000157  loss: 1.0841 (1.1065)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [370/431]  eta: 0:01:07  lr: 0.000157  loss: 1.1023 (1.1063)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [380/431]  eta: 0:00:56  lr: 0.000157  loss: 1.0486 (1.1047)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [390/431]  eta: 0:00:45  lr: 0.000157  loss: 1.0353 (1.1034)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [400/431]  eta: 0:00:34  lr: 0.000157  loss: 1.0294 (1.1017)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:293]  [410/431]  eta: 0:00:23  lr: 0.000157  loss: 1.0188 (1.1016)  time: 1.1028  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:293]  [420/431]  eta: 0:00:12  lr: 0.000157  loss: 1.0583 (1.1017)  time: 1.1035  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:293]  [430/431]  eta: 0:00:01  lr: 0.000157  loss: 1.0897 (1.1015)  time: 1.0996  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:293] Total time: 0:07:58 (1.1100 s / it)\n",
      "Averaged stats: lr: 0.000157  loss: 1.0897 (1.1015)\n",
      "Valid: [epoch:293]  [ 0/14]  eta: 0:00:35  loss: 1.1027 (1.1027)  time: 2.5534  data: 2.3762  max mem: 15925\n",
      "Valid: [epoch:293]  [13/14]  eta: 0:00:00  loss: 1.0382 (1.0503)  time: 0.2709  data: 0.1698  max mem: 15925\n",
      "Valid: [epoch:293] Total time: 0:00:04 (0.2872 s / it)\n",
      "Averaged stats: loss: 1.0382 (1.0503)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_293_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:294]  [  0/431]  eta: 0:33:23  lr: 0.000157  loss: 1.1343 (1.1343)  time: 4.6491  data: 3.4919  max mem: 15925\n",
      "Train: [epoch:294]  [ 10/431]  eta: 0:09:43  lr: 0.000157  loss: 1.2328 (1.2267)  time: 1.3852  data: 0.3176  max mem: 15925\n",
      "Train: [epoch:294]  [ 20/431]  eta: 0:08:29  lr: 0.000157  loss: 1.1108 (1.1412)  time: 1.0693  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [ 30/431]  eta: 0:07:58  lr: 0.000157  loss: 1.0722 (1.1244)  time: 1.0858  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [ 40/431]  eta: 0:07:37  lr: 0.000157  loss: 1.0773 (1.1177)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [ 50/431]  eta: 0:07:19  lr: 0.000157  loss: 1.0300 (1.1010)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [ 60/431]  eta: 0:07:05  lr: 0.000157  loss: 0.9973 (1.0908)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [ 70/431]  eta: 0:06:51  lr: 0.000157  loss: 1.0448 (1.0999)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [ 80/431]  eta: 0:06:39  lr: 0.000157  loss: 1.0889 (1.1029)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [ 90/431]  eta: 0:06:26  lr: 0.000157  loss: 1.0727 (1.0967)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [100/431]  eta: 0:06:14  lr: 0.000157  loss: 1.0299 (1.0953)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [110/431]  eta: 0:06:02  lr: 0.000157  loss: 1.0356 (1.0957)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [120/431]  eta: 0:05:50  lr: 0.000157  loss: 1.0760 (1.0947)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [130/431]  eta: 0:05:39  lr: 0.000157  loss: 1.0647 (1.0970)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [140/431]  eta: 0:05:27  lr: 0.000157  loss: 1.0515 (1.1009)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [150/431]  eta: 0:05:16  lr: 0.000157  loss: 1.1257 (1.1027)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [160/431]  eta: 0:05:04  lr: 0.000157  loss: 1.1242 (1.1045)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [170/431]  eta: 0:04:52  lr: 0.000157  loss: 1.0719 (1.1013)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [180/431]  eta: 0:04:41  lr: 0.000157  loss: 1.0505 (1.1027)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [190/431]  eta: 0:04:30  lr: 0.000157  loss: 1.0935 (1.1023)  time: 1.1196  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [200/431]  eta: 0:04:19  lr: 0.000157  loss: 1.1194 (1.1034)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [210/431]  eta: 0:04:07  lr: 0.000157  loss: 1.1279 (1.1071)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [220/431]  eta: 0:03:56  lr: 0.000157  loss: 1.1090 (1.1076)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [230/431]  eta: 0:03:44  lr: 0.000157  loss: 1.0877 (1.1071)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [240/431]  eta: 0:03:33  lr: 0.000157  loss: 1.1121 (1.1087)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [250/431]  eta: 0:03:22  lr: 0.000157  loss: 1.1344 (1.1095)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [260/431]  eta: 0:03:11  lr: 0.000157  loss: 1.1090 (1.1091)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [270/431]  eta: 0:02:59  lr: 0.000157  loss: 1.0885 (1.1087)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [280/431]  eta: 0:02:48  lr: 0.000157  loss: 1.0521 (1.1087)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [290/431]  eta: 0:02:37  lr: 0.000157  loss: 1.0769 (1.1088)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [300/431]  eta: 0:02:26  lr: 0.000157  loss: 1.0912 (1.1096)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [310/431]  eta: 0:02:15  lr: 0.000157  loss: 1.0694 (1.1093)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [320/431]  eta: 0:02:03  lr: 0.000157  loss: 1.0484 (1.1076)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [330/431]  eta: 0:01:52  lr: 0.000157  loss: 1.0618 (1.1076)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [340/431]  eta: 0:01:41  lr: 0.000157  loss: 1.0956 (1.1074)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [350/431]  eta: 0:01:30  lr: 0.000157  loss: 1.0805 (1.1059)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [360/431]  eta: 0:01:19  lr: 0.000157  loss: 1.0633 (1.1048)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [370/431]  eta: 0:01:08  lr: 0.000157  loss: 1.0384 (1.1034)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [380/431]  eta: 0:00:56  lr: 0.000157  loss: 1.0639 (1.1039)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [390/431]  eta: 0:00:45  lr: 0.000157  loss: 1.0657 (1.1042)  time: 1.1129  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:294]  [400/431]  eta: 0:00:34  lr: 0.000157  loss: 1.0624 (1.1036)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [410/431]  eta: 0:00:23  lr: 0.000157  loss: 1.0417 (1.1034)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:294]  [420/431]  eta: 0:00:12  lr: 0.000157  loss: 1.0997 (1.1049)  time: 1.1105  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:294]  [430/431]  eta: 0:00:01  lr: 0.000157  loss: 1.1072 (1.1043)  time: 1.1075  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:294] Total time: 0:08:00 (1.1154 s / it)\n",
      "Averaged stats: lr: 0.000157  loss: 1.1072 (1.1043)\n",
      "Valid: [epoch:294]  [ 0/14]  eta: 0:00:36  loss: 0.9389 (0.9389)  time: 2.5842  data: 2.4101  max mem: 15925\n",
      "Valid: [epoch:294]  [13/14]  eta: 0:00:00  loss: 1.0420 (1.0495)  time: 0.2755  data: 0.1722  max mem: 15925\n",
      "Valid: [epoch:294] Total time: 0:00:04 (0.2922 s / it)\n",
      "Averaged stats: loss: 1.0420 (1.0495)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_294_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:295]  [  0/431]  eta: 0:34:36  lr: 0.000157  loss: 1.2805 (1.2805)  time: 4.8184  data: 3.6139  max mem: 15925\n",
      "Train: [epoch:295]  [ 10/431]  eta: 0:09:34  lr: 0.000157  loss: 1.2266 (1.1827)  time: 1.3643  data: 0.3287  max mem: 15925\n",
      "Train: [epoch:295]  [ 20/431]  eta: 0:08:21  lr: 0.000157  loss: 1.1492 (1.1656)  time: 1.0397  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [ 30/431]  eta: 0:07:50  lr: 0.000157  loss: 1.1257 (1.1547)  time: 1.0701  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [ 40/431]  eta: 0:07:31  lr: 0.000157  loss: 1.1171 (1.1411)  time: 1.0889  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [ 50/431]  eta: 0:07:16  lr: 0.000157  loss: 1.1077 (1.1328)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [ 60/431]  eta: 0:07:01  lr: 0.000157  loss: 1.1230 (1.1344)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [ 70/431]  eta: 0:06:49  lr: 0.000157  loss: 1.1220 (1.1385)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [ 80/431]  eta: 0:06:36  lr: 0.000157  loss: 1.0577 (1.1256)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [ 90/431]  eta: 0:06:24  lr: 0.000157  loss: 1.0205 (1.1156)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [100/431]  eta: 0:06:12  lr: 0.000157  loss: 1.0205 (1.1088)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [110/431]  eta: 0:06:01  lr: 0.000157  loss: 1.0143 (1.1019)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [120/431]  eta: 0:05:49  lr: 0.000157  loss: 0.9985 (1.1007)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [130/431]  eta: 0:05:38  lr: 0.000157  loss: 1.0610 (1.1022)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [140/431]  eta: 0:05:27  lr: 0.000157  loss: 1.0647 (1.0989)  time: 1.1225  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [150/431]  eta: 0:05:15  lr: 0.000157  loss: 1.0729 (1.1024)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [160/431]  eta: 0:05:03  lr: 0.000157  loss: 1.1538 (1.1079)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [170/431]  eta: 0:04:52  lr: 0.000157  loss: 1.1340 (1.1076)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [180/431]  eta: 0:04:41  lr: 0.000157  loss: 1.1169 (1.1086)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [190/431]  eta: 0:04:29  lr: 0.000157  loss: 1.1267 (1.1086)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [200/431]  eta: 0:04:18  lr: 0.000157  loss: 1.1267 (1.1071)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [210/431]  eta: 0:04:07  lr: 0.000157  loss: 1.0208 (1.1045)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [220/431]  eta: 0:03:55  lr: 0.000157  loss: 1.0427 (1.1048)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [230/431]  eta: 0:03:44  lr: 0.000157  loss: 1.0890 (1.1050)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [240/431]  eta: 0:03:33  lr: 0.000157  loss: 1.0722 (1.1048)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [250/431]  eta: 0:03:22  lr: 0.000157  loss: 1.0500 (1.1013)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [260/431]  eta: 0:03:10  lr: 0.000157  loss: 1.0583 (1.1015)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [270/431]  eta: 0:02:59  lr: 0.000157  loss: 1.0734 (1.0993)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [280/431]  eta: 0:02:48  lr: 0.000157  loss: 1.0734 (1.0989)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [290/431]  eta: 0:02:37  lr: 0.000157  loss: 1.1026 (1.0992)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [300/431]  eta: 0:02:26  lr: 0.000157  loss: 1.1160 (1.1001)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [310/431]  eta: 0:02:14  lr: 0.000157  loss: 1.0839 (1.0994)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [320/431]  eta: 0:02:03  lr: 0.000157  loss: 1.1090 (1.1015)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [330/431]  eta: 0:01:52  lr: 0.000157  loss: 1.1362 (1.1020)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [340/431]  eta: 0:01:41  lr: 0.000157  loss: 1.1085 (1.1024)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [350/431]  eta: 0:01:30  lr: 0.000157  loss: 1.0801 (1.1017)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [360/431]  eta: 0:01:19  lr: 0.000157  loss: 1.0709 (1.1016)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [370/431]  eta: 0:01:07  lr: 0.000157  loss: 1.0231 (1.0999)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [380/431]  eta: 0:00:56  lr: 0.000157  loss: 1.0245 (1.0987)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [390/431]  eta: 0:00:45  lr: 0.000157  loss: 1.0751 (1.0997)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [400/431]  eta: 0:00:34  lr: 0.000157  loss: 1.1236 (1.0994)  time: 1.1189  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:295]  [410/431]  eta: 0:00:23  lr: 0.000157  loss: 1.1236 (1.1002)  time: 1.1159  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:295]  [420/431]  eta: 0:00:12  lr: 0.000157  loss: 1.1222 (1.1000)  time: 1.1095  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:295]  [430/431]  eta: 0:00:01  lr: 0.000157  loss: 1.1222 (1.1007)  time: 1.1127  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:295] Total time: 0:08:00 (1.1149 s / it)\n",
      "Averaged stats: lr: 0.000157  loss: 1.1222 (1.1007)\n",
      "Valid: [epoch:295]  [ 0/14]  eta: 0:00:36  loss: 0.9550 (0.9550)  time: 2.6288  data: 2.4585  max mem: 15925\n",
      "Valid: [epoch:295]  [13/14]  eta: 0:00:00  loss: 1.0333 (1.0435)  time: 0.2848  data: 0.1757  max mem: 15925\n",
      "Valid: [epoch:295] Total time: 0:00:04 (0.3015 s / it)\n",
      "Averaged stats: loss: 1.0333 (1.0435)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_295_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:296]  [  0/431]  eta: 0:31:35  lr: 0.000157  loss: 1.1969 (1.1969)  time: 4.3975  data: 3.1010  max mem: 15925\n",
      "Train: [epoch:296]  [ 10/431]  eta: 0:09:28  lr: 0.000157  loss: 1.1782 (1.1450)  time: 1.3510  data: 0.2821  max mem: 15925\n",
      "Train: [epoch:296]  [ 20/431]  eta: 0:08:18  lr: 0.000157  loss: 1.0683 (1.1186)  time: 1.0545  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [ 30/431]  eta: 0:07:49  lr: 0.000157  loss: 1.0313 (1.0977)  time: 1.0732  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [ 40/431]  eta: 0:07:29  lr: 0.000157  loss: 1.0691 (1.1082)  time: 1.0821  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [ 50/431]  eta: 0:07:14  lr: 0.000157  loss: 1.1182 (1.1171)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [ 60/431]  eta: 0:07:00  lr: 0.000157  loss: 1.0857 (1.1066)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [ 70/431]  eta: 0:06:48  lr: 0.000157  loss: 1.0719 (1.1106)  time: 1.1112  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:296]  [ 80/431]  eta: 0:06:35  lr: 0.000157  loss: 1.0816 (1.1102)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [ 90/431]  eta: 0:06:23  lr: 0.000157  loss: 1.0361 (1.1012)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [100/431]  eta: 0:06:11  lr: 0.000157  loss: 1.0412 (1.1007)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [110/431]  eta: 0:06:00  lr: 0.000157  loss: 1.0478 (1.0996)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [120/431]  eta: 0:05:48  lr: 0.000157  loss: 1.0632 (1.1005)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [130/431]  eta: 0:05:36  lr: 0.000157  loss: 1.0632 (1.0993)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [140/431]  eta: 0:05:25  lr: 0.000157  loss: 1.0846 (1.1014)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [150/431]  eta: 0:05:14  lr: 0.000157  loss: 1.0910 (1.1024)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [160/431]  eta: 0:05:02  lr: 0.000157  loss: 1.0910 (1.1051)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [170/431]  eta: 0:04:51  lr: 0.000157  loss: 0.9995 (1.1000)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [180/431]  eta: 0:04:40  lr: 0.000157  loss: 1.0519 (1.1020)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [190/431]  eta: 0:04:28  lr: 0.000157  loss: 1.0596 (1.0996)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [200/431]  eta: 0:04:17  lr: 0.000157  loss: 1.0508 (1.1003)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [210/431]  eta: 0:04:06  lr: 0.000157  loss: 1.0292 (1.0982)  time: 1.1055  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:296]  [220/431]  eta: 0:03:55  lr: 0.000157  loss: 1.0409 (1.1000)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [230/431]  eta: 0:03:43  lr: 0.000157  loss: 1.0586 (1.0976)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [240/431]  eta: 0:03:32  lr: 0.000157  loss: 1.0404 (1.0967)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [250/431]  eta: 0:03:21  lr: 0.000157  loss: 1.0820 (1.0971)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [260/431]  eta: 0:03:10  lr: 0.000157  loss: 1.0755 (1.0988)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [270/431]  eta: 0:02:59  lr: 0.000157  loss: 1.0510 (1.0989)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [280/431]  eta: 0:02:48  lr: 0.000157  loss: 1.1058 (1.1001)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [290/431]  eta: 0:02:37  lr: 0.000157  loss: 1.0890 (1.0993)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [300/431]  eta: 0:02:25  lr: 0.000157  loss: 1.0606 (1.0996)  time: 1.1252  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [310/431]  eta: 0:02:14  lr: 0.000157  loss: 1.0606 (1.0999)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [320/431]  eta: 0:02:03  lr: 0.000157  loss: 1.0513 (1.1001)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [330/431]  eta: 0:01:52  lr: 0.000157  loss: 1.0496 (1.0998)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [340/431]  eta: 0:01:41  lr: 0.000157  loss: 1.0625 (1.1006)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [350/431]  eta: 0:01:30  lr: 0.000157  loss: 1.0650 (1.1006)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [360/431]  eta: 0:01:19  lr: 0.000157  loss: 1.0722 (1.1014)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [370/431]  eta: 0:01:07  lr: 0.000157  loss: 1.0787 (1.1012)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [380/431]  eta: 0:00:56  lr: 0.000157  loss: 1.0549 (1.1025)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [390/431]  eta: 0:00:45  lr: 0.000157  loss: 1.1280 (1.1035)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [400/431]  eta: 0:00:34  lr: 0.000157  loss: 1.1241 (1.1035)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:296]  [410/431]  eta: 0:00:23  lr: 0.000157  loss: 1.1241 (1.1049)  time: 1.1124  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:296]  [420/431]  eta: 0:00:12  lr: 0.000157  loss: 1.1005 (1.1042)  time: 1.1113  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:296]  [430/431]  eta: 0:00:01  lr: 0.000157  loss: 1.0826 (1.1037)  time: 1.1076  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:296] Total time: 0:07:59 (1.1132 s / it)\n",
      "Averaged stats: lr: 0.000157  loss: 1.0826 (1.1037)\n",
      "Valid: [epoch:296]  [ 0/14]  eta: 0:00:35  loss: 1.1051 (1.1051)  time: 2.5535  data: 2.4035  max mem: 15925\n",
      "Valid: [epoch:296]  [13/14]  eta: 0:00:00  loss: 1.0511 (1.0558)  time: 0.2730  data: 0.1718  max mem: 15925\n",
      "Valid: [epoch:296] Total time: 0:00:04 (0.2869 s / it)\n",
      "Averaged stats: loss: 1.0511 (1.0558)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_296_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.056%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:297]  [  0/431]  eta: 0:34:10  lr: 0.000156  loss: 1.2231 (1.2231)  time: 4.7565  data: 3.5940  max mem: 15925\n",
      "Train: [epoch:297]  [ 10/431]  eta: 0:09:30  lr: 0.000156  loss: 1.1050 (1.1151)  time: 1.3560  data: 0.3269  max mem: 15925\n",
      "Train: [epoch:297]  [ 20/431]  eta: 0:08:18  lr: 0.000156  loss: 1.1050 (1.1395)  time: 1.0360  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [ 30/431]  eta: 0:07:48  lr: 0.000156  loss: 1.0711 (1.1126)  time: 1.0663  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:297]  [ 40/431]  eta: 0:07:30  lr: 0.000156  loss: 1.0629 (1.1041)  time: 1.0862  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [ 50/431]  eta: 0:07:14  lr: 0.000156  loss: 1.0923 (1.1074)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [ 60/431]  eta: 0:07:00  lr: 0.000156  loss: 1.0555 (1.0938)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [ 70/431]  eta: 0:06:47  lr: 0.000156  loss: 1.0660 (1.1015)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [ 80/431]  eta: 0:06:35  lr: 0.000156  loss: 1.0917 (1.1059)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [ 90/431]  eta: 0:06:23  lr: 0.000156  loss: 1.0994 (1.1106)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [100/431]  eta: 0:06:11  lr: 0.000156  loss: 1.0925 (1.1068)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [110/431]  eta: 0:06:00  lr: 0.000156  loss: 1.0539 (1.1048)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [120/431]  eta: 0:05:48  lr: 0.000156  loss: 1.0914 (1.1015)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [130/431]  eta: 0:05:36  lr: 0.000156  loss: 1.0914 (1.0998)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [140/431]  eta: 0:05:25  lr: 0.000156  loss: 1.0525 (1.0962)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [150/431]  eta: 0:05:13  lr: 0.000156  loss: 1.0505 (1.0942)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [160/431]  eta: 0:05:02  lr: 0.000156  loss: 1.0632 (1.0920)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [170/431]  eta: 0:04:51  lr: 0.000156  loss: 1.1080 (1.0946)  time: 1.1178  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [180/431]  eta: 0:04:40  lr: 0.000156  loss: 1.1080 (1.0971)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [190/431]  eta: 0:04:29  lr: 0.000156  loss: 1.1049 (1.0992)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [200/431]  eta: 0:04:17  lr: 0.000156  loss: 1.0914 (1.0974)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [210/431]  eta: 0:04:06  lr: 0.000156  loss: 1.0534 (1.0972)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [220/431]  eta: 0:03:55  lr: 0.000156  loss: 1.0195 (1.0964)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [230/431]  eta: 0:03:44  lr: 0.000156  loss: 1.0307 (1.0970)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [240/431]  eta: 0:03:32  lr: 0.000156  loss: 1.0587 (1.0973)  time: 1.1098  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:297]  [250/431]  eta: 0:03:21  lr: 0.000156  loss: 1.1277 (1.1010)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [260/431]  eta: 0:03:10  lr: 0.000156  loss: 1.1277 (1.1016)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [270/431]  eta: 0:02:59  lr: 0.000156  loss: 1.1035 (1.1033)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [280/431]  eta: 0:02:48  lr: 0.000156  loss: 1.1075 (1.1046)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [290/431]  eta: 0:02:36  lr: 0.000156  loss: 1.0674 (1.1032)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [300/431]  eta: 0:02:25  lr: 0.000156  loss: 1.0758 (1.1039)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [310/431]  eta: 0:02:14  lr: 0.000156  loss: 1.1127 (1.1044)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [320/431]  eta: 0:02:03  lr: 0.000156  loss: 1.0681 (1.1042)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [330/431]  eta: 0:01:52  lr: 0.000156  loss: 1.1345 (1.1054)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [340/431]  eta: 0:01:41  lr: 0.000156  loss: 1.0860 (1.1044)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [350/431]  eta: 0:01:29  lr: 0.000156  loss: 1.1041 (1.1055)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [360/431]  eta: 0:01:18  lr: 0.000156  loss: 1.0916 (1.1044)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [370/431]  eta: 0:01:07  lr: 0.000156  loss: 1.0876 (1.1050)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [380/431]  eta: 0:00:56  lr: 0.000156  loss: 1.1085 (1.1049)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [390/431]  eta: 0:00:45  lr: 0.000156  loss: 1.0808 (1.1037)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [400/431]  eta: 0:00:34  lr: 0.000156  loss: 1.0692 (1.1042)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:297]  [410/431]  eta: 0:00:23  lr: 0.000156  loss: 1.0462 (1.1028)  time: 1.1063  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:297]  [420/431]  eta: 0:00:12  lr: 0.000156  loss: 1.0319 (1.1019)  time: 1.1026  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:297]  [430/431]  eta: 0:00:01  lr: 0.000156  loss: 1.0542 (1.1018)  time: 1.1022  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:297] Total time: 0:07:58 (1.1107 s / it)\n",
      "Averaged stats: lr: 0.000156  loss: 1.0542 (1.1018)\n",
      "Valid: [epoch:297]  [ 0/14]  eta: 0:00:34  loss: 1.0832 (1.0832)  time: 2.4806  data: 2.3276  max mem: 15925\n",
      "Valid: [epoch:297]  [13/14]  eta: 0:00:00  loss: 1.0404 (1.0473)  time: 0.2522  data: 0.1663  max mem: 15925\n",
      "Valid: [epoch:297] Total time: 0:00:03 (0.2682 s / it)\n",
      "Averaged stats: loss: 1.0404 (1.0473)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_297_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:298]  [  0/431]  eta: 0:31:11  lr: 0.000156  loss: 1.0057 (1.0057)  time: 4.3429  data: 3.1126  max mem: 15925\n",
      "Train: [epoch:298]  [ 10/431]  eta: 0:09:25  lr: 0.000156  loss: 1.1258 (1.1200)  time: 1.3424  data: 0.2831  max mem: 15925\n",
      "Train: [epoch:298]  [ 20/431]  eta: 0:08:16  lr: 0.000156  loss: 1.0880 (1.1022)  time: 1.0517  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [ 30/431]  eta: 0:07:46  lr: 0.000156  loss: 1.0477 (1.0811)  time: 1.0655  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [ 40/431]  eta: 0:07:28  lr: 0.000156  loss: 1.0804 (1.0874)  time: 1.0849  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [ 50/431]  eta: 0:07:13  lr: 0.000156  loss: 1.0672 (1.0761)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [ 60/431]  eta: 0:07:00  lr: 0.000156  loss: 1.0146 (1.0717)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [ 70/431]  eta: 0:06:47  lr: 0.000156  loss: 1.1003 (1.0858)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [ 80/431]  eta: 0:06:35  lr: 0.000156  loss: 1.1175 (1.0887)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [ 90/431]  eta: 0:06:23  lr: 0.000156  loss: 1.1217 (1.0959)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [100/431]  eta: 0:06:11  lr: 0.000156  loss: 1.1195 (1.0968)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [110/431]  eta: 0:05:59  lr: 0.000156  loss: 1.0192 (1.0896)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [120/431]  eta: 0:05:48  lr: 0.000156  loss: 1.0186 (1.0882)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [130/431]  eta: 0:05:36  lr: 0.000156  loss: 1.0685 (1.0891)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [140/431]  eta: 0:05:25  lr: 0.000156  loss: 1.0748 (1.0894)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [150/431]  eta: 0:05:14  lr: 0.000156  loss: 1.0843 (1.0898)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [160/431]  eta: 0:05:02  lr: 0.000156  loss: 1.0677 (1.0901)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [170/431]  eta: 0:04:51  lr: 0.000156  loss: 1.0797 (1.0904)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [180/431]  eta: 0:04:40  lr: 0.000156  loss: 1.0854 (1.0902)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [190/431]  eta: 0:04:28  lr: 0.000156  loss: 1.0336 (1.0912)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [200/431]  eta: 0:04:17  lr: 0.000156  loss: 1.0725 (1.0929)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [210/431]  eta: 0:04:06  lr: 0.000156  loss: 1.0429 (1.0900)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [220/431]  eta: 0:03:55  lr: 0.000156  loss: 1.0212 (1.0919)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [230/431]  eta: 0:03:43  lr: 0.000156  loss: 1.1030 (1.0952)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [240/431]  eta: 0:03:32  lr: 0.000156  loss: 1.1030 (1.0970)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [250/431]  eta: 0:03:21  lr: 0.000156  loss: 1.1092 (1.0998)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [260/431]  eta: 0:03:10  lr: 0.000156  loss: 1.1365 (1.1000)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [270/431]  eta: 0:02:59  lr: 0.000156  loss: 1.2292 (1.1062)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [280/431]  eta: 0:02:48  lr: 0.000156  loss: 1.1065 (1.1049)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [290/431]  eta: 0:02:36  lr: 0.000156  loss: 1.0167 (1.1020)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [300/431]  eta: 0:02:25  lr: 0.000156  loss: 1.0388 (1.1023)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [310/431]  eta: 0:02:14  lr: 0.000156  loss: 1.0887 (1.1017)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [320/431]  eta: 0:02:03  lr: 0.000156  loss: 1.0810 (1.1007)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [330/431]  eta: 0:01:52  lr: 0.000156  loss: 1.1025 (1.1020)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [340/431]  eta: 0:01:41  lr: 0.000156  loss: 1.1474 (1.1040)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [350/431]  eta: 0:01:30  lr: 0.000156  loss: 1.1333 (1.1062)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [360/431]  eta: 0:01:18  lr: 0.000156  loss: 1.0726 (1.1051)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [370/431]  eta: 0:01:07  lr: 0.000156  loss: 1.0295 (1.1041)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [380/431]  eta: 0:00:56  lr: 0.000156  loss: 1.1142 (1.1056)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [390/431]  eta: 0:00:45  lr: 0.000156  loss: 1.1238 (1.1057)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [400/431]  eta: 0:00:34  lr: 0.000156  loss: 1.0544 (1.1040)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:298]  [410/431]  eta: 0:00:23  lr: 0.000156  loss: 1.0477 (1.1043)  time: 1.1070  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:298]  [420/431]  eta: 0:00:12  lr: 0.000156  loss: 1.0556 (1.1030)  time: 1.1049  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:298]  [430/431]  eta: 0:00:01  lr: 0.000156  loss: 1.0525 (1.1025)  time: 1.1043  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:298] Total time: 0:07:59 (1.1129 s / it)\n",
      "Averaged stats: lr: 0.000156  loss: 1.0525 (1.1025)\n",
      "Valid: [epoch:298]  [ 0/14]  eta: 0:00:57  loss: 0.9547 (0.9547)  time: 4.1218  data: 3.9539  max mem: 15925\n",
      "Valid: [epoch:298]  [13/14]  eta: 0:00:00  loss: 1.0338 (1.0426)  time: 0.3859  data: 0.2825  max mem: 15925\n",
      "Valid: [epoch:298] Total time: 0:00:05 (0.4037 s / it)\n",
      "Averaged stats: loss: 1.0338 (1.0426)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_298_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:299]  [  0/431]  eta: 0:30:56  lr: 0.000156  loss: 1.2498 (1.2498)  time: 4.3066  data: 3.0915  max mem: 15925\n",
      "Train: [epoch:299]  [ 10/431]  eta: 0:09:16  lr: 0.000156  loss: 1.0820 (1.1201)  time: 1.3213  data: 0.2812  max mem: 15925\n",
      "Train: [epoch:299]  [ 20/431]  eta: 0:08:11  lr: 0.000156  loss: 1.0756 (1.1166)  time: 1.0405  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [ 30/431]  eta: 0:07:42  lr: 0.000156  loss: 1.0756 (1.1173)  time: 1.0611  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [ 40/431]  eta: 0:07:24  lr: 0.000156  loss: 1.0726 (1.1097)  time: 1.0764  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [ 50/431]  eta: 0:07:09  lr: 0.000156  loss: 1.0709 (1.1073)  time: 1.0879  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [ 60/431]  eta: 0:06:57  lr: 0.000156  loss: 1.0102 (1.0944)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [ 70/431]  eta: 0:06:45  lr: 0.000156  loss: 1.0106 (1.0874)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [ 80/431]  eta: 0:06:34  lr: 0.000156  loss: 1.0638 (1.0893)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [ 90/431]  eta: 0:06:23  lr: 0.000156  loss: 1.0960 (1.0905)  time: 1.1217  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [100/431]  eta: 0:06:11  lr: 0.000156  loss: 1.0906 (1.0925)  time: 1.1244  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [110/431]  eta: 0:06:00  lr: 0.000156  loss: 1.0865 (1.0940)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [120/431]  eta: 0:05:49  lr: 0.000156  loss: 1.0926 (1.0945)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [130/431]  eta: 0:05:37  lr: 0.000156  loss: 1.0935 (1.0930)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [140/431]  eta: 0:05:26  lr: 0.000156  loss: 1.0832 (1.0901)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [150/431]  eta: 0:05:14  lr: 0.000156  loss: 1.0980 (1.0926)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [160/431]  eta: 0:05:03  lr: 0.000156  loss: 1.1237 (1.0940)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [170/431]  eta: 0:04:52  lr: 0.000156  loss: 1.0255 (1.0926)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [180/431]  eta: 0:04:40  lr: 0.000156  loss: 1.0457 (1.0925)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [190/431]  eta: 0:04:29  lr: 0.000156  loss: 1.0576 (1.0923)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [200/431]  eta: 0:04:18  lr: 0.000156  loss: 1.0438 (1.0920)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [210/431]  eta: 0:04:07  lr: 0.000156  loss: 1.0637 (1.0924)  time: 1.1202  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [220/431]  eta: 0:03:55  lr: 0.000156  loss: 1.0619 (1.0910)  time: 1.1213  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [230/431]  eta: 0:03:44  lr: 0.000156  loss: 1.0306 (1.0901)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [240/431]  eta: 0:03:33  lr: 0.000156  loss: 1.0917 (1.0917)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [250/431]  eta: 0:03:22  lr: 0.000156  loss: 1.0917 (1.0928)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [260/431]  eta: 0:03:11  lr: 0.000156  loss: 1.1650 (1.0972)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [270/431]  eta: 0:02:59  lr: 0.000156  loss: 1.1496 (1.0992)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [280/431]  eta: 0:02:48  lr: 0.000156  loss: 1.0750 (1.0981)  time: 1.1212  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [290/431]  eta: 0:02:37  lr: 0.000156  loss: 1.0556 (1.0985)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [300/431]  eta: 0:02:26  lr: 0.000156  loss: 1.1372 (1.1002)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [310/431]  eta: 0:02:15  lr: 0.000156  loss: 1.1210 (1.1006)  time: 1.1151  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [320/431]  eta: 0:02:04  lr: 0.000156  loss: 1.0958 (1.1002)  time: 1.1206  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [330/431]  eta: 0:01:52  lr: 0.000156  loss: 1.1240 (1.1020)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [340/431]  eta: 0:01:41  lr: 0.000156  loss: 1.1174 (1.1026)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [350/431]  eta: 0:01:30  lr: 0.000156  loss: 1.1111 (1.1024)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [360/431]  eta: 0:01:19  lr: 0.000156  loss: 1.1111 (1.1029)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [370/431]  eta: 0:01:08  lr: 0.000156  loss: 1.0622 (1.1028)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [380/431]  eta: 0:00:56  lr: 0.000156  loss: 1.0620 (1.1025)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [390/431]  eta: 0:00:45  lr: 0.000156  loss: 1.0620 (1.1020)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [400/431]  eta: 0:00:34  lr: 0.000156  loss: 1.0677 (1.1026)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [410/431]  eta: 0:00:23  lr: 0.000156  loss: 1.0356 (1.1015)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:299]  [420/431]  eta: 0:00:12  lr: 0.000156  loss: 1.0622 (1.1016)  time: 1.1069  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:299]  [430/431]  eta: 0:00:01  lr: 0.000156  loss: 1.0845 (1.1016)  time: 1.1117  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:299] Total time: 0:08:01 (1.1166 s / it)\n",
      "Averaged stats: lr: 0.000156  loss: 1.0845 (1.1016)\n",
      "Valid: [epoch:299]  [ 0/14]  eta: 0:00:36  loss: 1.0072 (1.0072)  time: 2.6078  data: 2.4591  max mem: 15925\n",
      "Valid: [epoch:299]  [13/14]  eta: 0:00:00  loss: 1.0293 (1.0399)  time: 0.2868  data: 0.1758  max mem: 15925\n",
      "Valid: [epoch:299] Total time: 0:00:04 (0.3037 s / it)\n",
      "Averaged stats: loss: 1.0293 (1.0399)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_299_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.040%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:300]  [  0/431]  eta: 0:34:47  lr: 0.000156  loss: 1.0230 (1.0230)  time: 4.8429  data: 3.6077  max mem: 15925\n",
      "Train: [epoch:300]  [ 10/431]  eta: 0:09:45  lr: 0.000156  loss: 1.1627 (1.1819)  time: 1.3917  data: 0.3282  max mem: 15925\n",
      "Train: [epoch:300]  [ 20/431]  eta: 0:08:26  lr: 0.000156  loss: 1.1119 (1.1405)  time: 1.0506  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [ 30/431]  eta: 0:07:53  lr: 0.000156  loss: 1.1119 (1.1360)  time: 1.0655  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [ 40/431]  eta: 0:07:34  lr: 0.000156  loss: 1.0725 (1.1251)  time: 1.0881  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [ 50/431]  eta: 0:07:18  lr: 0.000156  loss: 1.0414 (1.1159)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [ 60/431]  eta: 0:07:04  lr: 0.000156  loss: 1.0417 (1.1137)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [ 70/431]  eta: 0:06:51  lr: 0.000156  loss: 1.0605 (1.1070)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [ 80/431]  eta: 0:06:39  lr: 0.000156  loss: 1.0900 (1.1065)  time: 1.1199  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [ 90/431]  eta: 0:06:28  lr: 0.000156  loss: 1.0995 (1.1143)  time: 1.1271  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:300]  [100/431]  eta: 0:06:15  lr: 0.000156  loss: 1.0613 (1.1099)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [110/431]  eta: 0:06:04  lr: 0.000156  loss: 1.0484 (1.1082)  time: 1.1190  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [120/431]  eta: 0:05:52  lr: 0.000156  loss: 1.0450 (1.1061)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [130/431]  eta: 0:05:40  lr: 0.000156  loss: 1.0682 (1.1038)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [140/431]  eta: 0:05:28  lr: 0.000156  loss: 1.0779 (1.1035)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [150/431]  eta: 0:05:17  lr: 0.000156  loss: 1.0991 (1.1055)  time: 1.1204  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [160/431]  eta: 0:05:05  lr: 0.000156  loss: 1.0988 (1.1067)  time: 1.1208  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [170/431]  eta: 0:04:54  lr: 0.000156  loss: 1.0988 (1.1077)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [180/431]  eta: 0:04:42  lr: 0.000156  loss: 1.0764 (1.1059)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [190/431]  eta: 0:04:31  lr: 0.000156  loss: 1.0754 (1.1070)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [200/431]  eta: 0:04:19  lr: 0.000156  loss: 1.0924 (1.1071)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [210/431]  eta: 0:04:08  lr: 0.000156  loss: 1.1774 (1.1087)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [220/431]  eta: 0:03:57  lr: 0.000156  loss: 1.1774 (1.1117)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [230/431]  eta: 0:03:45  lr: 0.000156  loss: 1.1932 (1.1134)  time: 1.1216  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [240/431]  eta: 0:03:34  lr: 0.000156  loss: 1.1359 (1.1126)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [250/431]  eta: 0:03:23  lr: 0.000156  loss: 1.0675 (1.1105)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [260/431]  eta: 0:03:12  lr: 0.000156  loss: 1.0928 (1.1115)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [270/431]  eta: 0:03:00  lr: 0.000156  loss: 1.1093 (1.1115)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [280/431]  eta: 0:02:49  lr: 0.000156  loss: 1.0812 (1.1095)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [290/431]  eta: 0:02:38  lr: 0.000156  loss: 1.0457 (1.1078)  time: 1.1164  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [300/431]  eta: 0:02:26  lr: 0.000156  loss: 1.0751 (1.1084)  time: 1.1207  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [310/431]  eta: 0:02:15  lr: 0.000156  loss: 1.0910 (1.1076)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [320/431]  eta: 0:02:04  lr: 0.000156  loss: 1.0606 (1.1072)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [330/431]  eta: 0:01:53  lr: 0.000156  loss: 1.0733 (1.1075)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [340/431]  eta: 0:01:41  lr: 0.000156  loss: 1.1315 (1.1086)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [350/431]  eta: 0:01:30  lr: 0.000156  loss: 1.1208 (1.1086)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [360/431]  eta: 0:01:19  lr: 0.000156  loss: 1.0712 (1.1076)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [370/431]  eta: 0:01:08  lr: 0.000156  loss: 1.0327 (1.1058)  time: 1.1243  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [380/431]  eta: 0:00:57  lr: 0.000156  loss: 1.0452 (1.1046)  time: 1.1226  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [390/431]  eta: 0:00:45  lr: 0.000156  loss: 1.0850 (1.1047)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [400/431]  eta: 0:00:34  lr: 0.000156  loss: 1.0938 (1.1044)  time: 1.1133  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:300]  [410/431]  eta: 0:00:23  lr: 0.000156  loss: 1.0446 (1.1040)  time: 1.1156  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:300]  [420/431]  eta: 0:00:12  lr: 0.000156  loss: 1.0230 (1.1039)  time: 1.1137  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:300]  [430/431]  eta: 0:00:01  lr: 0.000156  loss: 1.0599 (1.1036)  time: 1.1163  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:300] Total time: 0:08:02 (1.1201 s / it)\n",
      "Averaged stats: lr: 0.000156  loss: 1.0599 (1.1036)\n",
      "Valid: [epoch:300]  [ 0/14]  eta: 0:00:35  loss: 1.0907 (1.0907)  time: 2.5548  data: 2.3912  max mem: 15925\n",
      "Valid: [epoch:300]  [13/14]  eta: 0:00:00  loss: 1.0316 (1.0401)  time: 0.2768  data: 0.1709  max mem: 15925\n",
      "Valid: [epoch:300] Total time: 0:00:04 (0.2937 s / it)\n",
      "Averaged stats: loss: 1.0316 (1.0401)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_300_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.040%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:301]  [  0/431]  eta: 0:36:13  lr: 0.000156  loss: 1.0973 (1.0973)  time: 5.0421  data: 3.8340  max mem: 15925\n",
      "Train: [epoch:301]  [ 10/431]  eta: 0:09:56  lr: 0.000156  loss: 1.0973 (1.1262)  time: 1.4179  data: 0.3488  max mem: 15925\n",
      "Train: [epoch:301]  [ 20/431]  eta: 0:08:35  lr: 0.000156  loss: 1.0860 (1.1060)  time: 1.0644  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [ 30/431]  eta: 0:08:01  lr: 0.000156  loss: 1.0762 (1.1011)  time: 1.0794  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [ 40/431]  eta: 0:07:39  lr: 0.000156  loss: 1.0762 (1.1052)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [ 50/431]  eta: 0:07:21  lr: 0.000156  loss: 1.0711 (1.0993)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [ 60/431]  eta: 0:07:06  lr: 0.000156  loss: 1.0381 (1.0919)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [ 70/431]  eta: 0:06:52  lr: 0.000156  loss: 1.0548 (1.0911)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [ 80/431]  eta: 0:06:39  lr: 0.000156  loss: 1.0735 (1.0908)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [ 90/431]  eta: 0:06:27  lr: 0.000156  loss: 1.0499 (1.0832)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [100/431]  eta: 0:06:14  lr: 0.000156  loss: 1.0248 (1.0806)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [110/431]  eta: 0:06:02  lr: 0.000156  loss: 1.0373 (1.0777)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [120/431]  eta: 0:05:50  lr: 0.000156  loss: 1.0560 (1.0751)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [130/431]  eta: 0:05:39  lr: 0.000156  loss: 1.0757 (1.0771)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [140/431]  eta: 0:05:27  lr: 0.000156  loss: 1.0936 (1.0825)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [150/431]  eta: 0:05:15  lr: 0.000156  loss: 1.0810 (1.0821)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [160/431]  eta: 0:05:04  lr: 0.000156  loss: 1.0778 (1.0822)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [170/431]  eta: 0:04:52  lr: 0.000156  loss: 1.0782 (1.0860)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [180/431]  eta: 0:04:41  lr: 0.000156  loss: 1.0554 (1.0861)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [190/431]  eta: 0:04:29  lr: 0.000156  loss: 1.0671 (1.0888)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [200/431]  eta: 0:04:18  lr: 0.000156  loss: 1.0379 (1.0847)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [210/431]  eta: 0:04:06  lr: 0.000156  loss: 0.9839 (1.0842)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [220/431]  eta: 0:03:55  lr: 0.000156  loss: 1.0551 (1.0855)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [230/431]  eta: 0:03:44  lr: 0.000156  loss: 1.0775 (1.0867)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [240/431]  eta: 0:03:33  lr: 0.000156  loss: 1.1077 (1.0878)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [250/431]  eta: 0:03:21  lr: 0.000156  loss: 1.1004 (1.0874)  time: 1.1145  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [260/431]  eta: 0:03:10  lr: 0.000156  loss: 1.1100 (1.0887)  time: 1.1064  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:301]  [270/431]  eta: 0:02:59  lr: 0.000156  loss: 1.0824 (1.0892)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [280/431]  eta: 0:02:48  lr: 0.000156  loss: 1.0808 (1.0896)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [290/431]  eta: 0:02:37  lr: 0.000156  loss: 1.0990 (1.0910)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [300/431]  eta: 0:02:25  lr: 0.000156  loss: 1.1094 (1.0931)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [310/431]  eta: 0:02:14  lr: 0.000156  loss: 1.1106 (1.0936)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [320/431]  eta: 0:02:03  lr: 0.000156  loss: 1.1106 (1.0941)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [330/431]  eta: 0:01:52  lr: 0.000156  loss: 1.1551 (1.0958)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [340/431]  eta: 0:01:41  lr: 0.000156  loss: 1.1613 (1.0979)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [350/431]  eta: 0:01:30  lr: 0.000156  loss: 1.0877 (1.0977)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [360/431]  eta: 0:01:18  lr: 0.000156  loss: 1.0721 (1.0982)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [370/431]  eta: 0:01:07  lr: 0.000156  loss: 1.0964 (1.0995)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [380/431]  eta: 0:00:56  lr: 0.000156  loss: 1.0964 (1.0993)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [390/431]  eta: 0:00:45  lr: 0.000156  loss: 1.0823 (1.0990)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [400/431]  eta: 0:00:34  lr: 0.000156  loss: 1.0934 (1.0991)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [410/431]  eta: 0:00:23  lr: 0.000156  loss: 1.1585 (1.1015)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:301]  [420/431]  eta: 0:00:12  lr: 0.000156  loss: 1.0922 (1.1006)  time: 1.1089  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:301]  [430/431]  eta: 0:00:01  lr: 0.000156  loss: 1.0697 (1.1006)  time: 1.1077  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:301] Total time: 0:07:59 (1.1114 s / it)\n",
      "Averaged stats: lr: 0.000156  loss: 1.0697 (1.1006)\n",
      "Valid: [epoch:301]  [ 0/14]  eta: 0:00:36  loss: 0.9337 (0.9337)  time: 2.5858  data: 2.4279  max mem: 15925\n",
      "Valid: [epoch:301]  [13/14]  eta: 0:00:00  loss: 1.0368 (1.0446)  time: 0.2706  data: 0.1735  max mem: 15925\n",
      "Valid: [epoch:301] Total time: 0:00:03 (0.2856 s / it)\n",
      "Averaged stats: loss: 1.0368 (1.0446)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_301_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:302]  [  0/431]  eta: 0:35:34  lr: 0.000155  loss: 1.1223 (1.1223)  time: 4.9519  data: 3.8561  max mem: 15925\n",
      "Train: [epoch:302]  [ 10/431]  eta: 0:09:43  lr: 0.000155  loss: 1.1104 (1.0904)  time: 1.3854  data: 0.3507  max mem: 15925\n",
      "Train: [epoch:302]  [ 20/431]  eta: 0:08:23  lr: 0.000155  loss: 1.1104 (1.1244)  time: 1.0385  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [ 30/431]  eta: 0:07:49  lr: 0.000155  loss: 1.1062 (1.1142)  time: 1.0524  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [ 40/431]  eta: 0:07:29  lr: 0.000155  loss: 1.0714 (1.1021)  time: 1.0696  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [ 50/431]  eta: 0:07:14  lr: 0.000155  loss: 1.0982 (1.1011)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [ 60/431]  eta: 0:07:00  lr: 0.000155  loss: 1.0843 (1.0928)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [ 70/431]  eta: 0:06:46  lr: 0.000155  loss: 1.0310 (1.0884)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [ 80/431]  eta: 0:06:34  lr: 0.000155  loss: 1.0436 (1.0902)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [ 90/431]  eta: 0:06:22  lr: 0.000155  loss: 1.0436 (1.0868)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [100/431]  eta: 0:06:10  lr: 0.000155  loss: 1.0330 (1.0834)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [110/431]  eta: 0:05:58  lr: 0.000155  loss: 1.0699 (1.0829)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [120/431]  eta: 0:05:47  lr: 0.000155  loss: 1.0737 (1.0837)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [130/431]  eta: 0:05:35  lr: 0.000155  loss: 1.0652 (1.0827)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [140/431]  eta: 0:05:24  lr: 0.000155  loss: 1.0720 (1.0861)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [150/431]  eta: 0:05:12  lr: 0.000155  loss: 1.0720 (1.0867)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [160/431]  eta: 0:05:01  lr: 0.000155  loss: 1.0831 (1.0932)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [170/431]  eta: 0:04:50  lr: 0.000155  loss: 1.0831 (1.0940)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [180/431]  eta: 0:04:39  lr: 0.000155  loss: 1.0650 (1.0946)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [190/431]  eta: 0:04:27  lr: 0.000155  loss: 1.0650 (1.0965)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [200/431]  eta: 0:04:16  lr: 0.000155  loss: 1.0689 (1.0976)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [210/431]  eta: 0:04:05  lr: 0.000155  loss: 1.0689 (1.0970)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [220/431]  eta: 0:03:54  lr: 0.000155  loss: 1.0515 (1.0965)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [230/431]  eta: 0:03:43  lr: 0.000155  loss: 1.1187 (1.0993)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [240/431]  eta: 0:03:31  lr: 0.000155  loss: 1.1173 (1.0978)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [250/431]  eta: 0:03:20  lr: 0.000155  loss: 1.0742 (1.0990)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [260/431]  eta: 0:03:09  lr: 0.000155  loss: 1.1061 (1.0987)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [270/431]  eta: 0:02:58  lr: 0.000155  loss: 1.0527 (1.0992)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [280/431]  eta: 0:02:47  lr: 0.000155  loss: 1.0326 (1.0988)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [290/431]  eta: 0:02:36  lr: 0.000155  loss: 1.0761 (1.0997)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [300/431]  eta: 0:02:25  lr: 0.000155  loss: 1.0610 (1.0994)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [310/431]  eta: 0:02:14  lr: 0.000155  loss: 1.0771 (1.1016)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [320/431]  eta: 0:02:02  lr: 0.000155  loss: 1.1050 (1.1040)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [330/431]  eta: 0:01:51  lr: 0.000155  loss: 1.1050 (1.1058)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [340/431]  eta: 0:01:40  lr: 0.000155  loss: 1.0752 (1.1049)  time: 1.1024  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:302]  [350/431]  eta: 0:01:29  lr: 0.000155  loss: 1.0213 (1.1046)  time: 1.0998  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:302]  [360/431]  eta: 0:01:18  lr: 0.000155  loss: 1.0477 (1.1041)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [370/431]  eta: 0:01:07  lr: 0.000155  loss: 1.0477 (1.1039)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [380/431]  eta: 0:00:56  lr: 0.000155  loss: 1.0448 (1.1041)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [390/431]  eta: 0:00:45  lr: 0.000155  loss: 1.0168 (1.1015)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [400/431]  eta: 0:00:34  lr: 0.000155  loss: 1.0177 (1.1017)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [410/431]  eta: 0:00:23  lr: 0.000155  loss: 1.0755 (1.1034)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:302]  [420/431]  eta: 0:00:12  lr: 0.000155  loss: 1.0755 (1.1026)  time: 1.0926  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:302]  [430/431]  eta: 0:00:01  lr: 0.000155  loss: 1.0648 (1.1023)  time: 1.0976  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:302] Total time: 0:07:57 (1.1069 s / it)\n",
      "Averaged stats: lr: 0.000155  loss: 1.0648 (1.1023)\n",
      "Valid: [epoch:302]  [ 0/14]  eta: 0:00:37  loss: 0.9372 (0.9372)  time: 2.6488  data: 2.4683  max mem: 15925\n",
      "Valid: [epoch:302]  [13/14]  eta: 0:00:00  loss: 1.0429 (1.0516)  time: 0.2786  data: 0.1764  max mem: 15925\n",
      "Valid: [epoch:302] Total time: 0:00:04 (0.2970 s / it)\n",
      "Averaged stats: loss: 1.0429 (1.0516)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_302_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.052%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:303]  [  0/431]  eta: 0:33:57  lr: 0.000155  loss: 1.0122 (1.0122)  time: 4.7276  data: 3.5640  max mem: 15925\n",
      "Train: [epoch:303]  [ 10/431]  eta: 0:09:37  lr: 0.000155  loss: 1.1239 (1.1770)  time: 1.3717  data: 0.3242  max mem: 15925\n",
      "Train: [epoch:303]  [ 20/431]  eta: 0:08:22  lr: 0.000155  loss: 1.1239 (1.1441)  time: 1.0465  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [ 30/431]  eta: 0:07:51  lr: 0.000155  loss: 1.0927 (1.1411)  time: 1.0698  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [ 40/431]  eta: 0:07:31  lr: 0.000155  loss: 1.0721 (1.1267)  time: 1.0845  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [ 50/431]  eta: 0:07:15  lr: 0.000155  loss: 1.0646 (1.1165)  time: 1.0918  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [ 60/431]  eta: 0:07:01  lr: 0.000155  loss: 1.0646 (1.1142)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [ 70/431]  eta: 0:06:49  lr: 0.000155  loss: 1.1353 (1.1252)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [ 80/431]  eta: 0:06:37  lr: 0.000155  loss: 1.1631 (1.1291)  time: 1.1188  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [ 90/431]  eta: 0:06:24  lr: 0.000155  loss: 1.1504 (1.1272)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [100/431]  eta: 0:06:13  lr: 0.000155  loss: 1.0581 (1.1189)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [110/431]  eta: 0:06:01  lr: 0.000155  loss: 1.0072 (1.1122)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [120/431]  eta: 0:05:50  lr: 0.000155  loss: 1.0341 (1.1118)  time: 1.1172  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [130/431]  eta: 0:05:38  lr: 0.000155  loss: 1.0507 (1.1084)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [140/431]  eta: 0:05:27  lr: 0.000155  loss: 1.0424 (1.1052)  time: 1.1175  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [150/431]  eta: 0:05:15  lr: 0.000155  loss: 1.0553 (1.1049)  time: 1.1176  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [160/431]  eta: 0:05:04  lr: 0.000155  loss: 1.0675 (1.1021)  time: 1.1197  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [170/431]  eta: 0:04:52  lr: 0.000155  loss: 1.0763 (1.1044)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [180/431]  eta: 0:04:41  lr: 0.000155  loss: 1.0506 (1.1015)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [190/431]  eta: 0:04:30  lr: 0.000155  loss: 1.0529 (1.1039)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [200/431]  eta: 0:04:18  lr: 0.000155  loss: 1.0717 (1.1005)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [210/431]  eta: 0:04:07  lr: 0.000155  loss: 1.0332 (1.0996)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [220/431]  eta: 0:03:55  lr: 0.000155  loss: 1.0682 (1.1035)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [230/431]  eta: 0:03:44  lr: 0.000155  loss: 1.1325 (1.1039)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [240/431]  eta: 0:03:33  lr: 0.000155  loss: 1.1155 (1.1043)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [250/431]  eta: 0:03:22  lr: 0.000155  loss: 1.0733 (1.1041)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [260/431]  eta: 0:03:10  lr: 0.000155  loss: 1.0519 (1.1047)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [270/431]  eta: 0:02:59  lr: 0.000155  loss: 1.0867 (1.1040)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [280/431]  eta: 0:02:48  lr: 0.000155  loss: 1.0491 (1.1037)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [290/431]  eta: 0:02:37  lr: 0.000155  loss: 1.0482 (1.1013)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [300/431]  eta: 0:02:25  lr: 0.000155  loss: 1.0823 (1.1018)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [310/431]  eta: 0:02:14  lr: 0.000155  loss: 1.0823 (1.1002)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [320/431]  eta: 0:02:03  lr: 0.000155  loss: 1.0378 (1.0984)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [330/431]  eta: 0:01:52  lr: 0.000155  loss: 1.0521 (1.0992)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [340/431]  eta: 0:01:41  lr: 0.000155  loss: 1.1273 (1.0998)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [350/431]  eta: 0:01:30  lr: 0.000155  loss: 1.0774 (1.0997)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [360/431]  eta: 0:01:19  lr: 0.000155  loss: 1.0591 (1.1005)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [370/431]  eta: 0:01:07  lr: 0.000155  loss: 1.1461 (1.1021)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [380/431]  eta: 0:00:56  lr: 0.000155  loss: 1.1413 (1.1028)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [390/431]  eta: 0:00:45  lr: 0.000155  loss: 1.1037 (1.1024)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [400/431]  eta: 0:00:34  lr: 0.000155  loss: 1.0541 (1.1012)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [410/431]  eta: 0:00:23  lr: 0.000155  loss: 1.0639 (1.1006)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:303]  [420/431]  eta: 0:00:12  lr: 0.000155  loss: 1.1029 (1.1019)  time: 1.0987  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:303]  [430/431]  eta: 0:00:01  lr: 0.000155  loss: 1.0799 (1.1018)  time: 1.1005  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:303] Total time: 0:07:59 (1.1126 s / it)\n",
      "Averaged stats: lr: 0.000155  loss: 1.0799 (1.1018)\n",
      "Valid: [epoch:303]  [ 0/14]  eta: 0:00:35  loss: 1.0925 (1.0925)  time: 2.5381  data: 2.3659  max mem: 15925\n",
      "Valid: [epoch:303]  [13/14]  eta: 0:00:00  loss: 1.0405 (1.0487)  time: 0.2692  data: 0.1691  max mem: 15925\n",
      "Valid: [epoch:303] Total time: 0:00:04 (0.2874 s / it)\n",
      "Averaged stats: loss: 1.0405 (1.0487)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_303_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:304]  [  0/431]  eta: 0:36:03  lr: 0.000155  loss: 1.3086 (1.3086)  time: 5.0199  data: 3.8280  max mem: 15925\n",
      "Train: [epoch:304]  [ 10/431]  eta: 0:09:50  lr: 0.000155  loss: 1.1468 (1.1609)  time: 1.4028  data: 0.3482  max mem: 15925\n",
      "Train: [epoch:304]  [ 20/431]  eta: 0:08:29  lr: 0.000155  loss: 1.1255 (1.1308)  time: 1.0497  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [ 30/431]  eta: 0:07:54  lr: 0.000155  loss: 1.0506 (1.0986)  time: 1.0644  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [ 40/431]  eta: 0:07:34  lr: 0.000155  loss: 1.0543 (1.1045)  time: 1.0796  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [ 50/431]  eta: 0:07:17  lr: 0.000155  loss: 1.0903 (1.0960)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [ 60/431]  eta: 0:07:03  lr: 0.000155  loss: 1.0818 (1.1031)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [ 70/431]  eta: 0:06:49  lr: 0.000155  loss: 1.0818 (1.1001)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [ 80/431]  eta: 0:06:36  lr: 0.000155  loss: 1.0323 (1.0998)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [ 90/431]  eta: 0:06:24  lr: 0.000155  loss: 1.0426 (1.0976)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [100/431]  eta: 0:06:12  lr: 0.000155  loss: 1.1421 (1.1025)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [110/431]  eta: 0:06:00  lr: 0.000155  loss: 1.1421 (1.1033)  time: 1.1077  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:304]  [120/431]  eta: 0:05:49  lr: 0.000155  loss: 1.0672 (1.0992)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [130/431]  eta: 0:05:37  lr: 0.000155  loss: 1.0363 (1.0969)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [140/431]  eta: 0:05:25  lr: 0.000155  loss: 1.0379 (1.0963)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [150/431]  eta: 0:05:14  lr: 0.000155  loss: 1.0711 (1.0952)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [160/431]  eta: 0:05:03  lr: 0.000155  loss: 1.1366 (1.1009)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [170/431]  eta: 0:04:51  lr: 0.000155  loss: 1.1366 (1.1001)  time: 1.0893  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [180/431]  eta: 0:04:40  lr: 0.000155  loss: 1.0478 (1.1000)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [190/431]  eta: 0:04:28  lr: 0.000155  loss: 1.0342 (1.0979)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [200/431]  eta: 0:04:17  lr: 0.000155  loss: 1.0597 (1.0996)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [210/431]  eta: 0:04:06  lr: 0.000155  loss: 1.0903 (1.0987)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [220/431]  eta: 0:03:54  lr: 0.000155  loss: 1.0903 (1.0990)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [230/431]  eta: 0:03:43  lr: 0.000155  loss: 1.0575 (1.0968)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [240/431]  eta: 0:03:32  lr: 0.000155  loss: 1.0504 (1.0970)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [250/431]  eta: 0:03:21  lr: 0.000155  loss: 1.0779 (1.0966)  time: 1.1094  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:304]  [260/431]  eta: 0:03:10  lr: 0.000155  loss: 1.0779 (1.0974)  time: 1.1105  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:304]  [270/431]  eta: 0:02:59  lr: 0.000155  loss: 1.0972 (1.0990)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [280/431]  eta: 0:02:47  lr: 0.000155  loss: 1.0945 (1.0982)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [290/431]  eta: 0:02:36  lr: 0.000155  loss: 1.0344 (1.0973)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [300/431]  eta: 0:02:25  lr: 0.000155  loss: 1.0860 (1.0987)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [310/431]  eta: 0:02:14  lr: 0.000155  loss: 1.1190 (1.0997)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [320/431]  eta: 0:02:03  lr: 0.000155  loss: 1.1282 (1.1000)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [330/431]  eta: 0:01:52  lr: 0.000155  loss: 1.1021 (1.1004)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [340/431]  eta: 0:01:41  lr: 0.000155  loss: 1.0932 (1.1007)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [350/431]  eta: 0:01:29  lr: 0.000155  loss: 1.0799 (1.1001)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [360/431]  eta: 0:01:18  lr: 0.000155  loss: 1.1146 (1.1012)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [370/431]  eta: 0:01:07  lr: 0.000155  loss: 1.1143 (1.1009)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [380/431]  eta: 0:00:56  lr: 0.000155  loss: 1.0782 (1.1007)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [390/431]  eta: 0:00:45  lr: 0.000155  loss: 1.1026 (1.1017)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [400/431]  eta: 0:00:34  lr: 0.000155  loss: 1.1102 (1.1018)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [410/431]  eta: 0:00:23  lr: 0.000155  loss: 1.1158 (1.1021)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:304]  [420/431]  eta: 0:00:12  lr: 0.000155  loss: 1.1408 (1.1035)  time: 1.0956  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:304]  [430/431]  eta: 0:00:01  lr: 0.000155  loss: 1.0984 (1.1025)  time: 1.1079  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:304] Total time: 0:07:58 (1.1098 s / it)\n",
      "Averaged stats: lr: 0.000155  loss: 1.0984 (1.1025)\n",
      "Valid: [epoch:304]  [ 0/14]  eta: 0:00:37  loss: 1.1005 (1.1005)  time: 2.6804  data: 2.4791  max mem: 15925\n",
      "Valid: [epoch:304]  [13/14]  eta: 0:00:00  loss: 1.0485 (1.0540)  time: 0.2849  data: 0.1772  max mem: 15925\n",
      "Valid: [epoch:304] Total time: 0:00:04 (0.2991 s / it)\n",
      "Averaged stats: loss: 1.0485 (1.0540)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_304_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.054%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:305]  [  0/431]  eta: 0:31:32  lr: 0.000155  loss: 1.0340 (1.0340)  time: 4.3906  data: 3.1029  max mem: 15925\n",
      "Train: [epoch:305]  [ 10/431]  eta: 0:09:30  lr: 0.000155  loss: 1.0868 (1.1081)  time: 1.3562  data: 0.2823  max mem: 15925\n",
      "Train: [epoch:305]  [ 20/431]  eta: 0:08:19  lr: 0.000155  loss: 1.0868 (1.1255)  time: 1.0567  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [ 30/431]  eta: 0:07:48  lr: 0.000155  loss: 1.0710 (1.1022)  time: 1.0657  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [ 40/431]  eta: 0:07:32  lr: 0.000155  loss: 1.0728 (1.0958)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [ 50/431]  eta: 0:07:15  lr: 0.000155  loss: 1.0893 (1.0948)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [ 60/431]  eta: 0:07:02  lr: 0.000155  loss: 1.0761 (1.0901)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [ 70/431]  eta: 0:06:48  lr: 0.000155  loss: 1.0761 (1.0933)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [ 80/431]  eta: 0:06:36  lr: 0.000155  loss: 1.1192 (1.0978)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [ 90/431]  eta: 0:06:24  lr: 0.000155  loss: 1.1062 (1.0944)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [100/431]  eta: 0:06:12  lr: 0.000155  loss: 1.0755 (1.0909)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [110/431]  eta: 0:06:00  lr: 0.000155  loss: 1.0627 (1.0895)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [120/431]  eta: 0:05:48  lr: 0.000155  loss: 1.0691 (1.0901)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [130/431]  eta: 0:05:37  lr: 0.000155  loss: 1.0927 (1.0884)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [140/431]  eta: 0:05:25  lr: 0.000155  loss: 1.0059 (1.0857)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [150/431]  eta: 0:05:14  lr: 0.000155  loss: 1.0140 (1.0873)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [160/431]  eta: 0:05:02  lr: 0.000155  loss: 1.0875 (1.0873)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [170/431]  eta: 0:04:51  lr: 0.000155  loss: 1.0875 (1.0889)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [180/431]  eta: 0:04:40  lr: 0.000155  loss: 1.1152 (1.0924)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [190/431]  eta: 0:04:28  lr: 0.000155  loss: 1.1152 (1.0952)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [200/431]  eta: 0:04:17  lr: 0.000155  loss: 1.0588 (1.0922)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [210/431]  eta: 0:04:06  lr: 0.000155  loss: 1.0567 (1.0939)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [220/431]  eta: 0:03:55  lr: 0.000155  loss: 1.0567 (1.0926)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [230/431]  eta: 0:03:43  lr: 0.000155  loss: 1.0563 (1.0921)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [240/431]  eta: 0:03:32  lr: 0.000155  loss: 1.0852 (1.0952)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [250/431]  eta: 0:03:21  lr: 0.000155  loss: 1.1532 (1.0980)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [260/431]  eta: 0:03:10  lr: 0.000155  loss: 1.1268 (1.0978)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [270/431]  eta: 0:02:59  lr: 0.000155  loss: 1.0577 (1.0965)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [280/431]  eta: 0:02:47  lr: 0.000155  loss: 1.0424 (1.0962)  time: 1.1030  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:305]  [290/431]  eta: 0:02:36  lr: 0.000155  loss: 1.0448 (1.0947)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [300/431]  eta: 0:02:25  lr: 0.000155  loss: 1.0838 (1.0972)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [310/431]  eta: 0:02:14  lr: 0.000155  loss: 1.0911 (1.0971)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [320/431]  eta: 0:02:03  lr: 0.000155  loss: 1.1037 (1.0981)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [330/431]  eta: 0:01:52  lr: 0.000155  loss: 1.1174 (1.0994)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [340/431]  eta: 0:01:40  lr: 0.000155  loss: 1.1174 (1.1011)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [350/431]  eta: 0:01:29  lr: 0.000155  loss: 1.0858 (1.0997)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [360/431]  eta: 0:01:18  lr: 0.000155  loss: 1.0195 (1.0984)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [370/431]  eta: 0:01:07  lr: 0.000155  loss: 1.0735 (1.0989)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [380/431]  eta: 0:00:56  lr: 0.000155  loss: 1.0871 (1.0999)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [390/431]  eta: 0:00:45  lr: 0.000155  loss: 1.0897 (1.1008)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [400/431]  eta: 0:00:34  lr: 0.000155  loss: 1.1071 (1.1019)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:305]  [410/431]  eta: 0:00:23  lr: 0.000155  loss: 1.0682 (1.1012)  time: 1.0944  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:305]  [420/431]  eta: 0:00:12  lr: 0.000155  loss: 1.0694 (1.1025)  time: 1.0925  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:305]  [430/431]  eta: 0:00:01  lr: 0.000155  loss: 1.0832 (1.1024)  time: 1.1049  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:305] Total time: 0:07:57 (1.1082 s / it)\n",
      "Averaged stats: lr: 0.000155  loss: 1.0832 (1.1024)\n",
      "Valid: [epoch:305]  [ 0/14]  eta: 0:00:33  loss: 1.1344 (1.1344)  time: 2.4076  data: 2.2588  max mem: 15925\n",
      "Valid: [epoch:305]  [13/14]  eta: 0:00:00  loss: 1.0328 (1.0435)  time: 0.2582  data: 0.1614  max mem: 15925\n",
      "Valid: [epoch:305] Total time: 0:00:03 (0.2716 s / it)\n",
      "Averaged stats: loss: 1.0328 (1.0435)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_305_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:306]  [  0/431]  eta: 0:35:38  lr: 0.000154  loss: 1.0447 (1.0447)  time: 4.9626  data: 3.7379  max mem: 15925\n",
      "Train: [epoch:306]  [ 10/431]  eta: 0:09:58  lr: 0.000154  loss: 1.0875 (1.1386)  time: 1.4209  data: 0.3400  max mem: 15925\n",
      "Train: [epoch:306]  [ 20/431]  eta: 0:08:36  lr: 0.000154  loss: 1.0875 (1.1192)  time: 1.0717  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [ 30/431]  eta: 0:07:59  lr: 0.000154  loss: 1.0812 (1.1005)  time: 1.0729  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [ 40/431]  eta: 0:07:38  lr: 0.000154  loss: 1.0763 (1.0900)  time: 1.0827  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [ 50/431]  eta: 0:07:20  lr: 0.000154  loss: 1.0763 (1.0982)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [ 60/431]  eta: 0:07:06  lr: 0.000154  loss: 1.0682 (1.0899)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [ 70/431]  eta: 0:06:52  lr: 0.000154  loss: 1.0483 (1.0887)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [ 80/431]  eta: 0:06:39  lr: 0.000154  loss: 1.0803 (1.0895)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [ 90/431]  eta: 0:06:26  lr: 0.000154  loss: 1.0815 (1.0915)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [100/431]  eta: 0:06:14  lr: 0.000154  loss: 1.0490 (1.0888)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [110/431]  eta: 0:06:02  lr: 0.000154  loss: 1.0415 (1.0857)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [120/431]  eta: 0:05:50  lr: 0.000154  loss: 1.0459 (1.0867)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [130/431]  eta: 0:05:38  lr: 0.000154  loss: 1.0833 (1.0892)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [140/431]  eta: 0:05:27  lr: 0.000154  loss: 1.0743 (1.0890)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [150/431]  eta: 0:05:15  lr: 0.000154  loss: 1.0854 (1.0905)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [160/431]  eta: 0:05:03  lr: 0.000154  loss: 1.0695 (1.0907)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [170/431]  eta: 0:04:52  lr: 0.000154  loss: 1.0384 (1.0921)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [180/431]  eta: 0:04:41  lr: 0.000154  loss: 1.0587 (1.0923)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [190/431]  eta: 0:04:29  lr: 0.000154  loss: 1.0913 (1.0953)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [200/431]  eta: 0:04:18  lr: 0.000154  loss: 1.1186 (1.0974)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [210/431]  eta: 0:04:06  lr: 0.000154  loss: 1.0794 (1.0956)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [220/431]  eta: 0:03:55  lr: 0.000154  loss: 1.0478 (1.0935)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [230/431]  eta: 0:03:44  lr: 0.000154  loss: 1.0478 (1.0929)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [240/431]  eta: 0:03:33  lr: 0.000154  loss: 1.0917 (1.0951)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [250/431]  eta: 0:03:21  lr: 0.000154  loss: 1.0917 (1.0946)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [260/431]  eta: 0:03:10  lr: 0.000154  loss: 1.0839 (1.0964)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [270/431]  eta: 0:02:59  lr: 0.000154  loss: 1.1263 (1.0993)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [280/431]  eta: 0:02:48  lr: 0.000154  loss: 1.0670 (1.0973)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [290/431]  eta: 0:02:37  lr: 0.000154  loss: 1.0371 (1.0965)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [300/431]  eta: 0:02:25  lr: 0.000154  loss: 1.1069 (1.0987)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [310/431]  eta: 0:02:14  lr: 0.000154  loss: 1.0760 (1.0982)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [320/431]  eta: 0:02:03  lr: 0.000154  loss: 1.0660 (1.0997)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [330/431]  eta: 0:01:52  lr: 0.000154  loss: 1.1388 (1.1013)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [340/431]  eta: 0:01:41  lr: 0.000154  loss: 1.1388 (1.1020)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [350/431]  eta: 0:01:30  lr: 0.000154  loss: 1.0954 (1.1026)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [360/431]  eta: 0:01:18  lr: 0.000154  loss: 1.0761 (1.1012)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [370/431]  eta: 0:01:07  lr: 0.000154  loss: 1.0842 (1.1015)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [380/431]  eta: 0:00:56  lr: 0.000154  loss: 1.1093 (1.1021)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [390/431]  eta: 0:00:45  lr: 0.000154  loss: 1.1192 (1.1044)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [400/431]  eta: 0:00:34  lr: 0.000154  loss: 1.0914 (1.1035)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [410/431]  eta: 0:00:23  lr: 0.000154  loss: 1.0914 (1.1040)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:306]  [420/431]  eta: 0:00:12  lr: 0.000154  loss: 1.0907 (1.1036)  time: 1.1131  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:306]  [430/431]  eta: 0:00:01  lr: 0.000154  loss: 1.0811 (1.1038)  time: 1.1030  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:306] Total time: 0:07:59 (1.1122 s / it)\n",
      "Averaged stats: lr: 0.000154  loss: 1.0811 (1.1038)\n",
      "Valid: [epoch:306]  [ 0/14]  eta: 0:00:35  loss: 1.0869 (1.0869)  time: 2.5650  data: 2.4142  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:306]  [13/14]  eta: 0:00:00  loss: 1.0361 (1.0437)  time: 0.2991  data: 0.1725  max mem: 15925\n",
      "Valid: [epoch:306] Total time: 0:00:04 (0.3138 s / it)\n",
      "Averaged stats: loss: 1.0361 (1.0437)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_306_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:307]  [  0/431]  eta: 0:30:50  lr: 0.000154  loss: 0.9923 (0.9923)  time: 4.2931  data: 3.0637  max mem: 15925\n",
      "Train: [epoch:307]  [ 10/431]  eta: 0:09:23  lr: 0.000154  loss: 1.0329 (1.1038)  time: 1.3381  data: 0.2788  max mem: 15925\n",
      "Train: [epoch:307]  [ 20/431]  eta: 0:08:14  lr: 0.000154  loss: 1.0772 (1.1125)  time: 1.0491  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:307]  [ 30/431]  eta: 0:07:47  lr: 0.000154  loss: 1.0365 (1.0884)  time: 1.0698  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [ 40/431]  eta: 0:07:26  lr: 0.000154  loss: 1.0365 (1.0857)  time: 1.0792  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [ 50/431]  eta: 0:07:11  lr: 0.000154  loss: 1.0840 (1.0942)  time: 1.0854  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [ 60/431]  eta: 0:06:59  lr: 0.000154  loss: 1.0840 (1.0945)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [ 70/431]  eta: 0:06:45  lr: 0.000154  loss: 1.1066 (1.0995)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [ 80/431]  eta: 0:06:34  lr: 0.000154  loss: 1.1314 (1.1073)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [ 90/431]  eta: 0:06:22  lr: 0.000154  loss: 1.1288 (1.1042)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [100/431]  eta: 0:06:10  lr: 0.000154  loss: 1.1142 (1.1070)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [110/431]  eta: 0:05:58  lr: 0.000154  loss: 1.0995 (1.1030)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [120/431]  eta: 0:05:47  lr: 0.000154  loss: 0.9915 (1.0936)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [130/431]  eta: 0:05:36  lr: 0.000154  loss: 1.0507 (1.0986)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [140/431]  eta: 0:05:24  lr: 0.000154  loss: 1.1352 (1.1001)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [150/431]  eta: 0:05:13  lr: 0.000154  loss: 1.0524 (1.0964)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [160/431]  eta: 0:05:01  lr: 0.000154  loss: 1.0445 (1.0960)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [170/431]  eta: 0:04:50  lr: 0.000154  loss: 1.0554 (1.0967)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [180/431]  eta: 0:04:39  lr: 0.000154  loss: 1.1094 (1.0996)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [190/431]  eta: 0:04:28  lr: 0.000154  loss: 1.1322 (1.1018)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [200/431]  eta: 0:04:17  lr: 0.000154  loss: 1.1165 (1.1039)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [210/431]  eta: 0:04:05  lr: 0.000154  loss: 1.0724 (1.1042)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [220/431]  eta: 0:03:54  lr: 0.000154  loss: 1.0678 (1.1036)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [230/431]  eta: 0:03:43  lr: 0.000154  loss: 1.0510 (1.1021)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [240/431]  eta: 0:03:32  lr: 0.000154  loss: 1.0707 (1.1028)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [250/431]  eta: 0:03:20  lr: 0.000154  loss: 1.0722 (1.1020)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [260/431]  eta: 0:03:09  lr: 0.000154  loss: 1.1084 (1.1044)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [270/431]  eta: 0:02:58  lr: 0.000154  loss: 1.1226 (1.1042)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [280/431]  eta: 0:02:47  lr: 0.000154  loss: 1.0488 (1.1019)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [290/431]  eta: 0:02:36  lr: 0.000154  loss: 1.0382 (1.1028)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [300/431]  eta: 0:02:25  lr: 0.000154  loss: 1.0560 (1.1032)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [310/431]  eta: 0:02:14  lr: 0.000154  loss: 1.0531 (1.1020)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [320/431]  eta: 0:02:03  lr: 0.000154  loss: 1.0496 (1.1012)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [330/431]  eta: 0:01:51  lr: 0.000154  loss: 1.0836 (1.1021)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [340/431]  eta: 0:01:40  lr: 0.000154  loss: 1.0906 (1.1023)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [350/431]  eta: 0:01:29  lr: 0.000154  loss: 1.1572 (1.1042)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [360/431]  eta: 0:01:18  lr: 0.000154  loss: 1.1085 (1.1042)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [370/431]  eta: 0:01:07  lr: 0.000154  loss: 1.0494 (1.1029)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [380/431]  eta: 0:00:56  lr: 0.000154  loss: 1.0294 (1.1022)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [390/431]  eta: 0:00:45  lr: 0.000154  loss: 1.0827 (1.1034)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [400/431]  eta: 0:00:34  lr: 0.000154  loss: 1.0983 (1.1038)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:307]  [410/431]  eta: 0:00:23  lr: 0.000154  loss: 1.0534 (1.1029)  time: 1.0900  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:307]  [420/431]  eta: 0:00:12  lr: 0.000154  loss: 1.0805 (1.1033)  time: 1.1009  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:307]  [430/431]  eta: 0:00:01  lr: 0.000154  loss: 1.0543 (1.1019)  time: 1.1000  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:307] Total time: 0:07:57 (1.1075 s / it)\n",
      "Averaged stats: lr: 0.000154  loss: 1.0543 (1.1019)\n",
      "Valid: [epoch:307]  [ 0/14]  eta: 0:00:36  loss: 1.0920 (1.0920)  time: 2.5865  data: 2.4569  max mem: 15925\n",
      "Valid: [epoch:307]  [13/14]  eta: 0:00:00  loss: 1.0319 (1.0414)  time: 0.2710  data: 0.1756  max mem: 15925\n",
      "Valid: [epoch:307] Total time: 0:00:04 (0.2868 s / it)\n",
      "Averaged stats: loss: 1.0319 (1.0414)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_307_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:308]  [  0/431]  eta: 0:35:41  lr: 0.000154  loss: 1.1055 (1.1055)  time: 4.9685  data: 3.5898  max mem: 15925\n",
      "Train: [epoch:308]  [ 10/431]  eta: 0:09:41  lr: 0.000154  loss: 1.1245 (1.1820)  time: 1.3805  data: 0.3265  max mem: 15925\n",
      "Train: [epoch:308]  [ 20/431]  eta: 0:08:23  lr: 0.000154  loss: 1.1207 (1.1596)  time: 1.0389  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [ 30/431]  eta: 0:07:53  lr: 0.000154  loss: 1.0537 (1.1311)  time: 1.0718  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [ 40/431]  eta: 0:07:32  lr: 0.000154  loss: 1.0443 (1.1201)  time: 1.0864  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [ 50/431]  eta: 0:07:17  lr: 0.000154  loss: 1.0369 (1.1066)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [ 60/431]  eta: 0:07:03  lr: 0.000154  loss: 1.0323 (1.0967)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [ 70/431]  eta: 0:06:49  lr: 0.000154  loss: 1.0395 (1.0932)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [ 80/431]  eta: 0:06:36  lr: 0.000154  loss: 1.0607 (1.0904)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [ 90/431]  eta: 0:06:24  lr: 0.000154  loss: 1.0623 (1.0880)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [100/431]  eta: 0:06:11  lr: 0.000154  loss: 1.0458 (1.0881)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [110/431]  eta: 0:06:00  lr: 0.000154  loss: 1.0588 (1.0930)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [120/431]  eta: 0:05:48  lr: 0.000154  loss: 1.1487 (1.0999)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [130/431]  eta: 0:05:36  lr: 0.000154  loss: 1.1010 (1.0950)  time: 1.1000  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:308]  [140/431]  eta: 0:05:25  lr: 0.000154  loss: 1.0466 (1.0972)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [150/431]  eta: 0:05:14  lr: 0.000154  loss: 1.0514 (1.0947)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [160/431]  eta: 0:05:03  lr: 0.000154  loss: 1.0350 (1.0941)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [170/431]  eta: 0:04:51  lr: 0.000154  loss: 1.0187 (1.0927)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [180/431]  eta: 0:04:39  lr: 0.000154  loss: 1.0334 (1.0910)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [190/431]  eta: 0:04:28  lr: 0.000154  loss: 1.0878 (1.0930)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [200/431]  eta: 0:04:17  lr: 0.000154  loss: 1.0985 (1.0914)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [210/431]  eta: 0:04:05  lr: 0.000154  loss: 1.0985 (1.0923)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [220/431]  eta: 0:03:54  lr: 0.000154  loss: 1.0665 (1.0931)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [230/431]  eta: 0:03:43  lr: 0.000154  loss: 1.0713 (1.0942)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [240/431]  eta: 0:03:32  lr: 0.000154  loss: 1.0971 (1.0951)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [250/431]  eta: 0:03:21  lr: 0.000154  loss: 1.0818 (1.0947)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [260/431]  eta: 0:03:09  lr: 0.000154  loss: 1.0938 (1.0949)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [270/431]  eta: 0:02:58  lr: 0.000154  loss: 1.1055 (1.0964)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [280/431]  eta: 0:02:47  lr: 0.000154  loss: 1.1055 (1.0971)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [290/431]  eta: 0:02:36  lr: 0.000154  loss: 1.0649 (1.0965)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [300/431]  eta: 0:02:25  lr: 0.000154  loss: 1.0943 (1.0971)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [310/431]  eta: 0:02:14  lr: 0.000154  loss: 1.1435 (1.0990)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [320/431]  eta: 0:02:03  lr: 0.000154  loss: 1.1184 (1.0986)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [330/431]  eta: 0:01:52  lr: 0.000154  loss: 1.0811 (1.0983)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [340/431]  eta: 0:01:40  lr: 0.000154  loss: 1.0516 (1.0978)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [350/431]  eta: 0:01:29  lr: 0.000154  loss: 1.0747 (1.0983)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [360/431]  eta: 0:01:18  lr: 0.000154  loss: 1.0840 (1.0981)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [370/431]  eta: 0:01:07  lr: 0.000154  loss: 1.1297 (1.0996)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [380/431]  eta: 0:00:56  lr: 0.000154  loss: 1.1024 (1.0992)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [390/431]  eta: 0:00:45  lr: 0.000154  loss: 1.0813 (1.0987)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [400/431]  eta: 0:00:34  lr: 0.000154  loss: 1.0922 (1.0991)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [410/431]  eta: 0:00:23  lr: 0.000154  loss: 1.1115 (1.1014)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:308]  [420/431]  eta: 0:00:12  lr: 0.000154  loss: 1.2005 (1.1032)  time: 1.1048  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:308]  [430/431]  eta: 0:00:01  lr: 0.000154  loss: 1.0950 (1.1025)  time: 1.1003  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:308] Total time: 0:07:57 (1.1076 s / it)\n",
      "Averaged stats: lr: 0.000154  loss: 1.0950 (1.1025)\n",
      "Valid: [epoch:308]  [ 0/14]  eta: 0:00:35  loss: 1.0955 (1.0955)  time: 2.5564  data: 2.3941  max mem: 15925\n",
      "Valid: [epoch:308]  [13/14]  eta: 0:00:00  loss: 1.0410 (1.0476)  time: 0.2741  data: 0.1711  max mem: 15925\n",
      "Valid: [epoch:308] Total time: 0:00:04 (0.2913 s / it)\n",
      "Averaged stats: loss: 1.0410 (1.0476)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_308_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.048%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:309]  [  0/431]  eta: 0:31:56  lr: 0.000154  loss: 1.0269 (1.0269)  time: 4.4464  data: 3.2253  max mem: 15925\n",
      "Train: [epoch:309]  [ 10/431]  eta: 0:09:31  lr: 0.000154  loss: 1.1675 (1.1554)  time: 1.3571  data: 0.2934  max mem: 15925\n",
      "Train: [epoch:309]  [ 20/431]  eta: 0:08:20  lr: 0.000154  loss: 1.1126 (1.1293)  time: 1.0570  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [ 30/431]  eta: 0:07:48  lr: 0.000154  loss: 1.0723 (1.1207)  time: 1.0658  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [ 40/431]  eta: 0:07:29  lr: 0.000154  loss: 1.0859 (1.1187)  time: 1.0769  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [ 50/431]  eta: 0:07:14  lr: 0.000154  loss: 1.1060 (1.1193)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [ 60/431]  eta: 0:07:00  lr: 0.000154  loss: 1.0912 (1.1137)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [ 70/431]  eta: 0:06:47  lr: 0.000154  loss: 1.0436 (1.1058)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [ 80/431]  eta: 0:06:35  lr: 0.000154  loss: 1.0805 (1.1067)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [ 90/431]  eta: 0:06:23  lr: 0.000154  loss: 1.0791 (1.0997)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [100/431]  eta: 0:06:10  lr: 0.000154  loss: 0.9988 (1.0997)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [110/431]  eta: 0:05:59  lr: 0.000154  loss: 1.0390 (1.0964)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [120/431]  eta: 0:05:48  lr: 0.000154  loss: 1.0390 (1.0938)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [130/431]  eta: 0:05:36  lr: 0.000154  loss: 1.0665 (1.0973)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [140/431]  eta: 0:05:25  lr: 0.000154  loss: 1.1360 (1.0994)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [150/431]  eta: 0:05:13  lr: 0.000154  loss: 1.1304 (1.1029)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [160/431]  eta: 0:05:02  lr: 0.000154  loss: 1.0713 (1.1030)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [170/431]  eta: 0:04:51  lr: 0.000154  loss: 1.0964 (1.1061)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [180/431]  eta: 0:04:39  lr: 0.000154  loss: 1.1123 (1.1058)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [190/431]  eta: 0:04:28  lr: 0.000154  loss: 1.1123 (1.1046)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [200/431]  eta: 0:04:17  lr: 0.000154  loss: 1.0837 (1.1024)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [210/431]  eta: 0:04:05  lr: 0.000154  loss: 1.1059 (1.1067)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [220/431]  eta: 0:03:54  lr: 0.000154  loss: 1.0967 (1.1054)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [230/431]  eta: 0:03:43  lr: 0.000154  loss: 1.1124 (1.1082)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [240/431]  eta: 0:03:32  lr: 0.000154  loss: 1.1450 (1.1078)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [250/431]  eta: 0:03:21  lr: 0.000154  loss: 1.1058 (1.1081)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [260/431]  eta: 0:03:10  lr: 0.000154  loss: 1.0473 (1.1068)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [270/431]  eta: 0:02:58  lr: 0.000154  loss: 1.0473 (1.1072)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [280/431]  eta: 0:02:47  lr: 0.000154  loss: 1.0832 (1.1056)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [290/431]  eta: 0:02:36  lr: 0.000154  loss: 1.0859 (1.1066)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [300/431]  eta: 0:02:25  lr: 0.000154  loss: 1.0931 (1.1063)  time: 1.1009  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:309]  [310/431]  eta: 0:02:14  lr: 0.000154  loss: 1.1251 (1.1084)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [320/431]  eta: 0:02:03  lr: 0.000154  loss: 1.1194 (1.1080)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [330/431]  eta: 0:01:52  lr: 0.000154  loss: 1.0924 (1.1087)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [340/431]  eta: 0:01:40  lr: 0.000154  loss: 1.0622 (1.1082)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [350/431]  eta: 0:01:29  lr: 0.000154  loss: 1.0622 (1.1085)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [360/431]  eta: 0:01:18  lr: 0.000154  loss: 1.0807 (1.1074)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [370/431]  eta: 0:01:07  lr: 0.000154  loss: 1.0317 (1.1056)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [380/431]  eta: 0:00:56  lr: 0.000154  loss: 1.0136 (1.1050)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [390/431]  eta: 0:00:45  lr: 0.000154  loss: 1.0703 (1.1045)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [400/431]  eta: 0:00:34  lr: 0.000154  loss: 1.0703 (1.1036)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [410/431]  eta: 0:00:23  lr: 0.000154  loss: 1.0857 (1.1038)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:309]  [420/431]  eta: 0:00:12  lr: 0.000154  loss: 1.1031 (1.1045)  time: 1.0993  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:309]  [430/431]  eta: 0:00:01  lr: 0.000154  loss: 1.0651 (1.1030)  time: 1.1034  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:309] Total time: 0:07:57 (1.1086 s / it)\n",
      "Averaged stats: lr: 0.000154  loss: 1.0651 (1.1030)\n",
      "Valid: [epoch:309]  [ 0/14]  eta: 0:00:36  loss: 1.0946 (1.0946)  time: 2.6134  data: 2.4429  max mem: 15925\n",
      "Valid: [epoch:309]  [13/14]  eta: 0:00:00  loss: 1.0359 (1.0451)  time: 0.2720  data: 0.1746  max mem: 15925\n",
      "Valid: [epoch:309] Total time: 0:00:04 (0.2858 s / it)\n",
      "Averaged stats: loss: 1.0359 (1.0451)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_309_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:310]  [  0/431]  eta: 0:35:21  lr: 0.000154  loss: 1.1651 (1.1651)  time: 4.9229  data: 3.7576  max mem: 15925\n",
      "Train: [epoch:310]  [ 10/431]  eta: 0:09:46  lr: 0.000154  loss: 1.1441 (1.1570)  time: 1.3923  data: 0.3418  max mem: 15925\n",
      "Train: [epoch:310]  [ 20/431]  eta: 0:08:27  lr: 0.000154  loss: 1.1651 (1.1624)  time: 1.0496  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [ 30/431]  eta: 0:07:54  lr: 0.000154  loss: 1.1258 (1.1486)  time: 1.0684  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [ 40/431]  eta: 0:07:33  lr: 0.000154  loss: 1.0955 (1.1315)  time: 1.0818  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [ 50/431]  eta: 0:07:17  lr: 0.000154  loss: 1.0899 (1.1246)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [ 60/431]  eta: 0:07:03  lr: 0.000154  loss: 1.0856 (1.1166)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [ 70/431]  eta: 0:06:49  lr: 0.000154  loss: 1.0991 (1.1180)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [ 80/431]  eta: 0:06:37  lr: 0.000154  loss: 1.0991 (1.1174)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [ 90/431]  eta: 0:06:24  lr: 0.000154  loss: 1.0730 (1.1129)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [100/431]  eta: 0:06:13  lr: 0.000154  loss: 1.0550 (1.1050)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [110/431]  eta: 0:06:00  lr: 0.000154  loss: 1.0550 (1.1030)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [120/431]  eta: 0:05:49  lr: 0.000154  loss: 1.0754 (1.0994)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [130/431]  eta: 0:05:37  lr: 0.000154  loss: 1.0529 (1.0973)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [140/431]  eta: 0:05:25  lr: 0.000154  loss: 1.0640 (1.0946)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [150/431]  eta: 0:05:14  lr: 0.000154  loss: 1.0640 (1.0954)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [160/431]  eta: 0:05:02  lr: 0.000154  loss: 1.0545 (1.0940)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [170/431]  eta: 0:04:51  lr: 0.000154  loss: 1.0612 (1.0929)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [180/431]  eta: 0:04:40  lr: 0.000154  loss: 1.0722 (1.0947)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [190/431]  eta: 0:04:28  lr: 0.000154  loss: 1.1318 (1.0967)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [200/431]  eta: 0:04:17  lr: 0.000154  loss: 1.1109 (1.0991)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [210/431]  eta: 0:04:06  lr: 0.000154  loss: 1.1083 (1.1013)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [220/431]  eta: 0:03:55  lr: 0.000154  loss: 1.0571 (1.1032)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [230/431]  eta: 0:03:43  lr: 0.000154  loss: 1.0682 (1.1038)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [240/431]  eta: 0:03:32  lr: 0.000154  loss: 1.1111 (1.1052)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [250/431]  eta: 0:03:21  lr: 0.000154  loss: 1.0406 (1.1045)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [260/431]  eta: 0:03:10  lr: 0.000154  loss: 1.0980 (1.1057)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [270/431]  eta: 0:02:59  lr: 0.000154  loss: 1.0980 (1.1043)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [280/431]  eta: 0:02:48  lr: 0.000154  loss: 1.0190 (1.1016)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [290/431]  eta: 0:02:36  lr: 0.000154  loss: 1.0358 (1.1010)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [300/431]  eta: 0:02:25  lr: 0.000154  loss: 1.0749 (1.0999)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [310/431]  eta: 0:02:14  lr: 0.000154  loss: 1.0766 (1.1010)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [320/431]  eta: 0:02:03  lr: 0.000154  loss: 1.1065 (1.1007)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [330/431]  eta: 0:01:52  lr: 0.000154  loss: 1.1065 (1.1016)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [340/431]  eta: 0:01:41  lr: 0.000154  loss: 1.1231 (1.1036)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [350/431]  eta: 0:01:29  lr: 0.000154  loss: 1.1128 (1.1049)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [360/431]  eta: 0:01:18  lr: 0.000154  loss: 1.1034 (1.1051)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [370/431]  eta: 0:01:07  lr: 0.000154  loss: 1.0447 (1.1055)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [380/431]  eta: 0:00:56  lr: 0.000154  loss: 1.0414 (1.1042)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [390/431]  eta: 0:00:45  lr: 0.000154  loss: 1.0468 (1.1045)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [400/431]  eta: 0:00:34  lr: 0.000154  loss: 1.0339 (1.1029)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [410/431]  eta: 0:00:23  lr: 0.000154  loss: 1.0216 (1.1014)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:310]  [420/431]  eta: 0:00:12  lr: 0.000154  loss: 1.0589 (1.1018)  time: 1.1031  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:310]  [430/431]  eta: 0:00:01  lr: 0.000154  loss: 1.0799 (1.1013)  time: 1.0987  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:310] Total time: 0:07:58 (1.1103 s / it)\n",
      "Averaged stats: lr: 0.000154  loss: 1.0799 (1.1013)\n",
      "Valid: [epoch:310]  [ 0/14]  eta: 0:00:36  loss: 1.0062 (1.0062)  time: 2.5939  data: 2.4495  max mem: 15925\n",
      "Valid: [epoch:310]  [13/14]  eta: 0:00:00  loss: 1.0304 (1.0423)  time: 0.2719  data: 0.1751  max mem: 15925\n",
      "Valid: [epoch:310] Total time: 0:00:04 (0.2872 s / it)\n",
      "Averaged stats: loss: 1.0304 (1.0423)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_310_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:311]  [  0/431]  eta: 0:33:00  lr: 0.000153  loss: 1.2010 (1.2010)  time: 4.5959  data: 3.3923  max mem: 15925\n",
      "Train: [epoch:311]  [ 10/431]  eta: 0:09:34  lr: 0.000153  loss: 1.1178 (1.1199)  time: 1.3642  data: 0.3086  max mem: 15925\n",
      "Train: [epoch:311]  [ 20/431]  eta: 0:08:20  lr: 0.000153  loss: 1.1178 (1.1380)  time: 1.0498  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [ 30/431]  eta: 0:07:48  lr: 0.000153  loss: 1.0946 (1.1294)  time: 1.0625  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:311]  [ 40/431]  eta: 0:07:30  lr: 0.000153  loss: 1.0340 (1.1117)  time: 1.0827  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [ 50/431]  eta: 0:07:13  lr: 0.000153  loss: 1.0340 (1.1052)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [ 60/431]  eta: 0:07:00  lr: 0.000153  loss: 1.0663 (1.1051)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [ 70/431]  eta: 0:06:47  lr: 0.000153  loss: 1.0553 (1.1009)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [ 80/431]  eta: 0:06:34  lr: 0.000153  loss: 1.0549 (1.1027)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [ 90/431]  eta: 0:06:22  lr: 0.000153  loss: 1.0546 (1.1027)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [100/431]  eta: 0:06:11  lr: 0.000153  loss: 1.0879 (1.1053)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [110/431]  eta: 0:05:59  lr: 0.000153  loss: 1.0879 (1.1060)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [120/431]  eta: 0:05:47  lr: 0.000153  loss: 1.0625 (1.1083)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [130/431]  eta: 0:05:36  lr: 0.000153  loss: 1.0559 (1.1049)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [140/431]  eta: 0:05:25  lr: 0.000153  loss: 1.0616 (1.1033)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [150/431]  eta: 0:05:13  lr: 0.000153  loss: 1.0616 (1.1022)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [160/431]  eta: 0:05:02  lr: 0.000153  loss: 1.0434 (1.1005)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [170/431]  eta: 0:04:51  lr: 0.000153  loss: 1.0272 (1.0973)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [180/431]  eta: 0:04:40  lr: 0.000153  loss: 1.0929 (1.0976)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [190/431]  eta: 0:04:28  lr: 0.000153  loss: 1.0934 (1.0978)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [200/431]  eta: 0:04:17  lr: 0.000153  loss: 1.0747 (1.0980)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [210/431]  eta: 0:04:06  lr: 0.000153  loss: 1.0609 (1.0960)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [220/431]  eta: 0:03:54  lr: 0.000153  loss: 1.0181 (1.0922)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [230/431]  eta: 0:03:43  lr: 0.000153  loss: 1.0591 (1.0928)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [240/431]  eta: 0:03:32  lr: 0.000153  loss: 1.0782 (1.0925)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [250/431]  eta: 0:03:21  lr: 0.000153  loss: 1.0688 (1.0931)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [260/431]  eta: 0:03:09  lr: 0.000153  loss: 1.1062 (1.0954)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [270/431]  eta: 0:02:58  lr: 0.000153  loss: 1.1215 (1.0962)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [280/431]  eta: 0:02:47  lr: 0.000153  loss: 1.0838 (1.0962)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [290/431]  eta: 0:02:36  lr: 0.000153  loss: 1.0838 (1.0976)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [300/431]  eta: 0:02:25  lr: 0.000153  loss: 1.1473 (1.1014)  time: 1.1044  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:311]  [310/431]  eta: 0:02:14  lr: 0.000153  loss: 1.1289 (1.1027)  time: 1.1072  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:311]  [320/431]  eta: 0:02:03  lr: 0.000153  loss: 1.1174 (1.1027)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [330/431]  eta: 0:01:52  lr: 0.000153  loss: 1.0774 (1.1021)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [340/431]  eta: 0:01:40  lr: 0.000153  loss: 1.0461 (1.1031)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [350/431]  eta: 0:01:29  lr: 0.000153  loss: 1.0707 (1.1039)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [360/431]  eta: 0:01:18  lr: 0.000153  loss: 1.1171 (1.1048)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [370/431]  eta: 0:01:07  lr: 0.000153  loss: 1.1027 (1.1039)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [380/431]  eta: 0:00:56  lr: 0.000153  loss: 1.0807 (1.1038)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [390/431]  eta: 0:00:45  lr: 0.000153  loss: 1.0807 (1.1025)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [400/431]  eta: 0:00:34  lr: 0.000153  loss: 1.0701 (1.1044)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:311]  [410/431]  eta: 0:00:23  lr: 0.000153  loss: 1.0734 (1.1037)  time: 1.0968  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:311]  [420/431]  eta: 0:00:12  lr: 0.000153  loss: 1.0660 (1.1029)  time: 1.0917  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:311]  [430/431]  eta: 0:00:01  lr: 0.000153  loss: 1.0547 (1.1020)  time: 1.0917  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:311] Total time: 0:07:57 (1.1081 s / it)\n",
      "Averaged stats: lr: 0.000153  loss: 1.0547 (1.1020)\n",
      "Valid: [epoch:311]  [ 0/14]  eta: 0:00:36  loss: 0.9777 (0.9777)  time: 2.6194  data: 2.4893  max mem: 15925\n",
      "Valid: [epoch:311]  [13/14]  eta: 0:00:00  loss: 1.0330 (1.0417)  time: 0.2751  data: 0.1779  max mem: 15925\n",
      "Valid: [epoch:311] Total time: 0:00:04 (0.2929 s / it)\n",
      "Averaged stats: loss: 1.0330 (1.0417)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_311_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:312]  [  0/431]  eta: 0:30:18  lr: 0.000153  loss: 1.1114 (1.1114)  time: 4.2185  data: 3.0025  max mem: 15925\n",
      "Train: [epoch:312]  [ 10/431]  eta: 0:09:19  lr: 0.000153  loss: 1.0810 (1.0991)  time: 1.3293  data: 0.2732  max mem: 15925\n",
      "Train: [epoch:312]  [ 20/431]  eta: 0:08:16  lr: 0.000153  loss: 1.0456 (1.0863)  time: 1.0570  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [ 30/431]  eta: 0:07:46  lr: 0.000153  loss: 1.0296 (1.0750)  time: 1.0705  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [ 40/431]  eta: 0:07:27  lr: 0.000153  loss: 1.0408 (1.0779)  time: 1.0808  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [ 50/431]  eta: 0:07:12  lr: 0.000153  loss: 1.0677 (1.0805)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [ 60/431]  eta: 0:06:59  lr: 0.000153  loss: 1.0514 (1.0757)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [ 70/431]  eta: 0:06:46  lr: 0.000153  loss: 1.0514 (1.0834)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [ 80/431]  eta: 0:06:34  lr: 0.000153  loss: 1.1126 (1.0894)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [ 90/431]  eta: 0:06:22  lr: 0.000153  loss: 1.0956 (1.0878)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [100/431]  eta: 0:06:11  lr: 0.000153  loss: 1.0956 (1.0913)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [110/431]  eta: 0:05:59  lr: 0.000153  loss: 1.0740 (1.0879)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [120/431]  eta: 0:05:47  lr: 0.000153  loss: 1.0733 (1.0903)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [130/431]  eta: 0:05:36  lr: 0.000153  loss: 1.0698 (1.0894)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [140/431]  eta: 0:05:24  lr: 0.000153  loss: 1.0584 (1.0951)  time: 1.1058  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:312]  [150/431]  eta: 0:05:13  lr: 0.000153  loss: 1.0529 (1.0927)  time: 1.1068  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:312]  [160/431]  eta: 0:05:02  lr: 0.000153  loss: 1.0624 (1.0959)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [170/431]  eta: 0:04:51  lr: 0.000153  loss: 1.0790 (1.0947)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [180/431]  eta: 0:04:39  lr: 0.000153  loss: 1.0950 (1.0965)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [190/431]  eta: 0:04:28  lr: 0.000153  loss: 1.0642 (1.0954)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [200/431]  eta: 0:04:17  lr: 0.000153  loss: 1.0383 (1.0932)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [210/431]  eta: 0:04:05  lr: 0.000153  loss: 1.0421 (1.0925)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [220/431]  eta: 0:03:54  lr: 0.000153  loss: 1.0432 (1.0936)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [230/431]  eta: 0:03:43  lr: 0.000153  loss: 1.0391 (1.0925)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [240/431]  eta: 0:03:32  lr: 0.000153  loss: 1.1156 (1.0951)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [250/431]  eta: 0:03:21  lr: 0.000153  loss: 1.1333 (1.0953)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [260/431]  eta: 0:03:09  lr: 0.000153  loss: 1.0815 (1.0953)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [270/431]  eta: 0:02:58  lr: 0.000153  loss: 1.0891 (1.0964)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [280/431]  eta: 0:02:47  lr: 0.000153  loss: 1.0608 (1.0945)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [290/431]  eta: 0:02:36  lr: 0.000153  loss: 1.0699 (1.0951)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [300/431]  eta: 0:02:25  lr: 0.000153  loss: 1.1446 (1.0981)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [310/431]  eta: 0:02:14  lr: 0.000153  loss: 1.1390 (1.0990)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [320/431]  eta: 0:02:03  lr: 0.000153  loss: 1.0947 (1.0985)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [330/431]  eta: 0:01:52  lr: 0.000153  loss: 1.0936 (1.0993)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [340/431]  eta: 0:01:40  lr: 0.000153  loss: 1.1013 (1.0988)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [350/431]  eta: 0:01:29  lr: 0.000153  loss: 1.1159 (1.1005)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [360/431]  eta: 0:01:18  lr: 0.000153  loss: 1.1064 (1.0992)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [370/431]  eta: 0:01:07  lr: 0.000153  loss: 1.0654 (1.1004)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [380/431]  eta: 0:00:56  lr: 0.000153  loss: 1.0994 (1.1011)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [390/431]  eta: 0:00:45  lr: 0.000153  loss: 1.0994 (1.1015)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [400/431]  eta: 0:00:34  lr: 0.000153  loss: 1.0825 (1.1016)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:312]  [410/431]  eta: 0:00:23  lr: 0.000153  loss: 1.0862 (1.1019)  time: 1.1023  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:312]  [420/431]  eta: 0:00:12  lr: 0.000153  loss: 1.0656 (1.1008)  time: 1.1041  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:312]  [430/431]  eta: 0:00:01  lr: 0.000153  loss: 1.0508 (1.1005)  time: 1.1072  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:312] Total time: 0:07:58 (1.1092 s / it)\n",
      "Averaged stats: lr: 0.000153  loss: 1.0508 (1.1005)\n",
      "Valid: [epoch:312]  [ 0/14]  eta: 0:00:36  loss: 0.9356 (0.9356)  time: 2.5826  data: 2.3942  max mem: 15925\n",
      "Valid: [epoch:312]  [13/14]  eta: 0:00:00  loss: 1.0380 (1.0455)  time: 0.2673  data: 0.1711  max mem: 15925\n",
      "Valid: [epoch:312] Total time: 0:00:03 (0.2819 s / it)\n",
      "Averaged stats: loss: 1.0380 (1.0455)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_312_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:313]  [  0/431]  eta: 0:32:44  lr: 0.000153  loss: 1.2129 (1.2129)  time: 4.5581  data: 3.3469  max mem: 15925\n",
      "Train: [epoch:313]  [ 10/431]  eta: 0:09:37  lr: 0.000153  loss: 1.1160 (1.1667)  time: 1.3723  data: 0.3045  max mem: 15925\n",
      "Train: [epoch:313]  [ 20/431]  eta: 0:08:23  lr: 0.000153  loss: 1.0813 (1.1309)  time: 1.0583  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [ 30/431]  eta: 0:07:53  lr: 0.000153  loss: 1.1079 (1.1324)  time: 1.0759  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [ 40/431]  eta: 0:07:34  lr: 0.000153  loss: 1.1135 (1.1338)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [ 50/431]  eta: 0:07:18  lr: 0.000153  loss: 1.0836 (1.1216)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [ 60/431]  eta: 0:07:04  lr: 0.000153  loss: 1.0537 (1.1191)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [ 70/431]  eta: 0:06:51  lr: 0.000153  loss: 1.0840 (1.1146)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [ 80/431]  eta: 0:06:38  lr: 0.000153  loss: 1.1055 (1.1211)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [ 90/431]  eta: 0:06:25  lr: 0.000153  loss: 1.0762 (1.1144)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [100/431]  eta: 0:06:13  lr: 0.000153  loss: 1.0581 (1.1094)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [110/431]  eta: 0:06:01  lr: 0.000153  loss: 1.0581 (1.1071)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [120/431]  eta: 0:05:49  lr: 0.000153  loss: 1.0465 (1.1037)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [130/431]  eta: 0:05:38  lr: 0.000153  loss: 1.0505 (1.1024)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [140/431]  eta: 0:05:26  lr: 0.000153  loss: 1.0043 (1.0972)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [150/431]  eta: 0:05:15  lr: 0.000153  loss: 1.0620 (1.0972)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [160/431]  eta: 0:05:03  lr: 0.000153  loss: 1.0799 (1.0988)  time: 1.1077  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:313]  [170/431]  eta: 0:04:52  lr: 0.000153  loss: 1.0515 (1.0995)  time: 1.1063  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:313]  [180/431]  eta: 0:04:41  lr: 0.000153  loss: 1.0703 (1.0990)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [190/431]  eta: 0:04:29  lr: 0.000153  loss: 1.1068 (1.0995)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [200/431]  eta: 0:04:18  lr: 0.000153  loss: 1.1213 (1.1022)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [210/431]  eta: 0:04:07  lr: 0.000153  loss: 1.1023 (1.0997)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [220/431]  eta: 0:03:55  lr: 0.000153  loss: 1.0616 (1.0996)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [230/431]  eta: 0:03:44  lr: 0.000153  loss: 1.1038 (1.1003)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [240/431]  eta: 0:03:33  lr: 0.000153  loss: 1.0904 (1.1015)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [250/431]  eta: 0:03:22  lr: 0.000153  loss: 1.0811 (1.1010)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [260/431]  eta: 0:03:10  lr: 0.000153  loss: 1.1027 (1.1024)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [270/431]  eta: 0:02:59  lr: 0.000153  loss: 1.0768 (1.1014)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [280/431]  eta: 0:02:48  lr: 0.000153  loss: 1.0414 (1.0992)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [290/431]  eta: 0:02:37  lr: 0.000153  loss: 1.0414 (1.0992)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [300/431]  eta: 0:02:26  lr: 0.000153  loss: 1.1188 (1.1001)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [310/431]  eta: 0:02:14  lr: 0.000153  loss: 1.0794 (1.0997)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [320/431]  eta: 0:02:03  lr: 0.000153  loss: 1.0676 (1.0989)  time: 1.1016  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:313]  [330/431]  eta: 0:01:52  lr: 0.000153  loss: 1.0877 (1.0985)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [340/431]  eta: 0:01:41  lr: 0.000153  loss: 1.1162 (1.0996)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [350/431]  eta: 0:01:30  lr: 0.000153  loss: 1.0801 (1.0998)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [360/431]  eta: 0:01:19  lr: 0.000153  loss: 1.0541 (1.0997)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [370/431]  eta: 0:01:07  lr: 0.000153  loss: 1.0639 (1.0997)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [380/431]  eta: 0:00:56  lr: 0.000153  loss: 1.0990 (1.1000)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [390/431]  eta: 0:00:45  lr: 0.000153  loss: 1.0979 (1.0997)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [400/431]  eta: 0:00:34  lr: 0.000153  loss: 1.0878 (1.0999)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [410/431]  eta: 0:00:23  lr: 0.000153  loss: 1.0878 (1.0998)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:313]  [420/431]  eta: 0:00:12  lr: 0.000153  loss: 1.0951 (1.1014)  time: 1.1009  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:313]  [430/431]  eta: 0:00:01  lr: 0.000153  loss: 1.1201 (1.1019)  time: 1.1086  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:313] Total time: 0:07:59 (1.1122 s / it)\n",
      "Averaged stats: lr: 0.000153  loss: 1.1201 (1.1019)\n",
      "Valid: [epoch:313]  [ 0/14]  eta: 0:00:39  loss: 1.0868 (1.0868)  time: 2.8291  data: 2.6660  max mem: 15925\n",
      "Valid: [epoch:313]  [13/14]  eta: 0:00:00  loss: 1.0359 (1.0447)  time: 0.2938  data: 0.1905  max mem: 15925\n",
      "Valid: [epoch:313] Total time: 0:00:04 (0.3093 s / it)\n",
      "Averaged stats: loss: 1.0359 (1.0447)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_313_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:314]  [  0/431]  eta: 0:34:33  lr: 0.000153  loss: 0.9663 (0.9663)  time: 4.8110  data: 3.4351  max mem: 15925\n",
      "Train: [epoch:314]  [ 10/431]  eta: 0:09:38  lr: 0.000153  loss: 1.0911 (1.1385)  time: 1.3734  data: 0.3125  max mem: 15925\n",
      "Train: [epoch:314]  [ 20/431]  eta: 0:08:27  lr: 0.000153  loss: 1.1245 (1.1473)  time: 1.0567  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [ 30/431]  eta: 0:07:57  lr: 0.000153  loss: 1.0699 (1.1310)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [ 40/431]  eta: 0:07:36  lr: 0.000153  loss: 1.0699 (1.1163)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [ 50/431]  eta: 0:07:20  lr: 0.000153  loss: 1.0989 (1.1209)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [ 60/431]  eta: 0:07:04  lr: 0.000153  loss: 1.0722 (1.1083)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [ 70/431]  eta: 0:06:51  lr: 0.000153  loss: 1.0085 (1.0985)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [ 80/431]  eta: 0:06:38  lr: 0.000153  loss: 1.0486 (1.1052)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [ 90/431]  eta: 0:06:25  lr: 0.000153  loss: 1.1026 (1.1090)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [100/431]  eta: 0:06:13  lr: 0.000153  loss: 1.0600 (1.1041)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [110/431]  eta: 0:06:01  lr: 0.000153  loss: 1.0112 (1.0978)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [120/431]  eta: 0:05:49  lr: 0.000153  loss: 1.0146 (1.0993)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [130/431]  eta: 0:05:37  lr: 0.000153  loss: 1.0686 (1.0985)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [140/431]  eta: 0:05:26  lr: 0.000153  loss: 1.0673 (1.0959)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [150/431]  eta: 0:05:14  lr: 0.000153  loss: 1.0624 (1.0959)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [160/431]  eta: 0:05:03  lr: 0.000153  loss: 1.0607 (1.0950)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [170/431]  eta: 0:04:51  lr: 0.000153  loss: 1.0605 (1.0959)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [180/431]  eta: 0:04:40  lr: 0.000153  loss: 1.1364 (1.0973)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [190/431]  eta: 0:04:29  lr: 0.000153  loss: 1.0944 (1.0973)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [200/431]  eta: 0:04:17  lr: 0.000153  loss: 1.0670 (1.0971)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [210/431]  eta: 0:04:06  lr: 0.000153  loss: 1.0558 (1.0936)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [220/431]  eta: 0:03:55  lr: 0.000153  loss: 1.0385 (1.0939)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [230/431]  eta: 0:03:44  lr: 0.000153  loss: 1.1187 (1.0952)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [240/431]  eta: 0:03:32  lr: 0.000153  loss: 1.1187 (1.0981)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [250/431]  eta: 0:03:21  lr: 0.000153  loss: 1.1181 (1.0991)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [260/431]  eta: 0:03:10  lr: 0.000153  loss: 1.0611 (1.0979)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [270/431]  eta: 0:02:59  lr: 0.000153  loss: 1.0269 (1.0965)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [280/431]  eta: 0:02:48  lr: 0.000153  loss: 1.0528 (1.0963)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [290/431]  eta: 0:02:37  lr: 0.000153  loss: 1.0505 (1.0966)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [300/431]  eta: 0:02:25  lr: 0.000153  loss: 1.1233 (1.0996)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [310/431]  eta: 0:02:14  lr: 0.000153  loss: 1.1296 (1.1006)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [320/431]  eta: 0:02:03  lr: 0.000153  loss: 1.0790 (1.1004)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [330/431]  eta: 0:01:52  lr: 0.000153  loss: 1.0790 (1.1015)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [340/431]  eta: 0:01:41  lr: 0.000153  loss: 1.1248 (1.1016)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [350/431]  eta: 0:01:30  lr: 0.000153  loss: 1.0933 (1.1019)  time: 1.1064  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:314]  [360/431]  eta: 0:01:18  lr: 0.000153  loss: 1.1614 (1.1040)  time: 1.1082  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:314]  [370/431]  eta: 0:01:07  lr: 0.000153  loss: 1.0778 (1.1038)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [380/431]  eta: 0:00:56  lr: 0.000153  loss: 1.0276 (1.1019)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [390/431]  eta: 0:00:45  lr: 0.000153  loss: 1.0173 (1.1013)  time: 1.1156  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [400/431]  eta: 0:00:34  lr: 0.000153  loss: 1.0536 (1.1010)  time: 1.1183  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [410/431]  eta: 0:00:23  lr: 0.000153  loss: 1.0560 (1.1014)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:314]  [420/431]  eta: 0:00:12  lr: 0.000153  loss: 1.0357 (1.1010)  time: 1.1088  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:314]  [430/431]  eta: 0:00:01  lr: 0.000153  loss: 1.0715 (1.1010)  time: 1.0984  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:314] Total time: 0:07:59 (1.1120 s / it)\n",
      "Averaged stats: lr: 0.000153  loss: 1.0715 (1.1010)\n",
      "Valid: [epoch:314]  [ 0/14]  eta: 0:00:34  loss: 1.0835 (1.0835)  time: 2.4896  data: 2.3345  max mem: 15925\n",
      "Valid: [epoch:314]  [13/14]  eta: 0:00:00  loss: 1.0426 (1.0482)  time: 0.2579  data: 0.1668  max mem: 15925\n",
      "Valid: [epoch:314] Total time: 0:00:03 (0.2734 s / it)\n",
      "Averaged stats: loss: 1.0426 (1.0482)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_314_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.048%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:315]  [  0/431]  eta: 0:31:44  lr: 0.000152  loss: 1.1133 (1.1133)  time: 4.4192  data: 3.1708  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:315]  [ 10/431]  eta: 0:09:29  lr: 0.000152  loss: 1.1136 (1.1323)  time: 1.3521  data: 0.2884  max mem: 15925\n",
      "Train: [epoch:315]  [ 20/431]  eta: 0:08:23  lr: 0.000152  loss: 1.0814 (1.1089)  time: 1.0657  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [ 30/431]  eta: 0:07:51  lr: 0.000152  loss: 1.0199 (1.0824)  time: 1.0770  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [ 40/431]  eta: 0:07:30  lr: 0.000152  loss: 1.0885 (1.0983)  time: 1.0772  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [ 50/431]  eta: 0:07:15  lr: 0.000152  loss: 1.1275 (1.1121)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [ 60/431]  eta: 0:07:00  lr: 0.000152  loss: 1.1025 (1.0986)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [ 70/431]  eta: 0:06:47  lr: 0.000152  loss: 1.0462 (1.0960)  time: 1.0924  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [ 80/431]  eta: 0:06:35  lr: 0.000152  loss: 1.0591 (1.0962)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [ 90/431]  eta: 0:06:22  lr: 0.000152  loss: 1.0751 (1.0985)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [100/431]  eta: 0:06:11  lr: 0.000152  loss: 1.0722 (1.0964)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [110/431]  eta: 0:05:59  lr: 0.000152  loss: 1.0672 (1.0932)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [120/431]  eta: 0:05:48  lr: 0.000152  loss: 1.0388 (1.0922)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [130/431]  eta: 0:05:36  lr: 0.000152  loss: 1.1178 (1.0946)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [140/431]  eta: 0:05:25  lr: 0.000152  loss: 1.1117 (1.0923)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [150/431]  eta: 0:05:13  lr: 0.000152  loss: 1.0570 (1.0960)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [160/431]  eta: 0:05:02  lr: 0.000152  loss: 1.0565 (1.0951)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [170/431]  eta: 0:04:50  lr: 0.000152  loss: 1.0266 (1.0912)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [180/431]  eta: 0:04:39  lr: 0.000152  loss: 1.1084 (1.0960)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [190/431]  eta: 0:04:28  lr: 0.000152  loss: 1.1084 (1.0967)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [200/431]  eta: 0:04:17  lr: 0.000152  loss: 1.0679 (1.0988)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [210/431]  eta: 0:04:06  lr: 0.000152  loss: 1.0454 (1.0974)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [220/431]  eta: 0:03:54  lr: 0.000152  loss: 1.1110 (1.1009)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [230/431]  eta: 0:03:43  lr: 0.000152  loss: 1.1260 (1.1032)  time: 1.1103  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:315]  [240/431]  eta: 0:03:32  lr: 0.000152  loss: 1.0891 (1.1027)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [250/431]  eta: 0:03:21  lr: 0.000152  loss: 1.0982 (1.1045)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [260/431]  eta: 0:03:10  lr: 0.000152  loss: 1.1442 (1.1063)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [270/431]  eta: 0:02:58  lr: 0.000152  loss: 1.1078 (1.1071)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [280/431]  eta: 0:02:47  lr: 0.000152  loss: 1.1078 (1.1078)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [290/431]  eta: 0:02:36  lr: 0.000152  loss: 1.0716 (1.1059)  time: 1.1030  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:315]  [300/431]  eta: 0:02:25  lr: 0.000152  loss: 1.0469 (1.1053)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [310/431]  eta: 0:02:14  lr: 0.000152  loss: 1.0469 (1.1034)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [320/431]  eta: 0:02:03  lr: 0.000152  loss: 1.0567 (1.1029)  time: 1.1169  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [330/431]  eta: 0:01:52  lr: 0.000152  loss: 1.1055 (1.1029)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [340/431]  eta: 0:01:41  lr: 0.000152  loss: 1.1604 (1.1060)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [350/431]  eta: 0:01:29  lr: 0.000152  loss: 1.1638 (1.1077)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [360/431]  eta: 0:01:18  lr: 0.000152  loss: 1.0974 (1.1077)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [370/431]  eta: 0:01:07  lr: 0.000152  loss: 1.0323 (1.1072)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [380/431]  eta: 0:00:56  lr: 0.000152  loss: 1.0503 (1.1070)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [390/431]  eta: 0:00:45  lr: 0.000152  loss: 1.0503 (1.1063)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [400/431]  eta: 0:00:34  lr: 0.000152  loss: 1.0441 (1.1049)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [410/431]  eta: 0:00:23  lr: 0.000152  loss: 1.0596 (1.1049)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:315]  [420/431]  eta: 0:00:12  lr: 0.000152  loss: 1.0259 (1.1030)  time: 1.1044  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:315]  [430/431]  eta: 0:00:01  lr: 0.000152  loss: 1.0131 (1.1016)  time: 1.1002  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:315] Total time: 0:07:58 (1.1106 s / it)\n",
      "Averaged stats: lr: 0.000152  loss: 1.0131 (1.1016)\n",
      "Valid: [epoch:315]  [ 0/14]  eta: 0:00:36  loss: 0.9524 (0.9524)  time: 2.5736  data: 2.3944  max mem: 15925\n",
      "Valid: [epoch:315]  [13/14]  eta: 0:00:00  loss: 1.0316 (1.0406)  time: 0.2826  data: 0.1711  max mem: 15925\n",
      "Valid: [epoch:315] Total time: 0:00:04 (0.2981 s / it)\n",
      "Averaged stats: loss: 1.0316 (1.0406)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_315_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:316]  [  0/431]  eta: 0:31:05  lr: 0.000152  loss: 1.1031 (1.1031)  time: 4.3278  data: 3.1307  max mem: 15925\n",
      "Train: [epoch:316]  [ 10/431]  eta: 0:09:23  lr: 0.000152  loss: 1.1706 (1.1573)  time: 1.3395  data: 0.2848  max mem: 15925\n",
      "Train: [epoch:316]  [ 20/431]  eta: 0:08:16  lr: 0.000152  loss: 1.1037 (1.1183)  time: 1.0511  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [ 30/431]  eta: 0:07:47  lr: 0.000152  loss: 1.0453 (1.1138)  time: 1.0691  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [ 40/431]  eta: 0:07:28  lr: 0.000152  loss: 1.0944 (1.1136)  time: 1.0820  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [ 50/431]  eta: 0:07:13  lr: 0.000152  loss: 1.1038 (1.1178)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [ 60/431]  eta: 0:07:00  lr: 0.000152  loss: 1.0872 (1.1099)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [ 70/431]  eta: 0:06:48  lr: 0.000152  loss: 1.0710 (1.1056)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [ 80/431]  eta: 0:06:35  lr: 0.000152  loss: 1.0738 (1.1040)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [ 90/431]  eta: 0:06:23  lr: 0.000152  loss: 1.0764 (1.1025)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [100/431]  eta: 0:06:11  lr: 0.000152  loss: 1.0642 (1.0966)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [110/431]  eta: 0:05:59  lr: 0.000152  loss: 1.0529 (1.0962)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [120/431]  eta: 0:05:48  lr: 0.000152  loss: 1.1070 (1.0962)  time: 1.1184  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [130/431]  eta: 0:05:37  lr: 0.000152  loss: 1.0610 (1.0946)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [140/431]  eta: 0:05:25  lr: 0.000152  loss: 1.0610 (1.0945)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [150/431]  eta: 0:05:14  lr: 0.000152  loss: 1.1217 (1.0977)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [160/431]  eta: 0:05:02  lr: 0.000152  loss: 1.1217 (1.0964)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [170/431]  eta: 0:04:51  lr: 0.000152  loss: 1.0535 (1.0937)  time: 1.1042  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:316]  [180/431]  eta: 0:04:40  lr: 0.000152  loss: 1.0832 (1.0966)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [190/431]  eta: 0:04:28  lr: 0.000152  loss: 1.1201 (1.0972)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [200/431]  eta: 0:04:17  lr: 0.000152  loss: 1.0954 (1.0983)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [210/431]  eta: 0:04:06  lr: 0.000152  loss: 1.0905 (1.1002)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [220/431]  eta: 0:03:54  lr: 0.000152  loss: 1.0688 (1.1003)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [230/431]  eta: 0:03:43  lr: 0.000152  loss: 1.0127 (1.0991)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [240/431]  eta: 0:03:32  lr: 0.000152  loss: 1.0571 (1.0977)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [250/431]  eta: 0:03:21  lr: 0.000152  loss: 1.0615 (1.0961)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [260/431]  eta: 0:03:09  lr: 0.000152  loss: 1.0499 (1.0948)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [270/431]  eta: 0:02:58  lr: 0.000152  loss: 1.0477 (1.0955)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [280/431]  eta: 0:02:47  lr: 0.000152  loss: 1.1351 (1.0964)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [290/431]  eta: 0:02:36  lr: 0.000152  loss: 1.0546 (1.0976)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [300/431]  eta: 0:02:25  lr: 0.000152  loss: 1.1158 (1.0989)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [310/431]  eta: 0:02:14  lr: 0.000152  loss: 1.1610 (1.1014)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [320/431]  eta: 0:02:03  lr: 0.000152  loss: 1.0836 (1.1003)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [330/431]  eta: 0:01:52  lr: 0.000152  loss: 1.1030 (1.1020)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [340/431]  eta: 0:01:40  lr: 0.000152  loss: 1.1030 (1.1008)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [350/431]  eta: 0:01:29  lr: 0.000152  loss: 1.0457 (1.1005)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [360/431]  eta: 0:01:18  lr: 0.000152  loss: 1.0227 (1.0984)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [370/431]  eta: 0:01:07  lr: 0.000152  loss: 1.0672 (1.0995)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [380/431]  eta: 0:00:56  lr: 0.000152  loss: 1.1008 (1.0990)  time: 1.1165  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [390/431]  eta: 0:00:45  lr: 0.000152  loss: 1.0863 (1.1000)  time: 1.1157  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [400/431]  eta: 0:00:34  lr: 0.000152  loss: 1.0534 (1.0998)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [410/431]  eta: 0:00:23  lr: 0.000152  loss: 1.0614 (1.1012)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:316]  [420/431]  eta: 0:00:12  lr: 0.000152  loss: 1.1082 (1.1010)  time: 1.1020  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:316]  [430/431]  eta: 0:00:01  lr: 0.000152  loss: 1.1082 (1.1013)  time: 1.0928  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:316] Total time: 0:07:58 (1.1092 s / it)\n",
      "Averaged stats: lr: 0.000152  loss: 1.1082 (1.1013)\n",
      "Valid: [epoch:316]  [ 0/14]  eta: 0:00:39  loss: 0.9518 (0.9518)  time: 2.7915  data: 2.6363  max mem: 15925\n",
      "Valid: [epoch:316]  [13/14]  eta: 0:00:00  loss: 1.0297 (1.0393)  time: 0.2866  data: 0.1884  max mem: 15925\n",
      "Valid: [epoch:316] Total time: 0:00:04 (0.3042 s / it)\n",
      "Averaged stats: loss: 1.0297 (1.0393)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_316_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.039%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:317]  [  0/431]  eta: 0:30:52  lr: 0.000152  loss: 1.1731 (1.1731)  time: 4.2992  data: 3.0435  max mem: 15925\n",
      "Train: [epoch:317]  [ 10/431]  eta: 0:09:26  lr: 0.000152  loss: 1.1042 (1.0901)  time: 1.3452  data: 0.2769  max mem: 15925\n",
      "Train: [epoch:317]  [ 20/431]  eta: 0:08:15  lr: 0.000152  loss: 1.0862 (1.1029)  time: 1.0505  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [ 30/431]  eta: 0:07:46  lr: 0.000152  loss: 1.0714 (1.0832)  time: 1.0649  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [ 40/431]  eta: 0:07:28  lr: 0.000152  loss: 1.0653 (1.0898)  time: 1.0872  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [ 50/431]  eta: 0:07:14  lr: 0.000152  loss: 1.0648 (1.0796)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [ 60/431]  eta: 0:07:00  lr: 0.000152  loss: 1.0021 (1.0730)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [ 70/431]  eta: 0:06:48  lr: 0.000152  loss: 1.0221 (1.0785)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [ 80/431]  eta: 0:06:35  lr: 0.000152  loss: 1.1427 (1.0870)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [ 90/431]  eta: 0:06:23  lr: 0.000152  loss: 1.0537 (1.0874)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [100/431]  eta: 0:06:11  lr: 0.000152  loss: 1.0300 (1.0819)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [110/431]  eta: 0:06:00  lr: 0.000152  loss: 1.0300 (1.0788)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [120/431]  eta: 0:05:48  lr: 0.000152  loss: 1.0593 (1.0826)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [130/431]  eta: 0:05:37  lr: 0.000152  loss: 1.0750 (1.0863)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [140/431]  eta: 0:05:25  lr: 0.000152  loss: 1.0780 (1.0881)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [150/431]  eta: 0:05:14  lr: 0.000152  loss: 1.0693 (1.0873)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [160/431]  eta: 0:05:02  lr: 0.000152  loss: 1.0751 (1.0888)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [170/431]  eta: 0:04:51  lr: 0.000152  loss: 1.1247 (1.0944)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [180/431]  eta: 0:04:39  lr: 0.000152  loss: 1.0739 (1.0926)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [190/431]  eta: 0:04:28  lr: 0.000152  loss: 1.0690 (1.0921)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [200/431]  eta: 0:04:17  lr: 0.000152  loss: 1.0691 (1.0919)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [210/431]  eta: 0:04:06  lr: 0.000152  loss: 1.0705 (1.0923)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [220/431]  eta: 0:03:54  lr: 0.000152  loss: 1.0705 (1.0920)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [230/431]  eta: 0:03:43  lr: 0.000152  loss: 1.0757 (1.0919)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [240/431]  eta: 0:03:32  lr: 0.000152  loss: 1.0757 (1.0904)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [250/431]  eta: 0:03:21  lr: 0.000152  loss: 1.1054 (1.0951)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [260/431]  eta: 0:03:10  lr: 0.000152  loss: 1.1559 (1.0963)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [270/431]  eta: 0:02:58  lr: 0.000152  loss: 1.0786 (1.0947)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [280/431]  eta: 0:02:47  lr: 0.000152  loss: 1.0706 (1.0956)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [290/431]  eta: 0:02:36  lr: 0.000152  loss: 1.0985 (1.0980)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [300/431]  eta: 0:02:25  lr: 0.000152  loss: 1.0805 (1.0982)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [310/431]  eta: 0:02:14  lr: 0.000152  loss: 1.0869 (1.0985)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [320/431]  eta: 0:02:03  lr: 0.000152  loss: 1.0911 (1.0986)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [330/431]  eta: 0:01:52  lr: 0.000152  loss: 1.1043 (1.1000)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [340/431]  eta: 0:01:40  lr: 0.000152  loss: 1.1048 (1.1003)  time: 1.1016  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:317]  [350/431]  eta: 0:01:29  lr: 0.000152  loss: 1.1249 (1.1012)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [360/431]  eta: 0:01:18  lr: 0.000152  loss: 1.0389 (1.1012)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [370/431]  eta: 0:01:07  lr: 0.000152  loss: 1.0319 (1.1017)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [380/431]  eta: 0:00:56  lr: 0.000152  loss: 1.0210 (1.1007)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [390/431]  eta: 0:00:45  lr: 0.000152  loss: 1.0678 (1.1014)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [400/431]  eta: 0:00:34  lr: 0.000152  loss: 1.0882 (1.1013)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [410/431]  eta: 0:00:23  lr: 0.000152  loss: 1.0882 (1.1021)  time: 1.0909  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:317]  [420/431]  eta: 0:00:12  lr: 0.000152  loss: 1.0722 (1.1015)  time: 1.0875  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:317]  [430/431]  eta: 0:00:01  lr: 0.000152  loss: 1.0708 (1.1015)  time: 1.1034  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:317] Total time: 0:07:57 (1.1080 s / it)\n",
      "Averaged stats: lr: 0.000152  loss: 1.0708 (1.1015)\n",
      "Valid: [epoch:317]  [ 0/14]  eta: 0:00:36  loss: 1.1276 (1.1276)  time: 2.6420  data: 2.4925  max mem: 15925\n",
      "Valid: [epoch:317]  [13/14]  eta: 0:00:00  loss: 1.0356 (1.0433)  time: 0.2888  data: 0.1781  max mem: 15925\n",
      "Valid: [epoch:317] Total time: 0:00:04 (0.3045 s / it)\n",
      "Averaged stats: loss: 1.0356 (1.0433)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_317_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:318]  [  0/431]  eta: 0:35:27  lr: 0.000152  loss: 1.0738 (1.0738)  time: 4.9367  data: 3.7912  max mem: 15925\n",
      "Train: [epoch:318]  [ 10/431]  eta: 0:09:47  lr: 0.000152  loss: 1.0738 (1.1010)  time: 1.3958  data: 0.3448  max mem: 15925\n",
      "Train: [epoch:318]  [ 20/431]  eta: 0:08:28  lr: 0.000152  loss: 1.0711 (1.0922)  time: 1.0518  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [ 30/431]  eta: 0:07:55  lr: 0.000152  loss: 1.0711 (1.0922)  time: 1.0702  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [ 40/431]  eta: 0:07:33  lr: 0.000152  loss: 1.0604 (1.0953)  time: 1.0803  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [ 50/431]  eta: 0:07:18  lr: 0.000152  loss: 1.0515 (1.0785)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [ 60/431]  eta: 0:07:03  lr: 0.000152  loss: 1.0325 (1.0763)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [ 70/431]  eta: 0:06:50  lr: 0.000152  loss: 1.0797 (1.0862)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [ 80/431]  eta: 0:06:38  lr: 0.000152  loss: 1.0992 (1.0918)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [ 90/431]  eta: 0:06:25  lr: 0.000152  loss: 1.0600 (1.0880)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [100/431]  eta: 0:06:13  lr: 0.000152  loss: 1.0550 (1.0929)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [110/431]  eta: 0:06:01  lr: 0.000152  loss: 1.0477 (1.0922)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [120/431]  eta: 0:05:49  lr: 0.000152  loss: 1.0477 (1.0888)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [130/431]  eta: 0:05:38  lr: 0.000152  loss: 1.0476 (1.0882)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [140/431]  eta: 0:05:26  lr: 0.000152  loss: 1.0476 (1.0887)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [150/431]  eta: 0:05:14  lr: 0.000152  loss: 1.0897 (1.0892)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [160/431]  eta: 0:05:03  lr: 0.000152  loss: 1.1086 (1.0926)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [170/431]  eta: 0:04:51  lr: 0.000152  loss: 1.1066 (1.0911)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [180/431]  eta: 0:04:40  lr: 0.000152  loss: 1.0602 (1.0901)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [190/431]  eta: 0:04:29  lr: 0.000152  loss: 1.0608 (1.0900)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [200/431]  eta: 0:04:18  lr: 0.000152  loss: 1.1170 (1.0942)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [210/431]  eta: 0:04:06  lr: 0.000152  loss: 1.1200 (1.0929)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [220/431]  eta: 0:03:55  lr: 0.000152  loss: 1.0473 (1.0932)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [230/431]  eta: 0:03:44  lr: 0.000152  loss: 1.1189 (1.0954)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [240/431]  eta: 0:03:33  lr: 0.000152  loss: 1.0952 (1.0958)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [250/431]  eta: 0:03:21  lr: 0.000152  loss: 1.0975 (1.0990)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [260/431]  eta: 0:03:10  lr: 0.000152  loss: 1.1132 (1.0997)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [270/431]  eta: 0:02:59  lr: 0.000152  loss: 1.0755 (1.0989)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [280/431]  eta: 0:02:48  lr: 0.000152  loss: 1.0335 (1.0983)  time: 1.1126  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [290/431]  eta: 0:02:37  lr: 0.000152  loss: 1.0601 (1.0994)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [300/431]  eta: 0:02:25  lr: 0.000152  loss: 1.0601 (1.0998)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [310/431]  eta: 0:02:14  lr: 0.000152  loss: 1.0726 (1.0990)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [320/431]  eta: 0:02:03  lr: 0.000152  loss: 1.0869 (1.0995)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [330/431]  eta: 0:01:52  lr: 0.000152  loss: 1.1163 (1.1011)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [340/431]  eta: 0:01:41  lr: 0.000152  loss: 1.1573 (1.1034)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [350/431]  eta: 0:01:30  lr: 0.000152  loss: 1.1374 (1.1035)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [360/431]  eta: 0:01:18  lr: 0.000152  loss: 1.0330 (1.1026)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [370/431]  eta: 0:01:07  lr: 0.000152  loss: 1.0289 (1.1007)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [380/431]  eta: 0:00:56  lr: 0.000152  loss: 1.0763 (1.1015)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [390/431]  eta: 0:00:45  lr: 0.000152  loss: 1.0899 (1.1012)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [400/431]  eta: 0:00:34  lr: 0.000152  loss: 1.0676 (1.1016)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [410/431]  eta: 0:00:23  lr: 0.000152  loss: 1.0780 (1.1007)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:318]  [420/431]  eta: 0:00:12  lr: 0.000152  loss: 1.0977 (1.1024)  time: 1.1047  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:318]  [430/431]  eta: 0:00:01  lr: 0.000152  loss: 1.1087 (1.1013)  time: 1.0968  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:318] Total time: 0:07:58 (1.1108 s / it)\n",
      "Averaged stats: lr: 0.000152  loss: 1.1087 (1.1013)\n",
      "Valid: [epoch:318]  [ 0/14]  eta: 0:00:35  loss: 1.0109 (1.0109)  time: 2.5404  data: 2.3732  max mem: 15925\n",
      "Valid: [epoch:318]  [13/14]  eta: 0:00:00  loss: 1.0347 (1.0443)  time: 0.2625  data: 0.1696  max mem: 15925\n",
      "Valid: [epoch:318] Total time: 0:00:03 (0.2802 s / it)\n",
      "Averaged stats: loss: 1.0347 (1.0443)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_318_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:319]  [  0/431]  eta: 0:34:29  lr: 0.000152  loss: 1.2105 (1.2105)  time: 4.8005  data: 3.6068  max mem: 15925\n",
      "Train: [epoch:319]  [ 10/431]  eta: 0:09:42  lr: 0.000152  loss: 1.1665 (1.2079)  time: 1.3826  data: 0.3281  max mem: 15925\n",
      "Train: [epoch:319]  [ 20/431]  eta: 0:08:21  lr: 0.000152  loss: 1.1105 (1.1295)  time: 1.0422  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:319]  [ 30/431]  eta: 0:07:52  lr: 0.000152  loss: 1.0224 (1.1043)  time: 1.0673  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [ 40/431]  eta: 0:07:32  lr: 0.000152  loss: 1.0343 (1.0873)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [ 50/431]  eta: 0:07:17  lr: 0.000152  loss: 1.0387 (1.0817)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [ 60/431]  eta: 0:07:02  lr: 0.000152  loss: 1.0387 (1.0769)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [ 70/431]  eta: 0:06:48  lr: 0.000152  loss: 1.0498 (1.0763)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [ 80/431]  eta: 0:06:35  lr: 0.000152  loss: 1.0607 (1.0760)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [ 90/431]  eta: 0:06:23  lr: 0.000152  loss: 1.0673 (1.0770)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [100/431]  eta: 0:06:10  lr: 0.000152  loss: 1.0621 (1.0757)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [110/431]  eta: 0:05:58  lr: 0.000152  loss: 1.0342 (1.0703)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [120/431]  eta: 0:05:47  lr: 0.000152  loss: 1.0342 (1.0734)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [130/431]  eta: 0:05:35  lr: 0.000152  loss: 1.0996 (1.0781)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [140/431]  eta: 0:05:24  lr: 0.000152  loss: 1.1135 (1.0831)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [150/431]  eta: 0:05:12  lr: 0.000152  loss: 1.0832 (1.0836)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [160/431]  eta: 0:05:01  lr: 0.000152  loss: 1.0832 (1.0849)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [170/431]  eta: 0:04:50  lr: 0.000152  loss: 1.0584 (1.0839)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [180/431]  eta: 0:04:39  lr: 0.000152  loss: 1.0705 (1.0882)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [190/431]  eta: 0:04:27  lr: 0.000152  loss: 1.1163 (1.0928)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [200/431]  eta: 0:04:16  lr: 0.000152  loss: 1.1135 (1.0938)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [210/431]  eta: 0:04:05  lr: 0.000152  loss: 1.0601 (1.0939)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [220/431]  eta: 0:03:54  lr: 0.000152  loss: 1.0601 (1.0934)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [230/431]  eta: 0:03:43  lr: 0.000152  loss: 1.0559 (1.0916)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [240/431]  eta: 0:03:32  lr: 0.000152  loss: 1.0239 (1.0903)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [250/431]  eta: 0:03:20  lr: 0.000152  loss: 1.0585 (1.0930)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [260/431]  eta: 0:03:09  lr: 0.000152  loss: 1.1394 (1.0938)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [270/431]  eta: 0:02:58  lr: 0.000152  loss: 1.1249 (1.0955)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [280/431]  eta: 0:02:47  lr: 0.000152  loss: 1.1126 (1.0971)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [290/431]  eta: 0:02:36  lr: 0.000152  loss: 1.1101 (1.0970)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [300/431]  eta: 0:02:25  lr: 0.000152  loss: 1.0995 (1.0988)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [310/431]  eta: 0:02:14  lr: 0.000152  loss: 1.0995 (1.0985)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [320/431]  eta: 0:02:02  lr: 0.000152  loss: 1.0624 (1.0983)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [330/431]  eta: 0:01:51  lr: 0.000152  loss: 1.0889 (1.0999)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [340/431]  eta: 0:01:40  lr: 0.000152  loss: 1.1088 (1.1014)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [350/431]  eta: 0:01:29  lr: 0.000152  loss: 1.1190 (1.1029)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [360/431]  eta: 0:01:18  lr: 0.000152  loss: 1.0926 (1.1029)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [370/431]  eta: 0:01:07  lr: 0.000152  loss: 1.0889 (1.1028)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [380/431]  eta: 0:00:56  lr: 0.000152  loss: 1.0707 (1.1030)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [390/431]  eta: 0:00:45  lr: 0.000152  loss: 1.0532 (1.1020)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [400/431]  eta: 0:00:34  lr: 0.000152  loss: 1.0172 (1.1001)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [410/431]  eta: 0:00:23  lr: 0.000152  loss: 1.0702 (1.1013)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:319]  [420/431]  eta: 0:00:12  lr: 0.000152  loss: 1.0793 (1.1011)  time: 1.0950  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:319]  [430/431]  eta: 0:00:01  lr: 0.000152  loss: 1.0457 (1.1009)  time: 1.1055  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:319] Total time: 0:07:57 (1.1069 s / it)\n",
      "Averaged stats: lr: 0.000152  loss: 1.0457 (1.1009)\n",
      "Valid: [epoch:319]  [ 0/14]  eta: 0:00:35  loss: 1.0879 (1.0879)  time: 2.5302  data: 2.3877  max mem: 15925\n",
      "Valid: [epoch:319]  [13/14]  eta: 0:00:00  loss: 1.0322 (1.0433)  time: 0.2644  data: 0.1706  max mem: 15925\n",
      "Valid: [epoch:319] Total time: 0:00:03 (0.2819 s / it)\n",
      "Averaged stats: loss: 1.0322 (1.0433)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_319_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:320]  [  0/431]  eta: 0:31:46  lr: 0.000151  loss: 1.1615 (1.1615)  time: 4.4231  data: 3.1487  max mem: 15925\n",
      "Train: [epoch:320]  [ 10/431]  eta: 0:09:28  lr: 0.000151  loss: 1.1061 (1.1260)  time: 1.3509  data: 0.2864  max mem: 15925\n",
      "Train: [epoch:320]  [ 20/431]  eta: 0:08:17  lr: 0.000151  loss: 1.0540 (1.0948)  time: 1.0492  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [ 30/431]  eta: 0:07:48  lr: 0.000151  loss: 1.0533 (1.0837)  time: 1.0668  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [ 40/431]  eta: 0:07:27  lr: 0.000151  loss: 1.1072 (1.1049)  time: 1.0738  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [ 50/431]  eta: 0:07:12  lr: 0.000151  loss: 1.1205 (1.1037)  time: 1.0822  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [ 60/431]  eta: 0:06:59  lr: 0.000151  loss: 1.0738 (1.1007)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [ 70/431]  eta: 0:06:46  lr: 0.000151  loss: 1.0738 (1.0989)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [ 80/431]  eta: 0:06:34  lr: 0.000151  loss: 1.1026 (1.1003)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [ 90/431]  eta: 0:06:21  lr: 0.000151  loss: 1.0939 (1.0976)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [100/431]  eta: 0:06:09  lr: 0.000151  loss: 1.0992 (1.1039)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [110/431]  eta: 0:05:58  lr: 0.000151  loss: 1.0992 (1.1000)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [120/431]  eta: 0:05:47  lr: 0.000151  loss: 1.0459 (1.0962)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [130/431]  eta: 0:05:35  lr: 0.000151  loss: 1.0846 (1.1001)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [140/431]  eta: 0:05:24  lr: 0.000151  loss: 1.0896 (1.0982)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [150/431]  eta: 0:05:12  lr: 0.000151  loss: 1.0772 (1.1009)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [160/431]  eta: 0:05:01  lr: 0.000151  loss: 1.0692 (1.0996)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [170/431]  eta: 0:04:50  lr: 0.000151  loss: 1.0645 (1.1003)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [180/431]  eta: 0:04:39  lr: 0.000151  loss: 1.0661 (1.0989)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [190/431]  eta: 0:04:27  lr: 0.000151  loss: 1.0702 (1.1007)  time: 1.1061  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:320]  [200/431]  eta: 0:04:16  lr: 0.000151  loss: 1.0788 (1.0998)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [210/431]  eta: 0:04:05  lr: 0.000151  loss: 1.0898 (1.1005)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [220/431]  eta: 0:03:54  lr: 0.000151  loss: 1.0413 (1.0974)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [230/431]  eta: 0:03:43  lr: 0.000151  loss: 1.0846 (1.0989)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [240/431]  eta: 0:03:31  lr: 0.000151  loss: 1.0885 (1.0985)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [250/431]  eta: 0:03:20  lr: 0.000151  loss: 1.0619 (1.0972)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [260/431]  eta: 0:03:09  lr: 0.000151  loss: 1.0739 (1.0983)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [270/431]  eta: 0:02:58  lr: 0.000151  loss: 1.0908 (1.0998)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [280/431]  eta: 0:02:47  lr: 0.000151  loss: 1.0942 (1.1004)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [290/431]  eta: 0:02:36  lr: 0.000151  loss: 1.0942 (1.1026)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [300/431]  eta: 0:02:25  lr: 0.000151  loss: 1.1159 (1.1039)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [310/431]  eta: 0:02:14  lr: 0.000151  loss: 1.0726 (1.1023)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [320/431]  eta: 0:02:02  lr: 0.000151  loss: 1.0363 (1.1008)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [330/431]  eta: 0:01:51  lr: 0.000151  loss: 1.0415 (1.1031)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [340/431]  eta: 0:01:40  lr: 0.000151  loss: 1.0808 (1.1038)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [350/431]  eta: 0:01:29  lr: 0.000151  loss: 1.0652 (1.1041)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [360/431]  eta: 0:01:18  lr: 0.000151  loss: 1.0553 (1.1032)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [370/431]  eta: 0:01:07  lr: 0.000151  loss: 1.0302 (1.1021)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [380/431]  eta: 0:00:56  lr: 0.000151  loss: 1.0577 (1.1021)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [390/431]  eta: 0:00:45  lr: 0.000151  loss: 1.0905 (1.1024)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [400/431]  eta: 0:00:34  lr: 0.000151  loss: 1.0824 (1.1016)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [410/431]  eta: 0:00:23  lr: 0.000151  loss: 1.0774 (1.1022)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:320]  [420/431]  eta: 0:00:12  lr: 0.000151  loss: 1.0774 (1.1020)  time: 1.1040  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:320]  [430/431]  eta: 0:00:01  lr: 0.000151  loss: 1.0577 (1.1018)  time: 1.1004  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:320] Total time: 0:07:57 (1.1075 s / it)\n",
      "Averaged stats: lr: 0.000151  loss: 1.0577 (1.1018)\n",
      "Valid: [epoch:320]  [ 0/14]  eta: 0:00:35  loss: 1.0915 (1.0915)  time: 2.5235  data: 2.3634  max mem: 15925\n",
      "Valid: [epoch:320]  [13/14]  eta: 0:00:00  loss: 1.0338 (1.0422)  time: 0.2781  data: 0.1689  max mem: 15925\n",
      "Valid: [epoch:320] Total time: 0:00:04 (0.2940 s / it)\n",
      "Averaged stats: loss: 1.0338 (1.0422)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_320_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:321]  [  0/431]  eta: 0:32:31  lr: 0.000151  loss: 1.0291 (1.0291)  time: 4.5280  data: 3.3484  max mem: 15925\n",
      "Train: [epoch:321]  [ 10/431]  eta: 0:09:30  lr: 0.000151  loss: 1.0649 (1.1009)  time: 1.3541  data: 0.3046  max mem: 15925\n",
      "Train: [epoch:321]  [ 20/431]  eta: 0:08:18  lr: 0.000151  loss: 1.0712 (1.1119)  time: 1.0465  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [ 30/431]  eta: 0:07:47  lr: 0.000151  loss: 1.0880 (1.0988)  time: 1.0627  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [ 40/431]  eta: 0:07:27  lr: 0.000151  loss: 1.0247 (1.0946)  time: 1.0758  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [ 50/431]  eta: 0:07:12  lr: 0.000151  loss: 1.0247 (1.0850)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [ 60/431]  eta: 0:06:59  lr: 0.000151  loss: 1.0347 (1.0798)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [ 70/431]  eta: 0:06:47  lr: 0.000151  loss: 1.0661 (1.0826)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [ 80/431]  eta: 0:06:34  lr: 0.000151  loss: 1.0608 (1.0830)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [ 90/431]  eta: 0:06:22  lr: 0.000151  loss: 1.0570 (1.0821)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [100/431]  eta: 0:06:11  lr: 0.000151  loss: 1.0911 (1.0885)  time: 1.1130  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:321]  [110/431]  eta: 0:05:59  lr: 0.000151  loss: 1.1447 (1.0941)  time: 1.1118  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:321]  [120/431]  eta: 0:05:48  lr: 0.000151  loss: 1.1447 (1.0956)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [130/431]  eta: 0:05:37  lr: 0.000151  loss: 1.1041 (1.0947)  time: 1.1181  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [140/431]  eta: 0:05:25  lr: 0.000151  loss: 1.0252 (1.0906)  time: 1.1161  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [150/431]  eta: 0:05:14  lr: 0.000151  loss: 1.0465 (1.0930)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [160/431]  eta: 0:05:03  lr: 0.000151  loss: 1.1033 (1.0972)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [170/431]  eta: 0:04:51  lr: 0.000151  loss: 1.1572 (1.0988)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [180/431]  eta: 0:04:40  lr: 0.000151  loss: 1.0388 (1.0964)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [190/431]  eta: 0:04:29  lr: 0.000151  loss: 1.1005 (1.1024)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [200/431]  eta: 0:04:17  lr: 0.000151  loss: 1.1458 (1.1042)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [210/431]  eta: 0:04:06  lr: 0.000151  loss: 1.0568 (1.1022)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [220/431]  eta: 0:03:55  lr: 0.000151  loss: 1.0479 (1.1019)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [230/431]  eta: 0:03:43  lr: 0.000151  loss: 1.0705 (1.1019)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [240/431]  eta: 0:03:32  lr: 0.000151  loss: 1.0897 (1.1033)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [250/431]  eta: 0:03:21  lr: 0.000151  loss: 1.1215 (1.1048)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [260/431]  eta: 0:03:10  lr: 0.000151  loss: 1.1215 (1.1077)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [270/431]  eta: 0:02:59  lr: 0.000151  loss: 1.0853 (1.1060)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [280/431]  eta: 0:02:47  lr: 0.000151  loss: 1.0751 (1.1063)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [290/431]  eta: 0:02:36  lr: 0.000151  loss: 1.0525 (1.1046)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [300/431]  eta: 0:02:25  lr: 0.000151  loss: 1.0390 (1.1047)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [310/431]  eta: 0:02:14  lr: 0.000151  loss: 1.1378 (1.1057)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [320/431]  eta: 0:02:03  lr: 0.000151  loss: 1.0764 (1.1052)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [330/431]  eta: 0:01:52  lr: 0.000151  loss: 1.0815 (1.1059)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [340/431]  eta: 0:01:41  lr: 0.000151  loss: 1.0815 (1.1054)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [350/431]  eta: 0:01:29  lr: 0.000151  loss: 1.0568 (1.1051)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [360/431]  eta: 0:01:18  lr: 0.000151  loss: 1.0600 (1.1053)  time: 1.1044  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:321]  [370/431]  eta: 0:01:07  lr: 0.000151  loss: 1.0536 (1.1043)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [380/431]  eta: 0:00:56  lr: 0.000151  loss: 1.0286 (1.1025)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [390/431]  eta: 0:00:45  lr: 0.000151  loss: 1.0392 (1.1027)  time: 1.1028  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:321]  [400/431]  eta: 0:00:34  lr: 0.000151  loss: 1.1102 (1.1043)  time: 1.1059  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:321]  [410/431]  eta: 0:00:23  lr: 0.000151  loss: 1.1102 (1.1034)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:321]  [420/431]  eta: 0:00:12  lr: 0.000151  loss: 1.0667 (1.1030)  time: 1.1013  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:321]  [430/431]  eta: 0:00:01  lr: 0.000151  loss: 1.0527 (1.1018)  time: 1.1021  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:321] Total time: 0:07:58 (1.1101 s / it)\n",
      "Averaged stats: lr: 0.000151  loss: 1.0527 (1.1018)\n",
      "Valid: [epoch:321]  [ 0/14]  eta: 0:00:34  loss: 1.1286 (1.1286)  time: 2.4978  data: 2.3160  max mem: 15925\n",
      "Valid: [epoch:321]  [13/14]  eta: 0:00:00  loss: 1.0335 (1.0415)  time: 0.2621  data: 0.1655  max mem: 15925\n",
      "Valid: [epoch:321] Total time: 0:00:03 (0.2795 s / it)\n",
      "Averaged stats: loss: 1.0335 (1.0415)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_321_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:322]  [  0/431]  eta: 0:33:59  lr: 0.000151  loss: 0.9735 (0.9735)  time: 4.7327  data: 3.5900  max mem: 15925\n",
      "Train: [epoch:322]  [ 10/431]  eta: 0:09:39  lr: 0.000151  loss: 1.1460 (1.1582)  time: 1.3776  data: 0.3266  max mem: 15925\n",
      "Train: [epoch:322]  [ 20/431]  eta: 0:08:25  lr: 0.000151  loss: 1.1168 (1.1297)  time: 1.0538  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [ 30/431]  eta: 0:07:51  lr: 0.000151  loss: 1.0792 (1.1101)  time: 1.0658  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [ 40/431]  eta: 0:07:31  lr: 0.000151  loss: 1.0878 (1.0979)  time: 1.0780  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [ 50/431]  eta: 0:07:15  lr: 0.000151  loss: 1.0401 (1.0875)  time: 1.0916  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [ 60/431]  eta: 0:07:01  lr: 0.000151  loss: 1.0119 (1.0776)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [ 70/431]  eta: 0:06:48  lr: 0.000151  loss: 1.0491 (1.0842)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [ 80/431]  eta: 0:06:36  lr: 0.000151  loss: 1.1127 (1.0841)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [ 90/431]  eta: 0:06:24  lr: 0.000151  loss: 1.0495 (1.0774)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [100/431]  eta: 0:06:12  lr: 0.000151  loss: 1.0609 (1.0807)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [110/431]  eta: 0:06:00  lr: 0.000151  loss: 1.0779 (1.0801)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [120/431]  eta: 0:05:48  lr: 0.000151  loss: 1.0846 (1.0837)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [130/431]  eta: 0:05:37  lr: 0.000151  loss: 1.1042 (1.0865)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [140/431]  eta: 0:05:26  lr: 0.000151  loss: 1.0693 (1.0852)  time: 1.1200  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [150/431]  eta: 0:05:14  lr: 0.000151  loss: 1.0549 (1.0829)  time: 1.1238  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [160/431]  eta: 0:05:03  lr: 0.000151  loss: 1.0706 (1.0866)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [170/431]  eta: 0:04:51  lr: 0.000151  loss: 1.0706 (1.0844)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [180/431]  eta: 0:04:40  lr: 0.000151  loss: 1.0296 (1.0837)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [190/431]  eta: 0:04:28  lr: 0.000151  loss: 1.0673 (1.0850)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [200/431]  eta: 0:04:17  lr: 0.000151  loss: 1.0347 (1.0838)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [210/431]  eta: 0:04:06  lr: 0.000151  loss: 1.0498 (1.0845)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [220/431]  eta: 0:03:55  lr: 0.000151  loss: 1.0701 (1.0866)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [230/431]  eta: 0:03:43  lr: 0.000151  loss: 1.1333 (1.0896)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [240/431]  eta: 0:03:32  lr: 0.000151  loss: 1.1557 (1.0933)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [250/431]  eta: 0:03:21  lr: 0.000151  loss: 1.1536 (1.0936)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [260/431]  eta: 0:03:10  lr: 0.000151  loss: 1.1001 (1.0947)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [270/431]  eta: 0:02:59  lr: 0.000151  loss: 1.0603 (1.0937)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [280/431]  eta: 0:02:47  lr: 0.000151  loss: 1.0104 (1.0915)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [290/431]  eta: 0:02:36  lr: 0.000151  loss: 1.0797 (1.0945)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [300/431]  eta: 0:02:25  lr: 0.000151  loss: 1.1455 (1.0951)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [310/431]  eta: 0:02:14  lr: 0.000151  loss: 1.0688 (1.0943)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [320/431]  eta: 0:02:03  lr: 0.000151  loss: 1.0639 (1.0952)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [330/431]  eta: 0:01:52  lr: 0.000151  loss: 1.1480 (1.0976)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [340/431]  eta: 0:01:41  lr: 0.000151  loss: 1.1620 (1.0998)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [350/431]  eta: 0:01:29  lr: 0.000151  loss: 1.1481 (1.1013)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [360/431]  eta: 0:01:18  lr: 0.000151  loss: 1.1249 (1.1017)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [370/431]  eta: 0:01:07  lr: 0.000151  loss: 1.0487 (1.1004)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [380/431]  eta: 0:00:56  lr: 0.000151  loss: 1.0489 (1.1009)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [390/431]  eta: 0:00:45  lr: 0.000151  loss: 1.1125 (1.1015)  time: 1.0896  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [400/431]  eta: 0:00:34  lr: 0.000151  loss: 1.0618 (1.1004)  time: 1.0873  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [410/431]  eta: 0:00:23  lr: 0.000151  loss: 1.0618 (1.1016)  time: 1.0896  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:322]  [420/431]  eta: 0:00:12  lr: 0.000151  loss: 1.1061 (1.1009)  time: 1.0953  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:322]  [430/431]  eta: 0:00:01  lr: 0.000151  loss: 1.0294 (1.1006)  time: 1.1036  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:322] Total time: 0:07:57 (1.1087 s / it)\n",
      "Averaged stats: lr: 0.000151  loss: 1.0294 (1.1006)\n",
      "Valid: [epoch:322]  [ 0/14]  eta: 0:00:35  loss: 1.0943 (1.0943)  time: 2.5286  data: 2.3744  max mem: 15925\n",
      "Valid: [epoch:322]  [13/14]  eta: 0:00:00  loss: 1.0329 (1.0425)  time: 0.2635  data: 0.1697  max mem: 15925\n",
      "Valid: [epoch:322] Total time: 0:00:03 (0.2828 s / it)\n",
      "Averaged stats: loss: 1.0329 (1.0425)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_322_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:323]  [  0/431]  eta: 0:32:02  lr: 0.000151  loss: 1.3634 (1.3634)  time: 4.4616  data: 3.3021  max mem: 15925\n",
      "Train: [epoch:323]  [ 10/431]  eta: 0:09:31  lr: 0.000151  loss: 1.1226 (1.1586)  time: 1.3584  data: 0.3004  max mem: 15925\n",
      "Train: [epoch:323]  [ 20/431]  eta: 0:08:21  lr: 0.000151  loss: 1.1136 (1.1195)  time: 1.0578  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [ 30/431]  eta: 0:07:50  lr: 0.000151  loss: 1.0320 (1.1108)  time: 1.0707  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [ 40/431]  eta: 0:07:30  lr: 0.000151  loss: 1.0908 (1.1107)  time: 1.0784  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:323]  [ 50/431]  eta: 0:07:13  lr: 0.000151  loss: 1.1330 (1.1119)  time: 1.0834  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [ 60/431]  eta: 0:07:00  lr: 0.000151  loss: 1.1330 (1.1044)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [ 70/431]  eta: 0:06:47  lr: 0.000151  loss: 1.0674 (1.1023)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [ 80/431]  eta: 0:06:34  lr: 0.000151  loss: 1.1239 (1.1049)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [ 90/431]  eta: 0:06:22  lr: 0.000151  loss: 1.0805 (1.1004)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [100/431]  eta: 0:06:10  lr: 0.000151  loss: 1.0948 (1.1032)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [110/431]  eta: 0:05:59  lr: 0.000151  loss: 1.0848 (1.1006)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [120/431]  eta: 0:05:47  lr: 0.000151  loss: 1.0324 (1.0987)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [130/431]  eta: 0:05:35  lr: 0.000151  loss: 1.0382 (1.0978)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [140/431]  eta: 0:05:24  lr: 0.000151  loss: 1.0379 (1.0942)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [150/431]  eta: 0:05:12  lr: 0.000151  loss: 1.0434 (1.0928)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [160/431]  eta: 0:05:01  lr: 0.000151  loss: 1.0735 (1.0937)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [170/431]  eta: 0:04:50  lr: 0.000151  loss: 1.0938 (1.0943)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [180/431]  eta: 0:04:38  lr: 0.000151  loss: 1.1562 (1.1001)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [190/431]  eta: 0:04:27  lr: 0.000151  loss: 1.1718 (1.1045)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [200/431]  eta: 0:04:16  lr: 0.000151  loss: 1.0718 (1.1041)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [210/431]  eta: 0:04:05  lr: 0.000151  loss: 1.0718 (1.1046)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [220/431]  eta: 0:03:54  lr: 0.000151  loss: 1.1152 (1.1045)  time: 1.0976  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:323]  [230/431]  eta: 0:03:42  lr: 0.000151  loss: 1.0553 (1.1022)  time: 1.0973  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:323]  [240/431]  eta: 0:03:31  lr: 0.000151  loss: 1.1305 (1.1035)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [250/431]  eta: 0:03:20  lr: 0.000151  loss: 1.1758 (1.1104)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [260/431]  eta: 0:03:09  lr: 0.000151  loss: 1.1577 (1.1098)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [270/431]  eta: 0:02:58  lr: 0.000151  loss: 1.1037 (1.1095)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [280/431]  eta: 0:02:47  lr: 0.000151  loss: 1.0868 (1.1091)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [290/431]  eta: 0:02:36  lr: 0.000151  loss: 1.0625 (1.1076)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [300/431]  eta: 0:02:25  lr: 0.000151  loss: 1.1088 (1.1092)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [310/431]  eta: 0:02:13  lr: 0.000151  loss: 1.1693 (1.1096)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [320/431]  eta: 0:02:02  lr: 0.000151  loss: 1.1476 (1.1087)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [330/431]  eta: 0:01:51  lr: 0.000151  loss: 1.0666 (1.1082)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [340/431]  eta: 0:01:40  lr: 0.000151  loss: 1.0505 (1.1057)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [350/431]  eta: 0:01:29  lr: 0.000151  loss: 1.0337 (1.1047)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [360/431]  eta: 0:01:18  lr: 0.000151  loss: 1.0416 (1.1044)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [370/431]  eta: 0:01:07  lr: 0.000151  loss: 1.0445 (1.1021)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [380/431]  eta: 0:00:56  lr: 0.000151  loss: 1.0062 (1.1015)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [390/431]  eta: 0:00:45  lr: 0.000151  loss: 1.0695 (1.1019)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [400/431]  eta: 0:00:34  lr: 0.000151  loss: 1.1084 (1.1020)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [410/431]  eta: 0:00:23  lr: 0.000151  loss: 1.1084 (1.1035)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:323]  [420/431]  eta: 0:00:12  lr: 0.000151  loss: 1.0501 (1.1023)  time: 1.0970  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:323]  [430/431]  eta: 0:00:01  lr: 0.000151  loss: 1.0256 (1.1009)  time: 1.1024  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:323] Total time: 0:07:56 (1.1060 s / it)\n",
      "Averaged stats: lr: 0.000151  loss: 1.0256 (1.1009)\n",
      "Valid: [epoch:323]  [ 0/14]  eta: 0:00:36  loss: 1.1284 (1.1284)  time: 2.5863  data: 2.4564  max mem: 15925\n",
      "Valid: [epoch:323]  [13/14]  eta: 0:00:00  loss: 1.0324 (1.0406)  time: 0.2762  data: 0.1755  max mem: 15925\n",
      "Valid: [epoch:323] Total time: 0:00:04 (0.2949 s / it)\n",
      "Averaged stats: loss: 1.0324 (1.0406)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_323_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:324]  [  0/431]  eta: 0:29:36  lr: 0.000150  loss: 1.1152 (1.1152)  time: 4.1226  data: 2.8665  max mem: 15925\n",
      "Train: [epoch:324]  [ 10/431]  eta: 0:09:22  lr: 0.000150  loss: 1.1616 (1.1891)  time: 1.3365  data: 0.2608  max mem: 15925\n",
      "Train: [epoch:324]  [ 20/431]  eta: 0:08:13  lr: 0.000150  loss: 1.1616 (1.1754)  time: 1.0549  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [ 30/431]  eta: 0:07:45  lr: 0.000150  loss: 1.1047 (1.1356)  time: 1.0649  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [ 40/431]  eta: 0:07:25  lr: 0.000150  loss: 1.0390 (1.1124)  time: 1.0771  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [ 50/431]  eta: 0:07:11  lr: 0.000150  loss: 1.0596 (1.1119)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [ 60/431]  eta: 0:06:58  lr: 0.000150  loss: 1.0596 (1.1052)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [ 70/431]  eta: 0:06:45  lr: 0.000150  loss: 1.0575 (1.1058)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [ 80/431]  eta: 0:06:33  lr: 0.000150  loss: 1.0690 (1.1043)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [ 90/431]  eta: 0:06:21  lr: 0.000150  loss: 1.0422 (1.0953)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [100/431]  eta: 0:06:09  lr: 0.000150  loss: 1.0403 (1.0922)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [110/431]  eta: 0:05:57  lr: 0.000150  loss: 1.0392 (1.0878)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [120/431]  eta: 0:05:46  lr: 0.000150  loss: 1.0392 (1.0909)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [130/431]  eta: 0:05:35  lr: 0.000150  loss: 1.0178 (1.0867)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [140/431]  eta: 0:05:23  lr: 0.000150  loss: 1.0178 (1.0848)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [150/431]  eta: 0:05:12  lr: 0.000150  loss: 1.0684 (1.0876)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [160/431]  eta: 0:05:00  lr: 0.000150  loss: 1.1417 (1.0934)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [170/431]  eta: 0:04:49  lr: 0.000150  loss: 1.1634 (1.0969)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [180/431]  eta: 0:04:38  lr: 0.000150  loss: 1.0838 (1.0946)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [190/431]  eta: 0:04:27  lr: 0.000150  loss: 1.0546 (1.0923)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [200/431]  eta: 0:04:16  lr: 0.000150  loss: 1.0582 (1.0913)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [210/431]  eta: 0:04:05  lr: 0.000150  loss: 1.0927 (1.0943)  time: 1.1079  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:324]  [220/431]  eta: 0:03:54  lr: 0.000150  loss: 1.0565 (1.0915)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [230/431]  eta: 0:03:42  lr: 0.000150  loss: 1.0147 (1.0908)  time: 1.1120  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [240/431]  eta: 0:03:31  lr: 0.000150  loss: 1.1476 (1.0944)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [250/431]  eta: 0:03:20  lr: 0.000150  loss: 1.1476 (1.0957)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [260/431]  eta: 0:03:09  lr: 0.000150  loss: 1.1307 (1.0983)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [270/431]  eta: 0:02:58  lr: 0.000150  loss: 1.0676 (1.0962)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [280/431]  eta: 0:02:47  lr: 0.000150  loss: 1.0465 (1.0977)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [290/431]  eta: 0:02:36  lr: 0.000150  loss: 1.0988 (1.0984)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [300/431]  eta: 0:02:25  lr: 0.000150  loss: 1.1104 (1.0995)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [310/431]  eta: 0:02:13  lr: 0.000150  loss: 1.0625 (1.0986)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [320/431]  eta: 0:02:02  lr: 0.000150  loss: 1.0471 (1.0970)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [330/431]  eta: 0:01:51  lr: 0.000150  loss: 1.0751 (1.0988)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [340/431]  eta: 0:01:40  lr: 0.000150  loss: 1.1403 (1.1025)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [350/431]  eta: 0:01:29  lr: 0.000150  loss: 1.1787 (1.1037)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [360/431]  eta: 0:01:18  lr: 0.000150  loss: 1.1573 (1.1049)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [370/431]  eta: 0:01:07  lr: 0.000150  loss: 1.0794 (1.1041)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [380/431]  eta: 0:00:56  lr: 0.000150  loss: 1.0402 (1.1036)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [390/431]  eta: 0:00:45  lr: 0.000150  loss: 1.0463 (1.1035)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [400/431]  eta: 0:00:34  lr: 0.000150  loss: 1.0532 (1.1024)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [410/431]  eta: 0:00:23  lr: 0.000150  loss: 1.0551 (1.1027)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:324]  [420/431]  eta: 0:00:12  lr: 0.000150  loss: 1.0842 (1.1023)  time: 1.0954  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:324]  [430/431]  eta: 0:00:01  lr: 0.000150  loss: 1.0578 (1.1013)  time: 1.0972  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:324] Total time: 0:07:56 (1.1063 s / it)\n",
      "Averaged stats: lr: 0.000150  loss: 1.0578 (1.1013)\n",
      "Valid: [epoch:324]  [ 0/14]  eta: 0:00:35  loss: 1.0937 (1.0937)  time: 2.5138  data: 2.3498  max mem: 15925\n",
      "Valid: [epoch:324]  [13/14]  eta: 0:00:00  loss: 1.0348 (1.0435)  time: 0.2746  data: 0.1679  max mem: 15925\n",
      "Valid: [epoch:324] Total time: 0:00:04 (0.2940 s / it)\n",
      "Averaged stats: loss: 1.0348 (1.0435)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_324_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:325]  [  0/431]  eta: 0:34:03  lr: 0.000150  loss: 1.2201 (1.2201)  time: 4.7409  data: 3.5364  max mem: 15925\n",
      "Train: [epoch:325]  [ 10/431]  eta: 0:09:40  lr: 0.000150  loss: 1.1839 (1.2013)  time: 1.3777  data: 0.3217  max mem: 15925\n",
      "Train: [epoch:325]  [ 20/431]  eta: 0:08:23  lr: 0.000150  loss: 1.1073 (1.1447)  time: 1.0487  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [ 30/431]  eta: 0:07:53  lr: 0.000150  loss: 1.0729 (1.1344)  time: 1.0719  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [ 40/431]  eta: 0:07:33  lr: 0.000150  loss: 1.0804 (1.1205)  time: 1.0895  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [ 50/431]  eta: 0:07:15  lr: 0.000150  loss: 1.0782 (1.1167)  time: 1.0857  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [ 60/431]  eta: 0:07:01  lr: 0.000150  loss: 1.0275 (1.0979)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [ 70/431]  eta: 0:06:47  lr: 0.000150  loss: 1.0140 (1.0994)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [ 80/431]  eta: 0:06:36  lr: 0.000150  loss: 1.0580 (1.1066)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [ 90/431]  eta: 0:06:24  lr: 0.000150  loss: 1.0479 (1.0992)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [100/431]  eta: 0:06:12  lr: 0.000150  loss: 1.0079 (1.0989)  time: 1.1127  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [110/431]  eta: 0:06:00  lr: 0.000150  loss: 1.0719 (1.0973)  time: 1.1152  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [120/431]  eta: 0:05:49  lr: 0.000150  loss: 1.0718 (1.0927)  time: 1.1131  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [130/431]  eta: 0:05:37  lr: 0.000150  loss: 1.0432 (1.0910)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [140/431]  eta: 0:05:26  lr: 0.000150  loss: 1.0872 (1.0947)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [150/431]  eta: 0:05:14  lr: 0.000150  loss: 1.1151 (1.0961)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [160/431]  eta: 0:05:03  lr: 0.000150  loss: 1.1151 (1.0993)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [170/431]  eta: 0:04:51  lr: 0.000150  loss: 1.0999 (1.0978)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [180/431]  eta: 0:04:40  lr: 0.000150  loss: 1.0497 (1.0964)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [190/431]  eta: 0:04:29  lr: 0.000150  loss: 1.0572 (1.0959)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [200/431]  eta: 0:04:17  lr: 0.000150  loss: 1.0901 (1.0955)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [210/431]  eta: 0:04:06  lr: 0.000150  loss: 1.1102 (1.0974)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [220/431]  eta: 0:03:55  lr: 0.000150  loss: 1.1205 (1.0980)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [230/431]  eta: 0:03:43  lr: 0.000150  loss: 1.1205 (1.0996)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [240/431]  eta: 0:03:32  lr: 0.000150  loss: 1.0935 (1.0982)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [250/431]  eta: 0:03:21  lr: 0.000150  loss: 1.0638 (1.0982)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [260/431]  eta: 0:03:10  lr: 0.000150  loss: 1.0700 (1.0990)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [270/431]  eta: 0:02:58  lr: 0.000150  loss: 1.1024 (1.1007)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [280/431]  eta: 0:02:47  lr: 0.000150  loss: 1.0864 (1.1013)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [290/431]  eta: 0:02:36  lr: 0.000150  loss: 1.0382 (1.1007)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [300/431]  eta: 0:02:25  lr: 0.000150  loss: 1.0537 (1.1007)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [310/431]  eta: 0:02:14  lr: 0.000150  loss: 1.0235 (1.0984)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [320/431]  eta: 0:02:03  lr: 0.000150  loss: 1.0235 (1.0990)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [330/431]  eta: 0:01:52  lr: 0.000150  loss: 1.1072 (1.1010)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [340/431]  eta: 0:01:40  lr: 0.000150  loss: 1.0746 (1.1001)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [350/431]  eta: 0:01:29  lr: 0.000150  loss: 1.0864 (1.1019)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [360/431]  eta: 0:01:18  lr: 0.000150  loss: 1.1150 (1.1032)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [370/431]  eta: 0:01:07  lr: 0.000150  loss: 1.0850 (1.1037)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [380/431]  eta: 0:00:56  lr: 0.000150  loss: 1.0843 (1.1038)  time: 1.1071  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:325]  [390/431]  eta: 0:00:45  lr: 0.000150  loss: 1.0843 (1.1034)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [400/431]  eta: 0:00:34  lr: 0.000150  loss: 1.0622 (1.1027)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [410/431]  eta: 0:00:23  lr: 0.000150  loss: 1.0454 (1.1019)  time: 1.1051  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:325]  [420/431]  eta: 0:00:12  lr: 0.000150  loss: 1.0454 (1.1022)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325]  [430/431]  eta: 0:00:01  lr: 0.000150  loss: 1.1185 (1.1028)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:325] Total time: 0:07:58 (1.1098 s / it)\n",
      "Averaged stats: lr: 0.000150  loss: 1.1185 (1.1028)\n",
      "Valid: [epoch:325]  [ 0/14]  eta: 0:00:34  loss: 1.0336 (1.0336)  time: 2.4902  data: 2.3394  max mem: 15925\n",
      "Valid: [epoch:325]  [13/14]  eta: 0:00:00  loss: 1.0336 (1.0440)  time: 0.2578  data: 0.1672  max mem: 15925\n",
      "Valid: [epoch:325] Total time: 0:00:03 (0.2718 s / it)\n",
      "Averaged stats: loss: 1.0336 (1.0440)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_325_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:326]  [  0/431]  eta: 0:32:23  lr: 0.000150  loss: 1.1049 (1.1049)  time: 4.5088  data: 3.3306  max mem: 15925\n",
      "Train: [epoch:326]  [ 10/431]  eta: 0:09:34  lr: 0.000150  loss: 1.1400 (1.1492)  time: 1.3648  data: 0.3030  max mem: 15925\n",
      "Train: [epoch:326]  [ 20/431]  eta: 0:08:22  lr: 0.000150  loss: 1.1231 (1.1610)  time: 1.0592  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [ 30/431]  eta: 0:07:51  lr: 0.000150  loss: 1.0904 (1.1290)  time: 1.0710  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [ 40/431]  eta: 0:07:30  lr: 0.000150  loss: 1.0363 (1.1133)  time: 1.0799  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [ 50/431]  eta: 0:07:14  lr: 0.000150  loss: 1.0410 (1.1106)  time: 1.0853  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [ 60/431]  eta: 0:06:59  lr: 0.000150  loss: 1.0146 (1.1012)  time: 1.0858  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [ 70/431]  eta: 0:06:47  lr: 0.000150  loss: 1.0272 (1.0988)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [ 80/431]  eta: 0:06:34  lr: 0.000150  loss: 1.0689 (1.1014)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [ 90/431]  eta: 0:06:22  lr: 0.000150  loss: 1.0225 (1.0920)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [100/431]  eta: 0:06:10  lr: 0.000150  loss: 0.9929 (1.0839)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [110/431]  eta: 0:05:58  lr: 0.000150  loss: 0.9996 (1.0807)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [120/431]  eta: 0:05:47  lr: 0.000150  loss: 1.0636 (1.0799)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [130/431]  eta: 0:05:35  lr: 0.000150  loss: 1.0912 (1.0833)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [140/431]  eta: 0:05:23  lr: 0.000150  loss: 1.0928 (1.0857)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [150/431]  eta: 0:05:12  lr: 0.000150  loss: 1.0482 (1.0852)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [160/431]  eta: 0:05:01  lr: 0.000150  loss: 1.0949 (1.0889)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [170/431]  eta: 0:04:49  lr: 0.000150  loss: 1.1263 (1.0911)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [180/431]  eta: 0:04:38  lr: 0.000150  loss: 1.0935 (1.0955)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [190/431]  eta: 0:04:27  lr: 0.000150  loss: 1.1274 (1.0968)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [200/431]  eta: 0:04:16  lr: 0.000150  loss: 1.0931 (1.0964)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [210/431]  eta: 0:04:05  lr: 0.000150  loss: 1.0604 (1.0971)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [220/431]  eta: 0:03:53  lr: 0.000150  loss: 1.0566 (1.0975)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [230/431]  eta: 0:03:42  lr: 0.000150  loss: 1.0513 (1.0955)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [240/431]  eta: 0:03:31  lr: 0.000150  loss: 1.1041 (1.0995)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [250/431]  eta: 0:03:20  lr: 0.000150  loss: 1.0828 (1.0974)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [260/431]  eta: 0:03:09  lr: 0.000150  loss: 1.0453 (1.0971)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [270/431]  eta: 0:02:58  lr: 0.000150  loss: 1.0617 (1.0979)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [280/431]  eta: 0:02:47  lr: 0.000150  loss: 1.0558 (1.0966)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [290/431]  eta: 0:02:36  lr: 0.000150  loss: 1.1059 (1.0993)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [300/431]  eta: 0:02:25  lr: 0.000150  loss: 1.1520 (1.1005)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [310/431]  eta: 0:02:13  lr: 0.000150  loss: 1.0785 (1.1006)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [320/431]  eta: 0:02:02  lr: 0.000150  loss: 1.0699 (1.1001)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [330/431]  eta: 0:01:51  lr: 0.000150  loss: 1.0699 (1.1001)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [340/431]  eta: 0:01:40  lr: 0.000150  loss: 1.0857 (1.1032)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [350/431]  eta: 0:01:29  lr: 0.000150  loss: 1.0865 (1.1034)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [360/431]  eta: 0:01:18  lr: 0.000150  loss: 1.0968 (1.1040)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [370/431]  eta: 0:01:07  lr: 0.000150  loss: 1.1033 (1.1032)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [380/431]  eta: 0:00:56  lr: 0.000150  loss: 1.0647 (1.1037)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [390/431]  eta: 0:00:45  lr: 0.000150  loss: 1.0515 (1.1034)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [400/431]  eta: 0:00:34  lr: 0.000150  loss: 1.1085 (1.1031)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [410/431]  eta: 0:00:23  lr: 0.000150  loss: 1.0682 (1.1018)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:326]  [420/431]  eta: 0:00:12  lr: 0.000150  loss: 1.0358 (1.1013)  time: 1.0974  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:326]  [430/431]  eta: 0:00:01  lr: 0.000150  loss: 1.0633 (1.1007)  time: 1.1058  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:326] Total time: 0:07:57 (1.1072 s / it)\n",
      "Averaged stats: lr: 0.000150  loss: 1.0633 (1.1007)\n",
      "Valid: [epoch:326]  [ 0/14]  eta: 0:00:34  loss: 1.0868 (1.0868)  time: 2.4827  data: 2.3389  max mem: 15925\n",
      "Valid: [epoch:326]  [13/14]  eta: 0:00:00  loss: 1.0427 (1.0485)  time: 0.2608  data: 0.1672  max mem: 15925\n",
      "Valid: [epoch:326] Total time: 0:00:03 (0.2802 s / it)\n",
      "Averaged stats: loss: 1.0427 (1.0485)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_326_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.048%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:327]  [  0/431]  eta: 0:32:58  lr: 0.000150  loss: 1.0589 (1.0589)  time: 4.5913  data: 3.4335  max mem: 15925\n",
      "Train: [epoch:327]  [ 10/431]  eta: 0:09:33  lr: 0.000150  loss: 1.1359 (1.1532)  time: 1.3618  data: 0.3123  max mem: 15925\n",
      "Train: [epoch:327]  [ 20/431]  eta: 0:08:21  lr: 0.000150  loss: 1.1359 (1.1507)  time: 1.0510  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [ 30/431]  eta: 0:07:51  lr: 0.000150  loss: 1.0770 (1.1329)  time: 1.0749  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [ 40/431]  eta: 0:07:30  lr: 0.000150  loss: 1.0772 (1.1261)  time: 1.0812  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [ 50/431]  eta: 0:07:14  lr: 0.000150  loss: 1.0944 (1.1203)  time: 1.0860  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [ 60/431]  eta: 0:07:00  lr: 0.000150  loss: 1.0596 (1.1084)  time: 1.0922  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:327]  [ 70/431]  eta: 0:06:47  lr: 0.000150  loss: 1.0202 (1.1003)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [ 80/431]  eta: 0:06:34  lr: 0.000150  loss: 1.1053 (1.1120)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [ 90/431]  eta: 0:06:22  lr: 0.000150  loss: 1.0896 (1.1084)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [100/431]  eta: 0:06:10  lr: 0.000150  loss: 1.0694 (1.1031)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [110/431]  eta: 0:05:59  lr: 0.000150  loss: 1.0270 (1.0972)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [120/431]  eta: 0:05:47  lr: 0.000150  loss: 1.0579 (1.0964)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [130/431]  eta: 0:05:36  lr: 0.000150  loss: 1.0565 (1.0929)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [140/431]  eta: 0:05:25  lr: 0.000150  loss: 1.0539 (1.0937)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [150/431]  eta: 0:05:13  lr: 0.000150  loss: 1.0678 (1.0942)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [160/431]  eta: 0:05:02  lr: 0.000150  loss: 1.0568 (1.0978)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [170/431]  eta: 0:04:50  lr: 0.000150  loss: 1.0409 (1.0937)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [180/431]  eta: 0:04:39  lr: 0.000150  loss: 1.0585 (1.0957)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [190/431]  eta: 0:04:28  lr: 0.000150  loss: 1.0935 (1.0959)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [200/431]  eta: 0:04:16  lr: 0.000150  loss: 1.0911 (1.0981)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [210/431]  eta: 0:04:05  lr: 0.000150  loss: 1.0723 (1.0964)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [220/431]  eta: 0:03:54  lr: 0.000150  loss: 1.0543 (1.0963)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [230/431]  eta: 0:03:43  lr: 0.000150  loss: 1.0355 (1.0960)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [240/431]  eta: 0:03:32  lr: 0.000150  loss: 1.0355 (1.0956)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [250/431]  eta: 0:03:20  lr: 0.000150  loss: 1.0431 (1.0942)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [260/431]  eta: 0:03:09  lr: 0.000150  loss: 1.1222 (1.0971)  time: 1.0925  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [270/431]  eta: 0:02:58  lr: 0.000150  loss: 1.0714 (1.0954)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [280/431]  eta: 0:02:47  lr: 0.000150  loss: 1.0426 (1.0957)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [290/431]  eta: 0:02:36  lr: 0.000150  loss: 1.0728 (1.0947)  time: 1.1154  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [300/431]  eta: 0:02:25  lr: 0.000150  loss: 1.0975 (1.0946)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [310/431]  eta: 0:02:14  lr: 0.000150  loss: 1.0386 (1.0935)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [320/431]  eta: 0:02:03  lr: 0.000150  loss: 1.0386 (1.0932)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [330/431]  eta: 0:01:51  lr: 0.000150  loss: 1.0859 (1.0956)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [340/431]  eta: 0:01:40  lr: 0.000150  loss: 1.1367 (1.0975)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [350/431]  eta: 0:01:29  lr: 0.000150  loss: 1.1367 (1.1009)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [360/431]  eta: 0:01:18  lr: 0.000150  loss: 1.0903 (1.1000)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [370/431]  eta: 0:01:07  lr: 0.000150  loss: 1.0903 (1.1004)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [380/431]  eta: 0:00:56  lr: 0.000150  loss: 1.0523 (1.1002)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [390/431]  eta: 0:00:45  lr: 0.000150  loss: 1.0385 (1.0996)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [400/431]  eta: 0:00:34  lr: 0.000150  loss: 1.0359 (1.0993)  time: 1.1108  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [410/431]  eta: 0:00:23  lr: 0.000150  loss: 1.0452 (1.0994)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:327]  [420/431]  eta: 0:00:12  lr: 0.000150  loss: 1.0575 (1.1005)  time: 1.0946  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:327]  [430/431]  eta: 0:00:01  lr: 0.000150  loss: 1.0911 (1.1007)  time: 1.0977  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:327] Total time: 0:07:57 (1.1078 s / it)\n",
      "Averaged stats: lr: 0.000150  loss: 1.0911 (1.1007)\n",
      "Valid: [epoch:327]  [ 0/14]  eta: 0:00:35  loss: 1.0034 (1.0034)  time: 2.5008  data: 2.3440  max mem: 15925\n",
      "Valid: [epoch:327]  [13/14]  eta: 0:00:00  loss: 1.0331 (1.0424)  time: 0.2622  data: 0.1675  max mem: 15925\n",
      "Valid: [epoch:327] Total time: 0:00:03 (0.2808 s / it)\n",
      "Averaged stats: loss: 1.0331 (1.0424)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_327_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:328]  [  0/431]  eta: 0:32:41  lr: 0.000150  loss: 1.2905 (1.2905)  time: 4.5513  data: 3.4039  max mem: 15925\n",
      "Train: [epoch:328]  [ 10/431]  eta: 0:09:33  lr: 0.000150  loss: 1.1910 (1.1773)  time: 1.3614  data: 0.3096  max mem: 15925\n",
      "Train: [epoch:328]  [ 20/431]  eta: 0:08:18  lr: 0.000150  loss: 1.1372 (1.1526)  time: 1.0454  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [ 30/431]  eta: 0:07:49  lr: 0.000150  loss: 1.0382 (1.1144)  time: 1.0653  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [ 40/431]  eta: 0:07:29  lr: 0.000150  loss: 1.0221 (1.1048)  time: 1.0856  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [ 50/431]  eta: 0:07:14  lr: 0.000150  loss: 1.1078 (1.1126)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [ 60/431]  eta: 0:07:00  lr: 0.000150  loss: 1.1233 (1.1086)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [ 70/431]  eta: 0:06:47  lr: 0.000150  loss: 1.0642 (1.1047)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [ 80/431]  eta: 0:06:35  lr: 0.000150  loss: 1.0925 (1.1139)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [ 90/431]  eta: 0:06:23  lr: 0.000150  loss: 1.0868 (1.1135)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [100/431]  eta: 0:06:11  lr: 0.000150  loss: 1.0483 (1.1122)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [110/431]  eta: 0:05:59  lr: 0.000150  loss: 1.0483 (1.1036)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [120/431]  eta: 0:05:48  lr: 0.000150  loss: 1.0616 (1.1105)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [130/431]  eta: 0:05:36  lr: 0.000150  loss: 1.0965 (1.1097)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [140/431]  eta: 0:05:25  lr: 0.000150  loss: 1.0965 (1.1119)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [150/431]  eta: 0:05:13  lr: 0.000150  loss: 1.0552 (1.1108)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [160/431]  eta: 0:05:02  lr: 0.000150  loss: 1.0552 (1.1089)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [170/431]  eta: 0:04:50  lr: 0.000150  loss: 1.0654 (1.1081)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [180/431]  eta: 0:04:39  lr: 0.000150  loss: 1.1036 (1.1065)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [190/431]  eta: 0:04:28  lr: 0.000150  loss: 1.0436 (1.1058)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [200/431]  eta: 0:04:16  lr: 0.000150  loss: 1.0344 (1.1030)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [210/431]  eta: 0:04:05  lr: 0.000150  loss: 1.0585 (1.1035)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [220/431]  eta: 0:03:54  lr: 0.000150  loss: 1.0714 (1.1050)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [230/431]  eta: 0:03:43  lr: 0.000150  loss: 1.0950 (1.1036)  time: 1.1053  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:328]  [240/431]  eta: 0:03:32  lr: 0.000150  loss: 1.0830 (1.1037)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [250/431]  eta: 0:03:21  lr: 0.000150  loss: 1.0787 (1.1031)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [260/431]  eta: 0:03:09  lr: 0.000150  loss: 1.0787 (1.1062)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [270/431]  eta: 0:02:58  lr: 0.000150  loss: 1.0530 (1.1048)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [280/431]  eta: 0:02:47  lr: 0.000150  loss: 1.0409 (1.1020)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [290/431]  eta: 0:02:36  lr: 0.000150  loss: 1.0125 (1.0993)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [300/431]  eta: 0:02:25  lr: 0.000150  loss: 1.0449 (1.1011)  time: 1.1146  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [310/431]  eta: 0:02:14  lr: 0.000150  loss: 1.1235 (1.1020)  time: 1.1162  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [320/431]  eta: 0:02:03  lr: 0.000150  loss: 1.0854 (1.1016)  time: 1.1139  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [330/431]  eta: 0:01:52  lr: 0.000150  loss: 1.0854 (1.1013)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [340/431]  eta: 0:01:40  lr: 0.000150  loss: 1.1103 (1.1033)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [350/431]  eta: 0:01:29  lr: 0.000150  loss: 1.1415 (1.1038)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [360/431]  eta: 0:01:18  lr: 0.000150  loss: 1.1090 (1.1047)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [370/431]  eta: 0:01:07  lr: 0.000150  loss: 1.0872 (1.1046)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [380/431]  eta: 0:00:56  lr: 0.000150  loss: 1.0809 (1.1039)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [390/431]  eta: 0:00:45  lr: 0.000150  loss: 1.0298 (1.1027)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [400/431]  eta: 0:00:34  lr: 0.000150  loss: 1.0098 (1.1017)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [410/431]  eta: 0:00:23  lr: 0.000150  loss: 1.0322 (1.1011)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:328]  [420/431]  eta: 0:00:12  lr: 0.000150  loss: 1.0718 (1.1021)  time: 1.0972  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:328]  [430/431]  eta: 0:00:01  lr: 0.000150  loss: 1.0920 (1.1022)  time: 1.0961  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:328] Total time: 0:07:57 (1.1077 s / it)\n",
      "Averaged stats: lr: 0.000150  loss: 1.0920 (1.1022)\n",
      "Valid: [epoch:328]  [ 0/14]  eta: 0:00:36  loss: 1.0916 (1.0916)  time: 2.5885  data: 2.4383  max mem: 15925\n",
      "Valid: [epoch:328]  [13/14]  eta: 0:00:00  loss: 1.0324 (1.0416)  time: 0.2718  data: 0.1743  max mem: 15925\n",
      "Valid: [epoch:328] Total time: 0:00:04 (0.2875 s / it)\n",
      "Averaged stats: loss: 1.0324 (1.0416)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_328_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:329]  [  0/431]  eta: 0:31:01  lr: 0.000149  loss: 1.0949 (1.0949)  time: 4.3190  data: 3.0813  max mem: 15925\n",
      "Train: [epoch:329]  [ 10/431]  eta: 0:09:21  lr: 0.000149  loss: 1.1216 (1.1231)  time: 1.3340  data: 0.2803  max mem: 15925\n",
      "Train: [epoch:329]  [ 20/431]  eta: 0:08:13  lr: 0.000149  loss: 1.1216 (1.1147)  time: 1.0461  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [ 30/431]  eta: 0:07:46  lr: 0.000149  loss: 1.0519 (1.0980)  time: 1.0710  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [ 40/431]  eta: 0:07:28  lr: 0.000149  loss: 1.0067 (1.0823)  time: 1.0881  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [ 50/431]  eta: 0:07:13  lr: 0.000149  loss: 1.0312 (1.0812)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [ 60/431]  eta: 0:06:59  lr: 0.000149  loss: 1.0544 (1.0768)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [ 70/431]  eta: 0:06:46  lr: 0.000149  loss: 1.0046 (1.0758)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [ 80/431]  eta: 0:06:34  lr: 0.000149  loss: 1.0601 (1.0844)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [ 90/431]  eta: 0:06:21  lr: 0.000149  loss: 1.0789 (1.0877)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [100/431]  eta: 0:06:10  lr: 0.000149  loss: 1.0481 (1.0849)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [110/431]  eta: 0:05:58  lr: 0.000149  loss: 1.0385 (1.0852)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [120/431]  eta: 0:05:46  lr: 0.000149  loss: 1.0592 (1.0870)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [130/431]  eta: 0:05:35  lr: 0.000149  loss: 1.0795 (1.0864)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [140/431]  eta: 0:05:23  lr: 0.000149  loss: 1.0755 (1.0870)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [150/431]  eta: 0:05:12  lr: 0.000149  loss: 1.0427 (1.0856)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [160/431]  eta: 0:05:00  lr: 0.000149  loss: 1.0476 (1.0851)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [170/431]  eta: 0:04:49  lr: 0.000149  loss: 1.0758 (1.0854)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [180/431]  eta: 0:04:38  lr: 0.000149  loss: 1.0508 (1.0866)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [190/431]  eta: 0:04:27  lr: 0.000149  loss: 1.0567 (1.0859)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [200/431]  eta: 0:04:15  lr: 0.000149  loss: 1.0724 (1.0893)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [210/431]  eta: 0:04:04  lr: 0.000149  loss: 1.1102 (1.0908)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [220/431]  eta: 0:03:53  lr: 0.000149  loss: 1.0844 (1.0935)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [230/431]  eta: 0:03:42  lr: 0.000149  loss: 1.1741 (1.0954)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [240/431]  eta: 0:03:31  lr: 0.000149  loss: 1.1090 (1.0961)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [250/431]  eta: 0:03:20  lr: 0.000149  loss: 1.0628 (1.0970)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [260/431]  eta: 0:03:09  lr: 0.000149  loss: 1.0577 (1.0965)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [270/431]  eta: 0:02:58  lr: 0.000149  loss: 1.0725 (1.0971)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [280/431]  eta: 0:02:47  lr: 0.000149  loss: 1.0865 (1.0985)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [290/431]  eta: 0:02:35  lr: 0.000149  loss: 1.0763 (1.0967)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [300/431]  eta: 0:02:24  lr: 0.000149  loss: 1.0834 (1.0998)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [310/431]  eta: 0:02:13  lr: 0.000149  loss: 1.0834 (1.1002)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [320/431]  eta: 0:02:02  lr: 0.000149  loss: 1.0683 (1.1024)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [330/431]  eta: 0:01:51  lr: 0.000149  loss: 1.1485 (1.1040)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [340/431]  eta: 0:01:40  lr: 0.000149  loss: 1.0933 (1.1040)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [350/431]  eta: 0:01:29  lr: 0.000149  loss: 1.0746 (1.1032)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [360/431]  eta: 0:01:18  lr: 0.000149  loss: 1.0359 (1.1027)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [370/431]  eta: 0:01:07  lr: 0.000149  loss: 1.0228 (1.1001)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [380/431]  eta: 0:00:56  lr: 0.000149  loss: 1.0378 (1.0997)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [390/431]  eta: 0:00:45  lr: 0.000149  loss: 1.0694 (1.0991)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [400/431]  eta: 0:00:34  lr: 0.000149  loss: 1.0699 (1.0998)  time: 1.0976  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:329]  [410/431]  eta: 0:00:23  lr: 0.000149  loss: 1.0942 (1.1006)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:329]  [420/431]  eta: 0:00:12  lr: 0.000149  loss: 1.0942 (1.1009)  time: 1.1063  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:329]  [430/431]  eta: 0:00:01  lr: 0.000149  loss: 1.0791 (1.1000)  time: 1.1012  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:329] Total time: 0:07:56 (1.1052 s / it)\n",
      "Averaged stats: lr: 0.000149  loss: 1.0791 (1.1000)\n",
      "Valid: [epoch:329]  [ 0/14]  eta: 0:00:34  loss: 1.0906 (1.0906)  time: 2.4864  data: 2.3459  max mem: 15925\n",
      "Valid: [epoch:329]  [13/14]  eta: 0:00:00  loss: 1.0358 (1.0428)  time: 0.2586  data: 0.1676  max mem: 15925\n",
      "Valid: [epoch:329] Total time: 0:00:03 (0.2740 s / it)\n",
      "Averaged stats: loss: 1.0358 (1.0428)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_329_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:330]  [  0/431]  eta: 0:33:21  lr: 0.000149  loss: 1.0841 (1.0841)  time: 4.6431  data: 3.4488  max mem: 15925\n",
      "Train: [epoch:330]  [ 10/431]  eta: 0:09:37  lr: 0.000149  loss: 1.0746 (1.0904)  time: 1.3713  data: 0.3137  max mem: 15925\n",
      "Train: [epoch:330]  [ 20/431]  eta: 0:08:20  lr: 0.000149  loss: 1.0501 (1.0653)  time: 1.0462  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [ 30/431]  eta: 0:07:49  lr: 0.000149  loss: 1.0016 (1.0629)  time: 1.0615  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [ 40/431]  eta: 0:07:30  lr: 0.000149  loss: 1.0043 (1.0739)  time: 1.0826  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [ 50/431]  eta: 0:07:13  lr: 0.000149  loss: 1.0517 (1.0707)  time: 1.0860  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [ 60/431]  eta: 0:07:00  lr: 0.000149  loss: 1.0438 (1.0676)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [ 70/431]  eta: 0:06:47  lr: 0.000149  loss: 1.0637 (1.0714)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [ 80/431]  eta: 0:06:35  lr: 0.000149  loss: 1.0829 (1.0786)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [ 90/431]  eta: 0:06:23  lr: 0.000149  loss: 1.0785 (1.0767)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [100/431]  eta: 0:06:10  lr: 0.000149  loss: 1.0529 (1.0784)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [110/431]  eta: 0:05:59  lr: 0.000149  loss: 1.0657 (1.0792)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [120/431]  eta: 0:05:47  lr: 0.000149  loss: 1.0594 (1.0800)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [130/431]  eta: 0:05:36  lr: 0.000149  loss: 1.0324 (1.0749)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [140/431]  eta: 0:05:24  lr: 0.000149  loss: 1.0612 (1.0785)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [150/431]  eta: 0:05:12  lr: 0.000149  loss: 1.0823 (1.0799)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [160/431]  eta: 0:05:01  lr: 0.000149  loss: 1.0722 (1.0804)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [170/431]  eta: 0:04:50  lr: 0.000149  loss: 1.0722 (1.0789)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [180/431]  eta: 0:04:38  lr: 0.000149  loss: 1.1017 (1.0848)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [190/431]  eta: 0:04:27  lr: 0.000149  loss: 1.1249 (1.0859)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [200/431]  eta: 0:04:16  lr: 0.000149  loss: 1.1028 (1.0892)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [210/431]  eta: 0:04:05  lr: 0.000149  loss: 1.1299 (1.0911)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [220/431]  eta: 0:03:54  lr: 0.000149  loss: 1.0708 (1.0908)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [230/431]  eta: 0:03:43  lr: 0.000149  loss: 1.1270 (1.0933)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [240/431]  eta: 0:03:31  lr: 0.000149  loss: 1.0410 (1.0917)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [250/431]  eta: 0:03:20  lr: 0.000149  loss: 1.0376 (1.0945)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [260/431]  eta: 0:03:09  lr: 0.000149  loss: 1.1154 (1.0946)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [270/431]  eta: 0:02:58  lr: 0.000149  loss: 1.1154 (1.0960)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [280/431]  eta: 0:02:47  lr: 0.000149  loss: 1.1225 (1.0973)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [290/431]  eta: 0:02:36  lr: 0.000149  loss: 1.0932 (1.0968)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [300/431]  eta: 0:02:25  lr: 0.000149  loss: 1.0979 (1.0986)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [310/431]  eta: 0:02:14  lr: 0.000149  loss: 1.1225 (1.0993)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [320/431]  eta: 0:02:03  lr: 0.000149  loss: 1.0569 (1.0983)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [330/431]  eta: 0:01:51  lr: 0.000149  loss: 1.0464 (1.0978)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [340/431]  eta: 0:01:40  lr: 0.000149  loss: 1.0919 (1.0995)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [350/431]  eta: 0:01:29  lr: 0.000149  loss: 1.0919 (1.0995)  time: 1.1042  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:330]  [360/431]  eta: 0:01:18  lr: 0.000149  loss: 1.0816 (1.0991)  time: 1.1050  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:330]  [370/431]  eta: 0:01:07  lr: 0.000149  loss: 1.0976 (1.0988)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [380/431]  eta: 0:00:56  lr: 0.000149  loss: 1.0788 (1.0981)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [390/431]  eta: 0:00:45  lr: 0.000149  loss: 1.0576 (1.0989)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [400/431]  eta: 0:00:34  lr: 0.000149  loss: 1.1257 (1.1000)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [410/431]  eta: 0:00:23  lr: 0.000149  loss: 1.1398 (1.1007)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:330]  [420/431]  eta: 0:00:12  lr: 0.000149  loss: 1.0528 (1.1006)  time: 1.1099  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:330]  [430/431]  eta: 0:00:01  lr: 0.000149  loss: 1.0305 (1.1003)  time: 1.1022  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:330] Total time: 0:07:57 (1.1077 s / it)\n",
      "Averaged stats: lr: 0.000149  loss: 1.0305 (1.1003)\n",
      "Valid: [epoch:330]  [ 0/14]  eta: 0:00:35  loss: 1.0333 (1.0333)  time: 2.5081  data: 2.3515  max mem: 15925\n",
      "Valid: [epoch:330]  [13/14]  eta: 0:00:00  loss: 1.0430 (1.0491)  time: 0.2630  data: 0.1681  max mem: 15925\n",
      "Valid: [epoch:330] Total time: 0:00:03 (0.2809 s / it)\n",
      "Averaged stats: loss: 1.0430 (1.0491)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_330_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:331]  [  0/431]  eta: 0:33:04  lr: 0.000149  loss: 1.2041 (1.2041)  time: 4.6034  data: 3.4372  max mem: 15925\n",
      "Train: [epoch:331]  [ 10/431]  eta: 0:09:38  lr: 0.000149  loss: 1.0483 (1.1121)  time: 1.3744  data: 0.3127  max mem: 15925\n",
      "Train: [epoch:331]  [ 20/431]  eta: 0:08:25  lr: 0.000149  loss: 1.0754 (1.1255)  time: 1.0618  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [ 30/431]  eta: 0:07:53  lr: 0.000149  loss: 1.0892 (1.1105)  time: 1.0748  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [ 40/431]  eta: 0:07:31  lr: 0.000149  loss: 1.0867 (1.1008)  time: 1.0764  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [ 50/431]  eta: 0:07:16  lr: 0.000149  loss: 1.0690 (1.0935)  time: 1.0890  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [ 60/431]  eta: 0:07:02  lr: 0.000149  loss: 1.0673 (1.0956)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [ 70/431]  eta: 0:06:49  lr: 0.000149  loss: 1.0917 (1.1016)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [ 80/431]  eta: 0:06:36  lr: 0.000149  loss: 1.0789 (1.0995)  time: 1.1034  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:331]  [ 90/431]  eta: 0:06:24  lr: 0.000149  loss: 1.0509 (1.0985)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [100/431]  eta: 0:06:12  lr: 0.000149  loss: 1.0675 (1.1004)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [110/431]  eta: 0:06:00  lr: 0.000149  loss: 1.1028 (1.1032)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [120/431]  eta: 0:05:48  lr: 0.000149  loss: 1.0556 (1.0984)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [130/431]  eta: 0:05:36  lr: 0.000149  loss: 1.0022 (1.0930)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [140/431]  eta: 0:05:25  lr: 0.000149  loss: 1.0827 (1.0949)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [150/431]  eta: 0:05:13  lr: 0.000149  loss: 1.1153 (1.0955)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [160/431]  eta: 0:05:02  lr: 0.000149  loss: 1.0410 (1.0929)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [170/431]  eta: 0:04:51  lr: 0.000149  loss: 1.0652 (1.0931)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [180/431]  eta: 0:04:40  lr: 0.000149  loss: 1.0652 (1.0922)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [190/431]  eta: 0:04:28  lr: 0.000149  loss: 1.0531 (1.0908)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [200/431]  eta: 0:04:17  lr: 0.000149  loss: 1.0732 (1.0904)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [210/431]  eta: 0:04:06  lr: 0.000149  loss: 1.0732 (1.0885)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [220/431]  eta: 0:03:55  lr: 0.000149  loss: 1.0790 (1.0900)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [230/431]  eta: 0:03:43  lr: 0.000149  loss: 1.1002 (1.0919)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [240/431]  eta: 0:03:32  lr: 0.000149  loss: 1.1009 (1.0929)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [250/431]  eta: 0:03:21  lr: 0.000149  loss: 1.0748 (1.0925)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [260/431]  eta: 0:03:10  lr: 0.000149  loss: 1.0589 (1.0931)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [270/431]  eta: 0:02:59  lr: 0.000149  loss: 1.0754 (1.0939)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [280/431]  eta: 0:02:47  lr: 0.000149  loss: 1.0510 (1.0925)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [290/431]  eta: 0:02:36  lr: 0.000149  loss: 1.0291 (1.0902)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [300/431]  eta: 0:02:25  lr: 0.000149  loss: 1.0873 (1.0916)  time: 1.0842  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [310/431]  eta: 0:02:14  lr: 0.000149  loss: 1.1660 (1.0946)  time: 1.0948  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:331]  [320/431]  eta: 0:02:03  lr: 0.000149  loss: 1.1329 (1.0947)  time: 1.1059  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:331]  [330/431]  eta: 0:01:52  lr: 0.000149  loss: 1.1267 (1.0967)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [340/431]  eta: 0:01:40  lr: 0.000149  loss: 1.1894 (1.0988)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [350/431]  eta: 0:01:29  lr: 0.000149  loss: 1.1620 (1.0997)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [360/431]  eta: 0:01:18  lr: 0.000149  loss: 1.1358 (1.1011)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [370/431]  eta: 0:01:07  lr: 0.000149  loss: 1.1052 (1.0993)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [380/431]  eta: 0:00:56  lr: 0.000149  loss: 1.1025 (1.1007)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [390/431]  eta: 0:00:45  lr: 0.000149  loss: 1.0837 (1.1000)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [400/431]  eta: 0:00:34  lr: 0.000149  loss: 1.0896 (1.1013)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [410/431]  eta: 0:00:23  lr: 0.000149  loss: 1.1431 (1.1027)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:331]  [420/431]  eta: 0:00:12  lr: 0.000149  loss: 1.1108 (1.1026)  time: 1.0991  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:331]  [430/431]  eta: 0:00:01  lr: 0.000149  loss: 1.0545 (1.1017)  time: 1.1048  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:331] Total time: 0:07:58 (1.1094 s / it)\n",
      "Averaged stats: lr: 0.000149  loss: 1.0545 (1.1017)\n",
      "Valid: [epoch:331]  [ 0/14]  eta: 0:00:34  loss: 1.0838 (1.0838)  time: 2.4432  data: 2.2929  max mem: 15925\n",
      "Valid: [epoch:331]  [13/14]  eta: 0:00:00  loss: 1.0342 (1.0422)  time: 0.2770  data: 0.1639  max mem: 15925\n",
      "Valid: [epoch:331] Total time: 0:00:04 (0.2929 s / it)\n",
      "Averaged stats: loss: 1.0342 (1.0422)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_331_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:332]  [  0/431]  eta: 0:31:43  lr: 0.000149  loss: 1.1567 (1.1567)  time: 4.4168  data: 3.2155  max mem: 15925\n",
      "Train: [epoch:332]  [ 10/431]  eta: 0:09:20  lr: 0.000149  loss: 1.0643 (1.0975)  time: 1.3317  data: 0.2926  max mem: 15925\n",
      "Train: [epoch:332]  [ 20/431]  eta: 0:08:11  lr: 0.000149  loss: 1.0643 (1.1091)  time: 1.0360  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:332]  [ 30/431]  eta: 0:07:43  lr: 0.000149  loss: 1.0588 (1.1022)  time: 1.0597  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [ 40/431]  eta: 0:07:25  lr: 0.000149  loss: 1.0588 (1.0965)  time: 1.0793  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [ 50/431]  eta: 0:07:10  lr: 0.000149  loss: 1.0722 (1.0941)  time: 1.0876  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [ 60/431]  eta: 0:06:57  lr: 0.000149  loss: 1.0071 (1.0774)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [ 70/431]  eta: 0:06:45  lr: 0.000149  loss: 1.0378 (1.0863)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [ 80/431]  eta: 0:06:33  lr: 0.000149  loss: 1.0838 (1.0816)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [ 90/431]  eta: 0:06:21  lr: 0.000149  loss: 1.0119 (1.0845)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [100/431]  eta: 0:06:09  lr: 0.000149  loss: 1.0569 (1.0807)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [110/431]  eta: 0:05:57  lr: 0.000149  loss: 1.0618 (1.0816)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [120/431]  eta: 0:05:46  lr: 0.000149  loss: 1.0752 (1.0831)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [130/431]  eta: 0:05:35  lr: 0.000149  loss: 1.1038 (1.0838)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [140/431]  eta: 0:05:24  lr: 0.000149  loss: 1.0989 (1.0865)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [150/431]  eta: 0:05:12  lr: 0.000149  loss: 1.0519 (1.0833)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [160/431]  eta: 0:05:01  lr: 0.000149  loss: 1.0712 (1.0909)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [170/431]  eta: 0:04:50  lr: 0.000149  loss: 1.0773 (1.0896)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [180/431]  eta: 0:04:39  lr: 0.000149  loss: 1.0773 (1.0919)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [190/431]  eta: 0:04:28  lr: 0.000149  loss: 1.1285 (1.0930)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [200/431]  eta: 0:04:17  lr: 0.000149  loss: 1.1285 (1.0962)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [210/431]  eta: 0:04:05  lr: 0.000149  loss: 1.1282 (1.0979)  time: 1.1170  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [220/431]  eta: 0:03:54  lr: 0.000149  loss: 1.0561 (1.0968)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [230/431]  eta: 0:03:43  lr: 0.000149  loss: 1.0544 (1.0963)  time: 1.1043  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:332]  [240/431]  eta: 0:03:32  lr: 0.000149  loss: 1.0685 (1.0967)  time: 1.0981  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:332]  [250/431]  eta: 0:03:20  lr: 0.000149  loss: 1.0685 (1.0968)  time: 1.0917  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:332]  [260/431]  eta: 0:03:09  lr: 0.000149  loss: 1.0602 (1.0978)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [270/431]  eta: 0:02:58  lr: 0.000149  loss: 1.0618 (1.0988)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [280/431]  eta: 0:02:47  lr: 0.000149  loss: 1.0849 (1.0980)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [290/431]  eta: 0:02:36  lr: 0.000149  loss: 1.0799 (1.0975)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [300/431]  eta: 0:02:25  lr: 0.000149  loss: 1.0654 (1.0988)  time: 1.0917  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [310/431]  eta: 0:02:14  lr: 0.000149  loss: 1.0413 (1.0981)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [320/431]  eta: 0:02:02  lr: 0.000149  loss: 1.0199 (1.0979)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [330/431]  eta: 0:01:51  lr: 0.000149  loss: 1.0866 (1.0981)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [340/431]  eta: 0:01:40  lr: 0.000149  loss: 1.1044 (1.1000)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [350/431]  eta: 0:01:29  lr: 0.000149  loss: 1.1044 (1.1004)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [360/431]  eta: 0:01:18  lr: 0.000149  loss: 1.0830 (1.0997)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [370/431]  eta: 0:01:07  lr: 0.000149  loss: 1.0421 (1.1002)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [380/431]  eta: 0:00:56  lr: 0.000149  loss: 1.0421 (1.0997)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [390/431]  eta: 0:00:45  lr: 0.000149  loss: 1.0523 (1.0994)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [400/431]  eta: 0:00:34  lr: 0.000149  loss: 1.1032 (1.1005)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [410/431]  eta: 0:00:23  lr: 0.000149  loss: 1.0822 (1.0997)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:332]  [420/431]  eta: 0:00:12  lr: 0.000149  loss: 1.0822 (1.1004)  time: 1.0991  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:332]  [430/431]  eta: 0:00:01  lr: 0.000149  loss: 1.0977 (1.0999)  time: 1.1014  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:332] Total time: 0:07:57 (1.1070 s / it)\n",
      "Averaged stats: lr: 0.000149  loss: 1.0977 (1.0999)\n",
      "Valid: [epoch:332]  [ 0/14]  eta: 0:00:34  loss: 1.0187 (1.0187)  time: 2.4368  data: 2.2746  max mem: 15925\n",
      "Valid: [epoch:332]  [13/14]  eta: 0:00:00  loss: 1.0408 (1.0500)  time: 0.2609  data: 0.1626  max mem: 15925\n",
      "Valid: [epoch:332] Total time: 0:00:03 (0.2773 s / it)\n",
      "Averaged stats: loss: 1.0408 (1.0500)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_332_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.050%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:333]  [  0/431]  eta: 0:33:49  lr: 0.000148  loss: 1.2460 (1.2460)  time: 4.7086  data: 3.3883  max mem: 15925\n",
      "Train: [epoch:333]  [ 10/431]  eta: 0:09:37  lr: 0.000148  loss: 1.1317 (1.1265)  time: 1.3715  data: 0.3082  max mem: 15925\n",
      "Train: [epoch:333]  [ 20/431]  eta: 0:08:24  lr: 0.000148  loss: 1.0553 (1.1029)  time: 1.0531  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [ 30/431]  eta: 0:07:52  lr: 0.000148  loss: 1.0594 (1.1062)  time: 1.0723  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [ 40/431]  eta: 0:07:31  lr: 0.000148  loss: 1.0721 (1.1137)  time: 1.0766  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [ 50/431]  eta: 0:07:15  lr: 0.000148  loss: 1.0920 (1.1070)  time: 1.0881  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [ 60/431]  eta: 0:07:00  lr: 0.000148  loss: 1.0744 (1.0993)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [ 70/431]  eta: 0:06:48  lr: 0.000148  loss: 1.1113 (1.1110)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [ 80/431]  eta: 0:06:35  lr: 0.000148  loss: 1.1361 (1.1103)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [ 90/431]  eta: 0:06:22  lr: 0.000148  loss: 1.0571 (1.1048)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [100/431]  eta: 0:06:10  lr: 0.000148  loss: 1.0368 (1.1020)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [110/431]  eta: 0:05:59  lr: 0.000148  loss: 1.0539 (1.1011)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [120/431]  eta: 0:05:47  lr: 0.000148  loss: 1.0539 (1.0976)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [130/431]  eta: 0:05:36  lr: 0.000148  loss: 1.0391 (1.0949)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [140/431]  eta: 0:05:24  lr: 0.000148  loss: 1.0445 (1.0947)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [150/431]  eta: 0:05:13  lr: 0.000148  loss: 1.0608 (1.0942)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [160/431]  eta: 0:05:02  lr: 0.000148  loss: 1.0448 (1.0937)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [170/431]  eta: 0:04:50  lr: 0.000148  loss: 1.0730 (1.0935)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [180/431]  eta: 0:04:39  lr: 0.000148  loss: 1.0825 (1.0968)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [190/431]  eta: 0:04:28  lr: 0.000148  loss: 1.1445 (1.1020)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [200/431]  eta: 0:04:16  lr: 0.000148  loss: 1.1011 (1.1018)  time: 1.0896  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [210/431]  eta: 0:04:05  lr: 0.000148  loss: 1.0652 (1.1019)  time: 1.0919  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [220/431]  eta: 0:03:54  lr: 0.000148  loss: 1.0652 (1.1014)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [230/431]  eta: 0:03:43  lr: 0.000148  loss: 1.0627 (1.0999)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [240/431]  eta: 0:03:31  lr: 0.000148  loss: 1.0784 (1.1016)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [250/431]  eta: 0:03:20  lr: 0.000148  loss: 1.0791 (1.1006)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [260/431]  eta: 0:03:09  lr: 0.000148  loss: 1.0845 (1.1019)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [270/431]  eta: 0:02:58  lr: 0.000148  loss: 1.0888 (1.1017)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [280/431]  eta: 0:02:47  lr: 0.000148  loss: 1.0984 (1.1009)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [290/431]  eta: 0:02:36  lr: 0.000148  loss: 1.0312 (1.0997)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [300/431]  eta: 0:02:25  lr: 0.000148  loss: 1.0824 (1.1006)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [310/431]  eta: 0:02:14  lr: 0.000148  loss: 1.1008 (1.1006)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [320/431]  eta: 0:02:02  lr: 0.000148  loss: 1.0469 (1.0995)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [330/431]  eta: 0:01:51  lr: 0.000148  loss: 1.1162 (1.1005)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [340/431]  eta: 0:01:40  lr: 0.000148  loss: 1.1274 (1.1009)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [350/431]  eta: 0:01:29  lr: 0.000148  loss: 1.1274 (1.1026)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [360/431]  eta: 0:01:18  lr: 0.000148  loss: 1.0955 (1.1030)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [370/431]  eta: 0:01:07  lr: 0.000148  loss: 1.0438 (1.1026)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [380/431]  eta: 0:00:56  lr: 0.000148  loss: 1.1074 (1.1033)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [390/431]  eta: 0:00:45  lr: 0.000148  loss: 1.1031 (1.1027)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [400/431]  eta: 0:00:34  lr: 0.000148  loss: 1.0317 (1.1013)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [410/431]  eta: 0:00:23  lr: 0.000148  loss: 1.0267 (1.1010)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:333]  [420/431]  eta: 0:00:12  lr: 0.000148  loss: 1.0510 (1.0999)  time: 1.0934  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:333]  [430/431]  eta: 0:00:01  lr: 0.000148  loss: 1.0639 (1.1010)  time: 1.1036  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:333] Total time: 0:07:56 (1.1061 s / it)\n",
      "Averaged stats: lr: 0.000148  loss: 1.0639 (1.1010)\n",
      "Valid: [epoch:333]  [ 0/14]  eta: 0:00:34  loss: 1.0888 (1.0888)  time: 2.4867  data: 2.3295  max mem: 15925\n",
      "Valid: [epoch:333]  [13/14]  eta: 0:00:00  loss: 1.0369 (1.0456)  time: 0.2740  data: 0.1665  max mem: 15925\n",
      "Valid: [epoch:333] Total time: 0:00:04 (0.2889 s / it)\n",
      "Averaged stats: loss: 1.0369 (1.0456)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_333_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:334]  [  0/431]  eta: 0:32:43  lr: 0.000148  loss: 1.1218 (1.1218)  time: 4.5550  data: 3.4156  max mem: 15925\n",
      "Train: [epoch:334]  [ 10/431]  eta: 0:09:29  lr: 0.000148  loss: 1.1041 (1.0947)  time: 1.3537  data: 0.3107  max mem: 15925\n",
      "Train: [epoch:334]  [ 20/431]  eta: 0:08:15  lr: 0.000148  loss: 1.0675 (1.0825)  time: 1.0378  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [ 30/431]  eta: 0:07:44  lr: 0.000148  loss: 1.0620 (1.1028)  time: 1.0524  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [ 40/431]  eta: 0:07:25  lr: 0.000148  loss: 1.0620 (1.0915)  time: 1.0712  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [ 50/431]  eta: 0:07:11  lr: 0.000148  loss: 1.0379 (1.0853)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [ 60/431]  eta: 0:06:58  lr: 0.000148  loss: 1.0434 (1.0839)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [ 70/431]  eta: 0:06:47  lr: 0.000148  loss: 1.0995 (1.0901)  time: 1.1182  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [ 80/431]  eta: 0:06:36  lr: 0.000148  loss: 1.1128 (1.0948)  time: 1.1256  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [ 90/431]  eta: 0:06:24  lr: 0.000148  loss: 1.0510 (1.0920)  time: 1.1203  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [100/431]  eta: 0:06:13  lr: 0.000148  loss: 1.0354 (1.0887)  time: 1.1233  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [110/431]  eta: 0:06:01  lr: 0.000148  loss: 1.1049 (1.0930)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [120/431]  eta: 0:05:49  lr: 0.000148  loss: 1.0853 (1.0914)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [130/431]  eta: 0:05:38  lr: 0.000148  loss: 1.0517 (1.0920)  time: 1.1138  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [140/431]  eta: 0:05:26  lr: 0.000148  loss: 1.0410 (1.0899)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [150/431]  eta: 0:05:15  lr: 0.000148  loss: 1.0410 (1.0930)  time: 1.1218  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [160/431]  eta: 0:05:04  lr: 0.000148  loss: 1.1112 (1.0936)  time: 1.1147  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [170/431]  eta: 0:04:52  lr: 0.000148  loss: 1.0900 (1.0945)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [180/431]  eta: 0:04:41  lr: 0.000148  loss: 1.0820 (1.0954)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [190/431]  eta: 0:04:29  lr: 0.000148  loss: 1.0815 (1.0955)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [200/431]  eta: 0:04:18  lr: 0.000148  loss: 1.1063 (1.0979)  time: 1.1142  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [210/431]  eta: 0:04:07  lr: 0.000148  loss: 1.1087 (1.1005)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [220/431]  eta: 0:03:56  lr: 0.000148  loss: 1.1225 (1.1022)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [230/431]  eta: 0:03:44  lr: 0.000148  loss: 1.1320 (1.1034)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [240/431]  eta: 0:03:33  lr: 0.000148  loss: 1.0485 (1.1017)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [250/431]  eta: 0:03:22  lr: 0.000148  loss: 1.0239 (1.1003)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [260/431]  eta: 0:03:10  lr: 0.000148  loss: 1.1422 (1.1039)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [270/431]  eta: 0:02:59  lr: 0.000148  loss: 1.1219 (1.1034)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [280/431]  eta: 0:02:48  lr: 0.000148  loss: 1.0379 (1.1012)  time: 1.1070  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:334]  [290/431]  eta: 0:02:37  lr: 0.000148  loss: 1.0457 (1.1018)  time: 1.1103  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:334]  [300/431]  eta: 0:02:26  lr: 0.000148  loss: 1.0872 (1.1023)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [310/431]  eta: 0:02:15  lr: 0.000148  loss: 1.0773 (1.1004)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [320/431]  eta: 0:02:03  lr: 0.000148  loss: 1.0773 (1.1015)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [330/431]  eta: 0:01:52  lr: 0.000148  loss: 1.0754 (1.1023)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [340/431]  eta: 0:01:41  lr: 0.000148  loss: 1.0840 (1.1023)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [350/431]  eta: 0:01:30  lr: 0.000148  loss: 1.1001 (1.1019)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [360/431]  eta: 0:01:19  lr: 0.000148  loss: 1.0619 (1.1011)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [370/431]  eta: 0:01:07  lr: 0.000148  loss: 1.0497 (1.1004)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [380/431]  eta: 0:00:56  lr: 0.000148  loss: 1.0544 (1.0995)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [390/431]  eta: 0:00:45  lr: 0.000148  loss: 1.0406 (1.0980)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [400/431]  eta: 0:00:34  lr: 0.000148  loss: 1.0406 (1.0983)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [410/431]  eta: 0:00:23  lr: 0.000148  loss: 1.0938 (1.0994)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:334]  [420/431]  eta: 0:00:12  lr: 0.000148  loss: 1.1141 (1.0993)  time: 1.1030  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:334]  [430/431]  eta: 0:00:01  lr: 0.000148  loss: 1.1510 (1.1008)  time: 1.1082  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:334] Total time: 0:07:59 (1.1131 s / it)\n",
      "Averaged stats: lr: 0.000148  loss: 1.1510 (1.1008)\n",
      "Valid: [epoch:334]  [ 0/14]  eta: 0:00:35  loss: 0.9783 (0.9783)  time: 2.5340  data: 2.3411  max mem: 15925\n",
      "Valid: [epoch:334]  [13/14]  eta: 0:00:00  loss: 1.0337 (1.0430)  time: 0.2814  data: 0.1673  max mem: 15925\n",
      "Valid: [epoch:334] Total time: 0:00:04 (0.2972 s / it)\n",
      "Averaged stats: loss: 1.0337 (1.0430)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_334_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:335]  [  0/431]  eta: 0:35:22  lr: 0.000148  loss: 1.0990 (1.0990)  time: 4.9242  data: 3.8308  max mem: 15925\n",
      "Train: [epoch:335]  [ 10/431]  eta: 0:09:47  lr: 0.000148  loss: 1.0990 (1.1185)  time: 1.3950  data: 0.3484  max mem: 15925\n",
      "Train: [epoch:335]  [ 20/431]  eta: 0:08:24  lr: 0.000148  loss: 1.1042 (1.1356)  time: 1.0432  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [ 30/431]  eta: 0:07:53  lr: 0.000148  loss: 1.1042 (1.1210)  time: 1.0635  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [ 40/431]  eta: 0:07:32  lr: 0.000148  loss: 1.0909 (1.1189)  time: 1.0827  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [ 50/431]  eta: 0:07:17  lr: 0.000148  loss: 1.0522 (1.1081)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [ 60/431]  eta: 0:07:03  lr: 0.000148  loss: 1.0578 (1.1072)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [ 70/431]  eta: 0:06:50  lr: 0.000148  loss: 1.0444 (1.1009)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [ 80/431]  eta: 0:06:37  lr: 0.000148  loss: 1.0569 (1.1039)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [ 90/431]  eta: 0:06:25  lr: 0.000148  loss: 1.0970 (1.1056)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [100/431]  eta: 0:06:13  lr: 0.000148  loss: 1.1052 (1.1045)  time: 1.1123  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:335]  [110/431]  eta: 0:06:01  lr: 0.000148  loss: 1.1086 (1.1061)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [120/431]  eta: 0:05:49  lr: 0.000148  loss: 1.0681 (1.1042)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [130/431]  eta: 0:05:37  lr: 0.000148  loss: 1.0681 (1.1028)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [140/431]  eta: 0:05:26  lr: 0.000148  loss: 1.0731 (1.1014)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [150/431]  eta: 0:05:14  lr: 0.000148  loss: 1.0825 (1.1036)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [160/431]  eta: 0:05:03  lr: 0.000148  loss: 1.1007 (1.1050)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [170/431]  eta: 0:04:51  lr: 0.000148  loss: 1.0512 (1.0997)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [180/431]  eta: 0:04:40  lr: 0.000148  loss: 1.0317 (1.1027)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [190/431]  eta: 0:04:29  lr: 0.000148  loss: 1.1158 (1.1042)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [200/431]  eta: 0:04:17  lr: 0.000148  loss: 1.1027 (1.1049)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [210/431]  eta: 0:04:06  lr: 0.000148  loss: 1.1231 (1.1050)  time: 1.1174  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [220/431]  eta: 0:03:55  lr: 0.000148  loss: 1.0536 (1.1026)  time: 1.1140  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [230/431]  eta: 0:03:44  lr: 0.000148  loss: 1.0920 (1.1066)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [240/431]  eta: 0:03:32  lr: 0.000148  loss: 1.1415 (1.1073)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [250/431]  eta: 0:03:21  lr: 0.000148  loss: 1.0947 (1.1062)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [260/431]  eta: 0:03:10  lr: 0.000148  loss: 1.0645 (1.1056)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [270/431]  eta: 0:02:59  lr: 0.000148  loss: 1.0949 (1.1063)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [280/431]  eta: 0:02:48  lr: 0.000148  loss: 1.0684 (1.1045)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [290/431]  eta: 0:02:37  lr: 0.000148  loss: 1.0582 (1.1042)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [300/431]  eta: 0:02:25  lr: 0.000148  loss: 1.0906 (1.1055)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [310/431]  eta: 0:02:14  lr: 0.000148  loss: 1.0710 (1.1045)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [320/431]  eta: 0:02:03  lr: 0.000148  loss: 1.0863 (1.1057)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [330/431]  eta: 0:01:52  lr: 0.000148  loss: 1.1262 (1.1065)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [340/431]  eta: 0:01:41  lr: 0.000148  loss: 1.1036 (1.1057)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [350/431]  eta: 0:01:30  lr: 0.000148  loss: 1.0873 (1.1054)  time: 1.1070  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:335]  [360/431]  eta: 0:01:18  lr: 0.000148  loss: 1.1063 (1.1056)  time: 1.1069  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:335]  [370/431]  eta: 0:01:07  lr: 0.000148  loss: 1.1063 (1.1058)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [380/431]  eta: 0:00:56  lr: 0.000148  loss: 1.0568 (1.1046)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [390/431]  eta: 0:00:45  lr: 0.000148  loss: 1.0531 (1.1039)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [400/431]  eta: 0:00:34  lr: 0.000148  loss: 1.0529 (1.1028)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [410/431]  eta: 0:00:23  lr: 0.000148  loss: 1.0356 (1.1019)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:335]  [420/431]  eta: 0:00:12  lr: 0.000148  loss: 1.0846 (1.1027)  time: 1.0981  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:335]  [430/431]  eta: 0:00:01  lr: 0.000148  loss: 1.0916 (1.1022)  time: 1.1042  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:335] Total time: 0:07:58 (1.1109 s / it)\n",
      "Averaged stats: lr: 0.000148  loss: 1.0916 (1.1022)\n",
      "Valid: [epoch:335]  [ 0/14]  eta: 0:00:34  loss: 0.9423 (0.9423)  time: 2.4877  data: 2.3522  max mem: 15925\n",
      "Valid: [epoch:335]  [13/14]  eta: 0:00:00  loss: 1.0472 (1.0520)  time: 0.2594  data: 0.1681  max mem: 15925\n",
      "Valid: [epoch:335] Total time: 0:00:03 (0.2762 s / it)\n",
      "Averaged stats: loss: 1.0472 (1.0520)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_335_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.052%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:336]  [  0/431]  eta: 0:35:27  lr: 0.000148  loss: 1.2549 (1.2549)  time: 4.9367  data: 3.8430  max mem: 15925\n",
      "Train: [epoch:336]  [ 10/431]  eta: 0:09:45  lr: 0.000148  loss: 1.1774 (1.1571)  time: 1.3906  data: 0.3495  max mem: 15925\n",
      "Train: [epoch:336]  [ 20/431]  eta: 0:08:25  lr: 0.000148  loss: 1.1110 (1.1180)  time: 1.0450  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [ 30/431]  eta: 0:07:51  lr: 0.000148  loss: 1.0659 (1.1009)  time: 1.0569  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [ 40/431]  eta: 0:07:31  lr: 0.000148  loss: 1.0637 (1.0999)  time: 1.0760  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [ 50/431]  eta: 0:07:15  lr: 0.000148  loss: 1.0994 (1.1017)  time: 1.0912  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [ 60/431]  eta: 0:07:01  lr: 0.000148  loss: 1.0524 (1.0926)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [ 70/431]  eta: 0:06:48  lr: 0.000148  loss: 1.0516 (1.0919)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [ 80/431]  eta: 0:06:36  lr: 0.000148  loss: 1.0994 (1.0981)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [ 90/431]  eta: 0:06:24  lr: 0.000148  loss: 1.0907 (1.1009)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [100/431]  eta: 0:06:12  lr: 0.000148  loss: 1.0794 (1.0996)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [110/431]  eta: 0:06:00  lr: 0.000148  loss: 1.0789 (1.1048)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [120/431]  eta: 0:05:49  lr: 0.000148  loss: 1.0724 (1.1013)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [130/431]  eta: 0:05:37  lr: 0.000148  loss: 1.0168 (1.0978)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [140/431]  eta: 0:05:26  lr: 0.000148  loss: 1.0470 (1.0961)  time: 1.1158  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [150/431]  eta: 0:05:14  lr: 0.000148  loss: 1.0384 (1.0964)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [160/431]  eta: 0:05:03  lr: 0.000148  loss: 1.0384 (1.0953)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [170/431]  eta: 0:04:51  lr: 0.000148  loss: 1.0892 (1.0971)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [180/431]  eta: 0:04:40  lr: 0.000148  loss: 1.1371 (1.1000)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [190/431]  eta: 0:04:29  lr: 0.000148  loss: 1.1481 (1.1011)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [200/431]  eta: 0:04:17  lr: 0.000148  loss: 1.1152 (1.1024)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [210/431]  eta: 0:04:06  lr: 0.000148  loss: 1.1152 (1.1046)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [220/431]  eta: 0:03:55  lr: 0.000148  loss: 1.0690 (1.1044)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [230/431]  eta: 0:03:44  lr: 0.000148  loss: 1.0773 (1.1042)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [240/431]  eta: 0:03:32  lr: 0.000148  loss: 1.0775 (1.1053)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [250/431]  eta: 0:03:21  lr: 0.000148  loss: 1.1084 (1.1062)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [260/431]  eta: 0:03:10  lr: 0.000148  loss: 1.0835 (1.1052)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [270/431]  eta: 0:02:59  lr: 0.000148  loss: 1.0714 (1.1042)  time: 1.1019  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:336]  [280/431]  eta: 0:02:48  lr: 0.000148  loss: 1.0714 (1.1026)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [290/431]  eta: 0:02:36  lr: 0.000148  loss: 1.0374 (1.1019)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [300/431]  eta: 0:02:25  lr: 0.000148  loss: 1.0395 (1.1035)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [310/431]  eta: 0:02:14  lr: 0.000148  loss: 1.0467 (1.1020)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [320/431]  eta: 0:02:03  lr: 0.000148  loss: 1.0627 (1.1032)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [330/431]  eta: 0:01:52  lr: 0.000148  loss: 1.1078 (1.1044)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [340/431]  eta: 0:01:41  lr: 0.000148  loss: 1.1332 (1.1061)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [350/431]  eta: 0:01:30  lr: 0.000148  loss: 1.1176 (1.1062)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [360/431]  eta: 0:01:18  lr: 0.000148  loss: 1.0206 (1.1033)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [370/431]  eta: 0:01:07  lr: 0.000148  loss: 1.0547 (1.1048)  time: 1.1092  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [380/431]  eta: 0:00:56  lr: 0.000148  loss: 1.1190 (1.1051)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [390/431]  eta: 0:00:45  lr: 0.000148  loss: 1.0343 (1.1031)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [400/431]  eta: 0:00:34  lr: 0.000148  loss: 1.0222 (1.1025)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [410/431]  eta: 0:00:23  lr: 0.000148  loss: 1.0619 (1.1027)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:336]  [420/431]  eta: 0:00:12  lr: 0.000148  loss: 1.0619 (1.1023)  time: 1.0982  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:336]  [430/431]  eta: 0:00:01  lr: 0.000148  loss: 1.0477 (1.1013)  time: 1.0988  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:336] Total time: 0:07:58 (1.1101 s / it)\n",
      "Averaged stats: lr: 0.000148  loss: 1.0477 (1.1013)\n",
      "Valid: [epoch:336]  [ 0/14]  eta: 0:00:36  loss: 1.1274 (1.1274)  time: 2.5798  data: 2.4087  max mem: 15925\n",
      "Valid: [epoch:336]  [13/14]  eta: 0:00:00  loss: 1.0316 (1.0408)  time: 0.2816  data: 0.1721  max mem: 15925\n",
      "Valid: [epoch:336] Total time: 0:00:04 (0.2983 s / it)\n",
      "Averaged stats: loss: 1.0316 (1.0408)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_336_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:337]  [  0/431]  eta: 0:32:20  lr: 0.000148  loss: 1.0115 (1.0115)  time: 4.5028  data: 3.3313  max mem: 15925\n",
      "Train: [epoch:337]  [ 10/431]  eta: 0:09:30  lr: 0.000148  loss: 1.0665 (1.0747)  time: 1.3547  data: 0.3030  max mem: 15925\n",
      "Train: [epoch:337]  [ 20/431]  eta: 0:08:19  lr: 0.000148  loss: 1.0912 (1.0815)  time: 1.0508  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [ 30/431]  eta: 0:07:48  lr: 0.000148  loss: 1.1430 (1.1081)  time: 1.0649  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [ 40/431]  eta: 0:07:29  lr: 0.000148  loss: 1.1465 (1.1066)  time: 1.0797  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [ 50/431]  eta: 0:07:12  lr: 0.000148  loss: 1.0846 (1.1052)  time: 1.0863  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [ 60/431]  eta: 0:06:59  lr: 0.000148  loss: 1.0846 (1.0985)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [ 70/431]  eta: 0:06:46  lr: 0.000148  loss: 1.0787 (1.0980)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [ 80/431]  eta: 0:06:34  lr: 0.000148  loss: 1.0928 (1.1065)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [ 90/431]  eta: 0:06:22  lr: 0.000148  loss: 1.0723 (1.1049)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [100/431]  eta: 0:06:10  lr: 0.000148  loss: 1.0721 (1.1086)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [110/431]  eta: 0:05:58  lr: 0.000148  loss: 1.1152 (1.1063)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [120/431]  eta: 0:05:47  lr: 0.000148  loss: 1.0987 (1.1041)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [130/431]  eta: 0:05:35  lr: 0.000148  loss: 1.0697 (1.1025)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [140/431]  eta: 0:05:24  lr: 0.000148  loss: 1.0500 (1.1015)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [150/431]  eta: 0:05:13  lr: 0.000148  loss: 1.0484 (1.1003)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [160/431]  eta: 0:05:02  lr: 0.000148  loss: 1.0328 (1.0976)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [170/431]  eta: 0:04:51  lr: 0.000148  loss: 1.0095 (1.0948)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [180/431]  eta: 0:04:39  lr: 0.000148  loss: 1.0203 (1.0943)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [190/431]  eta: 0:04:28  lr: 0.000148  loss: 1.0517 (1.0956)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [200/431]  eta: 0:04:16  lr: 0.000148  loss: 1.0353 (1.0920)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [210/431]  eta: 0:04:05  lr: 0.000148  loss: 1.0761 (1.0936)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [220/431]  eta: 0:03:54  lr: 0.000148  loss: 1.1332 (1.0948)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [230/431]  eta: 0:03:43  lr: 0.000148  loss: 1.1332 (1.0946)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [240/431]  eta: 0:03:32  lr: 0.000148  loss: 1.0630 (1.0938)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [250/431]  eta: 0:03:20  lr: 0.000148  loss: 1.0751 (1.0944)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [260/431]  eta: 0:03:09  lr: 0.000148  loss: 1.0804 (1.0955)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [270/431]  eta: 0:02:58  lr: 0.000148  loss: 1.1132 (1.0961)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [280/431]  eta: 0:02:47  lr: 0.000148  loss: 1.1132 (1.0973)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [290/431]  eta: 0:02:36  lr: 0.000148  loss: 1.1043 (1.0978)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [300/431]  eta: 0:02:25  lr: 0.000148  loss: 1.1682 (1.1007)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [310/431]  eta: 0:02:14  lr: 0.000148  loss: 1.1194 (1.0995)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [320/431]  eta: 0:02:02  lr: 0.000148  loss: 1.0609 (1.0984)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [330/431]  eta: 0:01:51  lr: 0.000148  loss: 1.0810 (1.0990)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [340/431]  eta: 0:01:40  lr: 0.000148  loss: 1.0810 (1.0985)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [350/431]  eta: 0:01:29  lr: 0.000148  loss: 1.0155 (1.0977)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [360/431]  eta: 0:01:18  lr: 0.000148  loss: 1.0665 (1.0981)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [370/431]  eta: 0:01:07  lr: 0.000148  loss: 1.0806 (1.1001)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [380/431]  eta: 0:00:56  lr: 0.000148  loss: 1.0738 (1.1000)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [390/431]  eta: 0:00:45  lr: 0.000148  loss: 1.0571 (1.1005)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [400/431]  eta: 0:00:34  lr: 0.000148  loss: 1.0696 (1.1003)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [410/431]  eta: 0:00:23  lr: 0.000148  loss: 1.0551 (1.1002)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:337]  [420/431]  eta: 0:00:12  lr: 0.000148  loss: 1.0197 (1.0988)  time: 1.0953  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:337]  [430/431]  eta: 0:00:01  lr: 0.000148  loss: 1.0499 (1.0991)  time: 1.1016  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:337] Total time: 0:07:57 (1.1079 s / it)\n",
      "Averaged stats: lr: 0.000148  loss: 1.0499 (1.0991)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:337]  [ 0/14]  eta: 0:00:36  loss: 0.9605 (0.9605)  time: 2.5766  data: 2.4467  max mem: 15925\n",
      "Valid: [epoch:337]  [13/14]  eta: 0:00:00  loss: 1.0410 (1.0471)  time: 0.2671  data: 0.1748  max mem: 15925\n",
      "Valid: [epoch:337] Total time: 0:00:03 (0.2819 s / it)\n",
      "Averaged stats: loss: 1.0410 (1.0471)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_337_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:338]  [  0/431]  eta: 0:35:09  lr: 0.000147  loss: 1.1337 (1.1337)  time: 4.8944  data: 3.7939  max mem: 15925\n",
      "Train: [epoch:338]  [ 10/431]  eta: 0:09:36  lr: 0.000147  loss: 0.9854 (1.0809)  time: 1.3697  data: 0.3451  max mem: 15925\n",
      "Train: [epoch:338]  [ 20/431]  eta: 0:08:20  lr: 0.000147  loss: 1.1442 (1.1274)  time: 1.0344  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [ 30/431]  eta: 0:07:50  lr: 0.000147  loss: 1.1415 (1.1154)  time: 1.0669  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [ 40/431]  eta: 0:07:31  lr: 0.000147  loss: 1.0598 (1.1000)  time: 1.0859  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [ 50/431]  eta: 0:07:15  lr: 0.000147  loss: 1.0483 (1.0975)  time: 1.0932  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [ 60/431]  eta: 0:07:01  lr: 0.000147  loss: 1.0483 (1.0901)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [ 70/431]  eta: 0:06:47  lr: 0.000147  loss: 1.0682 (1.0894)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [ 80/431]  eta: 0:06:35  lr: 0.000147  loss: 1.1068 (1.0919)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [ 90/431]  eta: 0:06:24  lr: 0.000147  loss: 1.1075 (1.0976)  time: 1.1135  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [100/431]  eta: 0:06:12  lr: 0.000147  loss: 1.0942 (1.0948)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [110/431]  eta: 0:06:00  lr: 0.000147  loss: 1.0942 (1.1006)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [120/431]  eta: 0:05:48  lr: 0.000147  loss: 1.1049 (1.0982)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [130/431]  eta: 0:05:37  lr: 0.000147  loss: 1.0340 (1.0933)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [140/431]  eta: 0:05:26  lr: 0.000147  loss: 1.0207 (1.0917)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [150/431]  eta: 0:05:14  lr: 0.000147  loss: 1.0790 (1.0929)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [160/431]  eta: 0:05:03  lr: 0.000147  loss: 1.1009 (1.0948)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [170/431]  eta: 0:04:51  lr: 0.000147  loss: 1.0917 (1.0973)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [180/431]  eta: 0:04:40  lr: 0.000147  loss: 1.0955 (1.0989)  time: 1.1144  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [190/431]  eta: 0:04:29  lr: 0.000147  loss: 1.0907 (1.0992)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [200/431]  eta: 0:04:17  lr: 0.000147  loss: 1.0778 (1.0987)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [210/431]  eta: 0:04:06  lr: 0.000147  loss: 1.1007 (1.1008)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [220/431]  eta: 0:03:55  lr: 0.000147  loss: 1.1462 (1.1030)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [230/431]  eta: 0:03:43  lr: 0.000147  loss: 1.1462 (1.1047)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [240/431]  eta: 0:03:32  lr: 0.000147  loss: 1.0879 (1.1025)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [250/431]  eta: 0:03:21  lr: 0.000147  loss: 1.0788 (1.1026)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [260/431]  eta: 0:03:10  lr: 0.000147  loss: 1.1018 (1.1045)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [270/431]  eta: 0:02:59  lr: 0.000147  loss: 1.1193 (1.1043)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [280/431]  eta: 0:02:47  lr: 0.000147  loss: 1.1236 (1.1061)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [290/431]  eta: 0:02:36  lr: 0.000147  loss: 1.1307 (1.1059)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [300/431]  eta: 0:02:25  lr: 0.000147  loss: 1.1096 (1.1097)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [310/431]  eta: 0:02:14  lr: 0.000147  loss: 1.0568 (1.1078)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [320/431]  eta: 0:02:03  lr: 0.000147  loss: 1.0405 (1.1076)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [330/431]  eta: 0:01:52  lr: 0.000147  loss: 1.0984 (1.1087)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [340/431]  eta: 0:01:41  lr: 0.000147  loss: 1.1182 (1.1097)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [350/431]  eta: 0:01:30  lr: 0.000147  loss: 1.1111 (1.1099)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [360/431]  eta: 0:01:18  lr: 0.000147  loss: 1.1001 (1.1091)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [370/431]  eta: 0:01:07  lr: 0.000147  loss: 1.1001 (1.1090)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [380/431]  eta: 0:00:56  lr: 0.000147  loss: 1.0044 (1.1065)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [390/431]  eta: 0:00:45  lr: 0.000147  loss: 1.0045 (1.1063)  time: 1.1004  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:338]  [400/431]  eta: 0:00:34  lr: 0.000147  loss: 1.0929 (1.1061)  time: 1.0954  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:338]  [410/431]  eta: 0:00:23  lr: 0.000147  loss: 1.1200 (1.1059)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:338]  [420/431]  eta: 0:00:12  lr: 0.000147  loss: 1.1082 (1.1050)  time: 1.0954  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:338]  [430/431]  eta: 0:00:01  lr: 0.000147  loss: 1.0423 (1.1042)  time: 1.1032  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:338] Total time: 0:07:58 (1.1096 s / it)\n",
      "Averaged stats: lr: 0.000147  loss: 1.0423 (1.1042)\n",
      "Valid: [epoch:338]  [ 0/14]  eta: 0:00:35  loss: 1.0094 (1.0094)  time: 2.5698  data: 2.4060  max mem: 15925\n",
      "Valid: [epoch:338]  [13/14]  eta: 0:00:00  loss: 1.0340 (1.0415)  time: 0.2645  data: 0.1719  max mem: 15925\n",
      "Valid: [epoch:338] Total time: 0:00:03 (0.2806 s / it)\n",
      "Averaged stats: loss: 1.0340 (1.0415)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_338_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:339]  [  0/431]  eta: 0:33:10  lr: 0.000147  loss: 1.0834 (1.0834)  time: 4.6190  data: 3.4449  max mem: 15925\n",
      "Train: [epoch:339]  [ 10/431]  eta: 0:09:35  lr: 0.000147  loss: 1.0834 (1.1180)  time: 1.3682  data: 0.3134  max mem: 15925\n",
      "Train: [epoch:339]  [ 20/431]  eta: 0:08:23  lr: 0.000147  loss: 1.0564 (1.1094)  time: 1.0542  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [ 30/431]  eta: 0:07:52  lr: 0.000147  loss: 1.0313 (1.0895)  time: 1.0746  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [ 40/431]  eta: 0:07:33  lr: 0.000147  loss: 1.0313 (1.0830)  time: 1.0918  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:339]  [ 50/431]  eta: 0:07:17  lr: 0.000147  loss: 1.0593 (1.0913)  time: 1.1039  data: 0.0004  max mem: 15925\n",
      "Train: [epoch:339]  [ 60/431]  eta: 0:07:03  lr: 0.000147  loss: 1.0371 (1.0819)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [ 70/431]  eta: 0:06:50  lr: 0.000147  loss: 1.0371 (1.0868)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [ 80/431]  eta: 0:06:37  lr: 0.000147  loss: 1.0830 (1.0886)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [ 90/431]  eta: 0:06:24  lr: 0.000147  loss: 1.0262 (1.0848)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [100/431]  eta: 0:06:13  lr: 0.000147  loss: 1.0363 (1.0838)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [110/431]  eta: 0:06:00  lr: 0.000147  loss: 1.0433 (1.0827)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [120/431]  eta: 0:05:49  lr: 0.000147  loss: 1.0506 (1.0798)  time: 1.0970  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:339]  [130/431]  eta: 0:05:37  lr: 0.000147  loss: 1.0778 (1.0826)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [140/431]  eta: 0:05:26  lr: 0.000147  loss: 1.0844 (1.0860)  time: 1.1134  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [150/431]  eta: 0:05:14  lr: 0.000147  loss: 1.0418 (1.0835)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [160/431]  eta: 0:05:03  lr: 0.000147  loss: 1.0565 (1.0835)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [170/431]  eta: 0:04:51  lr: 0.000147  loss: 1.0951 (1.0858)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [180/431]  eta: 0:04:40  lr: 0.000147  loss: 1.0939 (1.0873)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [190/431]  eta: 0:04:29  lr: 0.000147  loss: 1.0549 (1.0895)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [200/431]  eta: 0:04:17  lr: 0.000147  loss: 1.0549 (1.0883)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [210/431]  eta: 0:04:06  lr: 0.000147  loss: 1.0242 (1.0893)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [220/431]  eta: 0:03:54  lr: 0.000147  loss: 1.1239 (1.0915)  time: 1.0896  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [230/431]  eta: 0:03:43  lr: 0.000147  loss: 1.1239 (1.0932)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [240/431]  eta: 0:03:32  lr: 0.000147  loss: 1.0890 (1.0935)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [250/431]  eta: 0:03:21  lr: 0.000147  loss: 1.0850 (1.0937)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [260/431]  eta: 0:03:10  lr: 0.000147  loss: 1.0850 (1.0930)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [270/431]  eta: 0:02:59  lr: 0.000147  loss: 1.1167 (1.0942)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [280/431]  eta: 0:02:47  lr: 0.000147  loss: 1.1052 (1.0951)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [290/431]  eta: 0:02:36  lr: 0.000147  loss: 1.1036 (1.0965)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [300/431]  eta: 0:02:25  lr: 0.000147  loss: 1.0812 (1.0979)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [310/431]  eta: 0:02:14  lr: 0.000147  loss: 1.0865 (1.0984)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [320/431]  eta: 0:02:03  lr: 0.000147  loss: 1.0944 (1.0993)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [330/431]  eta: 0:01:52  lr: 0.000147  loss: 1.0824 (1.0991)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [340/431]  eta: 0:01:41  lr: 0.000147  loss: 1.0824 (1.1003)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [350/431]  eta: 0:01:29  lr: 0.000147  loss: 1.0797 (1.0997)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [360/431]  eta: 0:01:18  lr: 0.000147  loss: 1.0484 (1.0996)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [370/431]  eta: 0:01:07  lr: 0.000147  loss: 1.0628 (1.1003)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [380/431]  eta: 0:00:56  lr: 0.000147  loss: 1.0472 (1.0989)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [390/431]  eta: 0:00:45  lr: 0.000147  loss: 1.0643 (1.0999)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [400/431]  eta: 0:00:34  lr: 0.000147  loss: 1.0933 (1.0993)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [410/431]  eta: 0:00:23  lr: 0.000147  loss: 1.0890 (1.0999)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:339]  [420/431]  eta: 0:00:12  lr: 0.000147  loss: 1.1245 (1.1000)  time: 1.0990  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:339]  [430/431]  eta: 0:00:01  lr: 0.000147  loss: 1.1247 (1.1008)  time: 1.1038  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:339] Total time: 0:07:58 (1.1092 s / it)\n",
      "Averaged stats: lr: 0.000147  loss: 1.1247 (1.1008)\n",
      "Valid: [epoch:339]  [ 0/14]  eta: 0:00:35  loss: 1.0958 (1.0958)  time: 2.5441  data: 2.3971  max mem: 15925\n",
      "Valid: [epoch:339]  [13/14]  eta: 0:00:00  loss: 1.0456 (1.0526)  time: 0.2815  data: 0.1713  max mem: 15925\n",
      "Valid: [epoch:339] Total time: 0:00:04 (0.2971 s / it)\n",
      "Averaged stats: loss: 1.0456 (1.0526)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_339_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.053%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:340]  [  0/431]  eta: 0:33:35  lr: 0.000147  loss: 0.9671 (0.9671)  time: 4.6774  data: 3.5948  max mem: 15925\n",
      "Train: [epoch:340]  [ 10/431]  eta: 0:09:35  lr: 0.000147  loss: 1.0319 (1.0766)  time: 1.3666  data: 0.3270  max mem: 15925\n",
      "Train: [epoch:340]  [ 20/431]  eta: 0:08:20  lr: 0.000147  loss: 1.0299 (1.0812)  time: 1.0452  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [ 30/431]  eta: 0:07:51  lr: 0.000147  loss: 1.0299 (1.0761)  time: 1.0693  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [ 40/431]  eta: 0:07:30  lr: 0.000147  loss: 1.0590 (1.0791)  time: 1.0855  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [ 50/431]  eta: 0:07:15  lr: 0.000147  loss: 1.0393 (1.0735)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [ 60/431]  eta: 0:07:01  lr: 0.000147  loss: 1.0315 (1.0705)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [ 70/431]  eta: 0:06:49  lr: 0.000147  loss: 1.0812 (1.0747)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [ 80/431]  eta: 0:06:37  lr: 0.000147  loss: 1.0901 (1.0755)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [ 90/431]  eta: 0:06:25  lr: 0.000147  loss: 1.0328 (1.0754)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [100/431]  eta: 0:06:13  lr: 0.000147  loss: 1.0621 (1.0812)  time: 1.1078  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [110/431]  eta: 0:06:01  lr: 0.000147  loss: 1.1096 (1.0831)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [120/431]  eta: 0:05:49  lr: 0.000147  loss: 1.1043 (1.0851)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [130/431]  eta: 0:05:37  lr: 0.000147  loss: 1.1043 (1.0876)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [140/431]  eta: 0:05:25  lr: 0.000147  loss: 1.1026 (1.0885)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [150/431]  eta: 0:05:14  lr: 0.000147  loss: 1.1057 (1.0922)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [160/431]  eta: 0:05:03  lr: 0.000147  loss: 1.0683 (1.0916)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [170/431]  eta: 0:04:51  lr: 0.000147  loss: 1.0192 (1.0894)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [180/431]  eta: 0:04:40  lr: 0.000147  loss: 1.0125 (1.0900)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [190/431]  eta: 0:04:28  lr: 0.000147  loss: 1.0687 (1.0910)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [200/431]  eta: 0:04:17  lr: 0.000147  loss: 1.0851 (1.0914)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [210/431]  eta: 0:04:06  lr: 0.000147  loss: 1.0746 (1.0902)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [220/431]  eta: 0:03:55  lr: 0.000147  loss: 1.0577 (1.0895)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [230/431]  eta: 0:03:43  lr: 0.000147  loss: 1.1131 (1.0915)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [240/431]  eta: 0:03:32  lr: 0.000147  loss: 1.1282 (1.0928)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [250/431]  eta: 0:03:21  lr: 0.000147  loss: 1.0928 (1.0937)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [260/431]  eta: 0:03:10  lr: 0.000147  loss: 1.0910 (1.0941)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [270/431]  eta: 0:02:59  lr: 0.000147  loss: 1.0910 (1.0936)  time: 1.1177  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [280/431]  eta: 0:02:48  lr: 0.000147  loss: 1.0904 (1.0951)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [290/431]  eta: 0:02:36  lr: 0.000147  loss: 1.1081 (1.0964)  time: 1.0995  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:340]  [300/431]  eta: 0:02:25  lr: 0.000147  loss: 1.1271 (1.0989)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [310/431]  eta: 0:02:14  lr: 0.000147  loss: 1.1296 (1.0989)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [320/431]  eta: 0:02:03  lr: 0.000147  loss: 1.0673 (1.0983)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [330/431]  eta: 0:01:52  lr: 0.000147  loss: 1.0709 (1.1000)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [340/431]  eta: 0:01:41  lr: 0.000147  loss: 1.0965 (1.0996)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [350/431]  eta: 0:01:29  lr: 0.000147  loss: 1.0902 (1.0991)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [360/431]  eta: 0:01:18  lr: 0.000147  loss: 1.0775 (1.0987)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [370/431]  eta: 0:01:07  lr: 0.000147  loss: 1.0422 (1.0983)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [380/431]  eta: 0:00:56  lr: 0.000147  loss: 1.0376 (1.0970)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [390/431]  eta: 0:00:45  lr: 0.000147  loss: 1.0155 (1.0963)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [400/431]  eta: 0:00:34  lr: 0.000147  loss: 1.0396 (1.0961)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [410/431]  eta: 0:00:23  lr: 0.000147  loss: 1.0805 (1.0967)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:340]  [420/431]  eta: 0:00:12  lr: 0.000147  loss: 1.0850 (1.0971)  time: 1.1044  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:340]  [430/431]  eta: 0:00:01  lr: 0.000147  loss: 1.1113 (1.0980)  time: 1.1020  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:340] Total time: 0:07:58 (1.1096 s / it)\n",
      "Averaged stats: lr: 0.000147  loss: 1.1113 (1.0980)\n",
      "Valid: [epoch:340]  [ 0/14]  eta: 0:00:35  loss: 1.0295 (1.0295)  time: 2.5450  data: 2.3989  max mem: 15925\n",
      "Valid: [epoch:340]  [13/14]  eta: 0:00:00  loss: 1.0295 (1.0400)  time: 0.2664  data: 0.1714  max mem: 15925\n",
      "Valid: [epoch:340] Total time: 0:00:03 (0.2836 s / it)\n",
      "Averaged stats: loss: 1.0295 (1.0400)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_340_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.040%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:341]  [  0/431]  eta: 0:29:09  lr: 0.000147  loss: 1.1225 (1.1225)  time: 4.0582  data: 2.8810  max mem: 15925\n",
      "Train: [epoch:341]  [ 10/431]  eta: 0:09:14  lr: 0.000147  loss: 1.1403 (1.1420)  time: 1.3180  data: 0.2621  max mem: 15925\n",
      "Train: [epoch:341]  [ 20/431]  eta: 0:08:15  lr: 0.000147  loss: 1.0642 (1.1253)  time: 1.0624  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [ 30/431]  eta: 0:07:44  lr: 0.000147  loss: 1.0642 (1.1208)  time: 1.0707  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [ 40/431]  eta: 0:07:27  lr: 0.000147  loss: 1.0587 (1.1049)  time: 1.0784  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [ 50/431]  eta: 0:07:12  lr: 0.000147  loss: 1.0780 (1.1168)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [ 60/431]  eta: 0:06:58  lr: 0.000147  loss: 1.0820 (1.1016)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [ 70/431]  eta: 0:06:46  lr: 0.000147  loss: 1.0823 (1.1094)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [ 80/431]  eta: 0:06:33  lr: 0.000147  loss: 1.1121 (1.1130)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [ 90/431]  eta: 0:06:22  lr: 0.000147  loss: 1.0761 (1.1147)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [100/431]  eta: 0:06:10  lr: 0.000147  loss: 1.0162 (1.1072)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [110/431]  eta: 0:05:58  lr: 0.000147  loss: 1.0251 (1.1066)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [120/431]  eta: 0:05:47  lr: 0.000147  loss: 1.0704 (1.1040)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [130/431]  eta: 0:05:35  lr: 0.000147  loss: 1.0317 (1.1029)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [140/431]  eta: 0:05:24  lr: 0.000147  loss: 1.1196 (1.1075)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [150/431]  eta: 0:05:13  lr: 0.000147  loss: 1.1520 (1.1089)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [160/431]  eta: 0:05:01  lr: 0.000147  loss: 1.0650 (1.1084)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [170/431]  eta: 0:04:50  lr: 0.000147  loss: 1.0650 (1.1082)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [180/431]  eta: 0:04:39  lr: 0.000147  loss: 1.0746 (1.1051)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [190/431]  eta: 0:04:27  lr: 0.000147  loss: 1.0452 (1.1020)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [200/431]  eta: 0:04:16  lr: 0.000147  loss: 1.0372 (1.1012)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [210/431]  eta: 0:04:05  lr: 0.000147  loss: 1.0814 (1.1016)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [220/431]  eta: 0:03:54  lr: 0.000147  loss: 1.0289 (1.0996)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [230/431]  eta: 0:03:43  lr: 0.000147  loss: 1.0818 (1.1012)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [240/431]  eta: 0:03:32  lr: 0.000147  loss: 1.1242 (1.1030)  time: 1.1104  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [250/431]  eta: 0:03:20  lr: 0.000147  loss: 1.1043 (1.1031)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [260/431]  eta: 0:03:09  lr: 0.000147  loss: 1.1291 (1.1062)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [270/431]  eta: 0:02:58  lr: 0.000147  loss: 1.1367 (1.1068)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [280/431]  eta: 0:02:47  lr: 0.000147  loss: 1.0377 (1.1027)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [290/431]  eta: 0:02:36  lr: 0.000147  loss: 1.0328 (1.1023)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [300/431]  eta: 0:02:25  lr: 0.000147  loss: 1.0513 (1.1034)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [310/431]  eta: 0:02:14  lr: 0.000147  loss: 1.0291 (1.1026)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [320/431]  eta: 0:02:03  lr: 0.000147  loss: 1.0519 (1.1029)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [330/431]  eta: 0:01:51  lr: 0.000147  loss: 1.0727 (1.1028)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [340/431]  eta: 0:01:40  lr: 0.000147  loss: 1.1345 (1.1060)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [350/431]  eta: 0:01:29  lr: 0.000147  loss: 1.1205 (1.1053)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [360/431]  eta: 0:01:18  lr: 0.000147  loss: 1.1163 (1.1062)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [370/431]  eta: 0:01:07  lr: 0.000147  loss: 1.1163 (1.1053)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [380/431]  eta: 0:00:56  lr: 0.000147  loss: 1.0403 (1.1046)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [390/431]  eta: 0:00:45  lr: 0.000147  loss: 1.0891 (1.1049)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [400/431]  eta: 0:00:34  lr: 0.000147  loss: 1.0581 (1.1040)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [410/431]  eta: 0:00:23  lr: 0.000147  loss: 1.0332 (1.1039)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:341]  [420/431]  eta: 0:00:12  lr: 0.000147  loss: 1.0723 (1.1038)  time: 1.1042  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:341]  [430/431]  eta: 0:00:01  lr: 0.000147  loss: 1.0470 (1.1023)  time: 1.1034  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:341] Total time: 0:07:57 (1.1074 s / it)\n",
      "Averaged stats: lr: 0.000147  loss: 1.0470 (1.1023)\n",
      "Valid: [epoch:341]  [ 0/14]  eta: 0:00:36  loss: 1.1331 (1.1331)  time: 2.5889  data: 2.4353  max mem: 15925\n",
      "Valid: [epoch:341]  [13/14]  eta: 0:00:00  loss: 1.0442 (1.0490)  time: 0.2719  data: 0.1740  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:341] Total time: 0:00:04 (0.2902 s / it)\n",
      "Averaged stats: loss: 1.0442 (1.0490)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_341_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.049%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:342]  [  0/431]  eta: 0:33:59  lr: 0.000146  loss: 1.0881 (1.0881)  time: 4.7318  data: 3.5670  max mem: 15925\n",
      "Train: [epoch:342]  [ 10/431]  eta: 0:09:30  lr: 0.000146  loss: 1.1247 (1.1431)  time: 1.3549  data: 0.3244  max mem: 15925\n",
      "Train: [epoch:342]  [ 20/431]  eta: 0:08:19  lr: 0.000146  loss: 1.1247 (1.1403)  time: 1.0390  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [ 30/431]  eta: 0:07:48  lr: 0.000146  loss: 1.0624 (1.0987)  time: 1.0657  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [ 40/431]  eta: 0:07:29  lr: 0.000146  loss: 1.0499 (1.0988)  time: 1.0791  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [ 50/431]  eta: 0:07:14  lr: 0.000146  loss: 1.0632 (1.0959)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [ 60/431]  eta: 0:07:00  lr: 0.000146  loss: 1.0145 (1.0912)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [ 70/431]  eta: 0:06:47  lr: 0.000146  loss: 1.0823 (1.0956)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [ 80/431]  eta: 0:06:34  lr: 0.000146  loss: 1.0850 (1.0990)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [ 90/431]  eta: 0:06:21  lr: 0.000146  loss: 1.0533 (1.0953)  time: 1.0850  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [100/431]  eta: 0:06:09  lr: 0.000146  loss: 1.0674 (1.0963)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [110/431]  eta: 0:05:57  lr: 0.000146  loss: 1.0586 (1.0944)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [120/431]  eta: 0:05:46  lr: 0.000146  loss: 1.0255 (1.0928)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [130/431]  eta: 0:05:35  lr: 0.000146  loss: 1.0678 (1.0946)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [140/431]  eta: 0:05:23  lr: 0.000146  loss: 1.0756 (1.0918)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [150/431]  eta: 0:05:12  lr: 0.000146  loss: 1.0756 (1.0974)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [160/431]  eta: 0:05:01  lr: 0.000146  loss: 1.1606 (1.1006)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [170/431]  eta: 0:04:49  lr: 0.000146  loss: 1.1141 (1.0995)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [180/431]  eta: 0:04:38  lr: 0.000146  loss: 1.0989 (1.1004)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [190/431]  eta: 0:04:27  lr: 0.000146  loss: 1.1015 (1.1018)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [200/431]  eta: 0:04:16  lr: 0.000146  loss: 1.0738 (1.1011)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [210/431]  eta: 0:04:05  lr: 0.000146  loss: 1.0901 (1.1019)  time: 1.1018  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:342]  [220/431]  eta: 0:03:53  lr: 0.000146  loss: 1.0545 (1.1012)  time: 1.0971  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:342]  [230/431]  eta: 0:03:42  lr: 0.000146  loss: 1.0366 (1.0985)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [240/431]  eta: 0:03:31  lr: 0.000146  loss: 1.0721 (1.0988)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [250/431]  eta: 0:03:20  lr: 0.000146  loss: 1.1104 (1.0968)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [260/431]  eta: 0:03:09  lr: 0.000146  loss: 1.1118 (1.0984)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [270/431]  eta: 0:02:58  lr: 0.000146  loss: 1.0864 (1.0977)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [280/431]  eta: 0:02:47  lr: 0.000146  loss: 1.0834 (1.0980)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [290/431]  eta: 0:02:36  lr: 0.000146  loss: 1.0834 (1.0971)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [300/431]  eta: 0:02:24  lr: 0.000146  loss: 1.0306 (1.0954)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [310/431]  eta: 0:02:13  lr: 0.000146  loss: 1.0298 (1.0936)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [320/431]  eta: 0:02:02  lr: 0.000146  loss: 1.0544 (1.0941)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [330/431]  eta: 0:01:51  lr: 0.000146  loss: 1.0679 (1.0944)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [340/431]  eta: 0:01:40  lr: 0.000146  loss: 1.0643 (1.0954)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [350/431]  eta: 0:01:29  lr: 0.000146  loss: 1.1302 (1.0974)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [360/431]  eta: 0:01:18  lr: 0.000146  loss: 1.1535 (1.0999)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [370/431]  eta: 0:01:07  lr: 0.000146  loss: 1.1405 (1.1001)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [380/431]  eta: 0:00:56  lr: 0.000146  loss: 1.0820 (1.0997)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [390/431]  eta: 0:00:45  lr: 0.000146  loss: 1.0600 (1.0987)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [400/431]  eta: 0:00:34  lr: 0.000146  loss: 1.0555 (1.1004)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [410/431]  eta: 0:00:23  lr: 0.000146  loss: 1.0802 (1.1002)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:342]  [420/431]  eta: 0:00:12  lr: 0.000146  loss: 1.0657 (1.0999)  time: 1.1031  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:342]  [430/431]  eta: 0:00:01  lr: 0.000146  loss: 1.0843 (1.1002)  time: 1.1088  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:342] Total time: 0:07:56 (1.1056 s / it)\n",
      "Averaged stats: lr: 0.000146  loss: 1.0843 (1.1002)\n",
      "Valid: [epoch:342]  [ 0/14]  eta: 0:00:34  loss: 1.0961 (1.0961)  time: 2.4551  data: 2.2945  max mem: 15925\n",
      "Valid: [epoch:342]  [13/14]  eta: 0:00:00  loss: 1.0363 (1.0445)  time: 0.2736  data: 0.1640  max mem: 15925\n",
      "Valid: [epoch:342] Total time: 0:00:04 (0.2915 s / it)\n",
      "Averaged stats: loss: 1.0363 (1.0445)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_342_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:343]  [  0/431]  eta: 0:32:14  lr: 0.000146  loss: 1.1845 (1.1845)  time: 4.4875  data: 3.3217  max mem: 15925\n",
      "Train: [epoch:343]  [ 10/431]  eta: 0:09:30  lr: 0.000146  loss: 1.0379 (1.0662)  time: 1.3549  data: 0.3022  max mem: 15925\n",
      "Train: [epoch:343]  [ 20/431]  eta: 0:08:18  lr: 0.000146  loss: 1.0379 (1.0679)  time: 1.0493  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [ 30/431]  eta: 0:07:48  lr: 0.000146  loss: 1.0634 (1.0703)  time: 1.0665  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [ 40/431]  eta: 0:07:29  lr: 0.000146  loss: 1.0496 (1.0651)  time: 1.0814  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [ 50/431]  eta: 0:07:13  lr: 0.000146  loss: 1.1212 (1.0805)  time: 1.0865  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [ 60/431]  eta: 0:06:59  lr: 0.000146  loss: 1.1434 (1.0804)  time: 1.0930  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [ 70/431]  eta: 0:06:45  lr: 0.000146  loss: 1.0371 (1.0821)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [ 80/431]  eta: 0:06:33  lr: 0.000146  loss: 1.0404 (1.0823)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [ 90/431]  eta: 0:06:21  lr: 0.000146  loss: 1.0645 (1.0846)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [100/431]  eta: 0:06:09  lr: 0.000146  loss: 1.0853 (1.0896)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [110/431]  eta: 0:05:58  lr: 0.000146  loss: 1.0976 (1.0923)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [120/431]  eta: 0:05:46  lr: 0.000146  loss: 1.0927 (1.0934)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [130/431]  eta: 0:05:35  lr: 0.000146  loss: 1.0816 (1.0942)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [140/431]  eta: 0:05:24  lr: 0.000146  loss: 1.0816 (1.0962)  time: 1.1079  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:343]  [150/431]  eta: 0:05:12  lr: 0.000146  loss: 1.1164 (1.1011)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [160/431]  eta: 0:05:01  lr: 0.000146  loss: 1.0790 (1.1012)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [170/431]  eta: 0:04:50  lr: 0.000146  loss: 1.0790 (1.1015)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [180/431]  eta: 0:04:39  lr: 0.000146  loss: 1.0804 (1.1015)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [190/431]  eta: 0:04:27  lr: 0.000146  loss: 1.0737 (1.1026)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [200/431]  eta: 0:04:16  lr: 0.000146  loss: 1.0590 (1.1014)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [210/431]  eta: 0:04:05  lr: 0.000146  loss: 1.0385 (1.1005)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [220/431]  eta: 0:03:54  lr: 0.000146  loss: 1.1023 (1.1026)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [230/431]  eta: 0:03:43  lr: 0.000146  loss: 1.1204 (1.1031)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [240/431]  eta: 0:03:31  lr: 0.000146  loss: 1.0719 (1.1039)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [250/431]  eta: 0:03:20  lr: 0.000146  loss: 1.0427 (1.1025)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [260/431]  eta: 0:03:09  lr: 0.000146  loss: 1.0427 (1.1032)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [270/431]  eta: 0:02:58  lr: 0.000146  loss: 1.1023 (1.1040)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [280/431]  eta: 0:02:47  lr: 0.000146  loss: 1.1100 (1.1054)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [290/431]  eta: 0:02:36  lr: 0.000146  loss: 1.0966 (1.1042)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [300/431]  eta: 0:02:25  lr: 0.000146  loss: 1.0966 (1.1048)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [310/431]  eta: 0:02:14  lr: 0.000146  loss: 1.0499 (1.1030)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [320/431]  eta: 0:02:03  lr: 0.000146  loss: 1.0163 (1.1005)  time: 1.1111  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [330/431]  eta: 0:01:51  lr: 0.000146  loss: 1.0463 (1.1008)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [340/431]  eta: 0:01:40  lr: 0.000146  loss: 1.0781 (1.1017)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [350/431]  eta: 0:01:29  lr: 0.000146  loss: 1.0792 (1.1020)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [360/431]  eta: 0:01:18  lr: 0.000146  loss: 1.0798 (1.1021)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [370/431]  eta: 0:01:07  lr: 0.000146  loss: 1.0798 (1.1023)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [380/431]  eta: 0:00:56  lr: 0.000146  loss: 1.1021 (1.1028)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [390/431]  eta: 0:00:45  lr: 0.000146  loss: 1.1094 (1.1029)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [400/431]  eta: 0:00:34  lr: 0.000146  loss: 1.0633 (1.1012)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [410/431]  eta: 0:00:23  lr: 0.000146  loss: 1.0457 (1.1012)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:343]  [420/431]  eta: 0:00:12  lr: 0.000146  loss: 1.0959 (1.1006)  time: 1.1048  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:343]  [430/431]  eta: 0:00:01  lr: 0.000146  loss: 1.0694 (1.1002)  time: 1.1081  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:343] Total time: 0:07:57 (1.1069 s / it)\n",
      "Averaged stats: lr: 0.000146  loss: 1.0694 (1.1002)\n",
      "Valid: [epoch:343]  [ 0/14]  eta: 0:00:35  loss: 0.9774 (0.9774)  time: 2.5700  data: 2.4258  max mem: 15925\n",
      "Valid: [epoch:343]  [13/14]  eta: 0:00:00  loss: 1.0331 (1.0419)  time: 0.2630  data: 0.1733  max mem: 15925\n",
      "Valid: [epoch:343] Total time: 0:00:03 (0.2789 s / it)\n",
      "Averaged stats: loss: 1.0331 (1.0419)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_343_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:344]  [  0/431]  eta: 0:35:58  lr: 0.000146  loss: 0.9153 (0.9153)  time: 5.0090  data: 3.7989  max mem: 15925\n",
      "Train: [epoch:344]  [ 10/431]  eta: 0:09:42  lr: 0.000146  loss: 1.1413 (1.1157)  time: 1.3836  data: 0.3455  max mem: 15925\n",
      "Train: [epoch:344]  [ 20/431]  eta: 0:08:22  lr: 0.000146  loss: 1.1242 (1.1289)  time: 1.0337  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [ 30/431]  eta: 0:07:51  lr: 0.000146  loss: 1.1082 (1.1427)  time: 1.0615  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [ 40/431]  eta: 0:07:31  lr: 0.000146  loss: 1.0813 (1.1226)  time: 1.0821  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [ 50/431]  eta: 0:07:13  lr: 0.000146  loss: 1.0790 (1.1258)  time: 1.0808  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [ 60/431]  eta: 0:07:00  lr: 0.000146  loss: 1.0581 (1.1189)  time: 1.0890  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [ 70/431]  eta: 0:06:46  lr: 0.000146  loss: 1.0216 (1.1146)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [ 80/431]  eta: 0:06:34  lr: 0.000146  loss: 1.0400 (1.1114)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [ 90/431]  eta: 0:06:22  lr: 0.000146  loss: 1.0355 (1.1053)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [100/431]  eta: 0:06:10  lr: 0.000146  loss: 1.0384 (1.1038)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [110/431]  eta: 0:05:58  lr: 0.000146  loss: 1.0738 (1.1004)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [120/431]  eta: 0:05:47  lr: 0.000146  loss: 1.1071 (1.0981)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [130/431]  eta: 0:05:35  lr: 0.000146  loss: 1.0484 (1.0951)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [140/431]  eta: 0:05:24  lr: 0.000146  loss: 1.0560 (1.0946)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [150/431]  eta: 0:05:12  lr: 0.000146  loss: 1.1017 (1.0983)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [160/431]  eta: 0:05:01  lr: 0.000146  loss: 1.1503 (1.1018)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [170/431]  eta: 0:04:50  lr: 0.000146  loss: 1.1093 (1.0997)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [180/431]  eta: 0:04:38  lr: 0.000146  loss: 1.0395 (1.1001)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [190/431]  eta: 0:04:27  lr: 0.000146  loss: 1.1195 (1.1026)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [200/431]  eta: 0:04:16  lr: 0.000146  loss: 1.0990 (1.0992)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [210/431]  eta: 0:04:05  lr: 0.000146  loss: 1.0319 (1.0977)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [220/431]  eta: 0:03:53  lr: 0.000146  loss: 1.0462 (1.0957)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [230/431]  eta: 0:03:42  lr: 0.000146  loss: 1.0462 (1.0968)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [240/431]  eta: 0:03:31  lr: 0.000146  loss: 1.0900 (1.0975)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [250/431]  eta: 0:03:20  lr: 0.000146  loss: 1.0386 (1.0950)  time: 1.0861  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [260/431]  eta: 0:03:09  lr: 0.000146  loss: 1.0680 (1.0965)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [270/431]  eta: 0:02:58  lr: 0.000146  loss: 1.1254 (1.0991)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [280/431]  eta: 0:02:47  lr: 0.000146  loss: 1.0556 (1.0993)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [290/431]  eta: 0:02:35  lr: 0.000146  loss: 1.0497 (1.0979)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [300/431]  eta: 0:02:24  lr: 0.000146  loss: 1.0538 (1.1005)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [310/431]  eta: 0:02:13  lr: 0.000146  loss: 1.1075 (1.1005)  time: 1.1103  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:344]  [320/431]  eta: 0:02:02  lr: 0.000146  loss: 1.0787 (1.1003)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [330/431]  eta: 0:01:51  lr: 0.000146  loss: 1.1026 (1.1025)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [340/431]  eta: 0:01:40  lr: 0.000146  loss: 1.1026 (1.1024)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [350/431]  eta: 0:01:29  lr: 0.000146  loss: 1.0887 (1.1016)  time: 1.0859  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [360/431]  eta: 0:01:18  lr: 0.000146  loss: 1.0836 (1.1010)  time: 1.0882  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [370/431]  eta: 0:01:07  lr: 0.000146  loss: 1.0324 (1.1001)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [380/431]  eta: 0:00:56  lr: 0.000146  loss: 1.0503 (1.0996)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [390/431]  eta: 0:00:45  lr: 0.000146  loss: 1.0714 (1.0998)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [400/431]  eta: 0:00:34  lr: 0.000146  loss: 1.0756 (1.1003)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [410/431]  eta: 0:00:23  lr: 0.000146  loss: 1.0821 (1.0998)  time: 1.0883  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:344]  [420/431]  eta: 0:00:12  lr: 0.000146  loss: 1.0804 (1.1006)  time: 1.0933  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:344]  [430/431]  eta: 0:00:01  lr: 0.000146  loss: 1.0392 (1.0998)  time: 1.1038  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:344] Total time: 0:07:55 (1.1042 s / it)\n",
      "Averaged stats: lr: 0.000146  loss: 1.0392 (1.0998)\n",
      "Valid: [epoch:344]  [ 0/14]  eta: 0:00:35  loss: 1.0335 (1.0335)  time: 2.5042  data: 2.3262  max mem: 15925\n",
      "Valid: [epoch:344]  [13/14]  eta: 0:00:00  loss: 1.0335 (1.0425)  time: 0.2683  data: 0.1663  max mem: 15925\n",
      "Valid: [epoch:344] Total time: 0:00:03 (0.2855 s / it)\n",
      "Averaged stats: loss: 1.0335 (1.0425)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_344_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:345]  [  0/431]  eta: 0:35:06  lr: 0.000146  loss: 1.1557 (1.1557)  time: 4.8875  data: 3.7206  max mem: 15925\n",
      "Train: [epoch:345]  [ 10/431]  eta: 0:09:45  lr: 0.000146  loss: 1.0896 (1.1179)  time: 1.3912  data: 0.3384  max mem: 15925\n",
      "Train: [epoch:345]  [ 20/431]  eta: 0:08:25  lr: 0.000146  loss: 1.0913 (1.1197)  time: 1.0467  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [ 30/431]  eta: 0:07:52  lr: 0.000146  loss: 1.0913 (1.1007)  time: 1.0621  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [ 40/431]  eta: 0:07:32  lr: 0.000146  loss: 1.0674 (1.1113)  time: 1.0804  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [ 50/431]  eta: 0:07:15  lr: 0.000146  loss: 1.0641 (1.1095)  time: 1.0844  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [ 60/431]  eta: 0:07:00  lr: 0.000146  loss: 1.0601 (1.1026)  time: 1.0866  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [ 70/431]  eta: 0:06:48  lr: 0.000146  loss: 1.0603 (1.0978)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [ 80/431]  eta: 0:06:35  lr: 0.000146  loss: 1.0603 (1.0949)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [ 90/431]  eta: 0:06:22  lr: 0.000146  loss: 1.0560 (1.0930)  time: 1.0946  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [100/431]  eta: 0:06:10  lr: 0.000146  loss: 1.0560 (1.0915)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [110/431]  eta: 0:05:59  lr: 0.000146  loss: 1.0694 (1.0900)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [120/431]  eta: 0:05:47  lr: 0.000146  loss: 1.0983 (1.0914)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [130/431]  eta: 0:05:36  lr: 0.000146  loss: 1.0828 (1.0946)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [140/431]  eta: 0:05:24  lr: 0.000146  loss: 1.0828 (1.0954)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [150/431]  eta: 0:05:13  lr: 0.000146  loss: 1.0434 (1.0936)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [160/431]  eta: 0:05:01  lr: 0.000146  loss: 1.1354 (1.0980)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [170/431]  eta: 0:04:50  lr: 0.000146  loss: 1.1254 (1.0956)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [180/431]  eta: 0:04:38  lr: 0.000146  loss: 1.1208 (1.0993)  time: 1.0917  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [190/431]  eta: 0:04:27  lr: 0.000146  loss: 1.1316 (1.1005)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [200/431]  eta: 0:04:16  lr: 0.000146  loss: 1.1269 (1.1014)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [210/431]  eta: 0:04:05  lr: 0.000146  loss: 1.1138 (1.1016)  time: 1.0900  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [220/431]  eta: 0:03:53  lr: 0.000146  loss: 1.0973 (1.1014)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [230/431]  eta: 0:03:42  lr: 0.000146  loss: 1.0973 (1.1018)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [240/431]  eta: 0:03:31  lr: 0.000146  loss: 1.1004 (1.1011)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [250/431]  eta: 0:03:20  lr: 0.000146  loss: 1.0534 (1.1009)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [260/431]  eta: 0:03:09  lr: 0.000146  loss: 1.0534 (1.1016)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [270/431]  eta: 0:02:58  lr: 0.000146  loss: 1.1190 (1.1022)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [280/431]  eta: 0:02:47  lr: 0.000146  loss: 1.0868 (1.1016)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [290/431]  eta: 0:02:36  lr: 0.000146  loss: 1.0506 (1.1011)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [300/431]  eta: 0:02:24  lr: 0.000146  loss: 1.0686 (1.1013)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [310/431]  eta: 0:02:13  lr: 0.000146  loss: 1.0951 (1.1016)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [320/431]  eta: 0:02:02  lr: 0.000146  loss: 1.0350 (1.0983)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [330/431]  eta: 0:01:51  lr: 0.000146  loss: 1.0159 (1.0999)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [340/431]  eta: 0:01:40  lr: 0.000146  loss: 1.1380 (1.1014)  time: 1.0891  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [350/431]  eta: 0:01:29  lr: 0.000146  loss: 1.1010 (1.1013)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [360/431]  eta: 0:01:18  lr: 0.000146  loss: 1.1019 (1.1017)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [370/431]  eta: 0:01:07  lr: 0.000146  loss: 1.0926 (1.1009)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [380/431]  eta: 0:00:56  lr: 0.000146  loss: 1.0659 (1.1006)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [390/431]  eta: 0:00:45  lr: 0.000146  loss: 1.0747 (1.1005)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [400/431]  eta: 0:00:34  lr: 0.000146  loss: 1.0606 (1.1003)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:345]  [410/431]  eta: 0:00:23  lr: 0.000146  loss: 1.0530 (1.0995)  time: 1.0960  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:345]  [420/431]  eta: 0:00:12  lr: 0.000146  loss: 1.0620 (1.0999)  time: 1.0994  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:345]  [430/431]  eta: 0:00:01  lr: 0.000146  loss: 1.0776 (1.1005)  time: 1.1019  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:345] Total time: 0:07:56 (1.1049 s / it)\n",
      "Averaged stats: lr: 0.000146  loss: 1.0776 (1.1005)\n",
      "Valid: [epoch:345]  [ 0/14]  eta: 0:00:34  loss: 1.0957 (1.0957)  time: 2.4958  data: 2.3332  max mem: 15925\n",
      "Valid: [epoch:345]  [13/14]  eta: 0:00:00  loss: 1.0306 (1.0417)  time: 0.2668  data: 0.1668  max mem: 15925\n",
      "Valid: [epoch:345] Total time: 0:00:03 (0.2845 s / it)\n",
      "Averaged stats: loss: 1.0306 (1.0417)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_345_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:346]  [  0/431]  eta: 0:31:46  lr: 0.000146  loss: 1.1315 (1.1315)  time: 4.4238  data: 3.1896  max mem: 15925\n",
      "Train: [epoch:346]  [ 10/431]  eta: 0:09:30  lr: 0.000146  loss: 1.1315 (1.1624)  time: 1.3560  data: 0.2902  max mem: 15925\n",
      "Train: [epoch:346]  [ 20/431]  eta: 0:08:17  lr: 0.000146  loss: 1.1203 (1.1497)  time: 1.0507  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [ 30/431]  eta: 0:07:48  lr: 0.000146  loss: 1.0601 (1.1296)  time: 1.0651  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [ 40/431]  eta: 0:07:29  lr: 0.000146  loss: 1.0871 (1.1330)  time: 1.0861  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [ 50/431]  eta: 0:07:14  lr: 0.000146  loss: 1.1300 (1.1389)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [ 60/431]  eta: 0:07:00  lr: 0.000146  loss: 1.0611 (1.1199)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [ 70/431]  eta: 0:06:47  lr: 0.000146  loss: 1.0447 (1.1195)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [ 80/431]  eta: 0:06:35  lr: 0.000146  loss: 1.1240 (1.1212)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [ 90/431]  eta: 0:06:22  lr: 0.000146  loss: 1.0902 (1.1172)  time: 1.0937  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [100/431]  eta: 0:06:10  lr: 0.000146  loss: 1.0861 (1.1166)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [110/431]  eta: 0:05:58  lr: 0.000146  loss: 1.0470 (1.1113)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [120/431]  eta: 0:05:47  lr: 0.000146  loss: 1.0235 (1.1102)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [130/431]  eta: 0:05:36  lr: 0.000146  loss: 1.0189 (1.1031)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [140/431]  eta: 0:05:24  lr: 0.000146  loss: 1.0165 (1.1013)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [150/431]  eta: 0:05:13  lr: 0.000146  loss: 1.0606 (1.0991)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [160/431]  eta: 0:05:01  lr: 0.000146  loss: 1.0595 (1.0970)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [170/431]  eta: 0:04:50  lr: 0.000146  loss: 1.1063 (1.1009)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [180/431]  eta: 0:04:39  lr: 0.000146  loss: 1.1117 (1.1011)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [190/431]  eta: 0:04:27  lr: 0.000146  loss: 1.1060 (1.1002)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [200/431]  eta: 0:04:16  lr: 0.000146  loss: 1.0515 (1.0979)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [210/431]  eta: 0:04:05  lr: 0.000146  loss: 1.0511 (1.0969)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [220/431]  eta: 0:03:54  lr: 0.000146  loss: 1.0910 (1.0978)  time: 1.1148  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [230/431]  eta: 0:03:43  lr: 0.000146  loss: 1.0783 (1.0984)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [240/431]  eta: 0:03:31  lr: 0.000146  loss: 1.1524 (1.1007)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [250/431]  eta: 0:03:20  lr: 0.000146  loss: 1.1253 (1.0999)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [260/431]  eta: 0:03:09  lr: 0.000146  loss: 1.1253 (1.1022)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [270/431]  eta: 0:02:58  lr: 0.000146  loss: 1.0898 (1.1031)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [280/431]  eta: 0:02:47  lr: 0.000146  loss: 1.0794 (1.1035)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [290/431]  eta: 0:02:36  lr: 0.000146  loss: 1.1179 (1.1040)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [300/431]  eta: 0:02:25  lr: 0.000146  loss: 1.1134 (1.1047)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [310/431]  eta: 0:02:14  lr: 0.000146  loss: 1.0848 (1.1030)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [320/431]  eta: 0:02:03  lr: 0.000146  loss: 1.0755 (1.1021)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [330/431]  eta: 0:01:51  lr: 0.000146  loss: 1.0653 (1.1016)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [340/431]  eta: 0:01:40  lr: 0.000146  loss: 1.0653 (1.1025)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [350/431]  eta: 0:01:29  lr: 0.000146  loss: 1.1192 (1.1037)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [360/431]  eta: 0:01:18  lr: 0.000146  loss: 1.0602 (1.1025)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [370/431]  eta: 0:01:07  lr: 0.000146  loss: 1.0452 (1.1012)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [380/431]  eta: 0:00:56  lr: 0.000146  loss: 1.0452 (1.1002)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [390/431]  eta: 0:00:45  lr: 0.000146  loss: 1.0549 (1.0996)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [400/431]  eta: 0:00:34  lr: 0.000146  loss: 1.0596 (1.0999)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:346]  [410/431]  eta: 0:00:23  lr: 0.000146  loss: 1.0735 (1.0995)  time: 1.0903  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:346]  [420/431]  eta: 0:00:12  lr: 0.000146  loss: 1.0735 (1.0999)  time: 1.0980  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:346]  [430/431]  eta: 0:00:01  lr: 0.000146  loss: 1.0813 (1.0994)  time: 1.1091  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:346] Total time: 0:07:57 (1.1070 s / it)\n",
      "Averaged stats: lr: 0.000146  loss: 1.0813 (1.0994)\n",
      "Valid: [epoch:346]  [ 0/14]  eta: 0:00:35  loss: 1.0843 (1.0843)  time: 2.5534  data: 2.3909  max mem: 15925\n",
      "Valid: [epoch:346]  [13/14]  eta: 0:00:00  loss: 1.0327 (1.0408)  time: 0.2644  data: 0.1709  max mem: 15925\n",
      "Valid: [epoch:346] Total time: 0:00:03 (0.2800 s / it)\n",
      "Averaged stats: loss: 1.0327 (1.0408)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_346_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:347]  [  0/431]  eta: 0:34:07  lr: 0.000145  loss: 1.0676 (1.0676)  time: 4.7504  data: 3.6226  max mem: 15925\n",
      "Train: [epoch:347]  [ 10/431]  eta: 0:09:39  lr: 0.000145  loss: 1.1575 (1.1858)  time: 1.3755  data: 0.3295  max mem: 15925\n",
      "Train: [epoch:347]  [ 20/431]  eta: 0:08:23  lr: 0.000145  loss: 1.1089 (1.1393)  time: 1.0500  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [ 30/431]  eta: 0:07:52  lr: 0.000145  loss: 1.0674 (1.1127)  time: 1.0693  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [ 40/431]  eta: 0:07:31  lr: 0.000145  loss: 1.0913 (1.1076)  time: 1.0820  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [ 50/431]  eta: 0:07:16  lr: 0.000145  loss: 1.0711 (1.0996)  time: 1.0925  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [ 60/431]  eta: 0:07:02  lr: 0.000145  loss: 1.0711 (1.1041)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [ 70/431]  eta: 0:06:49  lr: 0.000145  loss: 1.1464 (1.1132)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [ 80/431]  eta: 0:06:37  lr: 0.000145  loss: 1.1531 (1.1134)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [ 90/431]  eta: 0:06:24  lr: 0.000145  loss: 1.1203 (1.1121)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [100/431]  eta: 0:06:12  lr: 0.000145  loss: 1.0619 (1.1061)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [110/431]  eta: 0:05:59  lr: 0.000145  loss: 1.0458 (1.1058)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [120/431]  eta: 0:05:47  lr: 0.000145  loss: 1.0905 (1.1110)  time: 1.0852  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [130/431]  eta: 0:05:36  lr: 0.000145  loss: 1.0854 (1.1076)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [140/431]  eta: 0:05:24  lr: 0.000145  loss: 1.0504 (1.1061)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [150/431]  eta: 0:05:13  lr: 0.000145  loss: 1.0143 (1.1038)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [160/431]  eta: 0:05:02  lr: 0.000145  loss: 1.0640 (1.1043)  time: 1.1004  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:347]  [170/431]  eta: 0:04:50  lr: 0.000145  loss: 1.0757 (1.1037)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [180/431]  eta: 0:04:39  lr: 0.000145  loss: 1.0770 (1.1029)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [190/431]  eta: 0:04:28  lr: 0.000145  loss: 1.0603 (1.1017)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [200/431]  eta: 0:04:16  lr: 0.000145  loss: 1.0468 (1.1007)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [210/431]  eta: 0:04:05  lr: 0.000145  loss: 1.0564 (1.1030)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [220/431]  eta: 0:03:54  lr: 0.000145  loss: 1.0430 (1.1024)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [230/431]  eta: 0:03:43  lr: 0.000145  loss: 1.0636 (1.1022)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [240/431]  eta: 0:03:32  lr: 0.000145  loss: 1.0848 (1.1025)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [250/431]  eta: 0:03:20  lr: 0.000145  loss: 1.0810 (1.1013)  time: 1.1035  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:347]  [260/431]  eta: 0:03:09  lr: 0.000145  loss: 1.0659 (1.1048)  time: 1.1018  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:347]  [270/431]  eta: 0:02:58  lr: 0.000145  loss: 1.0659 (1.1048)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [280/431]  eta: 0:02:47  lr: 0.000145  loss: 1.1421 (1.1060)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [290/431]  eta: 0:02:36  lr: 0.000145  loss: 1.0890 (1.1055)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [300/431]  eta: 0:02:25  lr: 0.000145  loss: 1.0808 (1.1048)  time: 1.0920  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [310/431]  eta: 0:02:13  lr: 0.000145  loss: 1.0808 (1.1035)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [320/431]  eta: 0:02:02  lr: 0.000145  loss: 1.0786 (1.1031)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [330/431]  eta: 0:01:51  lr: 0.000145  loss: 1.1010 (1.1041)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [340/431]  eta: 0:01:40  lr: 0.000145  loss: 1.1303 (1.1052)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [350/431]  eta: 0:01:29  lr: 0.000145  loss: 1.1107 (1.1053)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [360/431]  eta: 0:01:18  lr: 0.000145  loss: 1.0771 (1.1041)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [370/431]  eta: 0:01:07  lr: 0.000145  loss: 1.0501 (1.1038)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [380/431]  eta: 0:00:56  lr: 0.000145  loss: 1.0850 (1.1033)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [390/431]  eta: 0:00:45  lr: 0.000145  loss: 1.0876 (1.1024)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [400/431]  eta: 0:00:34  lr: 0.000145  loss: 1.0248 (1.1015)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [410/431]  eta: 0:00:23  lr: 0.000145  loss: 1.0343 (1.1007)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:347]  [420/431]  eta: 0:00:12  lr: 0.000145  loss: 1.1132 (1.1021)  time: 1.0905  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:347]  [430/431]  eta: 0:00:01  lr: 0.000145  loss: 1.1145 (1.1021)  time: 1.0975  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:347] Total time: 0:07:56 (1.1055 s / it)\n",
      "Averaged stats: lr: 0.000145  loss: 1.1145 (1.1021)\n",
      "Valid: [epoch:347]  [ 0/14]  eta: 0:00:34  loss: 1.0259 (1.0259)  time: 2.4908  data: 2.3319  max mem: 15925\n",
      "Valid: [epoch:347]  [13/14]  eta: 0:00:00  loss: 1.0339 (1.0446)  time: 0.2620  data: 0.1668  max mem: 15925\n",
      "Valid: [epoch:347] Total time: 0:00:03 (0.2781 s / it)\n",
      "Averaged stats: loss: 1.0339 (1.0446)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_347_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:348]  [  0/431]  eta: 0:31:18  lr: 0.000145  loss: 1.2226 (1.2226)  time: 4.3594  data: 3.1930  max mem: 15925\n",
      "Train: [epoch:348]  [ 10/431]  eta: 0:09:28  lr: 0.000145  loss: 1.1017 (1.1452)  time: 1.3506  data: 0.2905  max mem: 15925\n",
      "Train: [epoch:348]  [ 20/431]  eta: 0:08:19  lr: 0.000145  loss: 1.0740 (1.1110)  time: 1.0579  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [ 30/431]  eta: 0:07:49  lr: 0.000145  loss: 1.0509 (1.0890)  time: 1.0733  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [ 40/431]  eta: 0:07:28  lr: 0.000145  loss: 1.0211 (1.0743)  time: 1.0771  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [ 50/431]  eta: 0:07:14  lr: 0.000145  loss: 1.0657 (1.0765)  time: 1.0907  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [ 60/431]  eta: 0:07:00  lr: 0.000145  loss: 1.0303 (1.0657)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [ 70/431]  eta: 0:06:47  lr: 0.000145  loss: 1.0303 (1.0707)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [ 80/431]  eta: 0:06:34  lr: 0.000145  loss: 1.0857 (1.0758)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [ 90/431]  eta: 0:06:22  lr: 0.000145  loss: 1.1390 (1.0818)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [100/431]  eta: 0:06:10  lr: 0.000145  loss: 1.0643 (1.0809)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [110/431]  eta: 0:05:59  lr: 0.000145  loss: 1.0462 (1.0818)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [120/431]  eta: 0:05:47  lr: 0.000145  loss: 1.0655 (1.0835)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [130/431]  eta: 0:05:35  lr: 0.000145  loss: 1.0872 (1.0884)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [140/431]  eta: 0:05:24  lr: 0.000145  loss: 1.0780 (1.0870)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [150/431]  eta: 0:05:13  lr: 0.000145  loss: 1.0657 (1.0873)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [160/431]  eta: 0:05:01  lr: 0.000145  loss: 1.1110 (1.0918)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [170/431]  eta: 0:04:50  lr: 0.000145  loss: 1.0518 (1.0908)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [180/431]  eta: 0:04:39  lr: 0.000145  loss: 1.0335 (1.0892)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [190/431]  eta: 0:04:28  lr: 0.000145  loss: 1.0726 (1.0889)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [200/431]  eta: 0:04:16  lr: 0.000145  loss: 1.0900 (1.0885)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [210/431]  eta: 0:04:05  lr: 0.000145  loss: 1.0946 (1.0901)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [220/431]  eta: 0:03:54  lr: 0.000145  loss: 1.1403 (1.0920)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [230/431]  eta: 0:03:43  lr: 0.000145  loss: 1.0575 (1.0904)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [240/431]  eta: 0:03:31  lr: 0.000145  loss: 1.0575 (1.0927)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [250/431]  eta: 0:03:20  lr: 0.000145  loss: 1.1477 (1.0963)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [260/431]  eta: 0:03:09  lr: 0.000145  loss: 1.1325 (1.0961)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [270/431]  eta: 0:02:58  lr: 0.000145  loss: 1.0824 (1.0983)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [280/431]  eta: 0:02:47  lr: 0.000145  loss: 1.0824 (1.0969)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [290/431]  eta: 0:02:36  lr: 0.000145  loss: 1.0529 (1.0980)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [300/431]  eta: 0:02:25  lr: 0.000145  loss: 1.0956 (1.0988)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [310/431]  eta: 0:02:13  lr: 0.000145  loss: 1.1004 (1.0985)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [320/431]  eta: 0:02:02  lr: 0.000145  loss: 1.0943 (1.0985)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [330/431]  eta: 0:01:51  lr: 0.000145  loss: 1.0616 (1.0989)  time: 1.0984  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:348]  [340/431]  eta: 0:01:40  lr: 0.000145  loss: 1.1204 (1.1011)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [350/431]  eta: 0:01:29  lr: 0.000145  loss: 1.1204 (1.1015)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [360/431]  eta: 0:01:18  lr: 0.000145  loss: 1.0980 (1.1010)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [370/431]  eta: 0:01:07  lr: 0.000145  loss: 1.0915 (1.1018)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [380/431]  eta: 0:00:56  lr: 0.000145  loss: 1.0752 (1.1005)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [390/431]  eta: 0:00:45  lr: 0.000145  loss: 1.0614 (1.0996)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [400/431]  eta: 0:00:34  lr: 0.000145  loss: 1.0673 (1.1012)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [410/431]  eta: 0:00:23  lr: 0.000145  loss: 1.1091 (1.1017)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:348]  [420/431]  eta: 0:00:12  lr: 0.000145  loss: 1.0415 (1.1018)  time: 1.1111  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:348]  [430/431]  eta: 0:00:01  lr: 0.000145  loss: 1.0396 (1.1004)  time: 1.1057  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:348] Total time: 0:07:57 (1.1071 s / it)\n",
      "Averaged stats: lr: 0.000145  loss: 1.0396 (1.1004)\n",
      "Valid: [epoch:348]  [ 0/14]  eta: 0:00:33  loss: 0.9324 (0.9324)  time: 2.4134  data: 2.2634  max mem: 15925\n",
      "Valid: [epoch:348]  [13/14]  eta: 0:00:00  loss: 1.0348 (1.0425)  time: 0.2599  data: 0.1618  max mem: 15925\n",
      "Valid: [epoch:348] Total time: 0:00:03 (0.2750 s / it)\n",
      "Averaged stats: loss: 1.0348 (1.0425)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_348_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:349]  [  0/431]  eta: 0:32:52  lr: 0.000145  loss: 1.4169 (1.4169)  time: 4.5776  data: 3.4146  max mem: 15925\n",
      "Train: [epoch:349]  [ 10/431]  eta: 0:09:34  lr: 0.000145  loss: 1.1205 (1.1545)  time: 1.3636  data: 0.3106  max mem: 15925\n",
      "Train: [epoch:349]  [ 20/431]  eta: 0:08:18  lr: 0.000145  loss: 1.1011 (1.1333)  time: 1.0436  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [ 30/431]  eta: 0:07:48  lr: 0.000145  loss: 1.0865 (1.1074)  time: 1.0604  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [ 40/431]  eta: 0:07:28  lr: 0.000145  loss: 1.0635 (1.1038)  time: 1.0786  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [ 50/431]  eta: 0:07:13  lr: 0.000145  loss: 1.0630 (1.0979)  time: 1.0897  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [ 60/431]  eta: 0:07:00  lr: 0.000145  loss: 1.0292 (1.0870)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [ 70/431]  eta: 0:06:46  lr: 0.000145  loss: 1.0292 (1.0800)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [ 80/431]  eta: 0:06:34  lr: 0.000145  loss: 1.0926 (1.0859)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [ 90/431]  eta: 0:06:22  lr: 0.000145  loss: 1.0556 (1.0805)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [100/431]  eta: 0:06:10  lr: 0.000145  loss: 1.0165 (1.0772)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [110/431]  eta: 0:05:59  lr: 0.000145  loss: 1.0244 (1.0812)  time: 1.1137  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [120/431]  eta: 0:05:47  lr: 0.000145  loss: 1.0371 (1.0777)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [130/431]  eta: 0:05:36  lr: 0.000145  loss: 1.0199 (1.0765)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [140/431]  eta: 0:05:24  lr: 0.000145  loss: 1.0934 (1.0807)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [150/431]  eta: 0:05:13  lr: 0.000145  loss: 1.1006 (1.0802)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [160/431]  eta: 0:05:02  lr: 0.000145  loss: 1.0721 (1.0823)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [170/431]  eta: 0:04:50  lr: 0.000145  loss: 1.0772 (1.0829)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [180/431]  eta: 0:04:39  lr: 0.000145  loss: 1.0835 (1.0865)  time: 1.0951  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [190/431]  eta: 0:04:28  lr: 0.000145  loss: 1.0700 (1.0882)  time: 1.1013  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [200/431]  eta: 0:04:17  lr: 0.000145  loss: 1.0552 (1.0897)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [210/431]  eta: 0:04:05  lr: 0.000145  loss: 1.0572 (1.0884)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [220/431]  eta: 0:03:54  lr: 0.000145  loss: 1.0572 (1.0876)  time: 1.0880  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [230/431]  eta: 0:03:43  lr: 0.000145  loss: 1.0593 (1.0864)  time: 1.0855  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [240/431]  eta: 0:03:31  lr: 0.000145  loss: 1.0584 (1.0873)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [250/431]  eta: 0:03:20  lr: 0.000145  loss: 1.1052 (1.0886)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [260/431]  eta: 0:03:09  lr: 0.000145  loss: 1.1052 (1.0917)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [270/431]  eta: 0:02:58  lr: 0.000145  loss: 1.0706 (1.0923)  time: 1.0927  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [280/431]  eta: 0:02:47  lr: 0.000145  loss: 1.0773 (1.0924)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [290/431]  eta: 0:02:36  lr: 0.000145  loss: 1.1055 (1.0932)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [300/431]  eta: 0:02:25  lr: 0.000145  loss: 1.1353 (1.0947)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [310/431]  eta: 0:02:13  lr: 0.000145  loss: 1.1322 (1.0958)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [320/431]  eta: 0:02:02  lr: 0.000145  loss: 1.1087 (1.0958)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [330/431]  eta: 0:01:51  lr: 0.000145  loss: 1.1087 (1.0973)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [340/431]  eta: 0:01:40  lr: 0.000145  loss: 1.1447 (1.0985)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [350/431]  eta: 0:01:29  lr: 0.000145  loss: 1.1506 (1.1008)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [360/431]  eta: 0:01:18  lr: 0.000145  loss: 1.0939 (1.1003)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [370/431]  eta: 0:01:07  lr: 0.000145  loss: 1.0616 (1.0997)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [380/431]  eta: 0:00:56  lr: 0.000145  loss: 1.0468 (1.0994)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [390/431]  eta: 0:00:45  lr: 0.000145  loss: 1.0849 (1.1000)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [400/431]  eta: 0:00:34  lr: 0.000145  loss: 1.0858 (1.0996)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [410/431]  eta: 0:00:23  lr: 0.000145  loss: 1.0671 (1.0998)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:349]  [420/431]  eta: 0:00:12  lr: 0.000145  loss: 1.0781 (1.1002)  time: 1.0917  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:349]  [430/431]  eta: 0:00:01  lr: 0.000145  loss: 1.0916 (1.0996)  time: 1.0997  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:349] Total time: 0:07:56 (1.1066 s / it)\n",
      "Averaged stats: lr: 0.000145  loss: 1.0916 (1.0996)\n",
      "Valid: [epoch:349]  [ 0/14]  eta: 0:00:36  loss: 0.9830 (0.9830)  time: 2.6050  data: 2.4320  max mem: 15925\n",
      "Valid: [epoch:349]  [13/14]  eta: 0:00:00  loss: 1.0390 (1.0457)  time: 0.2987  data: 0.1738  max mem: 15925\n",
      "Valid: [epoch:349] Total time: 0:00:04 (0.3146 s / it)\n",
      "Averaged stats: loss: 1.0390 (1.0457)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_349_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:350]  [  0/431]  eta: 0:31:03  lr: 0.000145  loss: 1.1475 (1.1475)  time: 4.3244  data: 3.0957  max mem: 15925\n",
      "Train: [epoch:350]  [ 10/431]  eta: 0:09:25  lr: 0.000145  loss: 1.1977 (1.2098)  time: 1.3434  data: 0.2816  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:350]  [ 20/431]  eta: 0:08:13  lr: 0.000145  loss: 1.0820 (1.1437)  time: 1.0456  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [ 30/431]  eta: 0:07:45  lr: 0.000145  loss: 1.0459 (1.1382)  time: 1.0618  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [ 40/431]  eta: 0:07:26  lr: 0.000145  loss: 1.0396 (1.1147)  time: 1.0790  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [ 50/431]  eta: 0:07:11  lr: 0.000145  loss: 1.0253 (1.1040)  time: 1.0850  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [ 60/431]  eta: 0:06:58  lr: 0.000145  loss: 1.0477 (1.1026)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [ 70/431]  eta: 0:06:45  lr: 0.000145  loss: 1.0826 (1.1026)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [ 80/431]  eta: 0:06:33  lr: 0.000145  loss: 1.1284 (1.1088)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [ 90/431]  eta: 0:06:20  lr: 0.000145  loss: 1.0700 (1.1066)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [100/431]  eta: 0:06:08  lr: 0.000145  loss: 1.0673 (1.1053)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [110/431]  eta: 0:05:57  lr: 0.000145  loss: 1.0906 (1.1037)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [120/431]  eta: 0:05:45  lr: 0.000145  loss: 1.0655 (1.0992)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [130/431]  eta: 0:05:34  lr: 0.000145  loss: 1.0404 (1.0982)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [140/431]  eta: 0:05:23  lr: 0.000145  loss: 1.0659 (1.0955)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [150/431]  eta: 0:05:12  lr: 0.000145  loss: 1.0693 (1.0964)  time: 1.1044  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:350]  [160/431]  eta: 0:05:00  lr: 0.000145  loss: 1.0727 (1.0946)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [170/431]  eta: 0:04:49  lr: 0.000145  loss: 1.1141 (1.0986)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [180/431]  eta: 0:04:38  lr: 0.000145  loss: 1.1141 (1.0974)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [190/431]  eta: 0:04:27  lr: 0.000145  loss: 1.0695 (1.0988)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [200/431]  eta: 0:04:16  lr: 0.000145  loss: 1.0594 (1.0993)  time: 1.0996  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:350]  [210/431]  eta: 0:04:04  lr: 0.000145  loss: 1.0289 (1.0964)  time: 1.1003  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:350]  [220/431]  eta: 0:03:53  lr: 0.000145  loss: 1.0761 (1.0982)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [230/431]  eta: 0:03:42  lr: 0.000145  loss: 1.0907 (1.0982)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [240/431]  eta: 0:03:31  lr: 0.000145  loss: 1.0615 (1.0988)  time: 1.1049  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:350]  [250/431]  eta: 0:03:20  lr: 0.000145  loss: 1.0602 (1.0979)  time: 1.1021  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:350]  [260/431]  eta: 0:03:09  lr: 0.000145  loss: 1.0580 (1.0978)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [270/431]  eta: 0:02:58  lr: 0.000145  loss: 1.0276 (1.0957)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [280/431]  eta: 0:02:47  lr: 0.000145  loss: 1.0547 (1.0958)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [290/431]  eta: 0:02:36  lr: 0.000145  loss: 1.0814 (1.0955)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [300/431]  eta: 0:02:24  lr: 0.000145  loss: 1.0814 (1.0970)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [310/431]  eta: 0:02:13  lr: 0.000145  loss: 1.0679 (1.0972)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [320/431]  eta: 0:02:02  lr: 0.000145  loss: 1.0955 (1.0990)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [330/431]  eta: 0:01:51  lr: 0.000145  loss: 1.0843 (1.0980)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [340/431]  eta: 0:01:40  lr: 0.000145  loss: 1.0843 (1.0986)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [350/431]  eta: 0:01:29  lr: 0.000145  loss: 1.1174 (1.0999)  time: 1.1117  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [360/431]  eta: 0:01:18  lr: 0.000145  loss: 1.1191 (1.1006)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [370/431]  eta: 0:01:07  lr: 0.000145  loss: 1.0879 (1.1011)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [380/431]  eta: 0:00:56  lr: 0.000145  loss: 1.0879 (1.1011)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [390/431]  eta: 0:00:45  lr: 0.000145  loss: 1.0570 (1.1004)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [400/431]  eta: 0:00:34  lr: 0.000145  loss: 1.0244 (1.0998)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [410/431]  eta: 0:00:23  lr: 0.000145  loss: 1.0812 (1.1003)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:350]  [420/431]  eta: 0:00:12  lr: 0.000145  loss: 1.1118 (1.1011)  time: 1.0924  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:350]  [430/431]  eta: 0:00:01  lr: 0.000145  loss: 1.1038 (1.1016)  time: 1.0936  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:350] Total time: 0:07:56 (1.1062 s / it)\n",
      "Averaged stats: lr: 0.000145  loss: 1.1038 (1.1016)\n",
      "Valid: [epoch:350]  [ 0/14]  eta: 0:00:35  loss: 1.0930 (1.0930)  time: 2.5347  data: 2.3996  max mem: 15925\n",
      "Valid: [epoch:350]  [13/14]  eta: 0:00:00  loss: 1.0480 (1.0543)  time: 0.2757  data: 0.1715  max mem: 15925\n",
      "Valid: [epoch:350] Total time: 0:00:04 (0.2942 s / it)\n",
      "Averaged stats: loss: 1.0480 (1.0543)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_350_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.054%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:351]  [  0/431]  eta: 0:35:23  lr: 0.000144  loss: 1.2299 (1.2299)  time: 4.9275  data: 3.7793  max mem: 15925\n",
      "Train: [epoch:351]  [ 10/431]  eta: 0:09:49  lr: 0.000144  loss: 1.0295 (1.1086)  time: 1.4004  data: 0.3437  max mem: 15925\n",
      "Train: [epoch:351]  [ 20/431]  eta: 0:08:30  lr: 0.000144  loss: 1.0353 (1.1207)  time: 1.0588  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [ 30/431]  eta: 0:07:59  lr: 0.000144  loss: 1.0738 (1.1042)  time: 1.0830  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [ 40/431]  eta: 0:07:37  lr: 0.000144  loss: 1.1040 (1.1080)  time: 1.0915  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [ 50/431]  eta: 0:07:18  lr: 0.000144  loss: 1.1086 (1.1108)  time: 1.0821  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [ 60/431]  eta: 0:07:02  lr: 0.000144  loss: 1.0464 (1.1044)  time: 1.0794  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [ 70/431]  eta: 0:06:49  lr: 0.000144  loss: 1.0354 (1.1016)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [ 80/431]  eta: 0:06:37  lr: 0.000144  loss: 1.0732 (1.1073)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [ 90/431]  eta: 0:06:25  lr: 0.000144  loss: 1.0681 (1.1011)  time: 1.1141  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [100/431]  eta: 0:06:12  lr: 0.000144  loss: 1.0483 (1.0976)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [110/431]  eta: 0:06:00  lr: 0.000144  loss: 1.0501 (1.0954)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [120/431]  eta: 0:05:49  lr: 0.000144  loss: 1.0838 (1.0961)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [130/431]  eta: 0:05:37  lr: 0.000144  loss: 1.0924 (1.0956)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [140/431]  eta: 0:05:25  lr: 0.000144  loss: 1.0545 (1.0902)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [150/431]  eta: 0:05:14  lr: 0.000144  loss: 1.0396 (1.0877)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [160/431]  eta: 0:05:02  lr: 0.000144  loss: 1.0764 (1.0901)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [170/431]  eta: 0:04:51  lr: 0.000144  loss: 1.0636 (1.0878)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [180/431]  eta: 0:04:39  lr: 0.000144  loss: 1.0445 (1.0885)  time: 1.0915  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:351]  [190/431]  eta: 0:04:28  lr: 0.000144  loss: 1.0868 (1.0895)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [200/431]  eta: 0:04:17  lr: 0.000144  loss: 1.0868 (1.0906)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [210/431]  eta: 0:04:05  lr: 0.000144  loss: 1.0838 (1.0928)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [220/431]  eta: 0:03:54  lr: 0.000144  loss: 1.0756 (1.0924)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [230/431]  eta: 0:03:43  lr: 0.000144  loss: 1.0541 (1.0945)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [240/431]  eta: 0:03:32  lr: 0.000144  loss: 1.0541 (1.0955)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [250/431]  eta: 0:03:21  lr: 0.000144  loss: 1.0816 (1.0955)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [260/431]  eta: 0:03:09  lr: 0.000144  loss: 1.1180 (1.0975)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [270/431]  eta: 0:02:58  lr: 0.000144  loss: 1.1180 (1.0983)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [280/431]  eta: 0:02:47  lr: 0.000144  loss: 1.0761 (1.0975)  time: 1.1160  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [290/431]  eta: 0:02:36  lr: 0.000144  loss: 1.0761 (1.0976)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [300/431]  eta: 0:02:25  lr: 0.000144  loss: 1.1080 (1.0985)  time: 1.1009  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:351]  [310/431]  eta: 0:02:14  lr: 0.000144  loss: 1.1048 (1.0987)  time: 1.1095  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:351]  [320/431]  eta: 0:02:03  lr: 0.000144  loss: 1.0973 (1.0996)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [330/431]  eta: 0:01:52  lr: 0.000144  loss: 1.1066 (1.1001)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [340/431]  eta: 0:01:41  lr: 0.000144  loss: 1.1066 (1.1008)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [350/431]  eta: 0:01:29  lr: 0.000144  loss: 1.0806 (1.0996)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [360/431]  eta: 0:01:18  lr: 0.000144  loss: 1.0708 (1.0997)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [370/431]  eta: 0:01:07  lr: 0.000144  loss: 1.0708 (1.0993)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [380/431]  eta: 0:00:56  lr: 0.000144  loss: 1.0753 (1.0997)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [390/431]  eta: 0:00:45  lr: 0.000144  loss: 1.0470 (1.0991)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [400/431]  eta: 0:00:34  lr: 0.000144  loss: 1.0408 (1.0990)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:351]  [410/431]  eta: 0:00:23  lr: 0.000144  loss: 1.0657 (1.0995)  time: 1.0996  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:351]  [420/431]  eta: 0:00:12  lr: 0.000144  loss: 1.0779 (1.0996)  time: 1.1042  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:351]  [430/431]  eta: 0:00:01  lr: 0.000144  loss: 1.0779 (1.0998)  time: 1.1028  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:351] Total time: 0:07:58 (1.1090 s / it)\n",
      "Averaged stats: lr: 0.000144  loss: 1.0779 (1.0998)\n",
      "Valid: [epoch:351]  [ 0/14]  eta: 0:00:35  loss: 1.0241 (1.0241)  time: 2.5076  data: 2.3545  max mem: 15925\n",
      "Valid: [epoch:351]  [13/14]  eta: 0:00:00  loss: 1.0326 (1.0412)  time: 0.2620  data: 0.1683  max mem: 15925\n",
      "Valid: [epoch:351] Total time: 0:00:03 (0.2763 s / it)\n",
      "Averaged stats: loss: 1.0326 (1.0412)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_351_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:352]  [  0/431]  eta: 0:32:51  lr: 0.000144  loss: 1.0682 (1.0682)  time: 4.5737  data: 3.4117  max mem: 15925\n",
      "Train: [epoch:352]  [ 10/431]  eta: 0:09:31  lr: 0.000144  loss: 1.0828 (1.1484)  time: 1.3585  data: 0.3103  max mem: 15925\n",
      "Train: [epoch:352]  [ 20/431]  eta: 0:08:20  lr: 0.000144  loss: 1.0828 (1.1076)  time: 1.0504  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [ 30/431]  eta: 0:07:51  lr: 0.000144  loss: 1.0456 (1.0879)  time: 1.0735  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [ 40/431]  eta: 0:07:30  lr: 0.000144  loss: 1.0751 (1.0988)  time: 1.0845  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [ 50/431]  eta: 0:07:15  lr: 0.000144  loss: 1.0731 (1.0940)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [ 60/431]  eta: 0:07:01  lr: 0.000144  loss: 1.0351 (1.0776)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [ 70/431]  eta: 0:06:48  lr: 0.000144  loss: 1.0351 (1.0831)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [ 80/431]  eta: 0:06:36  lr: 0.000144  loss: 1.1172 (1.0922)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [ 90/431]  eta: 0:06:23  lr: 0.000144  loss: 1.1149 (1.0930)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [100/431]  eta: 0:06:11  lr: 0.000144  loss: 1.0487 (1.0875)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [110/431]  eta: 0:06:00  lr: 0.000144  loss: 1.0215 (1.0899)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [120/431]  eta: 0:05:48  lr: 0.000144  loss: 1.0535 (1.0885)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [130/431]  eta: 0:05:36  lr: 0.000144  loss: 1.0366 (1.0854)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [140/431]  eta: 0:05:25  lr: 0.000144  loss: 1.0366 (1.0839)  time: 1.0983  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [150/431]  eta: 0:05:13  lr: 0.000144  loss: 1.0436 (1.0837)  time: 1.0917  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [160/431]  eta: 0:05:02  lr: 0.000144  loss: 1.0395 (1.0830)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [170/431]  eta: 0:04:50  lr: 0.000144  loss: 1.0273 (1.0811)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [180/431]  eta: 0:04:39  lr: 0.000144  loss: 1.0350 (1.0799)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [190/431]  eta: 0:04:27  lr: 0.000144  loss: 1.0368 (1.0806)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [200/431]  eta: 0:04:16  lr: 0.000144  loss: 1.0534 (1.0812)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [210/431]  eta: 0:04:05  lr: 0.000144  loss: 1.0641 (1.0808)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [220/431]  eta: 0:03:54  lr: 0.000144  loss: 1.0479 (1.0794)  time: 1.0947  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [230/431]  eta: 0:03:42  lr: 0.000144  loss: 1.0474 (1.0823)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [240/431]  eta: 0:03:31  lr: 0.000144  loss: 1.1068 (1.0840)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [250/431]  eta: 0:03:20  lr: 0.000144  loss: 1.1068 (1.0858)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [260/431]  eta: 0:03:09  lr: 0.000144  loss: 1.0860 (1.0858)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [270/431]  eta: 0:02:58  lr: 0.000144  loss: 1.1402 (1.0892)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [280/431]  eta: 0:02:47  lr: 0.000144  loss: 1.1246 (1.0867)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [290/431]  eta: 0:02:36  lr: 0.000144  loss: 1.0183 (1.0883)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [300/431]  eta: 0:02:25  lr: 0.000144  loss: 1.1303 (1.0908)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [310/431]  eta: 0:02:14  lr: 0.000144  loss: 1.0874 (1.0907)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [320/431]  eta: 0:02:02  lr: 0.000144  loss: 1.0606 (1.0922)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [330/431]  eta: 0:01:51  lr: 0.000144  loss: 1.1027 (1.0951)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [340/431]  eta: 0:01:40  lr: 0.000144  loss: 1.1529 (1.0965)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [350/431]  eta: 0:01:29  lr: 0.000144  loss: 1.1036 (1.0985)  time: 1.0956  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:352]  [360/431]  eta: 0:01:18  lr: 0.000144  loss: 1.1582 (1.0999)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [370/431]  eta: 0:01:07  lr: 0.000144  loss: 1.1255 (1.0998)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [380/431]  eta: 0:00:56  lr: 0.000144  loss: 1.1174 (1.1006)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [390/431]  eta: 0:00:45  lr: 0.000144  loss: 1.0666 (1.1013)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [400/431]  eta: 0:00:34  lr: 0.000144  loss: 1.0843 (1.1020)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [410/431]  eta: 0:00:23  lr: 0.000144  loss: 1.1003 (1.1023)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:352]  [420/431]  eta: 0:00:12  lr: 0.000144  loss: 1.0665 (1.1025)  time: 1.1010  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:352]  [430/431]  eta: 0:00:01  lr: 0.000144  loss: 1.0555 (1.1015)  time: 1.1014  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:352] Total time: 0:07:56 (1.1063 s / it)\n",
      "Averaged stats: lr: 0.000144  loss: 1.0555 (1.1015)\n",
      "Valid: [epoch:352]  [ 0/14]  eta: 0:00:36  loss: 1.0086 (1.0086)  time: 2.5778  data: 2.4297  max mem: 15925\n",
      "Valid: [epoch:352]  [13/14]  eta: 0:00:00  loss: 1.0333 (1.0411)  time: 0.2666  data: 0.1736  max mem: 15925\n",
      "Valid: [epoch:352] Total time: 0:00:04 (0.2860 s / it)\n",
      "Averaged stats: loss: 1.0333 (1.0411)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_352_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:353]  [  0/431]  eta: 0:32:34  lr: 0.000144  loss: 1.1426 (1.1426)  time: 4.5344  data: 3.3112  max mem: 15925\n",
      "Train: [epoch:353]  [ 10/431]  eta: 0:09:30  lr: 0.000144  loss: 1.1280 (1.2093)  time: 1.3555  data: 0.3012  max mem: 15925\n",
      "Train: [epoch:353]  [ 20/431]  eta: 0:08:19  lr: 0.000144  loss: 1.1280 (1.1879)  time: 1.0498  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [ 30/431]  eta: 0:07:47  lr: 0.000144  loss: 1.0881 (1.1585)  time: 1.0621  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [ 40/431]  eta: 0:07:28  lr: 0.000144  loss: 1.0257 (1.1274)  time: 1.0751  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [ 50/431]  eta: 0:07:13  lr: 0.000144  loss: 1.0267 (1.1189)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [ 60/431]  eta: 0:06:59  lr: 0.000144  loss: 1.0588 (1.1155)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [ 70/431]  eta: 0:06:46  lr: 0.000144  loss: 1.0808 (1.1106)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [ 80/431]  eta: 0:06:34  lr: 0.000144  loss: 1.0812 (1.1136)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [ 90/431]  eta: 0:06:22  lr: 0.000144  loss: 1.0772 (1.1074)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [100/431]  eta: 0:06:10  lr: 0.000144  loss: 1.0212 (1.0996)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [110/431]  eta: 0:05:58  lr: 0.000144  loss: 1.0173 (1.1010)  time: 1.0961  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [120/431]  eta: 0:05:47  lr: 0.000144  loss: 1.0976 (1.1020)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [130/431]  eta: 0:05:35  lr: 0.000144  loss: 1.0705 (1.1027)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [140/431]  eta: 0:05:24  lr: 0.000144  loss: 1.0594 (1.0980)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [150/431]  eta: 0:05:12  lr: 0.000144  loss: 1.0679 (1.1017)  time: 1.1085  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [160/431]  eta: 0:05:01  lr: 0.000144  loss: 1.1222 (1.1042)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [170/431]  eta: 0:04:50  lr: 0.000144  loss: 1.1095 (1.1044)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [180/431]  eta: 0:04:39  lr: 0.000144  loss: 1.0841 (1.1043)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [190/431]  eta: 0:04:28  lr: 0.000144  loss: 1.1028 (1.1061)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [200/431]  eta: 0:04:16  lr: 0.000144  loss: 1.1008 (1.1035)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [210/431]  eta: 0:04:05  lr: 0.000144  loss: 1.0812 (1.1036)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [220/431]  eta: 0:03:54  lr: 0.000144  loss: 1.1070 (1.1033)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [230/431]  eta: 0:03:43  lr: 0.000144  loss: 1.0976 (1.1029)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [240/431]  eta: 0:03:32  lr: 0.000144  loss: 1.0624 (1.1011)  time: 1.1086  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [250/431]  eta: 0:03:21  lr: 0.000144  loss: 1.0684 (1.1021)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [260/431]  eta: 0:03:09  lr: 0.000144  loss: 1.0876 (1.1012)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [270/431]  eta: 0:02:58  lr: 0.000144  loss: 1.0402 (1.1008)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [280/431]  eta: 0:02:47  lr: 0.000144  loss: 1.0458 (1.1013)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [290/431]  eta: 0:02:36  lr: 0.000144  loss: 1.0955 (1.1007)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [300/431]  eta: 0:02:25  lr: 0.000144  loss: 1.0428 (1.0992)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [310/431]  eta: 0:02:14  lr: 0.000144  loss: 1.0788 (1.1007)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [320/431]  eta: 0:02:03  lr: 0.000144  loss: 1.0798 (1.1001)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [330/431]  eta: 0:01:52  lr: 0.000144  loss: 1.0728 (1.1014)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [340/431]  eta: 0:01:40  lr: 0.000144  loss: 1.1351 (1.1025)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [350/431]  eta: 0:01:29  lr: 0.000144  loss: 1.1519 (1.1043)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [360/431]  eta: 0:01:18  lr: 0.000144  loss: 1.1484 (1.1050)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [370/431]  eta: 0:01:07  lr: 0.000144  loss: 1.0968 (1.1050)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [380/431]  eta: 0:00:56  lr: 0.000144  loss: 1.0958 (1.1043)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [390/431]  eta: 0:00:45  lr: 0.000144  loss: 1.0603 (1.1036)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [400/431]  eta: 0:00:34  lr: 0.000144  loss: 1.0603 (1.1023)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [410/431]  eta: 0:00:23  lr: 0.000144  loss: 1.0547 (1.1016)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:353]  [420/431]  eta: 0:00:12  lr: 0.000144  loss: 1.0527 (1.1008)  time: 1.1046  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:353]  [430/431]  eta: 0:00:01  lr: 0.000144  loss: 1.0889 (1.1021)  time: 1.1080  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:353] Total time: 0:07:58 (1.1093 s / it)\n",
      "Averaged stats: lr: 0.000144  loss: 1.0889 (1.1021)\n",
      "Valid: [epoch:353]  [ 0/14]  eta: 0:00:34  loss: 1.0468 (1.0468)  time: 2.4754  data: 2.3022  max mem: 15925\n",
      "Valid: [epoch:353]  [13/14]  eta: 0:00:00  loss: 1.0468 (1.0523)  time: 0.2706  data: 0.1645  max mem: 15925\n",
      "Valid: [epoch:353] Total time: 0:00:04 (0.2875 s / it)\n",
      "Averaged stats: loss: 1.0468 (1.0523)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_353_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.052%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:354]  [  0/431]  eta: 0:35:10  lr: 0.000144  loss: 1.2072 (1.2072)  time: 4.8965  data: 3.7363  max mem: 15925\n",
      "Train: [epoch:354]  [ 10/431]  eta: 0:09:45  lr: 0.000144  loss: 1.0289 (1.0984)  time: 1.3909  data: 0.3398  max mem: 15925\n",
      "Train: [epoch:354]  [ 20/431]  eta: 0:08:27  lr: 0.000144  loss: 1.0616 (1.1321)  time: 1.0515  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [ 30/431]  eta: 0:07:53  lr: 0.000144  loss: 1.0906 (1.1087)  time: 1.0652  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:354]  [ 40/431]  eta: 0:07:32  lr: 0.000144  loss: 1.0661 (1.1039)  time: 1.0786  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [ 50/431]  eta: 0:07:16  lr: 0.000144  loss: 1.0449 (1.0987)  time: 1.0938  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [ 60/431]  eta: 0:07:02  lr: 0.000144  loss: 1.0748 (1.0969)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [ 70/431]  eta: 0:06:48  lr: 0.000144  loss: 1.1131 (1.1004)  time: 1.0936  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [ 80/431]  eta: 0:06:35  lr: 0.000144  loss: 1.1131 (1.1052)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [ 90/431]  eta: 0:06:23  lr: 0.000144  loss: 1.0986 (1.1020)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [100/431]  eta: 0:06:11  lr: 0.000144  loss: 1.0560 (1.0993)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [110/431]  eta: 0:05:59  lr: 0.000144  loss: 1.0514 (1.1015)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [120/431]  eta: 0:05:48  lr: 0.000144  loss: 1.0625 (1.0983)  time: 1.1095  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [130/431]  eta: 0:05:37  lr: 0.000144  loss: 1.0859 (1.1006)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [140/431]  eta: 0:05:25  lr: 0.000144  loss: 1.0410 (1.0967)  time: 1.1143  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [150/431]  eta: 0:05:14  lr: 0.000144  loss: 1.0410 (1.0966)  time: 1.1166  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [160/431]  eta: 0:05:03  lr: 0.000144  loss: 1.0738 (1.0968)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [170/431]  eta: 0:04:51  lr: 0.000144  loss: 1.0738 (1.0964)  time: 1.1103  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [180/431]  eta: 0:04:40  lr: 0.000144  loss: 1.0627 (1.0932)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [190/431]  eta: 0:04:29  lr: 0.000144  loss: 1.0525 (1.0941)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [200/431]  eta: 0:04:17  lr: 0.000144  loss: 1.1063 (1.0939)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [210/431]  eta: 0:04:06  lr: 0.000144  loss: 1.1063 (1.0955)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [220/431]  eta: 0:03:55  lr: 0.000144  loss: 1.0328 (1.0934)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [230/431]  eta: 0:03:44  lr: 0.000144  loss: 1.0895 (1.0941)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [240/431]  eta: 0:03:32  lr: 0.000144  loss: 1.1105 (1.0949)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [250/431]  eta: 0:03:21  lr: 0.000144  loss: 1.1370 (1.0960)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [260/431]  eta: 0:03:10  lr: 0.000144  loss: 1.1206 (1.0968)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [270/431]  eta: 0:02:59  lr: 0.000144  loss: 1.1058 (1.0971)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [280/431]  eta: 0:02:48  lr: 0.000144  loss: 1.0214 (1.0970)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [290/431]  eta: 0:02:36  lr: 0.000144  loss: 1.0745 (1.0973)  time: 1.1047  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [300/431]  eta: 0:02:25  lr: 0.000144  loss: 1.0951 (1.0984)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [310/431]  eta: 0:02:14  lr: 0.000144  loss: 1.0453 (1.0974)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [320/431]  eta: 0:02:03  lr: 0.000144  loss: 1.0453 (1.0985)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [330/431]  eta: 0:01:52  lr: 0.000144  loss: 1.1048 (1.0987)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [340/431]  eta: 0:01:41  lr: 0.000144  loss: 1.0977 (1.0992)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [350/431]  eta: 0:01:30  lr: 0.000144  loss: 1.0977 (1.1004)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [360/431]  eta: 0:01:18  lr: 0.000144  loss: 1.0708 (1.0990)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [370/431]  eta: 0:01:07  lr: 0.000144  loss: 1.0810 (1.0990)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [380/431]  eta: 0:00:56  lr: 0.000144  loss: 1.0810 (1.0995)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [390/431]  eta: 0:00:45  lr: 0.000144  loss: 1.0720 (1.0988)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:354]  [400/431]  eta: 0:00:34  lr: 0.000144  loss: 1.0720 (1.1001)  time: 1.1068  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:354]  [410/431]  eta: 0:00:23  lr: 0.000144  loss: 1.0828 (1.0994)  time: 1.1091  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:354]  [420/431]  eta: 0:00:12  lr: 0.000144  loss: 1.0806 (1.0983)  time: 1.1045  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:354]  [430/431]  eta: 0:00:01  lr: 0.000144  loss: 1.0903 (1.0986)  time: 1.0971  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:354] Total time: 0:07:58 (1.1104 s / it)\n",
      "Averaged stats: lr: 0.000144  loss: 1.0903 (1.0986)\n",
      "Valid: [epoch:354]  [ 0/14]  eta: 0:00:30  loss: 1.1284 (1.1284)  time: 2.1711  data: 1.9970  max mem: 15925\n",
      "Valid: [epoch:354]  [13/14]  eta: 0:00:00  loss: 1.0338 (1.0419)  time: 0.2399  data: 0.1427  max mem: 15925\n",
      "Valid: [epoch:354] Total time: 0:00:03 (0.2548 s / it)\n",
      "Averaged stats: loss: 1.0338 (1.0419)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_354_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.042%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:355]  [  0/431]  eta: 0:34:48  lr: 0.000144  loss: 1.1076 (1.1076)  time: 4.8458  data: 3.6952  max mem: 15925\n",
      "Train: [epoch:355]  [ 10/431]  eta: 0:09:45  lr: 0.000144  loss: 1.1058 (1.0769)  time: 1.3904  data: 0.3361  max mem: 15925\n",
      "Train: [epoch:355]  [ 20/431]  eta: 0:08:26  lr: 0.000144  loss: 1.1088 (1.1283)  time: 1.0514  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [ 30/431]  eta: 0:07:53  lr: 0.000144  loss: 1.0812 (1.1045)  time: 1.0671  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [ 40/431]  eta: 0:07:33  lr: 0.000144  loss: 1.0451 (1.1063)  time: 1.0816  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [ 50/431]  eta: 0:07:15  lr: 0.000144  loss: 1.0363 (1.0955)  time: 1.0832  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [ 60/431]  eta: 0:07:01  lr: 0.000144  loss: 1.0549 (1.0928)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [ 70/431]  eta: 0:06:48  lr: 0.000144  loss: 1.0800 (1.0968)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [ 80/431]  eta: 0:06:36  lr: 0.000144  loss: 1.0810 (1.0988)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [ 90/431]  eta: 0:06:24  lr: 0.000144  loss: 1.0676 (1.0913)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [100/431]  eta: 0:06:12  lr: 0.000144  loss: 1.0943 (1.0950)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [110/431]  eta: 0:06:00  lr: 0.000144  loss: 1.0945 (1.0922)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [120/431]  eta: 0:05:48  lr: 0.000144  loss: 1.0528 (1.0923)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [130/431]  eta: 0:05:37  lr: 0.000144  loss: 1.1096 (1.0992)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [140/431]  eta: 0:05:25  lr: 0.000144  loss: 1.1180 (1.0995)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [150/431]  eta: 0:05:14  lr: 0.000144  loss: 1.0909 (1.0981)  time: 1.1125  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [160/431]  eta: 0:05:02  lr: 0.000144  loss: 1.0911 (1.1024)  time: 1.0978  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [170/431]  eta: 0:04:51  lr: 0.000144  loss: 1.0961 (1.1000)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [180/431]  eta: 0:04:40  lr: 0.000144  loss: 1.0913 (1.0996)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [190/431]  eta: 0:04:28  lr: 0.000144  loss: 1.0913 (1.1027)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [200/431]  eta: 0:04:17  lr: 0.000144  loss: 1.0890 (1.1024)  time: 1.1060  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:355]  [210/431]  eta: 0:04:06  lr: 0.000144  loss: 1.0890 (1.1021)  time: 1.1093  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [220/431]  eta: 0:03:54  lr: 0.000144  loss: 1.0530 (1.1031)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [230/431]  eta: 0:03:43  lr: 0.000144  loss: 1.0687 (1.1013)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [240/431]  eta: 0:03:32  lr: 0.000144  loss: 1.0492 (1.0992)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [250/431]  eta: 0:03:21  lr: 0.000144  loss: 1.0654 (1.1000)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [260/431]  eta: 0:03:09  lr: 0.000144  loss: 1.1100 (1.1009)  time: 1.0887  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [270/431]  eta: 0:02:58  lr: 0.000144  loss: 1.1135 (1.1016)  time: 1.0888  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [280/431]  eta: 0:02:47  lr: 0.000144  loss: 1.1085 (1.1014)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [290/431]  eta: 0:02:36  lr: 0.000144  loss: 1.0252 (1.1000)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [300/431]  eta: 0:02:25  lr: 0.000144  loss: 1.1046 (1.1041)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [310/431]  eta: 0:02:14  lr: 0.000144  loss: 1.1304 (1.1020)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [320/431]  eta: 0:02:03  lr: 0.000144  loss: 1.0209 (1.1027)  time: 1.1051  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [330/431]  eta: 0:01:52  lr: 0.000144  loss: 1.0951 (1.1036)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [340/431]  eta: 0:01:40  lr: 0.000144  loss: 1.1093 (1.1033)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [350/431]  eta: 0:01:29  lr: 0.000144  loss: 1.0882 (1.1028)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [360/431]  eta: 0:01:18  lr: 0.000144  loss: 1.0758 (1.1047)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [370/431]  eta: 0:01:07  lr: 0.000144  loss: 1.0712 (1.1042)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [380/431]  eta: 0:00:56  lr: 0.000144  loss: 1.0477 (1.1040)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [390/431]  eta: 0:00:45  lr: 0.000144  loss: 1.0417 (1.1016)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [400/431]  eta: 0:00:34  lr: 0.000144  loss: 1.0495 (1.1020)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [410/431]  eta: 0:00:23  lr: 0.000144  loss: 1.0407 (1.1009)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:355]  [420/431]  eta: 0:00:12  lr: 0.000144  loss: 1.0244 (1.1000)  time: 1.1014  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:355]  [430/431]  eta: 0:00:01  lr: 0.000144  loss: 1.0968 (1.1005)  time: 1.0923  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:355] Total time: 0:07:57 (1.1080 s / it)\n",
      "Averaged stats: lr: 0.000144  loss: 1.0968 (1.1005)\n",
      "Valid: [epoch:355]  [ 0/14]  eta: 0:00:36  loss: 1.0369 (1.0369)  time: 2.5996  data: 2.4682  max mem: 15925\n",
      "Valid: [epoch:355]  [13/14]  eta: 0:00:00  loss: 1.0369 (1.0457)  time: 0.2695  data: 0.1764  max mem: 15925\n",
      "Valid: [epoch:355] Total time: 0:00:04 (0.2890 s / it)\n",
      "Averaged stats: loss: 1.0369 (1.0457)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_355_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:356]  [  0/431]  eta: 0:33:11  lr: 0.000143  loss: 1.0753 (1.0753)  time: 4.6209  data: 3.4454  max mem: 15925\n",
      "Train: [epoch:356]  [ 10/431]  eta: 0:09:26  lr: 0.000143  loss: 1.1468 (1.1782)  time: 1.3458  data: 0.3134  max mem: 15925\n",
      "Train: [epoch:356]  [ 20/431]  eta: 0:08:18  lr: 0.000143  loss: 1.0986 (1.1218)  time: 1.0426  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [ 30/431]  eta: 0:07:49  lr: 0.000143  loss: 1.0497 (1.1077)  time: 1.0733  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [ 40/431]  eta: 0:07:30  lr: 0.000143  loss: 1.0468 (1.1066)  time: 1.0858  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [ 50/431]  eta: 0:07:14  lr: 0.000143  loss: 1.0859 (1.1179)  time: 1.0913  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [ 60/431]  eta: 0:07:00  lr: 0.000143  loss: 1.0429 (1.1093)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [ 70/431]  eta: 0:06:48  lr: 0.000143  loss: 1.0346 (1.1195)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [ 80/431]  eta: 0:06:35  lr: 0.000143  loss: 1.0703 (1.1146)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [ 90/431]  eta: 0:06:23  lr: 0.000143  loss: 1.0675 (1.1091)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [100/431]  eta: 0:06:11  lr: 0.000143  loss: 1.0675 (1.1052)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [110/431]  eta: 0:05:59  lr: 0.000143  loss: 1.0333 (1.0998)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [120/431]  eta: 0:05:47  lr: 0.000143  loss: 1.0333 (1.0975)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [130/431]  eta: 0:05:36  lr: 0.000143  loss: 1.0497 (1.0992)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [140/431]  eta: 0:05:25  lr: 0.000143  loss: 1.0728 (1.1008)  time: 1.1155  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [150/431]  eta: 0:05:14  lr: 0.000143  loss: 1.1101 (1.1015)  time: 1.1084  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [160/431]  eta: 0:05:02  lr: 0.000143  loss: 1.1101 (1.1042)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [170/431]  eta: 0:04:51  lr: 0.000143  loss: 1.0978 (1.1029)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [180/431]  eta: 0:04:39  lr: 0.000143  loss: 1.1267 (1.1075)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [190/431]  eta: 0:04:28  lr: 0.000143  loss: 1.1796 (1.1089)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [200/431]  eta: 0:04:17  lr: 0.000143  loss: 1.0560 (1.1061)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [210/431]  eta: 0:04:05  lr: 0.000143  loss: 1.0242 (1.1031)  time: 1.1034  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [220/431]  eta: 0:03:54  lr: 0.000143  loss: 1.0242 (1.1019)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [230/431]  eta: 0:03:43  lr: 0.000143  loss: 1.0821 (1.1022)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [240/431]  eta: 0:03:32  lr: 0.000143  loss: 1.1182 (1.1038)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [250/431]  eta: 0:03:21  lr: 0.000143  loss: 1.0673 (1.1036)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [260/431]  eta: 0:03:10  lr: 0.000143  loss: 1.0323 (1.1023)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [270/431]  eta: 0:02:58  lr: 0.000143  loss: 1.0370 (1.1026)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [280/431]  eta: 0:02:47  lr: 0.000143  loss: 1.0494 (1.1011)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [290/431]  eta: 0:02:36  lr: 0.000143  loss: 1.0494 (1.0998)  time: 1.1090  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [300/431]  eta: 0:02:25  lr: 0.000143  loss: 1.0676 (1.0995)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [310/431]  eta: 0:02:14  lr: 0.000143  loss: 1.0676 (1.0984)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [320/431]  eta: 0:02:03  lr: 0.000143  loss: 1.0756 (1.0985)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [330/431]  eta: 0:01:52  lr: 0.000143  loss: 1.1186 (1.1014)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [340/431]  eta: 0:01:40  lr: 0.000143  loss: 1.1323 (1.1010)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [350/431]  eta: 0:01:29  lr: 0.000143  loss: 1.0816 (1.1014)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [360/431]  eta: 0:01:18  lr: 0.000143  loss: 1.0810 (1.1004)  time: 1.1068  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [370/431]  eta: 0:01:07  lr: 0.000143  loss: 1.0901 (1.0999)  time: 1.1132  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:356]  [380/431]  eta: 0:00:56  lr: 0.000143  loss: 1.0550 (1.0987)  time: 1.1153  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [390/431]  eta: 0:00:45  lr: 0.000143  loss: 1.0319 (1.0981)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [400/431]  eta: 0:00:34  lr: 0.000143  loss: 1.0278 (1.0974)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [410/431]  eta: 0:00:23  lr: 0.000143  loss: 1.0945 (1.0978)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:356]  [420/431]  eta: 0:00:12  lr: 0.000143  loss: 1.1087 (1.0980)  time: 1.0964  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:356]  [430/431]  eta: 0:00:01  lr: 0.000143  loss: 1.0860 (1.0977)  time: 1.1009  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:356] Total time: 0:07:58 (1.1095 s / it)\n",
      "Averaged stats: lr: 0.000143  loss: 1.0860 (1.0977)\n",
      "Valid: [epoch:356]  [ 0/14]  eta: 0:00:36  loss: 1.0160 (1.0160)  time: 2.6412  data: 2.4528  max mem: 15925\n",
      "Valid: [epoch:356]  [13/14]  eta: 0:00:00  loss: 1.0417 (1.0467)  time: 0.2856  data: 0.1753  max mem: 15925\n",
      "Valid: [epoch:356] Total time: 0:00:04 (0.3028 s / it)\n",
      "Averaged stats: loss: 1.0417 (1.0467)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_356_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:357]  [  0/431]  eta: 0:33:54  lr: 0.000143  loss: 0.9809 (0.9809)  time: 4.7199  data: 3.5901  max mem: 15925\n",
      "Train: [epoch:357]  [ 10/431]  eta: 0:09:29  lr: 0.000143  loss: 1.1368 (1.1271)  time: 1.3531  data: 0.3266  max mem: 15925\n",
      "Train: [epoch:357]  [ 20/431]  eta: 0:08:18  lr: 0.000143  loss: 1.1166 (1.1215)  time: 1.0378  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [ 30/431]  eta: 0:07:48  lr: 0.000143  loss: 1.0877 (1.1041)  time: 1.0656  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [ 40/431]  eta: 0:07:28  lr: 0.000143  loss: 1.0515 (1.0939)  time: 1.0790  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [ 50/431]  eta: 0:07:14  lr: 0.000143  loss: 1.0525 (1.0885)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [ 60/431]  eta: 0:07:01  lr: 0.000143  loss: 1.0362 (1.0889)  time: 1.1149  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [ 70/431]  eta: 0:06:49  lr: 0.000143  loss: 1.0292 (1.0892)  time: 1.1173  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [ 80/431]  eta: 0:06:36  lr: 0.000143  loss: 1.0932 (1.0920)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [ 90/431]  eta: 0:06:24  lr: 0.000143  loss: 1.0250 (1.0860)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [100/431]  eta: 0:06:12  lr: 0.000143  loss: 1.0169 (1.0845)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [110/431]  eta: 0:06:01  lr: 0.000143  loss: 1.0738 (1.0851)  time: 1.1101  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [120/431]  eta: 0:05:49  lr: 0.000143  loss: 1.0311 (1.0826)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [130/431]  eta: 0:05:37  lr: 0.000143  loss: 1.0619 (1.0840)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [140/431]  eta: 0:05:25  lr: 0.000143  loss: 1.0471 (1.0821)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [150/431]  eta: 0:05:14  lr: 0.000143  loss: 1.0514 (1.0849)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [160/431]  eta: 0:05:02  lr: 0.000143  loss: 1.0514 (1.0830)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [170/431]  eta: 0:04:51  lr: 0.000143  loss: 1.0384 (1.0837)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [180/431]  eta: 0:04:39  lr: 0.000143  loss: 1.0384 (1.0830)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [190/431]  eta: 0:04:28  lr: 0.000143  loss: 1.0858 (1.0892)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [200/431]  eta: 0:04:17  lr: 0.000143  loss: 1.1267 (1.0908)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [210/431]  eta: 0:04:05  lr: 0.000143  loss: 1.0677 (1.0903)  time: 1.0949  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [220/431]  eta: 0:03:54  lr: 0.000143  loss: 1.0677 (1.0912)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [230/431]  eta: 0:03:43  lr: 0.000143  loss: 1.0715 (1.0918)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [240/431]  eta: 0:03:32  lr: 0.000143  loss: 1.0820 (1.0931)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [250/431]  eta: 0:03:21  lr: 0.000143  loss: 1.1519 (1.0962)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [260/431]  eta: 0:03:09  lr: 0.000143  loss: 1.1519 (1.0963)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [270/431]  eta: 0:02:58  lr: 0.000143  loss: 1.1374 (1.0975)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [280/431]  eta: 0:02:47  lr: 0.000143  loss: 1.0790 (1.0955)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [290/431]  eta: 0:02:36  lr: 0.000143  loss: 1.0619 (1.0950)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [300/431]  eta: 0:02:25  lr: 0.000143  loss: 1.1070 (1.0965)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [310/431]  eta: 0:02:14  lr: 0.000143  loss: 1.1309 (1.0971)  time: 1.1119  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [320/431]  eta: 0:02:03  lr: 0.000143  loss: 1.0638 (1.0947)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [330/431]  eta: 0:01:52  lr: 0.000143  loss: 1.0849 (1.0963)  time: 1.1116  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [340/431]  eta: 0:01:41  lr: 0.000143  loss: 1.0866 (1.0967)  time: 1.1106  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [350/431]  eta: 0:01:29  lr: 0.000143  loss: 1.0778 (1.0973)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [360/431]  eta: 0:01:18  lr: 0.000143  loss: 1.1245 (1.0975)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [370/431]  eta: 0:01:07  lr: 0.000143  loss: 1.1011 (1.0983)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [380/431]  eta: 0:00:56  lr: 0.000143  loss: 1.1329 (1.1002)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [390/431]  eta: 0:00:45  lr: 0.000143  loss: 1.1266 (1.1000)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [400/431]  eta: 0:00:34  lr: 0.000143  loss: 1.0944 (1.1011)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [410/431]  eta: 0:00:23  lr: 0.000143  loss: 1.0983 (1.1012)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:357]  [420/431]  eta: 0:00:12  lr: 0.000143  loss: 1.0753 (1.1008)  time: 1.0985  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:357]  [430/431]  eta: 0:00:01  lr: 0.000143  loss: 1.0648 (1.1002)  time: 1.1063  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:357] Total time: 0:07:57 (1.1090 s / it)\n",
      "Averaged stats: lr: 0.000143  loss: 1.0648 (1.1002)\n",
      "Valid: [epoch:357]  [ 0/14]  eta: 0:00:36  loss: 1.0952 (1.0952)  time: 2.6081  data: 2.4572  max mem: 15925\n",
      "Valid: [epoch:357]  [13/14]  eta: 0:00:00  loss: 1.0389 (1.0465)  time: 0.2764  data: 0.1756  max mem: 15925\n",
      "Valid: [epoch:357] Total time: 0:00:04 (0.2944 s / it)\n",
      "Averaged stats: loss: 1.0389 (1.0465)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_357_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:358]  [  0/431]  eta: 0:32:52  lr: 0.000143  loss: 1.0556 (1.0556)  time: 4.5765  data: 3.4423  max mem: 15925\n",
      "Train: [epoch:358]  [ 10/431]  eta: 0:09:33  lr: 0.000143  loss: 1.1319 (1.1322)  time: 1.3615  data: 0.3131  max mem: 15925\n",
      "Train: [epoch:358]  [ 20/431]  eta: 0:08:20  lr: 0.000143  loss: 1.1290 (1.1211)  time: 1.0509  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [ 30/431]  eta: 0:07:48  lr: 0.000143  loss: 1.0420 (1.0959)  time: 1.0629  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [ 40/431]  eta: 0:07:30  lr: 0.000143  loss: 1.0527 (1.0908)  time: 1.0800  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [ 50/431]  eta: 0:07:15  lr: 0.000143  loss: 1.0631 (1.0852)  time: 1.1010  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:358]  [ 60/431]  eta: 0:07:01  lr: 0.000143  loss: 1.0526 (1.0884)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [ 70/431]  eta: 0:06:48  lr: 0.000143  loss: 1.0998 (1.0975)  time: 1.1088  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [ 80/431]  eta: 0:06:36  lr: 0.000143  loss: 1.1305 (1.1036)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [ 90/431]  eta: 0:06:24  lr: 0.000143  loss: 1.1219 (1.1011)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [100/431]  eta: 0:06:12  lr: 0.000143  loss: 1.0397 (1.0996)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [110/431]  eta: 0:06:00  lr: 0.000143  loss: 1.0510 (1.0961)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [120/431]  eta: 0:05:48  lr: 0.000143  loss: 1.0461 (1.0942)  time: 1.1023  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [130/431]  eta: 0:05:36  lr: 0.000143  loss: 1.0453 (1.0932)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [140/431]  eta: 0:05:25  lr: 0.000143  loss: 1.0461 (1.0913)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [150/431]  eta: 0:05:13  lr: 0.000143  loss: 1.0532 (1.0913)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [160/431]  eta: 0:05:02  lr: 0.000143  loss: 1.0576 (1.0920)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [170/431]  eta: 0:04:51  lr: 0.000143  loss: 1.0853 (1.0924)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [180/431]  eta: 0:04:40  lr: 0.000143  loss: 1.0853 (1.0953)  time: 1.1123  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [190/431]  eta: 0:04:28  lr: 0.000143  loss: 1.0941 (1.0971)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [200/431]  eta: 0:04:17  lr: 0.000143  loss: 1.0869 (1.0973)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [210/431]  eta: 0:04:06  lr: 0.000143  loss: 1.0853 (1.0976)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [220/431]  eta: 0:03:54  lr: 0.000143  loss: 1.0639 (1.0965)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [230/431]  eta: 0:03:43  lr: 0.000143  loss: 1.0916 (1.0981)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [240/431]  eta: 0:03:32  lr: 0.000143  loss: 1.1459 (1.1009)  time: 1.1098  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [250/431]  eta: 0:03:21  lr: 0.000143  loss: 1.1108 (1.1005)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [260/431]  eta: 0:03:10  lr: 0.000143  loss: 1.0512 (1.1001)  time: 1.1122  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [270/431]  eta: 0:02:59  lr: 0.000143  loss: 1.0778 (1.1003)  time: 1.1105  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [280/431]  eta: 0:02:48  lr: 0.000143  loss: 1.0932 (1.1007)  time: 1.1091  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [290/431]  eta: 0:02:36  lr: 0.000143  loss: 1.0830 (1.0997)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [300/431]  eta: 0:02:25  lr: 0.000143  loss: 1.0863 (1.1004)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [310/431]  eta: 0:02:14  lr: 0.000143  loss: 1.0977 (1.1005)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [320/431]  eta: 0:02:03  lr: 0.000143  loss: 1.1087 (1.1022)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [330/431]  eta: 0:01:52  lr: 0.000143  loss: 1.0848 (1.1017)  time: 1.1004  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [340/431]  eta: 0:01:41  lr: 0.000143  loss: 1.1011 (1.1027)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [350/431]  eta: 0:01:29  lr: 0.000143  loss: 1.0772 (1.1015)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [360/431]  eta: 0:01:18  lr: 0.000143  loss: 1.0724 (1.1027)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [370/431]  eta: 0:01:07  lr: 0.000143  loss: 1.0947 (1.1031)  time: 1.1083  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [380/431]  eta: 0:00:56  lr: 0.000143  loss: 1.0721 (1.1026)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [390/431]  eta: 0:00:45  lr: 0.000143  loss: 1.0575 (1.1021)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [400/431]  eta: 0:00:34  lr: 0.000143  loss: 1.0299 (1.1005)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [410/431]  eta: 0:00:23  lr: 0.000143  loss: 1.0459 (1.1002)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:358]  [420/431]  eta: 0:00:12  lr: 0.000143  loss: 1.0198 (1.0989)  time: 1.1051  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:358]  [430/431]  eta: 0:00:01  lr: 0.000143  loss: 1.0352 (1.0989)  time: 1.1039  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:358] Total time: 0:07:58 (1.1103 s / it)\n",
      "Averaged stats: lr: 0.000143  loss: 1.0352 (1.0989)\n",
      "Valid: [epoch:358]  [ 0/14]  eta: 0:00:36  loss: 0.9336 (0.9336)  time: 2.6046  data: 2.4621  max mem: 15925\n",
      "Valid: [epoch:358]  [13/14]  eta: 0:00:00  loss: 1.0369 (1.0441)  time: 0.2703  data: 0.1759  max mem: 15925\n",
      "Valid: [epoch:358] Total time: 0:00:03 (0.2850 s / it)\n",
      "Averaged stats: loss: 1.0369 (1.0441)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_358_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.038\n",
      "Best Epoch: 292.000\n",
      "Train: [epoch:359]  [  0/431]  eta: 0:35:16  lr: 0.000143  loss: 0.9981 (0.9981)  time: 4.9115  data: 3.7369  max mem: 15925\n",
      "Train: [epoch:359]  [ 10/431]  eta: 0:09:43  lr: 0.000143  loss: 1.1760 (1.1554)  time: 1.3851  data: 0.3399  max mem: 15925\n",
      "Train: [epoch:359]  [ 20/431]  eta: 0:08:27  lr: 0.000143  loss: 1.1318 (1.1429)  time: 1.0513  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:359]  [ 30/431]  eta: 0:07:55  lr: 0.000143  loss: 1.0910 (1.1149)  time: 1.0766  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:359]  [ 40/431]  eta: 0:07:34  lr: 0.000143  loss: 1.0712 (1.1096)  time: 1.0871  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [ 50/431]  eta: 0:07:17  lr: 0.000143  loss: 1.0903 (1.1110)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [ 60/431]  eta: 0:07:03  lr: 0.000143  loss: 1.0449 (1.1045)  time: 1.0941  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [ 70/431]  eta: 0:06:50  lr: 0.000143  loss: 1.0662 (1.1056)  time: 1.1062  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [ 80/431]  eta: 0:06:37  lr: 0.000143  loss: 1.0726 (1.0992)  time: 1.1099  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [ 90/431]  eta: 0:06:25  lr: 0.000143  loss: 1.0429 (1.0930)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [100/431]  eta: 0:06:12  lr: 0.000143  loss: 1.0294 (1.0891)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [110/431]  eta: 0:06:00  lr: 0.000143  loss: 1.0089 (1.0856)  time: 1.0923  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [120/431]  eta: 0:05:48  lr: 0.000143  loss: 0.9956 (1.0801)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [130/431]  eta: 0:05:36  lr: 0.000143  loss: 1.0622 (1.0893)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [140/431]  eta: 0:05:25  lr: 0.000143  loss: 1.1499 (1.0878)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [150/431]  eta: 0:05:13  lr: 0.000143  loss: 1.0834 (1.0911)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [160/431]  eta: 0:05:02  lr: 0.000143  loss: 1.0934 (1.0931)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [170/431]  eta: 0:04:50  lr: 0.000143  loss: 1.1226 (1.0981)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [180/431]  eta: 0:04:39  lr: 0.000143  loss: 1.1461 (1.1021)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [190/431]  eta: 0:04:28  lr: 0.000143  loss: 1.1442 (1.1053)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [200/431]  eta: 0:04:16  lr: 0.000143  loss: 1.1141 (1.1042)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [210/431]  eta: 0:04:05  lr: 0.000143  loss: 1.0489 (1.1041)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [220/431]  eta: 0:03:54  lr: 0.000143  loss: 1.0757 (1.1032)  time: 1.0951  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:359]  [230/431]  eta: 0:03:42  lr: 0.000143  loss: 1.0757 (1.1031)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [240/431]  eta: 0:03:31  lr: 0.000143  loss: 1.0680 (1.1026)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [250/431]  eta: 0:03:20  lr: 0.000143  loss: 1.0443 (1.0997)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [260/431]  eta: 0:03:09  lr: 0.000143  loss: 1.0789 (1.1017)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [270/431]  eta: 0:02:58  lr: 0.000143  loss: 1.0827 (1.1005)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [280/431]  eta: 0:02:47  lr: 0.000143  loss: 1.0645 (1.1005)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [290/431]  eta: 0:02:36  lr: 0.000143  loss: 1.0904 (1.1004)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [300/431]  eta: 0:02:25  lr: 0.000143  loss: 1.0712 (1.0997)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [310/431]  eta: 0:02:13  lr: 0.000143  loss: 1.0712 (1.0998)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [320/431]  eta: 0:02:02  lr: 0.000143  loss: 1.0975 (1.0999)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [330/431]  eta: 0:01:51  lr: 0.000143  loss: 1.0979 (1.0999)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [340/431]  eta: 0:01:40  lr: 0.000143  loss: 1.1047 (1.1015)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [350/431]  eta: 0:01:29  lr: 0.000143  loss: 1.1708 (1.1033)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [360/431]  eta: 0:01:18  lr: 0.000143  loss: 1.1091 (1.1036)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [370/431]  eta: 0:01:07  lr: 0.000143  loss: 1.1091 (1.1040)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [380/431]  eta: 0:00:56  lr: 0.000143  loss: 1.0855 (1.1028)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [390/431]  eta: 0:00:45  lr: 0.000143  loss: 1.0416 (1.1015)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [400/431]  eta: 0:00:34  lr: 0.000143  loss: 1.0403 (1.1015)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [410/431]  eta: 0:00:23  lr: 0.000143  loss: 1.0370 (1.1008)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:359]  [420/431]  eta: 0:00:12  lr: 0.000143  loss: 1.0505 (1.1013)  time: 1.1037  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:359]  [430/431]  eta: 0:00:01  lr: 0.000143  loss: 1.0343 (1.0995)  time: 1.1022  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:359] Total time: 0:07:56 (1.1057 s / it)\n",
      "Averaged stats: lr: 0.000143  loss: 1.0343 (1.0995)\n",
      "Valid: [epoch:359]  [ 0/14]  eta: 0:00:35  loss: 1.0873 (1.0873)  time: 2.5595  data: 2.4268  max mem: 15925\n",
      "Valid: [epoch:359]  [13/14]  eta: 0:00:00  loss: 1.0244 (1.0358)  time: 0.2643  data: 0.1734  max mem: 15925\n",
      "Valid: [epoch:359] Total time: 0:00:03 (0.2790 s / it)\n",
      "Averaged stats: loss: 1.0244 (1.0358)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_359_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.036%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:360]  [  0/431]  eta: 0:34:06  lr: 0.000142  loss: 1.0171 (1.0171)  time: 4.7478  data: 3.6619  max mem: 15925\n",
      "Train: [epoch:360]  [ 10/431]  eta: 0:09:38  lr: 0.000142  loss: 1.0874 (1.0949)  time: 1.3748  data: 0.3331  max mem: 15925\n",
      "Train: [epoch:360]  [ 20/431]  eta: 0:08:22  lr: 0.000142  loss: 1.0706 (1.0914)  time: 1.0469  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [ 30/431]  eta: 0:07:49  lr: 0.000142  loss: 1.0618 (1.0910)  time: 1.0600  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [ 40/431]  eta: 0:07:29  lr: 0.000142  loss: 1.0449 (1.0729)  time: 1.0745  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [ 50/431]  eta: 0:07:15  lr: 0.000142  loss: 1.0551 (1.0802)  time: 1.0969  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [ 60/431]  eta: 0:07:00  lr: 0.000142  loss: 1.0622 (1.0775)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [ 70/431]  eta: 0:06:47  lr: 0.000142  loss: 1.0361 (1.0762)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [ 80/431]  eta: 0:06:35  lr: 0.000142  loss: 1.0764 (1.0799)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [ 90/431]  eta: 0:06:23  lr: 0.000142  loss: 1.0764 (1.0796)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [100/431]  eta: 0:06:11  lr: 0.000142  loss: 1.0861 (1.0861)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [110/431]  eta: 0:05:59  lr: 0.000142  loss: 1.0722 (1.0844)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [120/431]  eta: 0:05:48  lr: 0.000142  loss: 1.0574 (1.0837)  time: 1.1109  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [130/431]  eta: 0:05:36  lr: 0.000142  loss: 1.0704 (1.0848)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [140/431]  eta: 0:05:25  lr: 0.000142  loss: 1.0628 (1.0859)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [150/431]  eta: 0:05:13  lr: 0.000142  loss: 1.0429 (1.0828)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [160/431]  eta: 0:05:02  lr: 0.000142  loss: 1.0319 (1.0850)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [170/431]  eta: 0:04:51  lr: 0.000142  loss: 1.0791 (1.0850)  time: 1.1150  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [180/431]  eta: 0:04:39  lr: 0.000142  loss: 1.0790 (1.0831)  time: 1.1096  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [190/431]  eta: 0:04:28  lr: 0.000142  loss: 1.0790 (1.0866)  time: 1.1017  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [200/431]  eta: 0:04:17  lr: 0.000142  loss: 1.0499 (1.0841)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [210/431]  eta: 0:04:05  lr: 0.000142  loss: 1.0431 (1.0827)  time: 1.0940  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [220/431]  eta: 0:03:54  lr: 0.000142  loss: 1.0445 (1.0853)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [230/431]  eta: 0:03:43  lr: 0.000142  loss: 1.0682 (1.0833)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [240/431]  eta: 0:03:32  lr: 0.000142  loss: 1.0908 (1.0854)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [250/431]  eta: 0:03:21  lr: 0.000142  loss: 1.1386 (1.0861)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [260/431]  eta: 0:03:09  lr: 0.000142  loss: 1.0717 (1.0856)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [270/431]  eta: 0:02:58  lr: 0.000142  loss: 1.0594 (1.0856)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [280/431]  eta: 0:02:47  lr: 0.000142  loss: 1.0684 (1.0866)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [290/431]  eta: 0:02:36  lr: 0.000142  loss: 1.1202 (1.0890)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [300/431]  eta: 0:02:25  lr: 0.000142  loss: 1.1212 (1.0927)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [310/431]  eta: 0:02:14  lr: 0.000142  loss: 1.1234 (1.0937)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [320/431]  eta: 0:02:03  lr: 0.000142  loss: 1.1407 (1.0959)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [330/431]  eta: 0:01:51  lr: 0.000142  loss: 1.0777 (1.0966)  time: 1.0954  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [340/431]  eta: 0:01:40  lr: 0.000142  loss: 1.0661 (1.0980)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [350/431]  eta: 0:01:29  lr: 0.000142  loss: 1.1162 (1.0994)  time: 1.0918  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:360]  [360/431]  eta: 0:01:18  lr: 0.000142  loss: 1.1110 (1.0996)  time: 1.0955  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:360]  [370/431]  eta: 0:01:07  lr: 0.000142  loss: 1.0588 (1.0983)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [380/431]  eta: 0:00:56  lr: 0.000142  loss: 1.0481 (1.0976)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [390/431]  eta: 0:00:45  lr: 0.000142  loss: 1.0808 (1.0994)  time: 1.0947  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:360]  [400/431]  eta: 0:00:34  lr: 0.000142  loss: 1.0854 (1.0988)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:360]  [410/431]  eta: 0:00:23  lr: 0.000142  loss: 1.1085 (1.1002)  time: 1.0931  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:360]  [420/431]  eta: 0:00:12  lr: 0.000142  loss: 1.0914 (1.0993)  time: 1.1033  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:360]  [430/431]  eta: 0:00:01  lr: 0.000142  loss: 1.0592 (1.0999)  time: 1.1102  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:360] Total time: 0:07:57 (1.1068 s / it)\n",
      "Averaged stats: lr: 0.000142  loss: 1.0592 (1.0999)\n",
      "Valid: [epoch:360]  [ 0/14]  eta: 0:00:37  loss: 1.0867 (1.0867)  time: 2.6437  data: 2.5443  max mem: 15925\n",
      "Valid: [epoch:360]  [13/14]  eta: 0:00:00  loss: 1.0346 (1.0434)  time: 0.2673  data: 0.1818  max mem: 15925\n",
      "Valid: [epoch:360] Total time: 0:00:03 (0.2818 s / it)\n",
      "Averaged stats: loss: 1.0346 (1.0434)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_360_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.043%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:361]  [  0/431]  eta: 0:35:24  lr: 0.000142  loss: 1.3277 (1.3277)  time: 4.9294  data: 3.5959  max mem: 15925\n",
      "Train: [epoch:361]  [ 10/431]  eta: 0:09:41  lr: 0.000142  loss: 1.1263 (1.1757)  time: 1.3816  data: 0.3271  max mem: 15925\n",
      "Train: [epoch:361]  [ 20/431]  eta: 0:08:23  lr: 0.000142  loss: 1.1263 (1.1804)  time: 1.0407  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [ 30/431]  eta: 0:07:51  lr: 0.000142  loss: 1.1074 (1.1522)  time: 1.0645  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [ 40/431]  eta: 0:07:31  lr: 0.000142  loss: 1.0541 (1.1251)  time: 1.0791  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [ 50/431]  eta: 0:07:15  lr: 0.000142  loss: 1.0524 (1.1164)  time: 1.0894  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [ 60/431]  eta: 0:07:01  lr: 0.000142  loss: 1.0780 (1.1150)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [ 70/431]  eta: 0:06:48  lr: 0.000142  loss: 1.0587 (1.1060)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [ 80/431]  eta: 0:06:35  lr: 0.000142  loss: 1.1119 (1.1147)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [ 90/431]  eta: 0:06:23  lr: 0.000142  loss: 1.1640 (1.1164)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [100/431]  eta: 0:06:12  lr: 0.000142  loss: 1.1296 (1.1145)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [110/431]  eta: 0:06:00  lr: 0.000142  loss: 1.0771 (1.1109)  time: 1.1132  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [120/431]  eta: 0:05:48  lr: 0.000142  loss: 1.0722 (1.1077)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [130/431]  eta: 0:05:37  lr: 0.000142  loss: 1.0770 (1.1095)  time: 1.1037  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [140/431]  eta: 0:05:25  lr: 0.000142  loss: 1.0944 (1.1088)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [150/431]  eta: 0:05:14  lr: 0.000142  loss: 1.0973 (1.1103)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [160/431]  eta: 0:05:02  lr: 0.000142  loss: 1.1015 (1.1090)  time: 1.1076  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [170/431]  eta: 0:04:51  lr: 0.000142  loss: 1.0722 (1.1090)  time: 1.1136  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [180/431]  eta: 0:04:40  lr: 0.000142  loss: 1.0320 (1.1050)  time: 1.1069  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [190/431]  eta: 0:04:28  lr: 0.000142  loss: 1.0387 (1.1070)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [200/431]  eta: 0:04:17  lr: 0.000142  loss: 1.0766 (1.1068)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [210/431]  eta: 0:04:06  lr: 0.000142  loss: 1.0392 (1.1017)  time: 1.1025  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [220/431]  eta: 0:03:54  lr: 0.000142  loss: 1.0392 (1.1029)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [230/431]  eta: 0:03:43  lr: 0.000142  loss: 1.1221 (1.1043)  time: 1.0987  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [240/431]  eta: 0:03:32  lr: 0.000142  loss: 1.0733 (1.1028)  time: 1.0921  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [250/431]  eta: 0:03:21  lr: 0.000142  loss: 1.0733 (1.1036)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [260/431]  eta: 0:03:10  lr: 0.000142  loss: 1.0955 (1.1042)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [270/431]  eta: 0:02:58  lr: 0.000142  loss: 1.1073 (1.1059)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [280/431]  eta: 0:02:47  lr: 0.000142  loss: 1.0988 (1.1068)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [290/431]  eta: 0:02:36  lr: 0.000142  loss: 1.0625 (1.1045)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [300/431]  eta: 0:02:25  lr: 0.000142  loss: 1.0504 (1.1044)  time: 1.1064  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [310/431]  eta: 0:02:14  lr: 0.000142  loss: 1.0562 (1.1044)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [320/431]  eta: 0:02:03  lr: 0.000142  loss: 1.0783 (1.1027)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [330/431]  eta: 0:01:52  lr: 0.000142  loss: 1.1048 (1.1042)  time: 1.1019  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [340/431]  eta: 0:01:40  lr: 0.000142  loss: 1.1274 (1.1049)  time: 1.1067  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [350/431]  eta: 0:01:29  lr: 0.000142  loss: 1.0551 (1.1032)  time: 1.1159  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [360/431]  eta: 0:01:18  lr: 0.000142  loss: 1.0616 (1.1033)  time: 1.1110  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [370/431]  eta: 0:01:07  lr: 0.000142  loss: 1.0616 (1.1032)  time: 1.1046  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [380/431]  eta: 0:00:56  lr: 0.000142  loss: 1.0614 (1.1034)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [390/431]  eta: 0:00:45  lr: 0.000142  loss: 1.0942 (1.1039)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [400/431]  eta: 0:00:34  lr: 0.000142  loss: 1.0791 (1.1035)  time: 1.0918  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:361]  [410/431]  eta: 0:00:23  lr: 0.000142  loss: 1.0491 (1.1026)  time: 1.0935  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:361]  [420/431]  eta: 0:00:12  lr: 0.000142  loss: 1.0667 (1.1029)  time: 1.1002  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:361]  [430/431]  eta: 0:00:01  lr: 0.000142  loss: 1.0667 (1.1022)  time: 1.1004  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:361] Total time: 0:07:57 (1.1087 s / it)\n",
      "Averaged stats: lr: 0.000142  loss: 1.0667 (1.1022)\n",
      "Valid: [epoch:361]  [ 0/14]  eta: 0:00:34  loss: 1.0827 (1.0827)  time: 2.4837  data: 2.3331  max mem: 15925\n",
      "Valid: [epoch:361]  [13/14]  eta: 0:00:00  loss: 1.0323 (1.0399)  time: 0.2576  data: 0.1667  max mem: 15925\n",
      "Valid: [epoch:361] Total time: 0:00:03 (0.2754 s / it)\n",
      "Averaged stats: loss: 1.0323 (1.0399)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_361_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.040%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:362]  [  0/431]  eta: 0:35:15  lr: 0.000142  loss: 1.0866 (1.0866)  time: 4.9087  data: 3.7378  max mem: 15925\n",
      "Train: [epoch:362]  [ 10/431]  eta: 0:09:47  lr: 0.000142  loss: 1.1545 (1.1968)  time: 1.3961  data: 0.3400  max mem: 15925\n",
      "Train: [epoch:362]  [ 20/431]  eta: 0:08:29  lr: 0.000142  loss: 1.1238 (1.1380)  time: 1.0562  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [ 30/431]  eta: 0:07:55  lr: 0.000142  loss: 1.0609 (1.1211)  time: 1.0692  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [ 40/431]  eta: 0:07:34  lr: 0.000142  loss: 1.0784 (1.1242)  time: 1.0823  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [ 50/431]  eta: 0:07:17  lr: 0.000142  loss: 1.0920 (1.1079)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [ 60/431]  eta: 0:07:04  lr: 0.000142  loss: 1.0578 (1.1066)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [ 70/431]  eta: 0:06:50  lr: 0.000142  loss: 1.1170 (1.1077)  time: 1.1102  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:362]  [ 80/431]  eta: 0:06:37  lr: 0.000142  loss: 1.1201 (1.1181)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [ 90/431]  eta: 0:06:25  lr: 0.000142  loss: 1.1310 (1.1149)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [100/431]  eta: 0:06:12  lr: 0.000142  loss: 1.1231 (1.1173)  time: 1.1009  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [110/431]  eta: 0:06:01  lr: 0.000142  loss: 1.1266 (1.1183)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [120/431]  eta: 0:05:49  lr: 0.000142  loss: 1.0730 (1.1145)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [130/431]  eta: 0:05:37  lr: 0.000142  loss: 1.0937 (1.1169)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [140/431]  eta: 0:05:26  lr: 0.000142  loss: 1.1592 (1.1236)  time: 1.1074  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [150/431]  eta: 0:05:14  lr: 0.000142  loss: 1.0750 (1.1200)  time: 1.1113  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [160/431]  eta: 0:05:03  lr: 0.000142  loss: 1.0554 (1.1176)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [170/431]  eta: 0:04:51  lr: 0.000142  loss: 1.0681 (1.1127)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [180/431]  eta: 0:04:40  lr: 0.000142  loss: 1.0800 (1.1140)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [190/431]  eta: 0:04:29  lr: 0.000142  loss: 1.1092 (1.1126)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [200/431]  eta: 0:04:17  lr: 0.000142  loss: 1.0910 (1.1127)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [210/431]  eta: 0:04:06  lr: 0.000142  loss: 1.0597 (1.1097)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [220/431]  eta: 0:03:55  lr: 0.000142  loss: 1.0749 (1.1112)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [230/431]  eta: 0:03:44  lr: 0.000142  loss: 1.1178 (1.1120)  time: 1.1057  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [240/431]  eta: 0:03:32  lr: 0.000142  loss: 1.1112 (1.1135)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [250/431]  eta: 0:03:21  lr: 0.000142  loss: 1.0841 (1.1115)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [260/431]  eta: 0:03:10  lr: 0.000142  loss: 1.0824 (1.1120)  time: 1.1124  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [270/431]  eta: 0:02:59  lr: 0.000142  loss: 1.0703 (1.1106)  time: 1.1049  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [280/431]  eta: 0:02:47  lr: 0.000142  loss: 1.0428 (1.1099)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [290/431]  eta: 0:02:36  lr: 0.000142  loss: 1.0722 (1.1089)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [300/431]  eta: 0:02:25  lr: 0.000142  loss: 1.0717 (1.1094)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [310/431]  eta: 0:02:14  lr: 0.000142  loss: 1.0717 (1.1086)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [320/431]  eta: 0:02:03  lr: 0.000142  loss: 1.0334 (1.1051)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [330/431]  eta: 0:01:52  lr: 0.000142  loss: 1.0623 (1.1081)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [340/431]  eta: 0:01:41  lr: 0.000142  loss: 1.1070 (1.1066)  time: 1.1100  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [350/431]  eta: 0:01:29  lr: 0.000142  loss: 1.0682 (1.1057)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [360/431]  eta: 0:01:18  lr: 0.000142  loss: 1.0925 (1.1067)  time: 1.0993  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [370/431]  eta: 0:01:07  lr: 0.000142  loss: 1.0674 (1.1053)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [380/431]  eta: 0:00:56  lr: 0.000142  loss: 1.0292 (1.1039)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [390/431]  eta: 0:00:45  lr: 0.000142  loss: 1.0256 (1.1017)  time: 1.0991  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [400/431]  eta: 0:00:34  lr: 0.000142  loss: 1.0309 (1.1015)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [410/431]  eta: 0:00:23  lr: 0.000142  loss: 1.0579 (1.1013)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:362]  [420/431]  eta: 0:00:12  lr: 0.000142  loss: 1.0893 (1.1027)  time: 1.0960  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:362]  [430/431]  eta: 0:00:01  lr: 0.000142  loss: 1.0884 (1.1016)  time: 1.1032  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:362] Total time: 0:07:58 (1.1092 s / it)\n",
      "Averaged stats: lr: 0.000142  loss: 1.0884 (1.1016)\n",
      "Valid: [epoch:362]  [ 0/14]  eta: 0:00:35  loss: 1.0224 (1.0224)  time: 2.5679  data: 2.4009  max mem: 15925\n",
      "Valid: [epoch:362]  [13/14]  eta: 0:00:00  loss: 1.0304 (1.0410)  time: 0.2810  data: 0.1716  max mem: 15925\n",
      "Valid: [epoch:362] Total time: 0:00:04 (0.2980 s / it)\n",
      "Averaged stats: loss: 1.0304 (1.0410)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_362_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.041%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:363]  [  0/431]  eta: 0:35:25  lr: 0.000142  loss: 1.0924 (1.0924)  time: 4.9325  data: 3.8151  max mem: 15925\n",
      "Train: [epoch:363]  [ 10/431]  eta: 0:09:44  lr: 0.000142  loss: 1.0975 (1.1117)  time: 1.3874  data: 0.3470  max mem: 15925\n",
      "Train: [epoch:363]  [ 20/431]  eta: 0:08:25  lr: 0.000142  loss: 1.0904 (1.0978)  time: 1.0440  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [ 30/431]  eta: 0:07:53  lr: 0.000142  loss: 1.0541 (1.0851)  time: 1.0690  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [ 40/431]  eta: 0:07:33  lr: 0.000142  loss: 1.0728 (1.0870)  time: 1.0870  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [ 50/431]  eta: 0:07:17  lr: 0.000142  loss: 1.0728 (1.0846)  time: 1.0985  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [ 60/431]  eta: 0:07:02  lr: 0.000142  loss: 1.0521 (1.0844)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [ 70/431]  eta: 0:06:49  lr: 0.000142  loss: 1.0816 (1.0880)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [ 80/431]  eta: 0:06:36  lr: 0.000142  loss: 1.1455 (1.1003)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [ 90/431]  eta: 0:06:24  lr: 0.000142  loss: 1.1173 (1.0992)  time: 1.0975  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [100/431]  eta: 0:06:12  lr: 0.000142  loss: 1.0205 (1.0949)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [110/431]  eta: 0:06:00  lr: 0.000142  loss: 1.0291 (1.0916)  time: 1.1128  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [120/431]  eta: 0:05:48  lr: 0.000142  loss: 1.1164 (1.0982)  time: 1.1097  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [130/431]  eta: 0:05:37  lr: 0.000142  loss: 1.1355 (1.1002)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [140/431]  eta: 0:05:25  lr: 0.000142  loss: 1.0532 (1.0950)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [150/431]  eta: 0:05:13  lr: 0.000142  loss: 1.0200 (1.0952)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [160/431]  eta: 0:05:02  lr: 0.000142  loss: 1.1009 (1.0975)  time: 1.0995  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [170/431]  eta: 0:04:50  lr: 0.000142  loss: 1.1009 (1.0986)  time: 1.0926  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [180/431]  eta: 0:04:39  lr: 0.000142  loss: 1.0633 (1.0980)  time: 1.0948  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [190/431]  eta: 0:04:28  lr: 0.000142  loss: 1.0453 (1.0964)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [200/431]  eta: 0:04:17  lr: 0.000142  loss: 1.0634 (1.0954)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [210/431]  eta: 0:04:05  lr: 0.000142  loss: 1.0758 (1.0966)  time: 1.1102  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [220/431]  eta: 0:03:54  lr: 0.000142  loss: 1.1153 (1.0966)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [230/431]  eta: 0:03:43  lr: 0.000142  loss: 1.0677 (1.0973)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [240/431]  eta: 0:03:32  lr: 0.000142  loss: 1.0538 (1.0963)  time: 1.1005  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:363]  [250/431]  eta: 0:03:20  lr: 0.000142  loss: 1.0377 (1.0955)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [260/431]  eta: 0:03:09  lr: 0.000142  loss: 1.0582 (1.0948)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [270/431]  eta: 0:02:58  lr: 0.000142  loss: 1.0975 (1.0964)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [280/431]  eta: 0:02:47  lr: 0.000142  loss: 1.1021 (1.0973)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [290/431]  eta: 0:02:36  lr: 0.000142  loss: 1.0693 (1.0964)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [300/431]  eta: 0:02:25  lr: 0.000142  loss: 1.1275 (1.0973)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [310/431]  eta: 0:02:14  lr: 0.000142  loss: 1.1410 (1.0968)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [320/431]  eta: 0:02:03  lr: 0.000142  loss: 1.0799 (1.0985)  time: 1.1087  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [330/431]  eta: 0:01:52  lr: 0.000142  loss: 1.1029 (1.0998)  time: 1.1040  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [340/431]  eta: 0:01:40  lr: 0.000142  loss: 1.0916 (1.0999)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [350/431]  eta: 0:01:29  lr: 0.000142  loss: 1.1154 (1.1014)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [360/431]  eta: 0:01:18  lr: 0.000142  loss: 1.0951 (1.1000)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [370/431]  eta: 0:01:07  lr: 0.000142  loss: 1.0689 (1.1005)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [380/431]  eta: 0:00:56  lr: 0.000142  loss: 1.0975 (1.1012)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [390/431]  eta: 0:00:45  lr: 0.000142  loss: 1.0836 (1.1012)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [400/431]  eta: 0:00:34  lr: 0.000142  loss: 1.0595 (1.0994)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [410/431]  eta: 0:00:23  lr: 0.000142  loss: 1.0595 (1.1002)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:363]  [420/431]  eta: 0:00:12  lr: 0.000142  loss: 1.0976 (1.1009)  time: 1.0980  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:363]  [430/431]  eta: 0:00:01  lr: 0.000142  loss: 1.0376 (1.0991)  time: 1.1023  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:363] Total time: 0:07:57 (1.1081 s / it)\n",
      "Averaged stats: lr: 0.000142  loss: 1.0376 (1.0991)\n",
      "Valid: [epoch:363]  [ 0/14]  eta: 0:00:36  loss: 0.9848 (0.9848)  time: 2.6415  data: 2.5005  max mem: 15925\n",
      "Valid: [epoch:363]  [13/14]  eta: 0:00:00  loss: 1.0408 (1.0460)  time: 0.2857  data: 0.1787  max mem: 15925\n",
      "Valid: [epoch:363] Total time: 0:00:04 (0.3045 s / it)\n",
      "Averaged stats: loss: 1.0408 (1.0460)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_363_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.046%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:364]  [  0/431]  eta: 0:31:46  lr: 0.000142  loss: 1.1973 (1.1973)  time: 4.4224  data: 3.2341  max mem: 15925\n",
      "Train: [epoch:364]  [ 10/431]  eta: 0:09:31  lr: 0.000142  loss: 1.1576 (1.1405)  time: 1.3566  data: 0.2942  max mem: 15925\n",
      "Train: [epoch:364]  [ 20/431]  eta: 0:08:21  lr: 0.000142  loss: 1.1541 (1.1452)  time: 1.0596  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [ 30/431]  eta: 0:07:49  lr: 0.000142  loss: 1.1204 (1.1296)  time: 1.0696  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [ 40/431]  eta: 0:07:30  lr: 0.000142  loss: 1.0750 (1.1133)  time: 1.0790  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [ 50/431]  eta: 0:07:14  lr: 0.000142  loss: 1.0318 (1.0981)  time: 1.0892  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [ 60/431]  eta: 0:07:00  lr: 0.000142  loss: 1.0034 (1.0866)  time: 1.0960  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [ 70/431]  eta: 0:06:47  lr: 0.000142  loss: 1.0558 (1.0885)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [ 80/431]  eta: 0:06:34  lr: 0.000142  loss: 1.0574 (1.0876)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [ 90/431]  eta: 0:06:22  lr: 0.000142  loss: 1.0325 (1.0869)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [100/431]  eta: 0:06:10  lr: 0.000142  loss: 1.0800 (1.0904)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [110/431]  eta: 0:05:58  lr: 0.000142  loss: 1.0355 (1.0851)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [120/431]  eta: 0:05:47  lr: 0.000142  loss: 0.9919 (1.0792)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [130/431]  eta: 0:05:35  lr: 0.000142  loss: 1.0082 (1.0791)  time: 1.1002  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [140/431]  eta: 0:05:24  lr: 0.000142  loss: 1.0615 (1.0818)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [150/431]  eta: 0:05:12  lr: 0.000142  loss: 1.0793 (1.0832)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [160/431]  eta: 0:05:01  lr: 0.000142  loss: 1.0831 (1.0855)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [170/431]  eta: 0:04:50  lr: 0.000142  loss: 1.0494 (1.0822)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [180/431]  eta: 0:04:39  lr: 0.000142  loss: 1.0976 (1.0854)  time: 1.1038  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [190/431]  eta: 0:04:27  lr: 0.000142  loss: 1.1173 (1.0876)  time: 1.1036  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [200/431]  eta: 0:04:16  lr: 0.000142  loss: 1.0761 (1.0885)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [210/431]  eta: 0:04:05  lr: 0.000142  loss: 1.0888 (1.0922)  time: 1.0953  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [220/431]  eta: 0:03:54  lr: 0.000142  loss: 1.0612 (1.0907)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [230/431]  eta: 0:03:43  lr: 0.000142  loss: 1.0442 (1.0890)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [240/431]  eta: 0:03:31  lr: 0.000142  loss: 1.0832 (1.0900)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [250/431]  eta: 0:03:20  lr: 0.000142  loss: 1.0674 (1.0897)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [260/431]  eta: 0:03:09  lr: 0.000142  loss: 1.0674 (1.0892)  time: 1.1010  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [270/431]  eta: 0:02:58  lr: 0.000142  loss: 1.0716 (1.0893)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [280/431]  eta: 0:02:47  lr: 0.000142  loss: 1.1121 (1.0921)  time: 1.1045  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [290/431]  eta: 0:02:36  lr: 0.000142  loss: 1.1121 (1.0929)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [300/431]  eta: 0:02:25  lr: 0.000142  loss: 1.1247 (1.0958)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [310/431]  eta: 0:02:13  lr: 0.000142  loss: 1.0799 (1.0962)  time: 1.1032  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [320/431]  eta: 0:02:02  lr: 0.000142  loss: 1.0474 (1.0946)  time: 1.1027  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [330/431]  eta: 0:01:51  lr: 0.000142  loss: 1.0473 (1.0948)  time: 1.0929  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [340/431]  eta: 0:01:40  lr: 0.000142  loss: 1.0982 (1.0966)  time: 1.0986  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [350/431]  eta: 0:01:29  lr: 0.000142  loss: 1.1472 (1.0984)  time: 1.1008  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [360/431]  eta: 0:01:18  lr: 0.000142  loss: 1.0994 (1.0976)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [370/431]  eta: 0:01:07  lr: 0.000142  loss: 1.0837 (1.0975)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [380/431]  eta: 0:00:56  lr: 0.000142  loss: 1.1027 (1.0984)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [390/431]  eta: 0:00:45  lr: 0.000142  loss: 1.0849 (1.0975)  time: 1.0934  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [400/431]  eta: 0:00:34  lr: 0.000142  loss: 1.0735 (1.0985)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:364]  [410/431]  eta: 0:00:23  lr: 0.000142  loss: 1.0907 (1.0987)  time: 1.1003  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:364]  [420/431]  eta: 0:00:12  lr: 0.000142  loss: 1.1049 (1.0993)  time: 1.1001  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:364]  [430/431]  eta: 0:00:01  lr: 0.000142  loss: 1.1111 (1.0997)  time: 1.1110  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:364] Total time: 0:07:56 (1.1062 s / it)\n",
      "Averaged stats: lr: 0.000142  loss: 1.1111 (1.0997)\n",
      "Valid: [epoch:364]  [ 0/14]  eta: 0:00:34  loss: 1.0539 (1.0539)  time: 2.4780  data: 2.3376  max mem: 15925\n",
      "Valid: [epoch:364]  [13/14]  eta: 0:00:00  loss: 1.0539 (1.0587)  time: 0.2577  data: 0.1671  max mem: 15925\n",
      "Valid: [epoch:364] Total time: 0:00:03 (0.2748 s / it)\n",
      "Averaged stats: loss: 1.0539 (1.0587)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_364_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.059%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:365]  [  0/431]  eta: 0:30:58  lr: 0.000141  loss: 1.1043 (1.1043)  time: 4.3116  data: 3.1390  max mem: 15925\n",
      "Train: [epoch:365]  [ 10/431]  eta: 0:09:14  lr: 0.000141  loss: 1.0922 (1.1110)  time: 1.3183  data: 0.2856  max mem: 15925\n",
      "Train: [epoch:365]  [ 20/431]  eta: 0:08:09  lr: 0.000141  loss: 1.0793 (1.0980)  time: 1.0344  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [ 30/431]  eta: 0:07:43  lr: 0.000141  loss: 1.0506 (1.0883)  time: 1.0668  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [ 40/431]  eta: 0:07:25  lr: 0.000141  loss: 1.0283 (1.0735)  time: 1.0858  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [ 50/431]  eta: 0:07:10  lr: 0.000141  loss: 1.0283 (1.0739)  time: 1.0908  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [ 60/431]  eta: 0:06:57  lr: 0.000141  loss: 1.0714 (1.0751)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [ 70/431]  eta: 0:06:45  lr: 0.000141  loss: 1.0700 (1.0735)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [ 80/431]  eta: 0:06:33  lr: 0.000141  loss: 1.0513 (1.0733)  time: 1.1058  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [ 90/431]  eta: 0:06:21  lr: 0.000141  loss: 1.0409 (1.0699)  time: 1.1059  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [100/431]  eta: 0:06:09  lr: 0.000141  loss: 1.0054 (1.0682)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [110/431]  eta: 0:05:58  lr: 0.000141  loss: 0.9890 (1.0648)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [120/431]  eta: 0:05:46  lr: 0.000141  loss: 1.0028 (1.0697)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [130/431]  eta: 0:05:35  lr: 0.000141  loss: 1.0569 (1.0727)  time: 1.1012  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [140/431]  eta: 0:05:24  lr: 0.000141  loss: 1.0474 (1.0730)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [150/431]  eta: 0:05:12  lr: 0.000141  loss: 1.0275 (1.0691)  time: 1.1024  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [160/431]  eta: 0:05:01  lr: 0.000141  loss: 1.0731 (1.0770)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [170/431]  eta: 0:04:50  lr: 0.000141  loss: 1.1386 (1.0764)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [180/431]  eta: 0:04:38  lr: 0.000141  loss: 1.0568 (1.0779)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [190/431]  eta: 0:04:27  lr: 0.000141  loss: 1.1034 (1.0828)  time: 1.1099  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:365]  [200/431]  eta: 0:04:16  lr: 0.000141  loss: 1.1034 (1.0830)  time: 1.1067  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:365]  [210/431]  eta: 0:04:05  lr: 0.000141  loss: 1.0770 (1.0846)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [220/431]  eta: 0:03:54  lr: 0.000141  loss: 1.0927 (1.0857)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [230/431]  eta: 0:03:43  lr: 0.000141  loss: 1.0661 (1.0859)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [240/431]  eta: 0:03:32  lr: 0.000141  loss: 1.0844 (1.0881)  time: 1.1112  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [250/431]  eta: 0:03:20  lr: 0.000141  loss: 1.0997 (1.0888)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [260/431]  eta: 0:03:09  lr: 0.000141  loss: 1.1032 (1.0917)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [270/431]  eta: 0:02:58  lr: 0.000141  loss: 1.1222 (1.0939)  time: 1.0945  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [280/431]  eta: 0:02:47  lr: 0.000141  loss: 1.1004 (1.0945)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [290/431]  eta: 0:02:36  lr: 0.000141  loss: 1.0688 (1.0926)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [300/431]  eta: 0:02:25  lr: 0.000141  loss: 1.0712 (1.0928)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [310/431]  eta: 0:02:13  lr: 0.000141  loss: 1.1231 (1.0947)  time: 1.0957  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [320/431]  eta: 0:02:02  lr: 0.000141  loss: 1.1017 (1.0946)  time: 1.0955  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [330/431]  eta: 0:01:51  lr: 0.000141  loss: 1.0777 (1.0961)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [340/431]  eta: 0:01:40  lr: 0.000141  loss: 1.0922 (1.0970)  time: 1.0962  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [350/431]  eta: 0:01:29  lr: 0.000141  loss: 1.1421 (1.0982)  time: 1.0973  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [360/431]  eta: 0:01:18  lr: 0.000141  loss: 1.1520 (1.1016)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [370/431]  eta: 0:01:07  lr: 0.000141  loss: 1.0439 (1.1010)  time: 1.1118  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [380/431]  eta: 0:00:56  lr: 0.000141  loss: 1.0703 (1.1010)  time: 1.0972  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [390/431]  eta: 0:00:45  lr: 0.000141  loss: 1.1040 (1.1006)  time: 1.0966  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [400/431]  eta: 0:00:34  lr: 0.000141  loss: 1.0981 (1.1017)  time: 1.1075  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [410/431]  eta: 0:00:23  lr: 0.000141  loss: 1.0981 (1.1014)  time: 1.1071  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:365]  [420/431]  eta: 0:00:12  lr: 0.000141  loss: 1.0906 (1.1013)  time: 1.1052  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:365]  [430/431]  eta: 0:00:01  lr: 0.000141  loss: 1.0310 (1.1007)  time: 1.1035  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:365] Total time: 0:07:56 (1.1063 s / it)\n",
      "Averaged stats: lr: 0.000141  loss: 1.0310 (1.1007)\n",
      "Valid: [epoch:365]  [ 0/14]  eta: 0:00:34  loss: 1.0116 (1.0116)  time: 2.4697  data: 2.3124  max mem: 15925\n",
      "Valid: [epoch:365]  [13/14]  eta: 0:00:00  loss: 1.0366 (1.0437)  time: 0.2640  data: 0.1653  max mem: 15925\n",
      "Valid: [epoch:365] Total time: 0:00:03 (0.2796 s / it)\n",
      "Averaged stats: loss: 1.0366 (1.0437)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_365_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.044%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:366]  [  0/431]  eta: 0:34:24  lr: 0.000141  loss: 1.1398 (1.1398)  time: 4.7910  data: 3.7127  max mem: 15925\n",
      "Train: [epoch:366]  [ 10/431]  eta: 0:09:43  lr: 0.000141  loss: 1.1233 (1.1200)  time: 1.3865  data: 0.3377  max mem: 15925\n",
      "Train: [epoch:366]  [ 20/431]  eta: 0:08:25  lr: 0.000141  loss: 1.0820 (1.1189)  time: 1.0513  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [ 30/431]  eta: 0:07:53  lr: 0.000141  loss: 1.0670 (1.0849)  time: 1.0669  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [ 40/431]  eta: 0:07:33  lr: 0.000141  loss: 1.0275 (1.0765)  time: 1.0878  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [ 50/431]  eta: 0:07:16  lr: 0.000141  loss: 1.0275 (1.0745)  time: 1.0898  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [ 60/431]  eta: 0:07:02  lr: 0.000141  loss: 1.0468 (1.0756)  time: 1.0952  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [ 70/431]  eta: 0:06:49  lr: 0.000141  loss: 1.1013 (1.0811)  time: 1.1079  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [ 80/431]  eta: 0:06:36  lr: 0.000141  loss: 1.0663 (1.0825)  time: 1.1052  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [ 90/431]  eta: 0:06:24  lr: 0.000141  loss: 1.0581 (1.0827)  time: 1.1022  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:366]  [100/431]  eta: 0:06:12  lr: 0.000141  loss: 1.0280 (1.0793)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [110/431]  eta: 0:06:00  lr: 0.000141  loss: 1.0482 (1.0795)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [120/431]  eta: 0:05:48  lr: 0.000141  loss: 1.1183 (1.0886)  time: 1.0959  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [130/431]  eta: 0:05:36  lr: 0.000141  loss: 1.0709 (1.0841)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [140/431]  eta: 0:05:25  lr: 0.000141  loss: 1.0399 (1.0837)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [150/431]  eta: 0:05:13  lr: 0.000141  loss: 1.0581 (1.0876)  time: 1.1044  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [160/431]  eta: 0:05:02  lr: 0.000141  loss: 1.0688 (1.0888)  time: 1.1094  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [170/431]  eta: 0:04:51  lr: 0.000141  loss: 1.0497 (1.0875)  time: 1.1072  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [180/431]  eta: 0:04:39  lr: 0.000141  loss: 1.0636 (1.0883)  time: 1.1014  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [190/431]  eta: 0:04:28  lr: 0.000141  loss: 1.0894 (1.0895)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [200/431]  eta: 0:04:17  lr: 0.000141  loss: 1.1079 (1.0932)  time: 1.1005  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [210/431]  eta: 0:04:06  lr: 0.000141  loss: 1.1406 (1.0938)  time: 1.1077  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [220/431]  eta: 0:03:54  lr: 0.000141  loss: 1.0684 (1.0938)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [230/431]  eta: 0:03:43  lr: 0.000141  loss: 1.0662 (1.0941)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [240/431]  eta: 0:03:32  lr: 0.000141  loss: 1.0552 (1.0928)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [250/431]  eta: 0:03:21  lr: 0.000141  loss: 1.0552 (1.0952)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [260/431]  eta: 0:03:09  lr: 0.000141  loss: 1.1174 (1.0969)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [270/431]  eta: 0:02:58  lr: 0.000141  loss: 1.1184 (1.0995)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [280/431]  eta: 0:02:47  lr: 0.000141  loss: 1.1113 (1.0994)  time: 1.0939  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [290/431]  eta: 0:02:36  lr: 0.000141  loss: 1.1096 (1.0999)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [300/431]  eta: 0:02:25  lr: 0.000141  loss: 1.1071 (1.1011)  time: 1.0970  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [310/431]  eta: 0:02:14  lr: 0.000141  loss: 1.0489 (1.0994)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [320/431]  eta: 0:02:02  lr: 0.000141  loss: 1.0184 (1.0971)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [330/431]  eta: 0:01:51  lr: 0.000141  loss: 1.0753 (1.0972)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [340/431]  eta: 0:01:40  lr: 0.000141  loss: 1.0773 (1.0976)  time: 1.0996  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [350/431]  eta: 0:01:29  lr: 0.000141  loss: 1.0422 (1.0973)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [360/431]  eta: 0:01:18  lr: 0.000141  loss: 1.0627 (1.0970)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [370/431]  eta: 0:01:07  lr: 0.000141  loss: 1.1206 (1.0977)  time: 1.0992  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [380/431]  eta: 0:00:56  lr: 0.000141  loss: 1.0776 (1.0968)  time: 1.0944  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [390/431]  eta: 0:00:45  lr: 0.000141  loss: 1.0437 (1.0980)  time: 1.0931  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [400/431]  eta: 0:00:34  lr: 0.000141  loss: 1.1199 (1.0987)  time: 1.0976  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [410/431]  eta: 0:00:23  lr: 0.000141  loss: 1.1199 (1.0999)  time: 1.0964  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:366]  [420/431]  eta: 0:00:12  lr: 0.000141  loss: 1.0522 (1.0989)  time: 1.0971  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:366]  [430/431]  eta: 0:00:01  lr: 0.000141  loss: 1.0268 (1.0993)  time: 1.0993  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:366] Total time: 0:07:56 (1.1061 s / it)\n",
      "Averaged stats: lr: 0.000141  loss: 1.0268 (1.0993)\n",
      "Valid: [epoch:366]  [ 0/14]  eta: 0:00:34  loss: 1.1358 (1.1358)  time: 2.4876  data: 2.3330  max mem: 15925\n",
      "Valid: [epoch:366]  [13/14]  eta: 0:00:00  loss: 1.0399 (1.0474)  time: 0.2599  data: 0.1667  max mem: 15925\n",
      "Valid: [epoch:366] Total time: 0:00:03 (0.2735 s / it)\n",
      "Averaged stats: loss: 1.0399 (1.0474)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_366_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.047%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:367]  [  0/431]  eta: 0:33:42  lr: 0.000141  loss: 1.1774 (1.1774)  time: 4.6926  data: 3.5128  max mem: 15925\n",
      "Train: [epoch:367]  [ 10/431]  eta: 0:09:38  lr: 0.000141  loss: 1.1394 (1.1388)  time: 1.3735  data: 0.3195  max mem: 15925\n",
      "Train: [epoch:367]  [ 20/431]  eta: 0:08:22  lr: 0.000141  loss: 1.1200 (1.1194)  time: 1.0488  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [ 30/431]  eta: 0:07:50  lr: 0.000141  loss: 1.1287 (1.1332)  time: 1.0635  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [ 40/431]  eta: 0:07:29  lr: 0.000141  loss: 1.0638 (1.1331)  time: 1.0753  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [ 50/431]  eta: 0:07:14  lr: 0.000141  loss: 1.0638 (1.1255)  time: 1.0874  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [ 60/431]  eta: 0:07:00  lr: 0.000141  loss: 1.0741 (1.1243)  time: 1.0990  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [ 70/431]  eta: 0:06:47  lr: 0.000141  loss: 1.0567 (1.1177)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [ 80/431]  eta: 0:06:35  lr: 0.000141  loss: 1.0567 (1.1114)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [ 90/431]  eta: 0:06:23  lr: 0.000141  loss: 1.0915 (1.1122)  time: 1.1055  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [100/431]  eta: 0:06:11  lr: 0.000141  loss: 1.0422 (1.1021)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [110/431]  eta: 0:05:59  lr: 0.000141  loss: 1.0236 (1.0958)  time: 1.0999  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [120/431]  eta: 0:05:47  lr: 0.000141  loss: 1.0150 (1.0933)  time: 1.1001  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [130/431]  eta: 0:05:36  lr: 0.000141  loss: 1.1349 (1.0981)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [140/431]  eta: 0:05:24  lr: 0.000141  loss: 1.0954 (1.0957)  time: 1.1054  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [150/431]  eta: 0:05:13  lr: 0.000141  loss: 1.0777 (1.0972)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [160/431]  eta: 0:05:02  lr: 0.000141  loss: 1.1135 (1.0957)  time: 1.1041  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [170/431]  eta: 0:04:50  lr: 0.000141  loss: 1.0612 (1.0954)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [180/431]  eta: 0:04:39  lr: 0.000141  loss: 1.0213 (1.0912)  time: 1.1050  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [190/431]  eta: 0:04:28  lr: 0.000141  loss: 1.0463 (1.0934)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [200/431]  eta: 0:04:16  lr: 0.000141  loss: 1.0624 (1.0928)  time: 1.0958  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [210/431]  eta: 0:04:05  lr: 0.000141  loss: 1.0392 (1.0923)  time: 1.0943  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [220/431]  eta: 0:03:54  lr: 0.000141  loss: 1.0462 (1.0911)  time: 1.0901  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [230/431]  eta: 0:03:43  lr: 0.000141  loss: 1.0689 (1.0929)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [240/431]  eta: 0:03:32  lr: 0.000141  loss: 1.1409 (1.0951)  time: 1.1089  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [250/431]  eta: 0:03:20  lr: 0.000141  loss: 1.0844 (1.0961)  time: 1.1063  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [260/431]  eta: 0:03:09  lr: 0.000141  loss: 1.0885 (1.0975)  time: 1.1018  data: 0.0002  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:367]  [270/431]  eta: 0:02:58  lr: 0.000141  loss: 1.0668 (1.0962)  time: 1.1020  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [280/431]  eta: 0:02:47  lr: 0.000141  loss: 1.0668 (1.0966)  time: 1.1015  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [290/431]  eta: 0:02:36  lr: 0.000141  loss: 1.0949 (1.0972)  time: 1.1033  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [300/431]  eta: 0:02:25  lr: 0.000141  loss: 1.0850 (1.0969)  time: 1.0979  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [310/431]  eta: 0:02:14  lr: 0.000141  loss: 1.0562 (1.0959)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [320/431]  eta: 0:02:03  lr: 0.000141  loss: 1.0348 (1.0950)  time: 1.1080  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [330/431]  eta: 0:01:51  lr: 0.000141  loss: 1.0749 (1.0978)  time: 1.1080  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:367]  [340/431]  eta: 0:01:40  lr: 0.000141  loss: 1.0889 (1.0977)  time: 1.0962  data: 0.0003  max mem: 15925\n",
      "Train: [epoch:367]  [350/431]  eta: 0:01:29  lr: 0.000141  loss: 1.0636 (1.0987)  time: 1.0981  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [360/431]  eta: 0:01:18  lr: 0.000141  loss: 1.0614 (1.0979)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [370/431]  eta: 0:01:07  lr: 0.000141  loss: 1.0940 (1.0980)  time: 1.1035  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [380/431]  eta: 0:00:56  lr: 0.000141  loss: 1.0774 (1.0977)  time: 1.1021  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [390/431]  eta: 0:00:45  lr: 0.000141  loss: 1.0708 (1.0984)  time: 1.1053  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [400/431]  eta: 0:00:34  lr: 0.000141  loss: 1.0841 (1.0993)  time: 1.1029  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [410/431]  eta: 0:00:23  lr: 0.000141  loss: 1.0857 (1.0990)  time: 1.0910  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:367]  [420/431]  eta: 0:00:12  lr: 0.000141  loss: 1.1274 (1.1005)  time: 1.0961  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:367]  [430/431]  eta: 0:00:01  lr: 0.000141  loss: 1.1274 (1.1004)  time: 1.1027  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:367] Total time: 0:07:57 (1.1070 s / it)\n",
      "Averaged stats: lr: 0.000141  loss: 1.1274 (1.1004)\n",
      "Valid: [epoch:367]  [ 0/14]  eta: 0:00:36  loss: 0.9643 (0.9643)  time: 2.6200  data: 2.4734  max mem: 15925\n",
      "Valid: [epoch:367]  [13/14]  eta: 0:00:00  loss: 1.0410 (1.0514)  time: 0.2712  data: 0.1768  max mem: 15925\n",
      "Valid: [epoch:367] Total time: 0:00:04 (0.2873 s / it)\n",
      "Averaged stats: loss: 1.0410 (1.0514)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_367_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.051%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:368]  [  0/431]  eta: 0:30:58  lr: 0.000141  loss: 1.1746 (1.1746)  time: 4.3119  data: 3.0784  max mem: 15925\n",
      "Train: [epoch:368]  [ 10/431]  eta: 0:09:19  lr: 0.000141  loss: 1.1746 (1.1691)  time: 1.3296  data: 0.2800  max mem: 15925\n",
      "Train: [epoch:368]  [ 20/431]  eta: 0:08:16  lr: 0.000141  loss: 1.1690 (1.1664)  time: 1.0518  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [ 30/431]  eta: 0:07:46  lr: 0.000141  loss: 1.0698 (1.1264)  time: 1.0723  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [ 40/431]  eta: 0:07:28  lr: 0.000141  loss: 1.0373 (1.1160)  time: 1.0816  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [ 50/431]  eta: 0:07:12  lr: 0.000141  loss: 1.0506 (1.1067)  time: 1.0904  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [ 60/431]  eta: 0:06:59  lr: 0.000141  loss: 1.0611 (1.0991)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [ 70/431]  eta: 0:06:46  lr: 0.000141  loss: 1.1031 (1.0997)  time: 1.0997  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [ 80/431]  eta: 0:06:33  lr: 0.000141  loss: 1.0845 (1.1008)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [ 90/431]  eta: 0:06:22  lr: 0.000141  loss: 1.0509 (1.0937)  time: 1.1060  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [100/431]  eta: 0:06:10  lr: 0.000141  loss: 1.0393 (1.0927)  time: 1.1066  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [110/431]  eta: 0:05:58  lr: 0.000141  loss: 1.0370 (1.0925)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [120/431]  eta: 0:05:46  lr: 0.000141  loss: 1.0854 (1.0941)  time: 1.0967  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [130/431]  eta: 0:05:35  lr: 0.000141  loss: 1.0881 (1.0930)  time: 1.1022  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [140/431]  eta: 0:05:23  lr: 0.000141  loss: 1.0639 (1.0939)  time: 1.0971  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [150/431]  eta: 0:05:12  lr: 0.000141  loss: 1.0888 (1.0958)  time: 1.1018  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [160/431]  eta: 0:05:01  lr: 0.000141  loss: 1.0993 (1.0963)  time: 1.1081  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [170/431]  eta: 0:04:49  lr: 0.000141  loss: 1.1141 (1.0986)  time: 1.0956  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [180/431]  eta: 0:04:38  lr: 0.000141  loss: 1.1503 (1.0998)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [190/431]  eta: 0:04:27  lr: 0.000141  loss: 1.0806 (1.0988)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [200/431]  eta: 0:04:16  lr: 0.000141  loss: 1.0619 (1.0980)  time: 1.0989  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [210/431]  eta: 0:04:05  lr: 0.000141  loss: 1.1093 (1.0997)  time: 1.1006  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [220/431]  eta: 0:03:53  lr: 0.000141  loss: 1.1385 (1.1006)  time: 1.1030  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [230/431]  eta: 0:03:42  lr: 0.000141  loss: 1.1166 (1.1024)  time: 1.0965  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [240/431]  eta: 0:03:31  lr: 0.000141  loss: 1.1166 (1.1034)  time: 1.1043  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [250/431]  eta: 0:03:20  lr: 0.000141  loss: 1.1269 (1.1046)  time: 1.1031  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [260/431]  eta: 0:03:09  lr: 0.000141  loss: 1.0977 (1.1048)  time: 1.1028  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [270/431]  eta: 0:02:58  lr: 0.000141  loss: 1.0676 (1.1038)  time: 1.1070  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [280/431]  eta: 0:02:47  lr: 0.000141  loss: 1.0636 (1.1029)  time: 1.0968  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [290/431]  eta: 0:02:36  lr: 0.000141  loss: 1.0633 (1.1029)  time: 1.0988  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [300/431]  eta: 0:02:25  lr: 0.000141  loss: 1.1299 (1.1047)  time: 1.1107  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [310/431]  eta: 0:02:14  lr: 0.000141  loss: 1.1395 (1.1056)  time: 1.1168  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [320/431]  eta: 0:02:02  lr: 0.000141  loss: 1.0813 (1.1053)  time: 1.1042  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [330/431]  eta: 0:01:51  lr: 0.000141  loss: 1.1021 (1.1067)  time: 1.0928  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [340/431]  eta: 0:01:40  lr: 0.000141  loss: 1.1341 (1.1062)  time: 1.1048  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [350/431]  eta: 0:01:29  lr: 0.000141  loss: 1.0958 (1.1057)  time: 1.1121  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [360/431]  eta: 0:01:18  lr: 0.000141  loss: 1.0958 (1.1068)  time: 1.1073  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [370/431]  eta: 0:01:07  lr: 0.000141  loss: 1.0861 (1.1064)  time: 1.1003  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [380/431]  eta: 0:00:56  lr: 0.000141  loss: 1.0664 (1.1056)  time: 1.0933  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [390/431]  eta: 0:00:45  lr: 0.000141  loss: 1.0319 (1.1042)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [400/431]  eta: 0:00:34  lr: 0.000141  loss: 1.0319 (1.1022)  time: 1.0942  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:368]  [410/431]  eta: 0:00:23  lr: 0.000141  loss: 1.0354 (1.1006)  time: 1.0953  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:368]  [420/431]  eta: 0:00:12  lr: 0.000141  loss: 1.0462 (1.1001)  time: 1.0980  data: 0.0001  max mem: 15925\n",
      "Train: [epoch:368]  [430/431]  eta: 0:00:01  lr: 0.000141  loss: 1.0661 (1.0997)  time: 1.0985  data: 0.0001  max mem: 15925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:368] Total time: 0:07:56 (1.1059 s / it)\n",
      "Averaged stats: lr: 0.000141  loss: 1.0661 (1.0997)\n",
      "Valid: [epoch:368]  [ 0/14]  eta: 0:00:36  loss: 0.9587 (0.9587)  time: 2.5864  data: 2.4627  max mem: 15925\n",
      "Valid: [epoch:368]  [13/14]  eta: 0:00:00  loss: 1.0387 (1.0450)  time: 0.2680  data: 0.1760  max mem: 15925\n",
      "Valid: [epoch:368] Total time: 0:00:04 (0.2889 s / it)\n",
      "Averaged stats: loss: 1.0387 (1.0450)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/epoch_368_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.045%\n",
      "Min loss: 1.036\n",
      "Best Epoch: 359.000\n",
      "Train: [epoch:369]  [  0/431]  eta: 0:30:59  lr: 0.000140  loss: 1.0758 (1.0758)  time: 4.3146  data: 3.1284  max mem: 15925\n",
      "Train: [epoch:369]  [ 10/431]  eta: 0:09:23  lr: 0.000140  loss: 1.0917 (1.1019)  time: 1.3379  data: 0.2846  max mem: 15925\n",
      "Train: [epoch:369]  [ 20/431]  eta: 0:08:14  lr: 0.000140  loss: 1.0599 (1.0919)  time: 1.0484  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [ 30/431]  eta: 0:07:46  lr: 0.000140  loss: 1.0213 (1.0792)  time: 1.0686  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [ 40/431]  eta: 0:07:28  lr: 0.000140  loss: 1.0213 (1.0772)  time: 1.0851  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [ 50/431]  eta: 0:07:13  lr: 0.000140  loss: 1.0576 (1.0876)  time: 1.0950  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [ 60/431]  eta: 0:07:00  lr: 0.000140  loss: 1.0576 (1.0814)  time: 1.1061  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [ 70/431]  eta: 0:06:47  lr: 0.000140  loss: 1.1246 (1.0866)  time: 1.1115  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [ 80/431]  eta: 0:06:35  lr: 0.000140  loss: 1.0987 (1.0870)  time: 1.1114  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [ 90/431]  eta: 0:06:23  lr: 0.000140  loss: 1.0780 (1.0896)  time: 1.1082  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [100/431]  eta: 0:06:11  lr: 0.000140  loss: 1.0962 (1.0939)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [110/431]  eta: 0:05:59  lr: 0.000140  loss: 1.0874 (1.0922)  time: 1.0914  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [120/431]  eta: 0:05:47  lr: 0.000140  loss: 1.0856 (1.0914)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [130/431]  eta: 0:05:35  lr: 0.000140  loss: 1.0694 (1.0916)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [140/431]  eta: 0:05:24  lr: 0.000140  loss: 1.0694 (1.0929)  time: 1.0982  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [150/431]  eta: 0:05:12  lr: 0.000140  loss: 1.0775 (1.0935)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [160/431]  eta: 0:05:01  lr: 0.000140  loss: 1.0695 (1.0928)  time: 1.0974  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [170/431]  eta: 0:04:50  lr: 0.000140  loss: 1.0760 (1.0939)  time: 1.1007  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [180/431]  eta: 0:04:38  lr: 0.000140  loss: 1.0872 (1.0946)  time: 1.0902  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [190/431]  eta: 0:04:27  lr: 0.000140  loss: 1.0910 (1.0947)  time: 1.0935  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [200/431]  eta: 0:04:16  lr: 0.000140  loss: 1.1173 (1.0980)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [210/431]  eta: 0:04:05  lr: 0.000140  loss: 1.1071 (1.0975)  time: 1.0984  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [220/431]  eta: 0:03:53  lr: 0.000140  loss: 1.0990 (1.0982)  time: 1.1016  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [230/431]  eta: 0:03:42  lr: 0.000140  loss: 1.1034 (1.0984)  time: 1.1039  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [240/431]  eta: 0:03:31  lr: 0.000140  loss: 1.0430 (1.0971)  time: 1.0963  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [250/431]  eta: 0:03:20  lr: 0.000140  loss: 1.0975 (1.0973)  time: 1.0977  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [260/431]  eta: 0:03:09  lr: 0.000140  loss: 1.1170 (1.0985)  time: 1.0994  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [270/431]  eta: 0:02:58  lr: 0.000140  loss: 1.1170 (1.0989)  time: 1.1000  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [280/431]  eta: 0:02:47  lr: 0.000140  loss: 1.0445 (1.0966)  time: 1.1065  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [290/431]  eta: 0:02:36  lr: 0.000140  loss: 1.0452 (1.0964)  time: 1.1011  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [300/431]  eta: 0:02:24  lr: 0.000140  loss: 1.0864 (1.0990)  time: 1.1026  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [310/431]  eta: 0:02:13  lr: 0.000140  loss: 1.1183 (1.0991)  time: 1.1129  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [320/431]  eta: 0:02:02  lr: 0.000140  loss: 1.0736 (1.0971)  time: 1.0998  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [330/431]  eta: 0:01:51  lr: 0.000140  loss: 1.0587 (1.0984)  time: 1.0899  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [340/431]  eta: 0:01:40  lr: 0.000140  loss: 1.0840 (1.0981)  time: 1.0865  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [350/431]  eta: 0:01:29  lr: 0.000140  loss: 1.0809 (1.0982)  time: 1.0922  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [360/431]  eta: 0:01:18  lr: 0.000140  loss: 1.0747 (1.0976)  time: 1.1056  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [370/431]  eta: 0:01:07  lr: 0.000140  loss: 1.0259 (1.0973)  time: 1.0980  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [380/431]  eta: 0:00:56  lr: 0.000140  loss: 1.0615 (1.0980)  time: 1.0890  data: 0.0002  max mem: 15925\n",
      "Train: [epoch:369]  [390/431]  eta: 0:00:45  lr: 0.000140  loss: 1.0615 (1.0968)  time: 1.0935  data: 0.0002  max mem: 15925\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--batch-size 16 \\\n",
    "--epochs 1000 \\\n",
    "--min-lr 5e-6 \\\n",
    "--lr 2e-4 \\\n",
    "--data-set 'Sinogram_DCM' \\\n",
    "--model-name 'CMT_Unet' \\\n",
    "--criterion 'L1 Loss' \\\n",
    "--output_dir '/workspace/sunggu/4.Dose_img2img/model/[Ours]CMT_Unet_L1_x1000' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]CMT_Unet_L1_x1000/low2high/' \\\n",
    "--validate-every 1 \\\n",
    "--num_workers 16 \\\n",
    "--criterion_mode 'not balance' \\\n",
    "--multiple_GT \"False\" \\\n",
    "--patch_training \"False\" \\\n",
    "--multi-gpu-mode 'DataParallel' \n",
    "# --multi-gpu-mode 'DataParallel' \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "import functools\n",
    "import pydicom\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore') \n",
    "\n",
    "\n",
    "def list_sort_nicely(l):   \n",
    "    def tryint(s):        \n",
    "        try:            \n",
    "            return int(s)        \n",
    "        except:            \n",
    "            return s\n",
    "        \n",
    "    def alphanum_key(s):\n",
    "        return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "    l.sort(key=alphanum_key)    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/20/*/*/*.dcm')) + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/20/*/*/*.dcm'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/X/*/*/*.dcm'))  + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/X/*/*/*.dcm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_hu(path):\n",
    "    # pydicom version...!\n",
    "    # referred from https://www.kaggle.com/gzuidhof/full-preprocessing-tutorial\n",
    "    # ref: pydicom.pixel_data_handlers.util.apply_modality_lut\n",
    "    # '''\n",
    "    # Awesome pydicom lut fuction...!\n",
    "    # ds  = pydicom.dcmread(fname)\n",
    "    # arr = ds.pixel_array\n",
    "    # hu  = apply_modality_lut(arr, ds)\n",
    "    # '''\n",
    "    dcm_image = pydicom.read_file(path)\n",
    "    image = dcm_image.pixel_array\n",
    "    image = image.astype(np.int16)\n",
    "    image[image == -2000] = 0\n",
    "\n",
    "    intercept = dcm_image.RescaleIntercept\n",
    "    slope     = dcm_image.RescaleSlope\n",
    "\n",
    "    if slope != 1:\n",
    "        image = slope * image.astype(np.float64)\n",
    "        image = image.astype(np.int16)\n",
    "\n",
    "    image += np.int16(intercept)\n",
    "    # print(image.shape) # (512, 512)\n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "def dicom_normalize(image, MIN_HU=-1024.0, MAX_HU=3071.0):   # I already check the max value is 3071.0\n",
    "   image = (image - MIN_HU) / (MAX_HU - MIN_HU)   # Range  0.0 ~ 1.0\n",
    "#    image = (image - 0.5) / 0.5                  # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n",
    "   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.transforms import *\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [Total]  number =  6899\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/20/*/*/*.dcm')) + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/20/*/*/*.dcm'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/X/*/*/*.dcm'))  + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/X/*/*/*.dcm'))\n",
    "\n",
    "files = [{\"n_20\": n_20, \"n_100\": n_100} for n_20, n_100 in zip(n_20_imgs, n_100_imgs)]            \n",
    "print(\"Train [Total]  number = \", len(n_20_imgs))\n",
    "\n",
    "# CT에 맞는 Augmentation\n",
    "\n",
    "transforms = Compose(\n",
    "    [\n",
    "        Lambdad(keys=[\"n_20\", \"n_100\"], func=get_pixels_hu),\n",
    "        Lambdad(keys=[\"n_20\", \"n_100\"], func=dicom_normalize),\n",
    "        AddChanneld(keys=[\"n_20\", \"n_100\"]),                 \n",
    "\n",
    "        # Crop  \n",
    "        # RandWeightedCropd(keys=[\"image\"], w_key=[\"image\"], spatial_size=(512,512,1), num_samples=1),\n",
    "        # RandSpatialCropd(keys=[\"image\"], roi_size=(512, 512), random_size=False, random_center=True),\n",
    "        # RandSpatialCropd(keys=[\"image\"], roi_size=(512,512,3), random_size=False, random_center=True),\n",
    "#         RandSpatialCropSamplesd(keys=[\"n_20\", \"n_100\"], roi_size=(64, 64), num_samples=8, random_center=True, random_size=False, meta_keys=None, allow_missing_keys=False), \n",
    "            # patch training, next(iter(loader)) output : list로 sample 만큼,,, 그 List 안에 (B, C, H, W)\n",
    "\n",
    "        # (45 degree rotation, vertical & horizontal flip & scaling)\n",
    "#         RandFlipd(keys=[\"n_20\", \"n_100\"], prob=0.1, spatial_axis=[0, 1], allow_missing_keys=False),\n",
    "#         RandRotated(keys=[\"n_20\", \"n_100\"], prob=0.1, range_x=np.pi/4, range_y=np.pi/4, range_z=0.0, keep_size=True, align_corners=False, allow_missing_keys=False),\n",
    "#         RandZoomd(keys=[\"n_20\", \"n_100\"], prob=0.1, min_zoom=0.5, max_zoom=2.0, align_corners=None, keep_size=True, allow_missing_keys=False),\n",
    "        ToTensord(keys=[\"n_20\", \"n_100\"]),\n",
    "    ]\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dataset(data=files, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_denormalize(image, MIN_HU=-1024.0, MAX_HU=3071.0):\n",
    "    # image = (image - 0.5) / 0.5           # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n",
    "    image = (MAX_HU - MIN_HU)*image + MIN_HU\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8702f9da90>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dicom_denormalize(t[470]['n_20'].squeeze()), 'gray', vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f85ae32d550>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dicom_denormalize(t[470]['n_100'].squeeze()), 'gray', vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2162e-06, dtype=torch.float64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000042162088882"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.2162e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000062162193206"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(6.2162e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.981746495508297"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(4.2162e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.59352195637245"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(6.2162e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-06"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-06"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list = read_log(path = '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/log.txt')\n",
    "\n",
    "train_lr   = [ log_list[i]['train_lr'] for i in range(len(log_list)) ]\n",
    "train_loss = [ log_list[i]['train_loss'] for i in range(len(log_list)) ]\n",
    "valid_loss = [ log_list[i]['valid_loss'] for i in range(len(log_list)) ]\n",
    "epoch      = [ log_list[i]['epoch'] for i in range(len(log_list)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(train_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(valid_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(np.argsort(valid_loss)[:10]) & set(np.argsort(train_loss)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py \\\n",
    "--training-mode 'sinogram' \\\n",
    "--data-set 'TEST_Sinogram_DCM' \\\n",
    "--model-name 'ED_CNN' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Test/png/[Privious]ED_CNN/epoch_999/' \\\n",
    "--num_workers 4 \\\n",
    "--pin-mem \\\n",
    "--range-minus1-plus1 'False' \\\n",
    "--teacher_forcing \"False\" \\\n",
    "--resume '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/epoch_999_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 978 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Original === \n",
    "PSNR avg: 54.4628 \n",
    "SSIM avg: 0.9956 \n",
    "RMSE avg: 7.9607\n",
    "\n",
    "\n",
    "Predictions === \n",
    "PSNR avg: 57.6190 \n",
    "SSIM avg: 0.9980 \n",
    "RMSE avg: 5.5423\n",
    "***********************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
