{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels==0.7.4\n",
    "# !pip install efficientnet-pytorch==0.6.3\n",
    "# !pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1.4.0 버전\n",
    "# !pip install torch==1.4.0\n",
    "# !pip install torchvision==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 1.6.0 버전\n",
    "# !pip install torch==1.6.0\n",
    "# !pip install torchvision==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('/workspace/sunggu'))\n",
    "sys.path.append(os.path.abspath('/workspace/sunggu/4.Dose_img2img'))\n",
    "sys.path.append(os.path.abspath('/workspace/sunggu/MONAI'))\n",
    "from sunggu_utils import check_value, take_list, plot_confusion_matrix, list_sort_nicely, find_dir, plot_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import skimage\n",
    "import monai\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss, GeneralizedDiceLoss, FocalLoss, TverskyLoss\n",
    "from monai.metrics import compute_meandice, DiceMetric, ConfusionMatrixMetric \n",
    "from monai.networks.layers import Norm\n",
    "from monai.networks.nets import UNet, highresnet\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadNiftid,\n",
    "    LoadNumpyd,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Lambdad,\n",
    "    ToTensord,\n",
    "    RandScaleIntensityd,\n",
    "    RandGaussianNoised,\n",
    "    RandFlipd,\n",
    "    RandZoomd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandShiftIntensityd,\n",
    "    SpatialPadd,\n",
    "    RandAffined,\n",
    "    CastToTyped,\n",
    "    DeleteItemsd,\n",
    "    FgBgToIndicesd,\n",
    "    Rand3DElasticd,\n",
    "    RandZoomd,\n",
    "    Rand2DElasticd,\n",
    "    RandWeightedCropd,\n",
    "    AsDiscrete,\n",
    "    SpatialPadd,\n",
    "    CenterSpatialCropd,\n",
    "    adaptor,\n",
    ")\n",
    "from monai.utils import first, set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 시드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "set_determinism(seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_low_images  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Train/noise20_b50f_5.0/*/*.npy'))\n",
    "train_high_images = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Train/noise100_b50f_5.0/*/*.npy'))\n",
    "\n",
    "valid_low_images  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Valid/noise20_b50f_5.0/*/*.npy'))\n",
    "valid_high_images = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Valid/noise100_b50f_5.0/*/*.npy'))\n",
    "\n",
    "total_low_list  = train_low_images  + valid_low_images\n",
    "total_high_list = train_high_images + valid_high_images\n",
    "\n",
    "train_files = [{\"low\": low_name, \"high\": high_name} for low_name, high_name in zip(total_low_list, total_high_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CT에 맞는 Augmentation\n",
    "from torchvision import transforms\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadNumpyd(keys=[\"low\", \"high\"]),\n",
    "        AddChanneld(keys=[\"low\", \"high\"]), \n",
    "        ToTensord(keys=[\"low\", \"high\"]),\n",
    "        Lambdad(keys=[\"low\", \"high\"], func=transforms.Normalize(mean=(0.5), std=(0.5))),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check transforms in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_windowing(x):\n",
    "    x = (x * 0.5) + 0.5 \n",
    "    x = np.clip(x, a_min=0.250, a_max=0.270)\n",
    "    x -= x.min()\n",
    "    x /= x.max()  \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "# check_loader = DataLoader(check_ds, batch_size=1, shuffle=False)\n",
    "# check_data = next(iter(check_loader))\n",
    "\n",
    "check_data = check_ds[160]\n",
    "\n",
    "print(check_data['low_meta_dict']['filename_or_obj'])\n",
    "print(check_data['high_meta_dict']['filename_or_obj'])\n",
    "\n",
    "low = (check_data[\"low\"][0])\n",
    "high = (check_data[\"high\"][0])\n",
    "print(f\"image shape: {low.shape}\")\n",
    "\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"low\")\n",
    "plt.imshow(visual_windowing(low), cmap=\"gray\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"high\")\n",
    "plt.imshow(visual_windowing(high), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "# from torchsampler.imbalanced import ImbalancedDatasetSampler, sunggu_ImbalancedDatasetSampler\n",
    "\n",
    "# cf) use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "print(\"CPU 갯수 = \", multiprocessing.cpu_count())\n",
    "\n",
    "# Cachedataset 이거 뭔가 문제가 있음...\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "# train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=0.5)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net_multi_gpu(net, init_type='normal', init_gain=0.02):\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    net.to('cuda')        \n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net\n",
    "\n",
    "def init_net_sigle_gpu(net, init_type='normal', init_gain=0.02):\n",
    "    net.to('cuda')        \n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from OT_CycleGAN_sunggu.models import *\n",
    "from Cyclegan_sunggu.util import *\n",
    "import itertools\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# 모델\n",
    "netG_low_2_high = OT_Generator(in_channels=1, out_channels=1, feature=64)\n",
    "netG_high_2_low = OT_Generator(in_channels=1, out_channels=1, feature=64)\n",
    "\n",
    "netD_low  = OT_Discriminator(in_channels=1, out_channels=1, feature=64)\n",
    "netD_high = OT_Discriminator(in_channels=1, out_channels=1, feature=64)\n",
    "\n",
    "# multi-gpu 사용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    \n",
    "    netG_low_2_high = init_net_multi_gpu(netG_low_2_high, init_type='normal', init_gain=0.02)\n",
    "    netG_high_2_low = init_net_multi_gpu(netG_high_2_low, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    netD_low        = init_net_multi_gpu(netD_low, init_type='normal', init_gain=0.02)\n",
    "    netD_high       = init_net_multi_gpu(netD_high, init_type='normal', init_gain=0.02)\n",
    "\n",
    "else :\n",
    "    netG_low_2_high = init_net_sigle_gpu(netG_low_2_high, init_type='normal', init_gain=0.02)\n",
    "    netG_high_2_low = init_net_sigle_gpu(netG_high_2_low, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    netD_low        = init_net_sigle_gpu(netD_low, init_type='normal', init_gain=0.02)\n",
    "    netD_high       = init_net_sigle_gpu(netD_high, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# input_size = (1,32,320,320)\n",
    "# summary(model.encoder, input_size, batch_size=-1, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 이어서 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 네트워크 불러오기\n",
    "checkpoint_dir = '/workspace/sunggu/4.Dose_img2img/model/OT_Cycle_Gan_2D_sinogram/epoch_55_model.pth'\n",
    "checkpoint = torch.load(checkpoint_dir)\n",
    "\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "netG_low_2_high.load_state_dict(checkpoint['netG_low_2_high_state_dict'])\n",
    "netG_high_2_low.load_state_dict(checkpoint['netG_high_2_low_state_dict'])\n",
    "netD_low.load_state_dict(checkpoint['netD_low_state_dict'])\n",
    "netD_high.load_state_dict(checkpoint['netD_high_state_dict'])\n",
    "     \n",
    "\n",
    "## multi-gpu 사용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    netG_low_2_high = torch.nn.DataParallel(netG_low_2_high)\n",
    "    netG_high_2_low = torch.nn.DataParallel(netG_high_2_low)\n",
    "    netD_low = torch.nn.DataParallel(netD_low)\n",
    "    netD_high = torch.nn.DataParallel(netD_high)\n",
    "    \n",
    "    \n",
    "netG_low_2_high.to('cuda')  \n",
    "netG_high_2_low.to('cuda')  \n",
    "netD_low.to('cuda')  \n",
    "netD_high.to('cuda')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_loss = nn.L1Loss()\n",
    "# gan_loss = nn.BCELoss()\n",
    "identity_loss = nn.L1Loss()\n",
    "\n",
    "learning_rate = 2e-4\n",
    "max_epochs = 1000\n",
    "\n",
    "# Optimizer 설정하기\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_low_2_high.parameters(), netG_high_2_low.parameters()), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(netD_low.parameters(), netD_high.parameters()), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "def lambda_rule(epoch, start_decay_epoch=100, total_epoch=1000):\n",
    "    lr = 1.0 - max(0, epoch - start_decay_epoch) / float(total_epoch)\n",
    "    return lr\n",
    "\n",
    "scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
    "scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=lambda_rule)\n",
    "\n",
    "# Generated image pool\n",
    "from Cyclegan_sunggu.image_pool import ImagePool\n",
    "num_pool = 0\n",
    "fake_low_pool  = ImagePool(num_pool)\n",
    "fake_high_pool = ImagePool(num_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "\n",
    "scheduler_G.load_state_dict(checkpoint['scheduler_G'])        \n",
    "scheduler_D.load_state_dict(checkpoint['scheduler_D'])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 필요한 Weight만 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Before\n",
    "# model_dict = model.state_dict()\n",
    "# print(\"이전 weight = \", model_dict['encoder._conv_stem.weight'][0])\n",
    "\n",
    "# load_dir = '/workspace/sunggu/1.Hemorrhage/monai_experiment/model/Efficient3d_conv2d_Aux/'\n",
    "# pretrained_dict =  torch.load(os.path.join(load_dir, \"epoch_0_best_metric_model.pth\")) \n",
    "\n",
    "# # 1. filter out unnecessary keys\n",
    "# pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "# model.load_state_dict(model_dict)\n",
    "\n",
    "# # After\n",
    "# print(\"이후 weight = \", model_dict['encoder._conv_stem.weight'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 그밖에 부수적인 functions 설정하기\n",
    "fn_tonumpy = lambda x: x.cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "fn_denorm  = lambda x: (x * 0.5) + 0.5\n",
    "fn_denorm_window  = visual_windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_penalty(critic, real_data, fake_data, device='cuda'):\n",
    "    \n",
    "    alpha = torch.rand_like(real_data).to(device)\n",
    "\n",
    "    interpolates = alpha*real_data + (1-alpha)*fake_data.detach()\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates = torch.autograd.Variable(interpolates, requires_grad = True)\n",
    "\n",
    "    critic_interpolates = critic(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=critic_interpolates, \n",
    "                                    inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(critic_interpolates.size()).to(device),\n",
    "                                    create_graph=True, \n",
    "                                    retain_graph=True, \n",
    "                                    only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    # Regularization hyperparameter for gradient penalty = 0.5, 공식 wgan-gp:10\n",
    "    gradient_penalty = 0.5 * ((gradients.norm(2, dim=1) - 1) ** 2).mean() \n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "## 네트워크 학습시키기\n",
    "st_epoch = 0\n",
    "num_D_iter = 5\n",
    "epoch_num = 1000\n",
    "val_interval = 10\n",
    "\n",
    "epoch_train_loss_list = list()\n",
    "epoch_val_loss_list = list()\n",
    "\n",
    "epoch_train_metric_list = list()\n",
    "epoch_val_metric_list = list()\n",
    "\n",
    "writer = SummaryWriter(log_dir='/workspace/sunggu/4.Dose_img2img/runs/OT_Cycle_Gan_2D_sinogram')\n",
    "root_dir = '/workspace/sunggu/4.Dose_img2img/model/OT_Cycle_Gan_2D_sinogram/'\n",
    "\n",
    "low2high_png_dir = '/workspace/sunggu/4.Dose_img2img/Predictions/png/'+'OT_Cycle_Gan_2D_sinogram'+'/low2high/'\n",
    "\n",
    "\n",
    "# 모델 save폴더 만들기\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir, mode=0o777)\n",
    "\n",
    "if not os.path.exists(low2high_png_dir):\n",
    "    os.makedirs(low2high_png_dir, mode=0o777)\n",
    "\n",
    "    \n",
    "for epoch in range(56, epoch_num):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch}/{epoch_num}\")\n",
    "    \n",
    "    # Model 선언\n",
    "    netG_low_2_high.train()\n",
    "    netG_high_2_low.train()\n",
    "    netD_low.train()\n",
    "    netD_high.train()\n",
    "    \n",
    "    # Loss 선언\n",
    "    loss_G_low2high_train = []\n",
    "    loss_G_high2low_train = []\n",
    "    \n",
    "    loss_D_low_train = []\n",
    "    loss_D_high_train = []\n",
    "    \n",
    "    loss_cycle_low_train = []\n",
    "    loss_cycle_high_train = []\n",
    "    \n",
    "    loss_ident_low_train = []\n",
    "    loss_ident_high_train = []\n",
    "    \n",
    "    D_iterator = tqdm(range(len(train_loader)), desc='Iteration', file=sys.stdout)  \n",
    "    for iteration in D_iterator:\n",
    "        \n",
    "        # Backward Discriminator  ################################################################        \n",
    "        for _ in range(num_D_iter):\n",
    "            batch_data = next(iter(train_loader))\n",
    "        \n",
    "            set_requires_grad([netD_low, netD_high], True)\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            input_low  = batch_data['low'].to(device)\n",
    "            input_high = batch_data['high'].to(device)\n",
    "\n",
    "            output_low  = netG_high_2_low(input_high)\n",
    "            output_high = netG_low_2_high(input_low)\n",
    "\n",
    "            recon_high  = netG_low_2_high(output_low)\n",
    "            recon_low   = netG_high_2_low(output_high)        \n",
    "\n",
    "            ident_low   = netG_high_2_low(input_low)\n",
    "            ident_high  = netG_low_2_high(input_high)         \n",
    "            \n",
    "                # LOW (Discriminator Gan Loss)\n",
    "            pred_real_low = netD_low(input_low)        \n",
    "            ### POOL\n",
    "            output_low    = fake_low_pool.query(output_low)\n",
    "            pred_fake_low = netD_low(output_low.detach())\n",
    "\n",
    "            loss_D_low_real = -pred_real_low.mean()\n",
    "            loss_D_low_fake = pred_fake_low.mean()\n",
    "            loss_D_low = loss_D_low_real + loss_D_low_fake + grad_penalty(netD_low, input_low, output_low)\n",
    "            \n",
    "\n",
    "                # HIGH (Discriminator Gan Loss)\n",
    "            pred_real_high = netD_high(input_high)\n",
    "            ### POOL\n",
    "            output_high    = fake_high_pool.query(output_high)\n",
    "            pred_fake_high = netD_high(output_high.detach())\n",
    "\n",
    "            loss_D_high_real = -pred_real_high.mean()\n",
    "            loss_D_high_fake = pred_fake_low.mean()\n",
    "            loss_D_high = loss_D_high_real + loss_D_high_fake + grad_penalty(netD_high, input_high, output_high)\n",
    "            \n",
    "            loss_D = loss_D_high + loss_D_low\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # 기록\n",
    "            loss_D_low_train  += [loss_D_low.item()]\n",
    "            loss_D_high_train += [loss_D_high.item()]\n",
    "        \n",
    "        # Backward Generator ################################################################\n",
    "        set_requires_grad([netD_low, netD_high], False)\n",
    "        optimizer_G.zero_grad()\n",
    "      \n",
    "            # Gerator Gan Loss\n",
    "        pred_fake_low  = netD_low(output_low)\n",
    "        pred_fake_high = netD_high(output_high)\n",
    "\n",
    "            # Gan loss\n",
    "        loss_G_low2high = -pred_fake_low.mean()\n",
    "        loss_G_high2low = -pred_fake_high.mean()\n",
    "\n",
    "            # Cycle Loss\n",
    "        loss_cycle_low  = cycle_loss(input_low, recon_low)\n",
    "        loss_cycle_high = cycle_loss(input_high, recon_high)\n",
    "\n",
    "            # Identity Loss\n",
    "        loss_ident_low  = identity_loss(input_low, ident_low)\n",
    "        loss_ident_high = identity_loss(input_high, ident_high)        \n",
    "\n",
    "        loss_G = (loss_G_low2high + loss_G_high2low) + \\\n",
    "                10.0 * (loss_cycle_low + loss_cycle_high) + \\\n",
    "                0.5 * (10.0 * loss_ident_low + 10.0 * loss_ident_high)\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 기록\n",
    "        loss_G_low2high_train += [loss_G_low2high.item()]\n",
    "        loss_G_high2low_train += [loss_G_high2low.item()]\n",
    "\n",
    "        loss_cycle_low_train  += [loss_cycle_low.item()]\n",
    "        loss_cycle_high_train += [loss_cycle_high.item()]\n",
    "\n",
    "        loss_ident_low_train  += [loss_ident_low.item()]\n",
    "        loss_ident_high_train += [loss_ident_high.item()]\n",
    "        \n",
    "        \n",
    "    print( \"Generator Loss       [Low to High] = %.4f\" %np.mean(loss_G_low2high_train) ) \n",
    "    print( \"Generator Loss       [High to Low] = %.4f\" %np.mean(loss_G_high2low_train) )\n",
    "    print( \"Discriminator LOss   [Low]         = %.4f\" %np.mean(loss_D_low_train) )\n",
    "    print( \"Discriminator Loss   [High]        = %.4f\" %np.mean(loss_D_high_train) )\n",
    "\n",
    "    print( \"Cycle Loss           [Low]         = %.4f\" %np.mean(loss_cycle_low_train) )\n",
    "    print( \"Cycle Loss           [High]        = %.4f\" %np.mean(loss_cycle_high_train) )\n",
    "\n",
    "    print( \"Identity Loss        [Low]         = %.4f\" %np.mean(loss_ident_low_train) )\n",
    "    print( \"Identity Loss        [High]        = %.4f\" %np.mean(loss_ident_high_train) )\n",
    "\n",
    "    total_loss = np.mean(loss_G_low2high_train) + np.mean(loss_G_high2low_train) +\\\n",
    "                 np.mean(loss_D_low_train)      + np.mean(loss_D_high_train)     +\\\n",
    "                 np.mean(loss_D_high_train)     + np.mean(loss_cycle_high_train) +\\\n",
    "                 np.mean(loss_ident_low_train)  + np.mean(loss_ident_high_train)\n",
    "\n",
    "    print( \"TOTAL Loss                         = %.4f\" %total_loss )\n",
    "\n",
    "    # Tensorboard 저장하기\n",
    "    input_low   = fn_denorm_window(fn_tonumpy((input_low)))\n",
    "    input_high  = fn_denorm_window(fn_tonumpy((input_high)))\n",
    "    output_high = fn_denorm_window(fn_tonumpy((output_high)))\n",
    "\n",
    "    input_low   = np.clip(input_low, a_min=0, a_max=1)\n",
    "    input_high  = np.clip(input_high, a_min=0, a_max=1)\n",
    "    output_high = np.clip(output_high, a_min=0, a_max=1)\n",
    "\n",
    "    # png Save\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_input_low.png',   input_low[0].squeeze(),   cmap=\"gray\")\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_output_high.png', output_high[0].squeeze(), cmap=\"gray\")\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_input_high.png',  input_high[0].squeeze(),  cmap=\"gray\")\n",
    "\n",
    "            \n",
    "    # Loss Write    \n",
    "    writer.add_scalar('loss_G_low2high', np.mean(loss_G_low2high_train), epoch)\n",
    "    writer.add_scalar('loss_G_high2low', np.mean(loss_G_high2low_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_D_low', np.mean(loss_D_low_train), epoch)\n",
    "    writer.add_scalar('loss_D_high', np.mean(loss_D_high_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_cycle_low', np.mean(loss_cycle_low_train), epoch)\n",
    "    writer.add_scalar('loss_cycle_high', np.mean(loss_cycle_high_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_ident_low', np.mean(loss_ident_low_train), epoch)\n",
    "    writer.add_scalar('loss_ident_high', np.mean(loss_ident_high_train), epoch)\n",
    "    \n",
    "    # 저장\n",
    "    if epoch % 5 == 0 or epoch == epoch_num:\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            checkpoint = {'epoch': epoch, \n",
    "                          'netG_low_2_high_state_dict': netG_low_2_high.module.state_dict(), \n",
    "                          'netG_high_2_low_state_dict': netG_high_2_low.module.state_dict(), \n",
    "                          'netD_low_state_dict': netD_low.module.state_dict(), \n",
    "                          'netD_high_state_dict': netD_high.module.state_dict(), \n",
    "                          'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                          'optimizer_D_state_dict': optimizer_D.state_dict(),  \n",
    "                          'scheduler_G': scheduler_G.state_dict(),\n",
    "                          'scheduler_D': scheduler_D.state_dict(),\n",
    "                         }                    \n",
    "\n",
    "        else:\n",
    "            checkpoint = {'epoch': epoch, \n",
    "                          'netG_low_2_high_state_dict': netG_low_2_high.state_dict(), \n",
    "                          'netG_high_2_low_state_dict': netG_high_2_low.state_dict(), \n",
    "                          'netD_low_state_dict': netD_low.state_dict(), \n",
    "                          'netD_high_state_dict': netD_high.state_dict(), \n",
    "                          'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                          'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                          'scheduler_G': scheduler_G.state_dict(),\n",
    "                          'scheduler_D': scheduler_D.state_dict(),\n",
    "                         }                         \n",
    "\n",
    "            torch.save(checkpoint, os.path.join(root_dir, \"epoch_\" + str(epoch) + \"_model.pth\"))        \n",
    "    \n",
    "    # Scheduler\n",
    "    writer.add_scalar('lr', optimizer_G.param_groups[0]['lr'], epoch)      \n",
    "    old_lr = optimizer_G.param_groups[0]['lr']\n",
    "    lr = optimizer_G.param_groups[0]['lr']\n",
    "    print('Learning Rate %.10f -> %.10f' % (old_lr, lr))\n",
    "\n",
    "    scheduler_G.step()    \n",
    "    scheduler_D.step()    \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    checkpoint = {'epoch': epoch, \n",
    "                  'netG_low_2_high_state_dict': netG_low_2_high.module.state_dict(), \n",
    "                  'netG_high_2_low_state_dict': netG_high_2_low.module.state_dict(), \n",
    "                  'netD_low_state_dict': netD_low.module.state_dict(), \n",
    "                  'netD_high_state_dict': netD_high.module.state_dict(), \n",
    "                  'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                  'optimizer_D_state_dict': optimizer_D.state_dict(),  \n",
    "                  'scheduler_G': scheduler_G.state_dict(),\n",
    "                  'scheduler_D': scheduler_D.state_dict(),\n",
    "                 }                    \n",
    "\n",
    "else:\n",
    "    checkpoint = {'epoch': epoch, \n",
    "                  'netG_low_2_high_state_dict': netG_low_2_high.state_dict(), \n",
    "                  'netG_high_2_low_state_dict': netG_high_2_low.state_dict(), \n",
    "                  'netD_low_state_dict': netD_low.state_dict(), \n",
    "                  'netD_high_state_dict': netD_high.state_dict(), \n",
    "                  'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                  'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                  'scheduler_G': scheduler_G.state_dict(),\n",
    "                  'scheduler_D': scheduler_D.state_dict(),\n",
    "                 }                         \n",
    "\n",
    "    torch.save(checkpoint, os.path.join(root_dir, \"epoch_\" + str(epoch) + \"_model.pth\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_network(net, name='network'):\n",
    "    \"\"\"Calculate and print the mean of average absolute(gradients)\n",
    "    Parameters:\n",
    "        net (torch network) -- Torch network\n",
    "        name (str) -- the name of the network\n",
    "    \"\"\"\n",
    "    mean = 0.0\n",
    "    count = 0\n",
    "    for param in net.parameters():\n",
    "        if param.grad is not None:\n",
    "            mean += torch.mean(torch.abs(param.grad.data))\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        mean = mean / count\n",
    "    print(name)\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 원본\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "## 네트워크 학습시키기\n",
    "st_epoch = 0\n",
    "\n",
    "epoch_num = 1000\n",
    "val_interval = 10\n",
    "\n",
    "epoch_train_loss_list = list()\n",
    "epoch_val_loss_list = list()\n",
    "\n",
    "epoch_train_metric_list = list()\n",
    "epoch_val_metric_list = list()\n",
    "\n",
    "writer = SummaryWriter(log_dir='/workspace/sunggu/4.Dose_img2img/runs/OT_Cycle_Gan_2D_sinogram')\n",
    "root_dir = '/workspace/sunggu/4.Dose_img2img/model/OT_Cycle_Gan_2D_sinogram/'\n",
    "\n",
    "low2high_png_dir = '/workspace/sunggu/4.Dose_img2img/Predictions/png/'+'OT_Cycle_Gan_2D_sinogram'+'/low2high/'\n",
    "\n",
    "\n",
    "# 모델 save폴더 만들기\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir, mode=0o777)\n",
    "\n",
    "if not os.path.exists(low2high_png_dir):\n",
    "    os.makedirs(low2high_png_dir, mode=0o777)\n",
    "\n",
    "\n",
    "    \n",
    "for epoch in range(41, epoch_num):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch}/{epoch_num}\")\n",
    "    \n",
    "    # Model 선언\n",
    "    netG_low_2_high.train()\n",
    "    netG_high_2_low.train()\n",
    "    netD_low.train()\n",
    "    netD_high.train()\n",
    "    \n",
    "    # Loss 선언\n",
    "    loss_G_low2high_train = []\n",
    "    loss_G_high2low_train = []\n",
    "    \n",
    "    loss_D_low_train = []\n",
    "    loss_D_high_train = []\n",
    "    \n",
    "    loss_cycle_low_train = []\n",
    "    loss_cycle_high_train = []\n",
    "    \n",
    "    loss_ident_low_train = []\n",
    "    loss_ident_high_train = []\n",
    "    \n",
    "    # Train Discriminator\n",
    "    train_iterator = tqdm(train_loader, desc='Discriminator', file=sys.stdout)    \n",
    "    for batch_data in train_iterator:\n",
    "        \n",
    "        set_requires_grad([netD_low, netD_high], True)\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        input_low = batch_data['low'].to(device)\n",
    "        input_high = batch_data['high'].to(device)\n",
    "\n",
    "        # LOW (Discriminator Gan Loss)\n",
    "        output_low = netG_high_2_low(input_high)\n",
    "        pred_real_low = netD_low(input_low)\n",
    "        \n",
    "        #### POOL\n",
    "        output_low = fake_low_pool.query(output_low)\n",
    "        pred_fake_low = netD_low(output_low.detach())\n",
    "        \n",
    "        loss_D_low_real = -pred_real_low.mean()\n",
    "        loss_D_low_fake = pred_fake_low.mean()\n",
    "        loss_D_low = loss_D_low_real + loss_D_low_fake + grad_penalty(netD_low, input_low, output_low)\n",
    "        loss_D_low.backward(retain_graph=True)\n",
    "        \n",
    "        # HIGH (Discriminator Gan Loss)\n",
    "        output_high = netG_low_2_high(input_low)\n",
    "        pred_real_high = netD_high(input_high)\n",
    "        \n",
    "        #### POOL\n",
    "        output_high = fake_high_pool.query(output_high)\n",
    "        pred_fake_high = netD_high(output_high.detach())\n",
    "        \n",
    "        loss_D_high_real = -pred_real_high.mean()\n",
    "        loss_D_high_fake = pred_fake_low.mean()\n",
    "        loss_D_high = loss_D_high_real + loss_D_high_fake + grad_penalty(netD_high, input_high, output_high)\n",
    "        loss_D_high.backward(retain_graph=True)\n",
    "        \n",
    "        loss_D = loss_D_high + loss_D_low\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # 기록\n",
    "        loss_D_low_train += [loss_D_low.item()]\n",
    "        loss_D_high_train += [loss_D_high.item()]\n",
    "        \n",
    "        \n",
    "    # Train Generator \n",
    "    train_iterator = tqdm(train_loader, desc='Generator', file=sys.stdout)    \n",
    "    for batch_data in train_iterator:      \n",
    "        \n",
    "        set_requires_grad([netD_low, netD_high], False)\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        input_low  = batch_data['low'].to(device)\n",
    "        input_high = batch_data['high'].to(device)\n",
    "        \n",
    "        output_low  = netG_high_2_low(input_high)\n",
    "        output_high = netG_low_2_high(input_low)\n",
    "        \n",
    "        recon_high = netG_low_2_high(output_low)\n",
    "        recon_low  = netG_high_2_low(output_high)\n",
    "\n",
    "        ident_low  = netG_high_2_low(input_low)\n",
    "        ident_high = netG_low_2_high(input_high)        \n",
    "        \n",
    "        # Gerator Gan Loss\n",
    "        pred_fake_low  = netD_low(output_low)\n",
    "        pred_fake_high = netD_high(output_high)\n",
    "        \n",
    "        # Gan loss\n",
    "        loss_G_low2high = -pred_fake_low.mean()\n",
    "        loss_G_high2low = -pred_fake_high.mean()\n",
    "            \n",
    "        # Cycle Loss\n",
    "        loss_cycle_low  = cycle_loss(input_low, recon_low)\n",
    "        loss_cycle_high = cycle_loss(input_high, recon_high)\n",
    "\n",
    "        # Identity Loss\n",
    "        loss_ident_low  = identity_loss(input_low, ident_low)\n",
    "        loss_ident_high = identity_loss(input_high, ident_high)        \n",
    "        \n",
    "        loss_G = (loss_G_low2high + loss_G_high2low) + 10.0*(loss_cycle_low + loss_cycle_high) + 0.5*(10.0*loss_ident_low + 10.0*loss_ident_high)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        \n",
    "        # 기록\n",
    "        loss_G_low2high_train += [loss_G_low2high.item()]\n",
    "        loss_G_high2low_train += [loss_G_high2low.item()]\n",
    "        \n",
    "        loss_cycle_low_train  += [loss_cycle_low.item()]\n",
    "        loss_cycle_high_train += [loss_cycle_high.item()]\n",
    "        \n",
    "        loss_ident_low_train  += [loss_ident_low.item()]\n",
    "        loss_ident_high_train += [loss_ident_high.item()]\n",
    "        \n",
    "        \n",
    "    print( \"Generator Loss       [Low to High] = %.4f\" %np.mean(loss_G_low2high_train) ) \n",
    "    print( \"Generator Loss       [High to Low] = %.4f\" %np.mean(loss_G_high2low_train) )\n",
    "    print( \"Discriminator LOss   [Low]         = %.4f\" %np.mean(loss_D_low_train) )\n",
    "    print( \"Discriminator Loss   [High]        = %.4f\" %np.mean(loss_D_high_train) )\n",
    "\n",
    "    print( \"Cycle Loss           [Low]         = %.4f\" %np.mean(loss_cycle_low_train) )\n",
    "    print( \"Cycle Loss           [High]        = %.4f\" %np.mean(loss_cycle_high_train) )\n",
    "\n",
    "    print( \"Identity Loss        [Low]         = %.4f\" %np.mean(loss_ident_low_train) )\n",
    "    print( \"Identity Loss        [High]        = %.4f\" %np.mean(loss_ident_high_train) )\n",
    "\n",
    "    total_loss = np.mean(loss_G_low2high_train) + np.mean(loss_G_high2low_train) +\\\n",
    "                 np.mean(loss_D_low_train)      + np.mean(loss_D_high_train)     +\\\n",
    "                 np.mean(loss_D_high_train)     + np.mean(loss_cycle_high_train) +\\\n",
    "                 np.mean(loss_ident_low_train)  + np.mean(loss_ident_high_train)\n",
    "\n",
    "    print( \"TOTAL Loss                         = %.4f\" %total_loss )\n",
    "\n",
    "    # Tensorboard 저장하기\n",
    "    input_low   = fn_denorm_window(fn_tonumpy((input_low)))\n",
    "    input_high  = fn_denorm_window(fn_tonumpy((input_high)))\n",
    "    output_high = fn_denorm_window(fn_tonumpy((output_high)))\n",
    "\n",
    "    input_low   = np.clip(input_low, a_min=0, a_max=1)\n",
    "    input_high  = np.clip(input_high, a_min=0, a_max=1)\n",
    "    output_high = np.clip(output_high, a_min=0, a_max=1)\n",
    "\n",
    "    # png Save\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_input_low.png',   input_low[0].squeeze(),   cmap=\"gray\")\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_output_high.png', output_high[0].squeeze(), cmap=\"gray\")\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_input_high.png',  input_high[0].squeeze(),  cmap=\"gray\")\n",
    "\n",
    "            \n",
    "    # Loss Write    \n",
    "    writer.add_scalar('loss_G_low2high', np.mean(loss_G_low2high_train), epoch)\n",
    "    writer.add_scalar('loss_G_high2low', np.mean(loss_G_high2low_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_D_low', np.mean(loss_D_low_train), epoch)\n",
    "    writer.add_scalar('loss_D_high', np.mean(loss_D_high_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_cycle_low', np.mean(loss_cycle_low_train), epoch)\n",
    "    writer.add_scalar('loss_cycle_high', np.mean(loss_cycle_high_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_ident_low', np.mean(loss_ident_low_train), epoch)\n",
    "    writer.add_scalar('loss_ident_high', np.mean(loss_ident_high_train), epoch)\n",
    "    \n",
    "    # 저장\n",
    "    if epoch % 5 == 0 or epoch == epoch_num:\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            checkpoint = {'epoch': epoch, \n",
    "                          'netG_low_2_high_state_dict': netG_low_2_high.module.state_dict(), \n",
    "                          'netG_high_2_low_state_dict': netG_high_2_low.module.state_dict(), \n",
    "                          'netD_low_state_dict': netD_low.module.state_dict(), \n",
    "                          'netD_high_state_dict': netD_high.module.state_dict(), \n",
    "                          'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                          'optimizer_D_state_dict': optimizer_D.state_dict(),  \n",
    "                          'scheduler_G': scheduler_G.state_dict(),\n",
    "                          'scheduler_D': scheduler_D.state_dict(),\n",
    "                         }                    \n",
    "\n",
    "        else:\n",
    "            checkpoint = {'epoch': epoch, \n",
    "                          'netG_low_2_high_state_dict': netG_low_2_high.state_dict(), \n",
    "                          'netG_high_2_low_state_dict': netG_high_2_low.state_dict(), \n",
    "                          'netD_low_state_dict': netD_low.state_dict(), \n",
    "                          'netD_high_state_dict': netD_high.state_dict(), \n",
    "                          'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                          'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                          'scheduler_G': scheduler_G.state_dict(),\n",
    "                          'scheduler_D': scheduler_D.state_dict(),\n",
    "                         }                         \n",
    "\n",
    "            torch.save(checkpoint, os.path.join(root_dir, \"epoch_\" + str(epoch) + \"_model.pth\"))        \n",
    "    \n",
    "    # Scheduler\n",
    "    writer.add_scalar('lr', optimizer_G.param_groups[0]['lr'], epoch)      \n",
    "    old_lr = optimizer_G.param_groups[0]['lr']\n",
    "    lr = optimizer_G.param_groups[0]['lr']\n",
    "    print('Learning Rate %.10f -> %.10f' % (old_lr, lr))\n",
    "\n",
    "    scheduler_G.step()    \n",
    "    scheduler_D.step()    \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.026px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
