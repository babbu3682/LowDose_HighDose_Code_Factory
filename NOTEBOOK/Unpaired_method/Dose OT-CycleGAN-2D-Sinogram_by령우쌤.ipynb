{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels==0.7.4\n",
    "# !pip install efficientnet-pytorch==0.6.3\n",
    "# !pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1.4.0 버전\n",
    "# !pip install torch==1.4.0\n",
    "# !pip install torchvision==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 1.6.0 버전\n",
    "# !pip install torch==1.6.0\n",
    "# !pip install torchvision==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  1 01:44:28 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "|100%   88C    P2   169W / 280W |  14728MiB / 24220MiB |     83%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX           Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 47%   70C    P2   272W / 280W |  14330MiB / 24220MiB |     81%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN RTX           Off  | 00000000:1D:00.0 Off |                  N/A |\n",
      "| 80%   86C    P2   111W / 280W |  17196MiB / 24220MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN RTX           Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "|132%   92C    P2   110W / 280W |  17196MiB / 24220MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN RTX           Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 41%   27C    P8    14W / 280W |  14702MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN RTX           Off  | 00000000:3F:00.0 Off |                  N/A |\n",
      "| 41%   32C    P8    17W / 280W |  14340MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN RTX           Off  | 00000000:40:00.0 Off |                  N/A |\n",
      "| 98%   88C    P2   180W / 280W |  22252MiB / 24220MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN RTX           Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 41%   47C    P8     4W / 280W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('/workspace/sunggu'))\n",
    "sys.path.append(os.path.abspath('/workspace/sunggu/4.Dose_img2img'))\n",
    "sys.path.append(os.path.abspath('/workspace/sunggu/MONAI'))\n",
    "from sunggu_utils import check_value, take_list, plot_confusion_matrix, list_sort_nicely, find_dir, plot_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0+unknown\n",
      "Python version: 3.6.9 (default, Jul 17 2020, 12:50:27)  [GCC 8.4.0]\n",
      "OS version: Linux (4.15.0-137-generic)\n",
      "Numpy version: 1.18.5\n",
      "Pytorch version: 1.6.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 3.2.0\n",
      "scikit-image version: 0.17.2\n",
      "Pillow version: 8.0.1\n",
      "Tensorboard version: 2.3.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.7.0\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.50.2\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import skimage\n",
    "import monai\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss, GeneralizedDiceLoss, FocalLoss, TverskyLoss\n",
    "from monai.metrics import compute_meandice, DiceMetric, ConfusionMatrixMetric \n",
    "from monai.networks.layers import Norm\n",
    "from monai.networks.nets import UNet, highresnet\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadNiftid,\n",
    "    LoadNumpyd,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Lambdad,\n",
    "    ToTensord,\n",
    "    RandScaleIntensityd,\n",
    "    RandGaussianNoised,\n",
    "    RandFlipd,\n",
    "    RandZoomd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandShiftIntensityd,\n",
    "    SpatialPadd,\n",
    "    RandAffined,\n",
    "    CastToTyped,\n",
    "    DeleteItemsd,\n",
    "    FgBgToIndicesd,\n",
    "    Rand3DElasticd,\n",
    "    RandZoomd,\n",
    "    Rand2DElasticd,\n",
    "    RandWeightedCropd,\n",
    "    AsDiscrete,\n",
    "    SpatialPadd,\n",
    "    CenterSpatialCropd,\n",
    "    adaptor,\n",
    ")\n",
    "from monai.utils import first, set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 시드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "set_determinism(seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_low_images  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Train/noise20_b50f_5.0/*/*.npy'))\n",
    "train_high_images = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Train/noise100_b50f_5.0/*/*.npy'))\n",
    "\n",
    "valid_low_images  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Valid/noise20_b50f_5.0/*/*.npy'))\n",
    "valid_high_images = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Valid/noise100_b50f_5.0/*/*.npy'))\n",
    "\n",
    "total_low_list  = train_low_images  + valid_low_images\n",
    "total_high_list = train_high_images + valid_high_images\n",
    "\n",
    "train_files = [{\"low\": low_name, \"high\": high_name} for low_name, high_name in zip(total_low_list, total_high_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CT에 맞는 Augmentation\n",
    "from torchvision import transforms\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadNumpyd(keys=[\"low\", \"high\"]),\n",
    "        AddChanneld(keys=[\"low\", \"high\"]), \n",
    "        ToTensord(keys=[\"low\", \"high\"]),\n",
    "        Lambdad(keys=[\"low\", \"high\"], func=transforms.Normalize(mean=(0.5), std=(0.5))),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check transforms in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_windowing(x):\n",
    "    x = (x * 0.5) + 0.5 \n",
    "    x = np.clip(x, a_min=0.250, a_max=0.270)\n",
    "    x -= x.min()\n",
    "    x /= x.max()  \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Train/noise20_b50f_5.0/HUH_JEO_F79/HUH_JEO_F79_001_20210218_184354_00026.npy\n",
      "/workspace/sunggu/4.Dose_img2img/dataset/sinogram_dataset/2D_dataset/Train/noise100_b50f_5.0/HUH_JEO_F79/HUH_JEO_F79_001_20210218_184251_00026.npy\n",
      "image shape: torch.Size([512, 512])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "# check_loader = DataLoader(check_ds, batch_size=1, shuffle=False)\n",
    "# check_data = next(iter(check_loader))\n",
    "\n",
    "check_data = check_ds[160]\n",
    "\n",
    "print(check_data['low_meta_dict']['filename_or_obj'])\n",
    "print(check_data['high_meta_dict']['filename_or_obj'])\n",
    "\n",
    "low = (check_data[\"low\"][0])\n",
    "high = (check_data[\"high\"][0])\n",
    "print(f\"image shape: {low.shape}\")\n",
    "\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"low\")\n",
    "plt.imshow(visual_windowing(low), cmap=\"gray\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"high\")\n",
    "plt.imshow(visual_windowing(high), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 갯수 =  32\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "# from torchsampler.imbalanced import ImbalancedDatasetSampler, sunggu_ImbalancedDatasetSampler\n",
    "\n",
    "# cf) use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "print(\"CPU 갯수 = \", multiprocessing.cpu_count())\n",
    "\n",
    "# Cachedataset 이거 뭔가 문제가 있음...\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "# train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=0.5)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net_multi_gpu(net, init_type='normal', init_gain=0.02):\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    net.to('cuda')        \n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net\n",
    "\n",
    "def init_net_sigle_gpu(net, init_type='normal', init_gain=0.02):\n",
    "    net.to('cuda')        \n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n"
     ]
    }
   ],
   "source": [
    "from OT_CycleGAN_sunggu.models import *\n",
    "from Cyclegan_sunggu.util import *\n",
    "import itertools\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# 모델\n",
    "netG_low_2_high = OT_Generator(in_channels=1, out_channels=1, feature=64)\n",
    "netG_high_2_low = OT_Generator(in_channels=1, out_channels=1, feature=64)\n",
    "\n",
    "netD_low  = OT_Discriminator(in_channels=1, out_channels=1, feature=64)\n",
    "netD_high = OT_Discriminator(in_channels=1, out_channels=1, feature=64)\n",
    "\n",
    "# multi-gpu 사용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    \n",
    "    netG_low_2_high = init_net_multi_gpu(netG_low_2_high, init_type='normal', init_gain=0.02)\n",
    "    netG_high_2_low = init_net_multi_gpu(netG_high_2_low, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    netD_low        = init_net_multi_gpu(netD_low, init_type='normal', init_gain=0.02)\n",
    "    netD_high       = init_net_multi_gpu(netD_high, init_type='normal', init_gain=0.02)\n",
    "\n",
    "else :\n",
    "    netG_low_2_high = init_net_sigle_gpu(netG_low_2_high, init_type='normal', init_gain=0.02)\n",
    "    netG_high_2_low = init_net_sigle_gpu(netG_high_2_low, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    netD_low        = init_net_sigle_gpu(netD_low, init_type='normal', init_gain=0.02)\n",
    "    netD_high       = init_net_sigle_gpu(netD_high, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# input_size = (1,32,320,320)\n",
    "# summary(model.encoder, input_size, batch_size=-1, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 이어서 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OT_Discriminator(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (leaky1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (instance2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (leaky2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (instance3): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (leaky3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  (instance4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (leaky4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (last): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  (output): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 네트워크 불러오기\n",
    "checkpoint_dir = '/workspace/sunggu/4.Dose_img2img/model/OT_Cycle_Gan_2D_sinogram_fullbatch/epoch_60_model.pth'\n",
    "checkpoint = torch.load(checkpoint_dir)\n",
    "\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "netG_low_2_high.load_state_dict(checkpoint['netG_low_2_high_state_dict'])\n",
    "netG_high_2_low.load_state_dict(checkpoint['netG_high_2_low_state_dict'])\n",
    "netD_low.load_state_dict(checkpoint['netD_low_state_dict'])\n",
    "netD_high.load_state_dict(checkpoint['netD_high_state_dict'])\n",
    "     \n",
    "\n",
    "## multi-gpu 사용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    netG_low_2_high = torch.nn.DataParallel(netG_low_2_high)\n",
    "    netG_high_2_low = torch.nn.DataParallel(netG_high_2_low)\n",
    "    netD_low = torch.nn.DataParallel(netD_low)\n",
    "    netD_high = torch.nn.DataParallel(netD_high)\n",
    "    \n",
    "    \n",
    "netG_low_2_high.to('cuda')  \n",
    "netG_high_2_low.to('cuda')  \n",
    "netD_low.to('cuda')  \n",
    "netD_high.to('cuda')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_loss = nn.L1Loss()\n",
    "# gan_loss = nn.BCELoss()\n",
    "identity_loss = nn.L1Loss()\n",
    "\n",
    "learning_rate = 2e-4\n",
    "max_epochs = 1000\n",
    "\n",
    "# Optimizer 설정하기\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_low_2_high.parameters(), netG_high_2_low.parameters()), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(netD_low.parameters(), netD_high.parameters()), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "def lambda_rule(epoch, start_decay_epoch=100, total_epoch=1000):\n",
    "    lr = 1.0 - max(0, epoch - start_decay_epoch) / float(total_epoch)\n",
    "    return lr\n",
    "\n",
    "scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
    "scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=lambda_rule)\n",
    "\n",
    "# Generated image pool\n",
    "from Cyclegan_sunggu.image_pool import ImagePool\n",
    "num_pool = 0\n",
    "fake_low_pool  = ImagePool(num_pool)\n",
    "fake_high_pool = ImagePool(num_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "\n",
    "scheduler_G.load_state_dict(checkpoint['scheduler_G'])        \n",
    "scheduler_D.load_state_dict(checkpoint['scheduler_D'])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 필요한 Weight만 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Before\n",
    "# model_dict = model.state_dict()\n",
    "# print(\"이전 weight = \", model_dict['encoder._conv_stem.weight'][0])\n",
    "\n",
    "# load_dir = '/workspace/sunggu/1.Hemorrhage/monai_experiment/model/Efficient3d_conv2d_Aux/'\n",
    "# pretrained_dict =  torch.load(os.path.join(load_dir, \"epoch_0_best_metric_model.pth\")) \n",
    "\n",
    "# # 1. filter out unnecessary keys\n",
    "# pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "# model.load_state_dict(model_dict)\n",
    "\n",
    "# # After\n",
    "# print(\"이후 weight = \", model_dict['encoder._conv_stem.weight'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 그밖에 부수적인 functions 설정하기\n",
    "fn_tonumpy = lambda x: x.cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "fn_denorm  = lambda x: (x * 0.5) + 0.5\n",
    "fn_denorm_window  = visual_windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_penalty(critic, real_data, fake_data, device='cuda'):\n",
    "    \n",
    "    alpha = torch.rand_like(real_data).to(device)\n",
    "\n",
    "    interpolates = alpha*real_data + (1-alpha)*fake_data.detach()\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates = torch.autograd.Variable(interpolates, requires_grad = True)\n",
    "\n",
    "    critic_interpolates = critic(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=critic_interpolates, \n",
    "                                    inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(critic_interpolates.size()).to(device),\n",
    "                                    create_graph=True, \n",
    "                                    retain_graph=True, \n",
    "                                    only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    # Regularization hyperparameter for gradient penalty = 0.5, 공식 wgan-gp:10\n",
    "    gradient_penalty = 0.5 * ((gradients.norm(2, dim=1) - 1) ** 2).mean() \n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 61/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:28<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:52<00:00,  1.62s/it]\n",
      "Generator Loss       [Low to High] = -0.9966\n",
      "Generator Loss       [High to Low] = -0.9833\n",
      "Discriminator LOss   [Low]         = 0.1052\n",
      "Discriminator Loss   [High]        = 0.0142\n",
      "Cycle Loss           [Low]         = 0.0027\n",
      "Cycle Loss           [High]        = 0.0027\n",
      "Identity Loss        [Low]         = 0.0022\n",
      "Identity Loss        [High]        = 0.0021\n",
      "TOTAL Loss                         = -1.8392\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 62/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:06<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:23<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.9967\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0749\n",
      "Discriminator Loss   [High]        = 0.0139\n",
      "Cycle Loss           [Low]         = 0.0027\n",
      "Cycle Loss           [High]        = 0.0026\n",
      "Identity Loss        [Low]         = 0.0022\n",
      "Identity Loss        [High]        = 0.0021\n",
      "TOTAL Loss                         = -1.8705\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 63/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:41<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [42:09<00:00,  1.46s/it]\n",
      "Generator Loss       [Low to High] = -0.9962\n",
      "Generator Loss       [High to Low] = -0.9833\n",
      "Discriminator LOss   [Low]         = 0.0372\n",
      "Discriminator Loss   [High]        = 0.0137\n",
      "Cycle Loss           [Low]         = 0.0027\n",
      "Cycle Loss           [High]        = 0.0026\n",
      "Identity Loss        [Low]         = 0.0022\n",
      "Identity Loss        [High]        = 0.0021\n",
      "TOTAL Loss                         = -1.9081\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 64/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:42<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:17<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9955\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0679\n",
      "Discriminator Loss   [High]        = 0.0133\n",
      "Cycle Loss           [Low]         = 0.0027\n",
      "Cycle Loss           [High]        = 0.0026\n",
      "Identity Loss        [Low]         = 0.0022\n",
      "Identity Loss        [High]        = 0.0021\n",
      "TOTAL Loss                         = -1.8774\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 65/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:41<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:22<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9855\n",
      "Generator Loss       [High to Low] = -0.9833\n",
      "Discriminator LOss   [Low]         = 0.0182\n",
      "Discriminator Loss   [High]        = 0.0113\n",
      "Cycle Loss           [Low]         = 0.0026\n",
      "Cycle Loss           [High]        = 0.0026\n",
      "Identity Loss        [Low]         = 0.0022\n",
      "Identity Loss        [High]        = 0.0021\n",
      "TOTAL Loss                         = -1.9212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 66/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:41<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:21<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9808\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.1444\n",
      "Discriminator Loss   [High]        = -0.0007\n",
      "Cycle Loss           [Low]         = 0.0026\n",
      "Cycle Loss           [High]        = 0.0026\n",
      "Identity Loss        [Low]         = 0.0022\n",
      "Identity Loss        [High]        = 0.0021\n",
      "TOTAL Loss                         = -1.8144\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 67/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:59<00:00,  1.45it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:39<00:00,  1.44s/it]\n",
      "Generator Loss       [Low to High] = -0.9778\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0086\n",
      "Discriminator Loss   [High]        = -0.0030\n",
      "Cycle Loss           [Low]         = 0.0026\n",
      "Cycle Loss           [High]        = 0.0026\n",
      "Identity Loss        [Low]         = 0.0022\n",
      "Identity Loss        [High]        = 0.0020\n",
      "TOTAL Loss                         = -1.9518\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 68/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:43<00:00,  1.46it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:26<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9771\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0085\n",
      "Discriminator Loss   [High]        = -0.0049\n",
      "Cycle Loss           [Low]         = 0.0026\n",
      "Cycle Loss           [High]        = 0.0025\n",
      "Identity Loss        [Low]         = 0.0022\n",
      "Identity Loss        [High]        = 0.0020\n",
      "TOTAL Loss                         = -1.9551\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 69/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:42<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:24<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9763\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0062\n",
      "Discriminator Loss   [High]        = -0.0062\n",
      "Cycle Loss           [Low]         = 0.0026\n",
      "Cycle Loss           [High]        = 0.0025\n",
      "Identity Loss        [Low]         = 0.0021\n",
      "Identity Loss        [High]        = 0.0020\n",
      "TOTAL Loss                         = -1.9594\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 70/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:39<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:29<00:00,  1.44s/it]\n",
      "Generator Loss       [Low to High] = -0.9834\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0059\n",
      "Discriminator Loss   [High]        = -0.0067\n",
      "Cycle Loss           [Low]         = 0.0026\n",
      "Cycle Loss           [High]        = 0.0025\n",
      "Identity Loss        [Low]         = 0.0021\n",
      "Identity Loss        [High]        = 0.0020\n",
      "TOTAL Loss                         = -1.9677\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 71/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:44<00:00,  1.46it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:38<00:00,  1.44s/it]\n",
      "Generator Loss       [Low to High] = -0.9911\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0881\n",
      "Discriminator Loss   [High]        = 0.0078\n",
      "Cycle Loss           [Low]         = 0.0026\n",
      "Cycle Loss           [High]        = 0.0025\n",
      "Identity Loss        [Low]         = 0.0021\n",
      "Identity Loss        [High]        = 0.0020\n",
      "TOTAL Loss                         = -1.8642\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 72/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:42<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:11<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9766\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0712\n",
      "Discriminator Loss   [High]        = -0.0013\n",
      "Cycle Loss           [Low]         = 0.0025\n",
      "Cycle Loss           [High]        = 0.0025\n",
      "Identity Loss        [Low]         = 0.0021\n",
      "Identity Loss        [High]        = 0.0020\n",
      "TOTAL Loss                         = -1.8849\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 73/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:37<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:07<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.9754\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0082\n",
      "Discriminator Loss   [High]        = -0.0071\n",
      "Cycle Loss           [Low]         = 0.0025\n",
      "Cycle Loss           [High]        = 0.0024\n",
      "Identity Loss        [Low]         = 0.0021\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9582\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 74/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:35<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:12<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9755\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0062\n",
      "Discriminator Loss   [High]        = -0.0081\n",
      "Cycle Loss           [Low]         = 0.0025\n",
      "Cycle Loss           [High]        = 0.0025\n",
      "Identity Loss        [Low]         = 0.0021\n",
      "Identity Loss        [High]        = 0.0020\n",
      "TOTAL Loss                         = -1.9624\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 75/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:43<00:00,  1.46it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:32<00:00,  1.44s/it]\n",
      "Generator Loss       [Low to High] = -0.9754\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0068\n",
      "Discriminator Loss   [High]        = -0.0082\n",
      "Cycle Loss           [Low]         = 0.0025\n",
      "Cycle Loss           [High]        = 0.0024\n",
      "Identity Loss        [Low]         = 0.0021\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9620\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 76/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:41<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:32<00:00,  1.44s/it]\n",
      "Generator Loss       [Low to High] = -0.9735\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0051\n",
      "Discriminator Loss   [High]        = -0.0081\n",
      "Cycle Loss           [Low]         = 0.0025\n",
      "Cycle Loss           [High]        = 0.0024\n",
      "Identity Loss        [Low]         = 0.0021\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9618\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 77/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:37<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:18<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9875\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0375\n",
      "Discriminator Loss   [High]        = -0.0044\n",
      "Cycle Loss           [Low]         = 0.0025\n",
      "Cycle Loss           [High]        = 0.0024\n",
      "Identity Loss        [Low]         = 0.0020\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9359\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 78/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:37<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:03<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.9753\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0041\n",
      "Discriminator Loss   [High]        = -0.0044\n",
      "Cycle Loss           [Low]         = 0.0025\n",
      "Cycle Loss           [High]        = 0.0024\n",
      "Identity Loss        [Low]         = 0.0020\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9570\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 79/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:34<00:00,  1.48it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:04<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.9739\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0039\n",
      "Discriminator Loss   [High]        = -0.0072\n",
      "Cycle Loss           [Low]         = 0.0024\n",
      "Cycle Loss           [High]        = 0.0024\n",
      "Identity Loss        [Low]         = 0.0020\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 80/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:34<00:00,  1.48it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:03<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.9752\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0040\n",
      "Discriminator Loss   [High]        = -0.0094\n",
      "Cycle Loss           [Low]         = 0.0024\n",
      "Cycle Loss           [High]        = 0.0024\n",
      "Identity Loss        [Low]         = 0.0020\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9674\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 81/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:34<00:00,  1.48it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:02<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.9769\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0063\n",
      "Discriminator Loss   [High]        = -0.0084\n",
      "Cycle Loss           [Low]         = 0.0024\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0020\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9649\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 82/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:34<00:00,  1.48it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:59<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.9733\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0292\n",
      "Discriminator Loss   [High]        = -0.0094\n",
      "Cycle Loss           [Low]         = 0.0024\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0020\n",
      "Identity Loss        [High]        = 0.0019\n",
      "TOTAL Loss                         = -1.9405\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 83/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:35<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:06<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.9735\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0021\n",
      "Discriminator Loss   [High]        = -0.0075\n",
      "Cycle Loss           [Low]         = 0.0024\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0020\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.9637\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 84/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:35<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:13<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9730\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0022\n",
      "Discriminator Loss   [High]        = -0.0098\n",
      "Cycle Loss           [Low]         = 0.0024\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0020\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.9679\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 85/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:39<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:17<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9728\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0021\n",
      "Discriminator Loss   [High]        = -0.0098\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.9678\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 86/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:38<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:18<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9714\n",
      "Generator Loss       [High to Low] = -0.9834\n",
      "Discriminator LOss   [Low]         = 0.0023\n",
      "Discriminator Loss   [High]        = -0.0108\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.9681\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 87/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:41<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:24<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.9747\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0682\n",
      "Discriminator Loss   [High]        = -0.0075\n",
      "Cycle Loss           [Low]         = 0.0024\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.8991\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 88/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:51<00:00,  1.45it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:22<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.9729\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0015\n",
      "Discriminator Loss   [High]        = -0.0096\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.9685\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 89/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:49<00:00,  1.32it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [47:33<00:00,  1.65s/it]\n",
      "Generator Loss       [Low to High] = -0.9727\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0017\n",
      "Discriminator Loss   [High]        = -0.0109\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.9705\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 90/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:47<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:39<00:00,  1.58s/it]\n",
      "Generator Loss       [Low to High] = -0.9720\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0020\n",
      "Discriminator Loss   [High]        = -0.0114\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0023\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.9707\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 91/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:53<00:00,  1.38it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [47:22<00:00,  1.64s/it]\n",
      "Generator Loss       [Low to High] = -0.9706\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0022\n",
      "Discriminator Loss   [High]        = -0.0122\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0018\n",
      "TOTAL Loss                         = -1.9707\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 92/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:46<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:24<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.9718\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0032\n",
      "Discriminator Loss   [High]        = -0.0120\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9705\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 93/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:43<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:17<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.9697\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0027\n",
      "Discriminator Loss   [High]        = -0.0130\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 94/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:43<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:10<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.9697\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0018\n",
      "Discriminator Loss   [High]        = -0.0133\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9724\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 95/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:41<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:56<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.9697\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0037\n",
      "Discriminator Loss   [High]        = -0.0139\n",
      "Cycle Loss           [Low]         = 0.0023\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0019\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9717\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 96/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:41<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:57<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.9782\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0018\n",
      "Discriminator Loss   [High]        = -0.0135\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9814\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 97/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:50<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.9698\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0015\n",
      "Discriminator Loss   [High]        = -0.0110\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9684\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 98/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:45<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.9703\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0023\n",
      "Discriminator Loss   [High]        = -0.0129\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9718\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 99/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:51<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.9697\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0015\n",
      "Discriminator Loss   [High]        = -0.0134\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9730\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 100/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:51<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.9732\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0135\n",
      "Discriminator Loss   [High]        = -0.0134\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0022\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9644\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 101/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:50<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.9692\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0005\n",
      "Discriminator Loss   [High]        = -0.0127\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9722\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 102/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:40<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:55<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.9887\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0031\n",
      "Discriminator Loss   [High]        = -0.0114\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9864\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 103/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:57<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.9850\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0010\n",
      "Discriminator Loss   [High]        = 0.0019\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9584\n",
      "Learning Rate 0.0002000000 -> 0.0002000000\n",
      "----------\n",
      "epoch 104/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:40<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:53<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.9845\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0004\n",
      "Discriminator Loss   [High]        = 0.0011\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9600\n",
      "Learning Rate 0.0001998000 -> 0.0001998000\n",
      "----------\n",
      "epoch 105/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:38<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:41<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.9846\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0019\n",
      "Discriminator Loss   [High]        = 0.0026\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9557\n",
      "Learning Rate 0.0001996000 -> 0.0001996000\n",
      "----------\n",
      "epoch 106/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:35<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:33<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.9802\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0008\n",
      "Discriminator Loss   [High]        = 0.0008\n",
      "Cycle Loss           [Low]         = 0.0022\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.9560\n",
      "Learning Rate 0.0001994000 -> 0.0001994000\n",
      "----------\n",
      "epoch 107/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:34<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:29<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0507\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0050\n",
      "Discriminator Loss   [High]        = -0.3774\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0017\n",
      "TOTAL Loss                         = -1.7788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001992000 -> 0.0001992000\n",
      "----------\n",
      "epoch 108/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:33<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:24<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0493\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0018\n",
      "Discriminator Loss   [High]        = -0.9340\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0018\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.8937\n",
      "Learning Rate 0.0001990000 -> 0.0001990000\n",
      "----------\n",
      "epoch 109/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:34<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:40<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0490\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0013\n",
      "Discriminator Loss   [High]        = -0.9343\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.8947\n",
      "Learning Rate 0.0001988000 -> 0.0001988000\n",
      "----------\n",
      "epoch 110/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:47<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.4624\n",
      "Discriminator Loss   [High]        = -0.9411\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0021\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.4307\n",
      "Learning Rate 0.0001986000 -> 0.0001986000\n",
      "----------\n",
      "epoch 111/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:33<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:20<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0082\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9049\n",
      "Learning Rate 0.0001984000 -> 0.0001984000\n",
      "----------\n",
      "epoch 112/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:32<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:25<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0328\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0017\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9113\n",
      "Learning Rate 0.0001982000 -> 0.0001982000\n",
      "----------\n",
      "epoch 113/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:30<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:18<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0328\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0008\n",
      "Discriminator Loss   [High]        = -0.9508\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9119\n",
      "Learning Rate 0.0001980000 -> 0.0001980000\n",
      "----------\n",
      "epoch 114/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:29<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:19<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0328\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0006\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9124\n",
      "Learning Rate 0.0001978000 -> 0.0001978000\n",
      "----------\n",
      "epoch 115/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:29<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:13<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0328\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0007\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9124\n",
      "Learning Rate 0.0001976000 -> 0.0001976000\n",
      "----------\n",
      "epoch 116/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:28<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:11<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0009\n",
      "Discriminator Loss   [High]        = -0.9498\n",
      "Cycle Loss           [Low]         = 0.0021\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9099\n",
      "Learning Rate 0.0001974000 -> 0.0001974000\n",
      "----------\n",
      "epoch 117/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:30<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:16<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0010\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9122\n",
      "Learning Rate 0.0001972000 -> 0.0001972000\n",
      "----------\n",
      "epoch 118/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:28<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:06<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0328\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0012\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9121\n",
      "Learning Rate 0.0001970000 -> 0.0001970000\n",
      "----------\n",
      "epoch 119/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:08<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0009\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9123\n",
      "Learning Rate 0.0001968000 -> 0.0001968000\n",
      "----------\n",
      "epoch 120/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:28<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:04<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0328\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0040\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9093\n",
      "Learning Rate 0.0001966000 -> 0.0001966000\n",
      "----------\n",
      "epoch 121/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:04<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9837\n",
      "Discriminator LOss   [Low]         = 0.0003\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0020\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0016\n",
      "TOTAL Loss                         = -2.9127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001964000 -> 0.0001964000\n",
      "----------\n",
      "epoch 122/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:59<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9838\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9512\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9133\n",
      "Learning Rate 0.0001962000 -> 0.0001962000\n",
      "----------\n",
      "epoch 123/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:23<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:02<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9836\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9192\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.8491\n",
      "Learning Rate 0.0001960000 -> 0.0001960000\n",
      "----------\n",
      "epoch 124/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:23<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:05<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9836\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9490\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9089\n",
      "Learning Rate 0.0001958000 -> 0.0001958000\n",
      "----------\n",
      "epoch 125/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:24<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:00<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9503\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0017\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9115\n",
      "Learning Rate 0.0001956000 -> 0.0001956000\n",
      "----------\n",
      "epoch 126/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:23<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:55<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9507\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9125\n",
      "Learning Rate 0.0001954000 -> 0.0001954000\n",
      "----------\n",
      "epoch 127/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:22<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:02<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0009\n",
      "Discriminator Loss   [High]        = -0.9508\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9118\n",
      "Learning Rate 0.0001952000 -> 0.0001952000\n",
      "----------\n",
      "epoch 128/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:24<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:53<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9127\n",
      "Learning Rate 0.0001950000 -> 0.0001950000\n",
      "----------\n",
      "epoch 129/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:26<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:59<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9128\n",
      "Learning Rate 0.0001948000 -> 0.0001948000\n",
      "----------\n",
      "epoch 130/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:22<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:00<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0006\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0020\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9123\n",
      "Learning Rate 0.0001946000 -> 0.0001946000\n",
      "----------\n",
      "epoch 131/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:18<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:41<00:00,  1.51s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9128\n",
      "Learning Rate 0.0001944000 -> 0.0001944000\n",
      "----------\n",
      "epoch 132/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:19<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:46<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9508\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9126\n",
      "Learning Rate 0.0001942000 -> 0.0001942000\n",
      "----------\n",
      "epoch 133/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:19<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:45<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9126\n",
      "Learning Rate 0.0001940000 -> 0.0001940000\n",
      "----------\n",
      "epoch 134/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:20<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:52<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0004\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0019\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9124\n",
      "Learning Rate 0.0001938000 -> 0.0001938000\n",
      "----------\n",
      "epoch 135/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:21<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:58<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001936000 -> 0.0001936000\n",
      "----------\n",
      "epoch 136/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:26<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:12<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0015\n",
      "TOTAL Loss                         = -2.9127\n",
      "Learning Rate 0.0001934000 -> 0.0001934000\n",
      "----------\n",
      "epoch 137/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:18<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9832\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9504\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0016\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9115\n",
      "Learning Rate 0.0001932000 -> 0.0001932000\n",
      "----------\n",
      "epoch 138/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:31<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:06<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9832\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9507\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9121\n",
      "Learning Rate 0.0001930000 -> 0.0001930000\n",
      "----------\n",
      "epoch 139/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:04<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9832\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9508\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9123\n",
      "Learning Rate 0.0001928000 -> 0.0001928000\n",
      "----------\n",
      "epoch 140/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:26<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:03<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9832\n",
      "Discriminator LOss   [Low]         = 0.0003\n",
      "Discriminator Loss   [High]        = -0.9508\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9121\n",
      "Learning Rate 0.0001926000 -> 0.0001926000\n",
      "----------\n",
      "epoch 141/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:26<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:05<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9832\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9507\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9123\n",
      "Learning Rate 0.0001924000 -> 0.0001924000\n",
      "----------\n",
      "epoch 142/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:06<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9832\n",
      "Discriminator LOss   [Low]         = 0.0005\n",
      "Discriminator Loss   [High]        = -0.9508\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9120\n",
      "Learning Rate 0.0001922000 -> 0.0001922000\n",
      "----------\n",
      "epoch 143/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:24<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:04<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9832\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9507\n",
      "Cycle Loss           [Low]         = 0.0019\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9122\n",
      "Learning Rate 0.0001920000 -> 0.0001920000\n",
      "----------\n",
      "epoch 144/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:26<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:03<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9268\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.8648\n",
      "Learning Rate 0.0001918000 -> 0.0001918000\n",
      "----------\n",
      "epoch 145/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:24<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:10<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0009\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9124\n",
      "Learning Rate 0.0001916000 -> 0.0001916000\n",
      "----------\n",
      "epoch 146/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:26<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:58<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9133\n",
      "Learning Rate 0.0001914000 -> 0.0001914000\n",
      "----------\n",
      "epoch 147/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:20<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:49<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9133\n",
      "Learning Rate 0.0001912000 -> 0.0001912000\n",
      "----------\n",
      "epoch 148/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [22:54<00:00,  1.26it/s] \n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:23<00:00,  1.61s/it]  \n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0018\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9133\n",
      "Learning Rate 0.0001910000 -> 0.0001910000\n",
      "----------\n",
      "epoch 149/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:24<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:43<00:00,  1.51s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001908000 -> 0.0001908000\n",
      "----------\n",
      "epoch 150/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:17<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:50<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0004\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9131\n",
      "Learning Rate 0.0001906000 -> 0.0001906000\n",
      "----------\n",
      "epoch 151/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:19<00:00,  1.35it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:42<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9134\n",
      "Learning Rate 0.0001904000 -> 0.0001904000\n",
      "----------\n",
      "epoch 152/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:08<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0014\n",
      "TOTAL Loss                         = -2.9133\n",
      "Learning Rate 0.0001902000 -> 0.0001902000\n",
      "----------\n",
      "epoch 153/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:42<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:19<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9135\n",
      "Learning Rate 0.0001900000 -> 0.0001900000\n",
      "----------\n",
      "epoch 154/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:44<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:11<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0009\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9128\n",
      "Learning Rate 0.0001898000 -> 0.0001898000\n",
      "----------\n",
      "epoch 155/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:48<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:17<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9509\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9133\n",
      "Learning Rate 0.0001896000 -> 0.0001896000\n",
      "----------\n",
      "epoch 156/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:49<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:20<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9134\n",
      "Learning Rate 0.0001894000 -> 0.0001894000\n",
      "----------\n",
      "epoch 157/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:46<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:41<00:00,  1.58s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9135\n",
      "Learning Rate 0.0001892000 -> 0.0001892000\n",
      "----------\n",
      "epoch 158/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:52<00:00,  1.38it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:21<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9134\n",
      "Learning Rate 0.0001890000 -> 0.0001890000\n",
      "----------\n",
      "epoch 159/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:43<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:20<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9510\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9134\n",
      "Learning Rate 0.0001888000 -> 0.0001888000\n",
      "----------\n",
      "epoch 160/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:43<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:18<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9137\n",
      "Learning Rate 0.0001886000 -> 0.0001886000\n",
      "----------\n",
      "epoch 161/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:42<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:08<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0004\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0018\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0015\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9132\n",
      "Learning Rate 0.0001884000 -> 0.0001884000\n",
      "----------\n",
      "epoch 162/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:42<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:35<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9137\n",
      "Learning Rate 0.0001882000 -> 0.0001882000\n",
      "----------\n",
      "epoch 163/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:38<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:45<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9835\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9511\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0017\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001880000 -> 0.0001880000\n",
      "----------\n",
      "epoch 164/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:50<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9522\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9318\n",
      "Learning Rate 0.0001878000 -> 0.0001878000\n",
      "----------\n",
      "epoch 165/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:40<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:51<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 2.3517\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -0.6093\n",
      "Learning Rate 0.0001876000 -> 0.0001876000\n",
      "----------\n",
      "epoch 166/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:34<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:32<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0070\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9541\n",
      "Learning Rate 0.0001874000 -> 0.0001874000\n",
      "----------\n",
      "epoch 167/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:30<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:35<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0016\n",
      "Discriminator Loss   [High]        = -0.9666\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9594\n",
      "Learning Rate 0.0001872000 -> 0.0001872000\n",
      "----------\n",
      "epoch 168/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:38<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:24<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0013\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9598\n",
      "Learning Rate 0.0001870000 -> 0.0001870000\n",
      "----------\n",
      "epoch 169/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:16<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0010\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9601\n",
      "Learning Rate 0.0001868000 -> 0.0001868000\n",
      "----------\n",
      "epoch 170/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:27<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0006\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9605\n",
      "Learning Rate 0.0001866000 -> 0.0001866000\n",
      "----------\n",
      "epoch 171/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:32<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:29<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0004\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9607\n",
      "Learning Rate 0.0001864000 -> 0.0001864000\n",
      "----------\n",
      "epoch 172/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:37<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:41<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0003\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9608\n",
      "Learning Rate 0.0001862000 -> 0.0001862000\n",
      "----------\n",
      "epoch 173/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:34<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:40<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9609\n",
      "Learning Rate 0.0001860000 -> 0.0001860000\n",
      "----------\n",
      "epoch 174/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:50<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:31<00:00,  1.58s/it]\n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9610\n",
      "Learning Rate 0.0001858000 -> 0.0001858000\n",
      "----------\n",
      "epoch 175/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:53<00:00,  1.38it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:06<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9611\n",
      "Learning Rate 0.0001856000 -> 0.0001856000\n",
      "----------\n",
      "epoch 176/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:37<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:03<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9612\n",
      "Learning Rate 0.0001854000 -> 0.0001854000\n",
      "----------\n",
      "epoch 177/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:40<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:09<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9668\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001852000 -> 0.0001852000\n",
      "----------\n",
      "epoch 178/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:06<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9615\n",
      "Learning Rate 0.0001850000 -> 0.0001850000\n",
      "----------\n",
      "epoch 179/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:11<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9615\n",
      "Learning Rate 0.0001848000 -> 0.0001848000\n",
      "----------\n",
      "epoch 180/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:42<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [52:22<00:00,  1.81s/it]  \n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0008\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0017\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0014\n",
      "Identity Loss        [High]        = 0.0013\n",
      "TOTAL Loss                         = -2.9610\n",
      "Learning Rate 0.0001846000 -> 0.0001846000\n",
      "----------\n",
      "epoch 181/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [25:45<00:00,  1.12it/s]  \n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:06<00:00,  1.56s/it] \n",
      "Generator Loss       [Low to High] = -0.0327\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9612\n",
      "Learning Rate 0.0001844000 -> 0.0001844000\n",
      "----------\n",
      "epoch 182/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:37<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:55<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0326\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0016\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9613\n",
      "Learning Rate 0.0001842000 -> 0.0001842000\n",
      "----------\n",
      "epoch 183/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:43<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:27<00:00,  1.57s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9668\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9614\n",
      "Learning Rate 0.0001840000 -> 0.0001840000\n",
      "----------\n",
      "epoch 184/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:00<00:00,  1.38it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:46<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9615\n",
      "Learning Rate 0.0001838000 -> 0.0001838000\n",
      "----------\n",
      "epoch 185/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:45<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9668\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9613\n",
      "Learning Rate 0.0001836000 -> 0.0001836000\n",
      "----------\n",
      "epoch 186/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:39<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9615\n",
      "Learning Rate 0.0001834000 -> 0.0001834000\n",
      "----------\n",
      "epoch 187/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:50<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:34<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9615\n",
      "Learning Rate 0.0001832000 -> 0.0001832000\n",
      "----------\n",
      "epoch 188/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:37<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:05<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0006\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9609\n",
      "Learning Rate 0.0001830000 -> 0.0001830000\n",
      "----------\n",
      "epoch 189/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [43:03<00:00,  1.49s/it]  \n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:25<00:00,  1.54s/it] \n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9616\n",
      "Learning Rate 0.0001828000 -> 0.0001828000\n",
      "----------\n",
      "epoch 190/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:00<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9617\n",
      "Learning Rate 0.0001826000 -> 0.0001826000\n",
      "----------\n",
      "epoch 191/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:06<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0004\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001824000 -> 0.0001824000\n",
      "----------\n",
      "epoch 192/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:30<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:24<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9617\n",
      "Learning Rate 0.0001822000 -> 0.0001822000\n",
      "----------\n",
      "epoch 193/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:36<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:34<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9618\n",
      "Learning Rate 0.0001820000 -> 0.0001820000\n",
      "----------\n",
      "epoch 194/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:36<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:27<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0005\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9612\n",
      "Learning Rate 0.0001818000 -> 0.0001818000\n",
      "----------\n",
      "epoch 195/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:59<00:00,  1.38it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:15<00:00,  1.60s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9618\n",
      "Learning Rate 0.0001816000 -> 0.0001816000\n",
      "----------\n",
      "epoch 196/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:02<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:34<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9618\n",
      "Learning Rate 0.0001814000 -> 0.0001814000\n",
      "----------\n",
      "epoch 197/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:02<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:11<00:00,  1.60s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9619\n",
      "Learning Rate 0.0001812000 -> 0.0001812000\n",
      "----------\n",
      "epoch 198/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:56<00:00,  1.38it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:05<00:00,  1.60s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0004\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9614\n",
      "Learning Rate 0.0001810000 -> 0.0001810000\n",
      "----------\n",
      "epoch 199/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:58<00:00,  1.38it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:12<00:00,  1.60s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0016\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9618\n",
      "Learning Rate 0.0001808000 -> 0.0001808000\n",
      "----------\n",
      "epoch 200/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:00<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:45<00:00,  1.62s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9619\n",
      "Learning Rate 0.0001806000 -> 0.0001806000\n",
      "----------\n",
      "epoch 201/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:07<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [47:03<00:00,  1.63s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0006\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9613\n",
      "Learning Rate 0.0001804000 -> 0.0001804000\n",
      "----------\n",
      "epoch 202/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:05<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:41<00:00,  1.62s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9619\n",
      "Learning Rate 0.0001802000 -> 0.0001802000\n",
      "----------\n",
      "epoch 203/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:03<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:34<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9618\n",
      "Learning Rate 0.0001800000 -> 0.0001800000\n",
      "----------\n",
      "epoch 204/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:04<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:27<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0323\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9616\n",
      "Learning Rate 0.0001798000 -> 0.0001798000\n",
      "----------\n",
      "epoch 205/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:04<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:26<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0323\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0011\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001796000 -> 0.0001796000\n",
      "----------\n",
      "epoch 206/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:03<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:29<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001794000 -> 0.0001794000\n",
      "----------\n",
      "epoch 207/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:07<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:31<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9619\n",
      "Learning Rate 0.0001792000 -> 0.0001792000\n",
      "----------\n",
      "epoch 208/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:06<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:37<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001790000 -> 0.0001790000\n",
      "----------\n",
      "epoch 209/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:07<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:36<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001788000 -> 0.0001788000\n",
      "----------\n",
      "epoch 210/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:05<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:32<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0015\n",
      "Identity Loss        [Low]         = 0.0013\n",
      "Identity Loss        [High]        = 0.0012\n",
      "TOTAL Loss                         = -2.9619\n",
      "Learning Rate 0.0001786000 -> 0.0001786000\n",
      "----------\n",
      "epoch 211/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:03<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:25<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0027\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9594\n",
      "Learning Rate 0.0001784000 -> 0.0001784000\n",
      "----------\n",
      "epoch 212/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [21:04<00:00,  1.37it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [46:31<00:00,  1.61s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9618\n",
      "Learning Rate 0.0001782000 -> 0.0001782000\n",
      "----------\n",
      "epoch 213/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:44<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:11<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001780000 -> 0.0001780000\n",
      "----------\n",
      "epoch 214/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:36<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:14<00:00,  1.43s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001778000 -> 0.0001778000\n",
      "----------\n",
      "epoch 215/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:37<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:06<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9621\n",
      "Learning Rate 0.0001776000 -> 0.0001776000\n",
      "----------\n",
      "epoch 216/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:36<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:06<00:00,  1.42s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0004\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9616\n",
      "Learning Rate 0.0001774000 -> 0.0001774000\n",
      "----------\n",
      "epoch 217/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:33<00:00,  1.48it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [42:12<00:00,  1.46s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001772000 -> 0.0001772000\n",
      "----------\n",
      "epoch 218/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:40<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:41<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9621\n",
      "Learning Rate 0.0001770000 -> 0.0001770000\n",
      "----------\n",
      "epoch 219/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:41<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:51<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001768000 -> 0.0001768000\n",
      "----------\n",
      "epoch 220/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:45<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001766000 -> 0.0001766000\n",
      "----------\n",
      "epoch 221/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:41<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:55<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0030\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9591\n",
      "Learning Rate 0.0001764000 -> 0.0001764000\n",
      "----------\n",
      "epoch 222/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:42<00:00,  1.39it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:49<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0325\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001762000 -> 0.0001762000\n",
      "----------\n",
      "epoch 223/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:40<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:39<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9669\n",
      "Cycle Loss           [Low]         = 0.0015\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001760000 -> 0.0001760000\n",
      "----------\n",
      "epoch 224/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:34<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:16<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9621\n",
      "Learning Rate 0.0001758000 -> 0.0001758000\n",
      "----------\n",
      "epoch 225/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:29<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:13<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9621\n",
      "Learning Rate 0.0001756000 -> 0.0001756000\n",
      "----------\n",
      "epoch 226/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:13<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9621\n",
      "Learning Rate 0.0001754000 -> 0.0001754000\n",
      "----------\n",
      "epoch 227/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:12<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001752000 -> 0.0001752000\n",
      "----------\n",
      "epoch 228/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:29<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:14<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9618\n",
      "Learning Rate 0.0001750000 -> 0.0001750000\n",
      "----------\n",
      "epoch 229/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:03<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001748000 -> 0.0001748000\n",
      "----------\n",
      "epoch 230/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:23<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:59<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001746000 -> 0.0001746000\n",
      "----------\n",
      "epoch 231/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:10<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001744000 -> 0.0001744000\n",
      "----------\n",
      "epoch 232/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:11<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9621\n",
      "Learning Rate 0.0001742000 -> 0.0001742000\n",
      "----------\n",
      "epoch 233/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:13<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0026\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001740000 -> 0.0001740000\n",
      "----------\n",
      "epoch 234/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:00<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0014\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001738000 -> 0.0001738000\n",
      "----------\n",
      "epoch 235/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:07<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9623\n",
      "Learning Rate 0.0001736000 -> 0.0001736000\n",
      "----------\n",
      "epoch 236/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:12<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001734000 -> 0.0001734000\n",
      "----------\n",
      "epoch 237/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:15<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001732000 -> 0.0001732000\n",
      "----------\n",
      "epoch 238/1000\n",
      "D_Iteration:   0%|          | 0/1733 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_Iteration: 100%|██████████| 1733/1733 [20:31<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:33<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9621\n",
      "Learning Rate 0.0001730000 -> 0.0001730000\n",
      "----------\n",
      "epoch 239/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:37<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:48<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001728000 -> 0.0001728000\n",
      "----------\n",
      "epoch 240/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:38<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:48<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001726000 -> 0.0001726000\n",
      "----------\n",
      "epoch 241/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:37<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:49<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9620\n",
      "Learning Rate 0.0001724000 -> 0.0001724000\n",
      "----------\n",
      "epoch 242/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:40<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:45<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0012\n",
      "Identity Loss        [High]        = 0.0011\n",
      "TOTAL Loss                         = -2.9622\n",
      "Learning Rate 0.0001722000 -> 0.0001722000\n",
      "----------\n",
      "epoch 243/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:38<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:49<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9667\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9617\n",
      "Learning Rate 0.0001720000 -> 0.0001720000\n",
      "----------\n",
      "epoch 244/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:32<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9623\n",
      "Learning Rate 0.0001718000 -> 0.0001718000\n",
      "----------\n",
      "epoch 245/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:36<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [45:01<00:00,  1.56s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9623\n",
      "Learning Rate 0.0001716000 -> 0.0001716000\n",
      "----------\n",
      "epoch 246/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:39<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:48<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0003\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9621\n",
      "Learning Rate 0.0001714000 -> 0.0001714000\n",
      "----------\n",
      "epoch 247/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:36<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:38<00:00,  1.55s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9623\n",
      "Learning Rate 0.0001712000 -> 0.0001712000\n",
      "----------\n",
      "epoch 248/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:37<00:00,  1.40it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:27<00:00,  1.54s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001710000 -> 0.0001710000\n",
      "----------\n",
      "epoch 249/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:28<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:17<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9623\n",
      "Learning Rate 0.0001708000 -> 0.0001708000\n",
      "----------\n",
      "epoch 250/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:30<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:10<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9623\n",
      "Learning Rate 0.0001706000 -> 0.0001706000\n",
      "----------\n",
      "epoch 251/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:27<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:14<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001704000 -> 0.0001704000\n",
      "----------\n",
      "epoch 252/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:12<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001702000 -> 0.0001702000\n",
      "----------\n",
      "epoch 253/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:22<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:04<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9623\n",
      "Learning Rate 0.0001700000 -> 0.0001700000\n",
      "----------\n",
      "epoch 254/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:24<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:58<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001698000 -> 0.0001698000\n",
      "----------\n",
      "epoch 255/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:22<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:53<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0014\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001696000 -> 0.0001696000\n",
      "----------\n",
      "epoch 256/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:21<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:01<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001694000 -> 0.0001694000\n",
      "----------\n",
      "epoch 257/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:20<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:48<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001692000 -> 0.0001692000\n",
      "----------\n",
      "epoch 258/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:18<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:49<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001690000 -> 0.0001690000\n",
      "----------\n",
      "epoch 259/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:19<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:53<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001688000 -> 0.0001688000\n",
      "----------\n",
      "epoch 260/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:18<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:42<00:00,  1.51s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9625\n",
      "Learning Rate 0.0001686000 -> 0.0001686000\n",
      "----------\n",
      "epoch 261/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:21<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:54<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9623\n",
      "Learning Rate 0.0001684000 -> 0.0001684000\n",
      "----------\n",
      "epoch 262/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:24<00:00,  1.42it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:20<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n",
      "Learning Rate 0.0001682000 -> 0.0001682000\n",
      "----------\n",
      "epoch 263/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:28<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:18<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9625\n",
      "Learning Rate 0.0001680000 -> 0.0001680000\n",
      "----------\n",
      "epoch 264/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:31<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:14<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0013\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9625\n",
      "Learning Rate 0.0001678000 -> 0.0001678000\n",
      "----------\n",
      "epoch 265/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:28<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:12<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001676000 -> 0.0001676000\n",
      "----------\n",
      "epoch 266/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:26<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [44:04<00:00,  1.53s/it]\n",
      "Generator Loss       [Low to High] = -0.0324\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9670\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9625\n",
      "Learning Rate 0.0001674000 -> 0.0001674000\n",
      "----------\n",
      "epoch 267/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [20:25<00:00,  1.41it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [43:51<00:00,  1.52s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0094\n",
      "Discriminator Loss   [High]        = -0.9769\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9569\n",
      "Learning Rate 0.0001672000 -> 0.0001672000\n",
      "----------\n",
      "epoch 268/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:31<00:00,  1.48it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:25<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9786\n",
      "Learning Rate 0.0001670000 -> 0.0001670000\n",
      "----------\n",
      "epoch 269/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:23<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001668000 -> 0.0001668000\n",
      "----------\n",
      "epoch 270/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:21<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:21<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9786\n",
      "Learning Rate 0.0001666000 -> 0.0001666000\n",
      "----------\n",
      "epoch 271/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:18<00:00,  1.50it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:21<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001664000 -> 0.0001664000\n",
      "----------\n",
      "epoch 272/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:22<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001662000 -> 0.0001662000\n",
      "----------\n",
      "epoch 273/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:22<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9831\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9786\n",
      "Learning Rate 0.0001660000 -> 0.0001660000\n",
      "----------\n",
      "epoch 274/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:21<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:24<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001658000 -> 0.0001658000\n",
      "----------\n",
      "epoch 275/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:24<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0003\n",
      "Discriminator Loss   [High]        = -0.9831\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9783\n",
      "Learning Rate 0.0001656000 -> 0.0001656000\n",
      "----------\n",
      "epoch 276/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:21<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:22<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0003\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9785\n",
      "Learning Rate 0.0001654000 -> 0.0001654000\n",
      "----------\n",
      "epoch 277/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:23<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001652000 -> 0.0001652000\n",
      "----------\n",
      "epoch 278/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:22<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001650000 -> 0.0001650000\n",
      "----------\n",
      "epoch 279/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:17<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001648000 -> 0.0001648000\n",
      "----------\n",
      "epoch 280/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:19<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001646000 -> 0.0001646000\n",
      "----------\n",
      "epoch 281/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:21<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0163\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001644000 -> 0.0001644000\n",
      "----------\n",
      "epoch 282/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:22<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001642000 -> 0.0001642000\n",
      "----------\n",
      "epoch 283/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.50it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:18<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9831\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001640000 -> 0.0001640000\n",
      "----------\n",
      "epoch 284/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:26<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0011\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001638000 -> 0.0001638000\n",
      "----------\n",
      "epoch 285/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:22<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:28<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0010\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001636000 -> 0.0001636000\n",
      "----------\n",
      "epoch 286/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:22<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:27<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001634000 -> 0.0001634000\n",
      "----------\n",
      "epoch 287/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:22<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:27<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001632000 -> 0.0001632000\n",
      "----------\n",
      "epoch 288/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:21<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:26<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001630000 -> 0.0001630000\n",
      "----------\n",
      "epoch 289/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:21<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:29<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001628000 -> 0.0001628000\n",
      "----------\n",
      "epoch 290/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:27<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0001\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0013\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001626000 -> 0.0001626000\n",
      "----------\n",
      "epoch 291/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:25<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001624000 -> 0.0001624000\n",
      "----------\n",
      "epoch 292/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:21<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001622000 -> 0.0001622000\n",
      "----------\n",
      "epoch 293/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:20<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001620000 -> 0.0001620000\n",
      "----------\n",
      "epoch 294/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:16<00:00,  1.39s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001618000 -> 0.0001618000\n",
      "----------\n",
      "epoch 295/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:16<00:00,  1.39s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001616000 -> 0.0001616000\n",
      "----------\n",
      "epoch 296/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:16<00:00,  1.39s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9787\n",
      "Learning Rate 0.0001614000 -> 0.0001614000\n",
      "----------\n",
      "epoch 297/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:16<00:00,  1.39s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001612000 -> 0.0001612000\n",
      "----------\n",
      "epoch 298/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:18<00:00,  1.50it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:16<00:00,  1.39s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001610000 -> 0.0001610000\n",
      "----------\n",
      "epoch 299/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:14<00:00,  1.39s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001608000 -> 0.0001608000\n",
      "----------\n",
      "epoch 300/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:30<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001606000 -> 0.0001606000\n",
      "----------\n",
      "epoch 301/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:25<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:23<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9831\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0012\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001604000 -> 0.0001604000\n",
      "----------\n",
      "epoch 302/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:23<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:38<00:00,  1.41s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001602000 -> 0.0001602000\n",
      "----------\n",
      "epoch 303/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:36<00:00,  1.41s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001600000 -> 0.0001600000\n",
      "----------\n",
      "epoch 304/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:12<00:00,  1.39s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9790\n",
      "Learning Rate 0.0001598000 -> 0.0001598000\n",
      "----------\n",
      "epoch 305/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:12<00:00,  1.39s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001596000 -> 0.0001596000\n",
      "----------\n",
      "epoch 306/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:19<00:00,  1.50it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:18<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001594000 -> 0.0001594000\n",
      "----------\n",
      "epoch 307/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:20<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:24<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001592000 -> 0.0001592000\n",
      "----------\n",
      "epoch 308/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_Iteration: 100%|██████████| 1733/1733 [19:21<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:32<00:00,  1.40s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9831\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9789\n",
      "Learning Rate 0.0001590000 -> 0.0001590000\n",
      "----------\n",
      "epoch 309/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:25<00:00,  1.49it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [41:44<00:00,  1.45s/it]  \n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0000\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9790\n",
      "Learning Rate 0.0001588000 -> 0.0001588000\n",
      "----------\n",
      "epoch 310/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:36<00:00,  1.47it/s]\n",
      "G_Iteration: 100%|██████████| 1733/1733 [40:42<00:00,  1.41s/it]\n",
      "Generator Loss       [Low to High] = -0.0162\n",
      "Generator Loss       [High to Low] = -0.9994\n",
      "Discriminator LOss   [Low]         = 0.0002\n",
      "Discriminator Loss   [High]        = -0.9832\n",
      "Cycle Loss           [Low]         = 0.0012\n",
      "Cycle Loss           [High]        = 0.0011\n",
      "Identity Loss        [Low]         = 0.0010\n",
      "Identity Loss        [High]        = 0.0009\n",
      "TOTAL Loss                         = -2.9788\n",
      "Learning Rate 0.0001586000 -> 0.0001586000\n",
      "----------\n",
      "epoch 311/1000\n",
      "D_Iteration: 100%|██████████| 1733/1733 [19:31<00:00,  1.48it/s]\n",
      "G_Iteration:  87%|████████▋ | 1514/1733 [35:39<05:08,  1.41s/it]"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "## 네트워크 학습시키기\n",
    "st_epoch = 0\n",
    "num_D_iter = 5\n",
    "epoch_num = 1000\n",
    "val_interval = 10\n",
    "\n",
    "epoch_train_loss_list = list()\n",
    "epoch_val_loss_list = list()\n",
    "\n",
    "epoch_train_metric_list = list()\n",
    "epoch_val_metric_list = list()\n",
    "\n",
    "writer = SummaryWriter(log_dir='/workspace/sunggu/4.Dose_img2img/runs/OT_Cycle_Gan_2D_sinogram_fullbatch')\n",
    "root_dir = '/workspace/sunggu/4.Dose_img2img/model/OT_Cycle_Gan_2D_sinogram_fullbatch/'\n",
    "\n",
    "low2high_png_dir = '/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/'+'OT_Cycle_Gan_2D_sinogram_fullbatch'+'/low2high/'\n",
    "\n",
    "\n",
    "# 모델 save폴더 만들기\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir, mode=0o777)\n",
    "\n",
    "if not os.path.exists(low2high_png_dir):\n",
    "    os.makedirs(low2high_png_dir, mode=0o777)\n",
    "\n",
    "    \n",
    "for epoch in range(61, epoch_num):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch}/{epoch_num}\")\n",
    "    \n",
    "    # Model 선언\n",
    "    netG_low_2_high.train()\n",
    "    netG_high_2_low.train()\n",
    "    netD_low.train()\n",
    "    netD_high.train()\n",
    "    \n",
    "    # Loss 선언\n",
    "    loss_G_low2high_train = []\n",
    "    loss_G_high2low_train = []\n",
    "    \n",
    "    loss_D_low_train = []\n",
    "    loss_D_high_train = []\n",
    "    \n",
    "    loss_cycle_low_train = []\n",
    "    loss_cycle_high_train = []\n",
    "    \n",
    "    loss_ident_low_train = []\n",
    "    loss_ident_high_train = []\n",
    "        \n",
    "    D_iterator = tqdm(train_loader, desc='D_Iteration', file=sys.stdout)  \n",
    "    for batch_data in D_iterator:\n",
    "        # Backward Discriminator  ################################################################        \n",
    "\n",
    "        set_requires_grad([netD_low, netD_high], True)\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        input_low  = batch_data['low'].to(device)\n",
    "        input_high = batch_data['high'].to(device)\n",
    "\n",
    "        output_low  = netG_high_2_low(input_high)\n",
    "        output_high = netG_low_2_high(input_low)\n",
    "     \n",
    "            # LOW (Discriminator Gan Loss)\n",
    "        pred_real_low = netD_low(input_low)        \n",
    "        ### POOL\n",
    "        output_low    = fake_low_pool.query(output_low)\n",
    "        pred_fake_low = netD_low(output_low.detach())\n",
    "\n",
    "        loss_D_low_real = -pred_real_low.mean()\n",
    "        loss_D_low_fake = pred_fake_low.mean()\n",
    "        loss_D_low = loss_D_low_real + loss_D_low_fake + grad_penalty(netD_low, input_low, output_low)\n",
    "\n",
    "            # HIGH (Discriminator Gan Loss)\n",
    "        pred_real_high = netD_high(input_high)\n",
    "        ### POOL\n",
    "        output_high    = fake_high_pool.query(output_high)\n",
    "        pred_fake_high = netD_high(output_high.detach())\n",
    "\n",
    "        loss_D_high_real = -pred_real_high.mean()\n",
    "        loss_D_high_fake = pred_fake_low.mean()\n",
    "        loss_D_high = loss_D_high_real + loss_D_high_fake + grad_penalty(netD_high, input_high, output_high)\n",
    "\n",
    "        loss_D = loss_D_high + loss_D_low\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # 기록\n",
    "        loss_D_low_train  += [loss_D_low.item()]\n",
    "        loss_D_high_train += [loss_D_high.item()]\n",
    "\n",
    "    G_iterator = tqdm(train_loader, desc='G_Iteration', file=sys.stdout)  \n",
    "    for batch_data in G_iterator:\n",
    "        # Backward Generator ################################################################\n",
    "        set_requires_grad([netD_low, netD_high], False)\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        input_low  = batch_data['low'].to(device)\n",
    "        input_high = batch_data['high'].to(device)        \n",
    "\n",
    "        output_low  = netG_high_2_low(input_high)\n",
    "        output_high = netG_low_2_high(input_low)\n",
    "        \n",
    "        recon_high  = netG_low_2_high(output_low)\n",
    "        recon_low   = netG_high_2_low(output_high)           \n",
    "        \n",
    "        ident_low   = netG_high_2_low(input_low)\n",
    "        ident_high  = netG_low_2_high(input_high)             \n",
    "        \n",
    "            # Gerator Gan Loss\n",
    "        pred_fake_low  = netD_low(output_low)\n",
    "        pred_fake_high = netD_high(output_high)\n",
    "\n",
    "            # Gan loss\n",
    "        loss_G_low2high = -pred_fake_low.mean()\n",
    "        loss_G_high2low = -pred_fake_high.mean()\n",
    "\n",
    "            # Cycle Loss\n",
    "        loss_cycle_low  = cycle_loss(input_low, recon_low)\n",
    "        loss_cycle_high = cycle_loss(input_high, recon_high)\n",
    "\n",
    "            # Identity Loss\n",
    "        loss_ident_low  = identity_loss(input_low, ident_low)\n",
    "        loss_ident_high = identity_loss(input_high, ident_high)        \n",
    "\n",
    "        loss_G = (loss_G_low2high + loss_G_high2low) + \\\n",
    "                10.0 * (loss_cycle_low + loss_cycle_high) + \\\n",
    "                0.5 * (10.0 * loss_ident_low + 10.0 * loss_ident_high)\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 기록\n",
    "        loss_G_low2high_train += [loss_G_low2high.item()]\n",
    "        loss_G_high2low_train += [loss_G_high2low.item()]\n",
    "\n",
    "        loss_cycle_low_train  += [loss_cycle_low.item()]\n",
    "        loss_cycle_high_train += [loss_cycle_high.item()]\n",
    "\n",
    "        loss_ident_low_train  += [loss_ident_low.item()]\n",
    "        loss_ident_high_train += [loss_ident_high.item()]\n",
    "        \n",
    "        \n",
    "    print( \"Generator Loss       [Low to High] = %.4f\" %np.mean(loss_G_low2high_train) ) \n",
    "    print( \"Generator Loss       [High to Low] = %.4f\" %np.mean(loss_G_high2low_train) )\n",
    "    print( \"Discriminator LOss   [Low]         = %.4f\" %np.mean(loss_D_low_train) )\n",
    "    print( \"Discriminator Loss   [High]        = %.4f\" %np.mean(loss_D_high_train) )\n",
    "\n",
    "    print( \"Cycle Loss           [Low]         = %.4f\" %np.mean(loss_cycle_low_train) )\n",
    "    print( \"Cycle Loss           [High]        = %.4f\" %np.mean(loss_cycle_high_train) )\n",
    "\n",
    "    print( \"Identity Loss        [Low]         = %.4f\" %np.mean(loss_ident_low_train) )\n",
    "    print( \"Identity Loss        [High]        = %.4f\" %np.mean(loss_ident_high_train) )\n",
    "\n",
    "    total_loss = np.mean(loss_G_low2high_train) + np.mean(loss_G_high2low_train) +\\\n",
    "                 np.mean(loss_D_low_train)      + np.mean(loss_D_high_train)     +\\\n",
    "                 np.mean(loss_D_high_train)     + np.mean(loss_cycle_high_train) +\\\n",
    "                 np.mean(loss_ident_low_train)  + np.mean(loss_ident_high_train)\n",
    "\n",
    "    print( \"TOTAL Loss                         = %.4f\" %total_loss )\n",
    "\n",
    "    # Tensorboard 저장하기\n",
    "    input_low   = fn_denorm_window(fn_tonumpy((input_low)))\n",
    "    input_high  = fn_denorm_window(fn_tonumpy((input_high)))\n",
    "    output_high = fn_denorm_window(fn_tonumpy((output_high)))\n",
    "\n",
    "    input_low   = np.clip(input_low, a_min=0, a_max=1)\n",
    "    input_high  = np.clip(input_high, a_min=0, a_max=1)\n",
    "    output_high = np.clip(output_high, a_min=0, a_max=1)\n",
    "\n",
    "    # png Save\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_input_low.png',   input_low[0].squeeze(),   cmap=\"gray\")\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_output_high.png', output_high[0].squeeze(), cmap=\"gray\")\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_input_high.png',  input_high[0].squeeze(),  cmap=\"gray\")\n",
    "\n",
    "            \n",
    "    # Loss Write    \n",
    "    writer.add_scalar('loss_G_low2high', np.mean(loss_G_low2high_train), epoch)\n",
    "    writer.add_scalar('loss_G_high2low', np.mean(loss_G_high2low_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_D_low', np.mean(loss_D_low_train), epoch)\n",
    "    writer.add_scalar('loss_D_high', np.mean(loss_D_high_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_cycle_low', np.mean(loss_cycle_low_train), epoch)\n",
    "    writer.add_scalar('loss_cycle_high', np.mean(loss_cycle_high_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_ident_low', np.mean(loss_ident_low_train), epoch)\n",
    "    writer.add_scalar('loss_ident_high', np.mean(loss_ident_high_train), epoch)\n",
    "    \n",
    "    # 저장\n",
    "    if epoch % 5 == 0 or epoch == epoch_num:\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            checkpoint = {'epoch': epoch, \n",
    "                          'netG_low_2_high_state_dict': netG_low_2_high.module.state_dict(), \n",
    "                          'netG_high_2_low_state_dict': netG_high_2_low.module.state_dict(), \n",
    "                          'netD_low_state_dict': netD_low.module.state_dict(), \n",
    "                          'netD_high_state_dict': netD_high.module.state_dict(), \n",
    "                          'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                          'optimizer_D_state_dict': optimizer_D.state_dict(),  \n",
    "                          'scheduler_G': scheduler_G.state_dict(),\n",
    "                          'scheduler_D': scheduler_D.state_dict(),\n",
    "                         }                    \n",
    "\n",
    "        else:\n",
    "            checkpoint = {'epoch': epoch, \n",
    "                          'netG_low_2_high_state_dict': netG_low_2_high.state_dict(), \n",
    "                          'netG_high_2_low_state_dict': netG_high_2_low.state_dict(), \n",
    "                          'netD_low_state_dict': netD_low.state_dict(), \n",
    "                          'netD_high_state_dict': netD_high.state_dict(), \n",
    "                          'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                          'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                          'scheduler_G': scheduler_G.state_dict(),\n",
    "                          'scheduler_D': scheduler_D.state_dict(),\n",
    "                         }                         \n",
    "\n",
    "            torch.save(checkpoint, os.path.join(root_dir, \"epoch_\" + str(epoch) + \"_model.pth\"))        \n",
    "    \n",
    "    # Scheduler\n",
    "    writer.add_scalar('lr', optimizer_G.param_groups[0]['lr'], epoch)      \n",
    "    old_lr = optimizer_G.param_groups[0]['lr']\n",
    "    lr = optimizer_G.param_groups[0]['lr']\n",
    "    print('Learning Rate %.10f -> %.10f' % (old_lr, lr))\n",
    "\n",
    "    scheduler_G.step()    \n",
    "    scheduler_D.step()    \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    checkpoint = {'epoch': epoch, \n",
    "                  'netG_low_2_high_state_dict': netG_low_2_high.module.state_dict(), \n",
    "                  'netG_high_2_low_state_dict': netG_high_2_low.module.state_dict(), \n",
    "                  'netD_low_state_dict': netD_low.module.state_dict(), \n",
    "                  'netD_high_state_dict': netD_high.module.state_dict(), \n",
    "                  'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                  'optimizer_D_state_dict': optimizer_D.state_dict(),  \n",
    "                  'scheduler_G': scheduler_G.state_dict(),\n",
    "                  'scheduler_D': scheduler_D.state_dict(),\n",
    "                 }                    \n",
    "\n",
    "else:\n",
    "    checkpoint = {'epoch': epoch, \n",
    "                  'netG_low_2_high_state_dict': netG_low_2_high.state_dict(), \n",
    "                  'netG_high_2_low_state_dict': netG_high_2_low.state_dict(), \n",
    "                  'netD_low_state_dict': netD_low.state_dict(), \n",
    "                  'netD_high_state_dict': netD_high.state_dict(), \n",
    "                  'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                  'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                  'scheduler_G': scheduler_G.state_dict(),\n",
    "                  'scheduler_D': scheduler_D.state_dict(),\n",
    "                 }                         \n",
    "\n",
    "    torch.save(checkpoint, os.path.join(root_dir, \"epoch_\" + str(epoch) + \"_model.pth\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_network(net, name='network'):\n",
    "    \"\"\"Calculate and print the mean of average absolute(gradients)\n",
    "    Parameters:\n",
    "        net (torch network) -- Torch network\n",
    "        name (str) -- the name of the network\n",
    "    \"\"\"\n",
    "    mean = 0.0\n",
    "    count = 0\n",
    "    for param in net.parameters():\n",
    "        if param.grad is not None:\n",
    "            mean += torch.mean(torch.abs(param.grad.data))\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        mean = mean / count\n",
    "    print(name)\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 원본\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "## 네트워크 학습시키기\n",
    "st_epoch = 0\n",
    "\n",
    "epoch_num = 1000\n",
    "val_interval = 10\n",
    "\n",
    "epoch_train_loss_list = list()\n",
    "epoch_val_loss_list = list()\n",
    "\n",
    "epoch_train_metric_list = list()\n",
    "epoch_val_metric_list = list()\n",
    "\n",
    "writer = SummaryWriter(log_dir='/workspace/sunggu/4.Dose_img2img/runs/OT_Cycle_Gan_2D_sinogram')\n",
    "root_dir = '/workspace/sunggu/4.Dose_img2img/model/OT_Cycle_Gan_2D_sinogram/'\n",
    "\n",
    "low2high_png_dir = '/workspace/sunggu/4.Dose_img2img/Predictions/png/'+'OT_Cycle_Gan_2D_sinogram'+'/low2high/'\n",
    "\n",
    "\n",
    "# 모델 save폴더 만들기\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir, mode=0o777)\n",
    "\n",
    "if not os.path.exists(low2high_png_dir):\n",
    "    os.makedirs(low2high_png_dir, mode=0o777)\n",
    "\n",
    "\n",
    "    \n",
    "for epoch in range(41, epoch_num):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch}/{epoch_num}\")\n",
    "    \n",
    "    # Model 선언\n",
    "    netG_low_2_high.train()\n",
    "    netG_high_2_low.train()\n",
    "    netD_low.train()\n",
    "    netD_high.train()\n",
    "    \n",
    "    # Loss 선언\n",
    "    loss_G_low2high_train = []\n",
    "    loss_G_high2low_train = []\n",
    "    \n",
    "    loss_D_low_train = []\n",
    "    loss_D_high_train = []\n",
    "    \n",
    "    loss_cycle_low_train = []\n",
    "    loss_cycle_high_train = []\n",
    "    \n",
    "    loss_ident_low_train = []\n",
    "    loss_ident_high_train = []\n",
    "    \n",
    "    # Train Discriminator\n",
    "    train_iterator = tqdm(train_loader, desc='Discriminator', file=sys.stdout)    \n",
    "    for batch_data in train_iterator:\n",
    "        \n",
    "        set_requires_grad([netD_low, netD_high], True)\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        input_low = batch_data['low'].to(device)\n",
    "        input_high = batch_data['high'].to(device)\n",
    "\n",
    "        # LOW (Discriminator Gan Loss)\n",
    "        output_low = netG_high_2_low(input_high)\n",
    "        pred_real_low = netD_low(input_low)\n",
    "        \n",
    "        #### POOL\n",
    "        output_low = fake_low_pool.query(output_low)\n",
    "        pred_fake_low = netD_low(output_low.detach())\n",
    "        \n",
    "        loss_D_low_real = -pred_real_low.mean()\n",
    "        loss_D_low_fake = pred_fake_low.mean()\n",
    "        loss_D_low = loss_D_low_real + loss_D_low_fake + grad_penalty(netD_low, input_low, output_low)\n",
    "        loss_D_low.backward(retain_graph=True)\n",
    "        \n",
    "        # HIGH (Discriminator Gan Loss)\n",
    "        output_high = netG_low_2_high(input_low)\n",
    "        pred_real_high = netD_high(input_high)\n",
    "        \n",
    "        #### POOL\n",
    "        output_high = fake_high_pool.query(output_high)\n",
    "        pred_fake_high = netD_high(output_high.detach())\n",
    "        \n",
    "        loss_D_high_real = -pred_real_high.mean()\n",
    "        loss_D_high_fake = pred_fake_low.mean()\n",
    "        loss_D_high = loss_D_high_real + loss_D_high_fake + grad_penalty(netD_high, input_high, output_high)\n",
    "        loss_D_high.backward(retain_graph=True)\n",
    "        \n",
    "        loss_D = loss_D_high + loss_D_low\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # 기록\n",
    "        loss_D_low_train += [loss_D_low.item()]\n",
    "        loss_D_high_train += [loss_D_high.item()]\n",
    "        \n",
    "        \n",
    "    # Train Generator \n",
    "    train_iterator = tqdm(train_loader, desc='Generator', file=sys.stdout)    \n",
    "    for batch_data in train_iterator:      \n",
    "        \n",
    "        set_requires_grad([netD_low, netD_high], False)\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        input_low  = batch_data['low'].to(device)\n",
    "        input_high = batch_data['high'].to(device)\n",
    "        \n",
    "        output_low  = netG_high_2_low(input_high)\n",
    "        output_high = netG_low_2_high(input_low)\n",
    "        \n",
    "        recon_high = netG_low_2_high(output_low)\n",
    "        recon_low  = netG_high_2_low(output_high)\n",
    "\n",
    "        ident_low  = netG_high_2_low(input_low)\n",
    "        ident_high = netG_low_2_high(input_high)        \n",
    "        \n",
    "        # Gerator Gan Loss\n",
    "        pred_fake_low  = netD_low(output_low)\n",
    "        pred_fake_high = netD_high(output_high)\n",
    "        \n",
    "        # Gan loss\n",
    "        loss_G_low2high = -pred_fake_low.mean()\n",
    "        loss_G_high2low = -pred_fake_high.mean()\n",
    "            \n",
    "        # Cycle Loss\n",
    "        loss_cycle_low  = cycle_loss(input_low, recon_low)\n",
    "        loss_cycle_high = cycle_loss(input_high, recon_high)\n",
    "\n",
    "        # Identity Loss\n",
    "        loss_ident_low  = identity_loss(input_low, ident_low)\n",
    "        loss_ident_high = identity_loss(input_high, ident_high)        \n",
    "        \n",
    "        loss_G = (loss_G_low2high + loss_G_high2low) + 10.0*(loss_cycle_low + loss_cycle_high) + 0.5*(10.0*loss_ident_low + 10.0*loss_ident_high)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        \n",
    "        # 기록\n",
    "        loss_G_low2high_train += [loss_G_low2high.item()]\n",
    "        loss_G_high2low_train += [loss_G_high2low.item()]\n",
    "        \n",
    "        loss_cycle_low_train  += [loss_cycle_low.item()]\n",
    "        loss_cycle_high_train += [loss_cycle_high.item()]\n",
    "        \n",
    "        loss_ident_low_train  += [loss_ident_low.item()]\n",
    "        loss_ident_high_train += [loss_ident_high.item()]\n",
    "        \n",
    "        \n",
    "    print( \"Generator Loss       [Low to High] = %.4f\" %np.mean(loss_G_low2high_train) ) \n",
    "    print( \"Generator Loss       [High to Low] = %.4f\" %np.mean(loss_G_high2low_train) )\n",
    "    print( \"Discriminator LOss   [Low]         = %.4f\" %np.mean(loss_D_low_train) )\n",
    "    print( \"Discriminator Loss   [High]        = %.4f\" %np.mean(loss_D_high_train) )\n",
    "\n",
    "    print( \"Cycle Loss           [Low]         = %.4f\" %np.mean(loss_cycle_low_train) )\n",
    "    print( \"Cycle Loss           [High]        = %.4f\" %np.mean(loss_cycle_high_train) )\n",
    "\n",
    "    print( \"Identity Loss        [Low]         = %.4f\" %np.mean(loss_ident_low_train) )\n",
    "    print( \"Identity Loss        [High]        = %.4f\" %np.mean(loss_ident_high_train) )\n",
    "\n",
    "    total_loss = np.mean(loss_G_low2high_train) + np.mean(loss_G_high2low_train) +\\\n",
    "                 np.mean(loss_D_low_train)      + np.mean(loss_D_high_train)     +\\\n",
    "                 np.mean(loss_D_high_train)     + np.mean(loss_cycle_high_train) +\\\n",
    "                 np.mean(loss_ident_low_train)  + np.mean(loss_ident_high_train)\n",
    "\n",
    "    print( \"TOTAL Loss                         = %.4f\" %total_loss )\n",
    "\n",
    "    # Tensorboard 저장하기\n",
    "    input_low   = fn_denorm_window(fn_tonumpy((input_low)))\n",
    "    input_high  = fn_denorm_window(fn_tonumpy((input_high)))\n",
    "    output_high = fn_denorm_window(fn_tonumpy((output_high)))\n",
    "\n",
    "    input_low   = np.clip(input_low, a_min=0, a_max=1)\n",
    "    input_high  = np.clip(input_high, a_min=0, a_max=1)\n",
    "    output_high = np.clip(output_high, a_min=0, a_max=1)\n",
    "\n",
    "    # png Save\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_input_low.png',   input_low[0].squeeze(),   cmap=\"gray\")\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_output_high.png', output_high[0].squeeze(), cmap=\"gray\")\n",
    "    plt.imsave(low2high_png_dir+'epoch_'+str(epoch)+'_input_high.png',  input_high[0].squeeze(),  cmap=\"gray\")\n",
    "\n",
    "            \n",
    "    # Loss Write    \n",
    "    writer.add_scalar('loss_G_low2high', np.mean(loss_G_low2high_train), epoch)\n",
    "    writer.add_scalar('loss_G_high2low', np.mean(loss_G_high2low_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_D_low', np.mean(loss_D_low_train), epoch)\n",
    "    writer.add_scalar('loss_D_high', np.mean(loss_D_high_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_cycle_low', np.mean(loss_cycle_low_train), epoch)\n",
    "    writer.add_scalar('loss_cycle_high', np.mean(loss_cycle_high_train), epoch)\n",
    "\n",
    "    writer.add_scalar('loss_ident_low', np.mean(loss_ident_low_train), epoch)\n",
    "    writer.add_scalar('loss_ident_high', np.mean(loss_ident_high_train), epoch)\n",
    "    \n",
    "    # 저장\n",
    "    if epoch % 5 == 0 or epoch == epoch_num:\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            checkpoint = {'epoch': epoch, \n",
    "                          'netG_low_2_high_state_dict': netG_low_2_high.module.state_dict(), \n",
    "                          'netG_high_2_low_state_dict': netG_high_2_low.module.state_dict(), \n",
    "                          'netD_low_state_dict': netD_low.module.state_dict(), \n",
    "                          'netD_high_state_dict': netD_high.module.state_dict(), \n",
    "                          'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                          'optimizer_D_state_dict': optimizer_D.state_dict(),  \n",
    "                          'scheduler_G': scheduler_G.state_dict(),\n",
    "                          'scheduler_D': scheduler_D.state_dict(),\n",
    "                         }                    \n",
    "\n",
    "        else:\n",
    "            checkpoint = {'epoch': epoch, \n",
    "                          'netG_low_2_high_state_dict': netG_low_2_high.state_dict(), \n",
    "                          'netG_high_2_low_state_dict': netG_high_2_low.state_dict(), \n",
    "                          'netD_low_state_dict': netD_low.state_dict(), \n",
    "                          'netD_high_state_dict': netD_high.state_dict(), \n",
    "                          'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                          'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                          'scheduler_G': scheduler_G.state_dict(),\n",
    "                          'scheduler_D': scheduler_D.state_dict(),\n",
    "                         }                         \n",
    "\n",
    "            torch.save(checkpoint, os.path.join(root_dir, \"epoch_\" + str(epoch) + \"_model.pth\"))        \n",
    "    \n",
    "    # Scheduler\n",
    "    writer.add_scalar('lr', optimizer_G.param_groups[0]['lr'], epoch)      \n",
    "    old_lr = optimizer_G.param_groups[0]['lr']\n",
    "    lr = optimizer_G.param_groups[0]['lr']\n",
    "    print('Learning Rate %.10f -> %.10f' % (old_lr, lr))\n",
    "\n",
    "    scheduler_G.step()    \n",
    "    scheduler_D.step()    \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.026px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
